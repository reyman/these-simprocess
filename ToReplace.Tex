



article Pumain D., Saint-Julien T., Sanders L., 1986, Urban dynamics of some French cities. European Journal of Operational Research, 25, 3-10. Ainsi que Pumain D., Saint-Julien T., Sanders L., 1987, Application of a dynamic urban model. Geographical Analysis, n°1-2, 152-168, dans la thèse de Lena Sanders (Sanders1984), et dans un ouvrage collectif en 1989 “Ville et auto-organisation” (Pumain1989 p 112).


Replication supports model validation because validation is a process that determines a correspondence between the outputs from an implemented model and real-world measures. If the replicated model produces different outputs than the original model then that raises questions as to which outputs correspond more to real world data. If the replicated model's outputs are closer to the real world data that lends support to the validity of the replicated model as compared with the original model. More importantly, model replication raises questions about the details of the original modeling decisions and how they correspond to the real world. These questions help clarify whether there is sufficient correspondence between the original model and the real world. Replication forces the model replicater to examine the face validity of the original model by re-evaluating the original mapping between the real world and the conceptual model, since the replicater must re-implement those same concepts.


--

Guermond va décider d'embaucher directement dans leur équipes des mathématiciens/informaticiens comme Patrice Langlois. Une décision qui permet de maintenir un certain niveau technique dans le laboratoire, d’introduire de nouvelles compétences comme par exemple la manipulation d'Automates Cellulaires courant des années 1980, tout en assurant une certaine continuité dans l’enseignement de la programmation et des statistiques aux Géographes dans ces universités.


%Il semble bien donc que cela soit par la construction d’une coopération entre géographes et disciplines plus rompu à l’informatique, comme les statistiques, mathématiques ou la physique que se construisent majoritairement en France les nouveaux modèles de simulations.

%Un autre marqueur intéressant peut être soulevé, révélateur d'une époque ou le programme informatique n'a pas encore acquis de valeur patrimoniale, est celui d'une absence totale de stratégie pour la sauvegarde des modèles de simulations ainsi réalisés. Pour un modèle tel que Simpop 1 réalisé au début des années 1990, il est étonnant de retrouver les compte rendu de réunion semaine par semaine parfaitement conservé sous leur formes papiers et éléctroniques, mais aucune version éléctronique archivé, la gestion de cet aspect étant délégué de façon implicite au travail des informaticiens. Comme si la seule vrai valeur du modèle résidait plus dans sa fonction explicative, formalisatrice, permise par sa construction, plutôt que dans sa capacité à produire et reproduire un résultat.


Il n’y a pas je crois de travail de synthèse existant permettant d'acter en géographie la “possible” expression de ce désengagement des géographes dans la formation en “programmation”, et la façon dont elle pourrait se traduire à la fois dans les enseignements et son impact sur les projets dans les laboratoires de recherche plus spécialisé dans la modélisation. Car cette démocratisation de l’outil, si elle permet le passage  plutot \enquote{heureux} de certaine de ces techniques dans le vocabulaire courant du géographe (AFC; ACP; SIG; etc.) \autocite{Pumain2002} elle dessert également une autre vision de l'informatique, celle de l'informatique \enquote{boite-noire}. En effet qu’advient t il de la programmation comme activité “créative” dès lors que son autre fonction principale définissant l’apprentissage de celle-ci comme un \enquote{passage obligé vers d’autres applications} disparait au profit de logiciel plus simple à utiliser ? \textcite[4]{LeBerre1987} cite ainsi à propos de sa formation à l’informatique opéré dans un contrat entre la DGRST et le groupe Dupont, \enquote{il m’a fait refuser l’utilisation de l’informatique presse bouton, dangereuse pour le travail scientifique, et qui malheureusement se répand avec la diffusion des micro-ordinateurs}

Ce mouvement est peut être en train de s'inverser avec les efforts de la génération de modélisateurs précédentes, et l'avénement de support plus accessible pour la modélisation. Si ce combat n'est pas encore gagné, un autre nous attend déjà, il s'agit maintenant de ré-engager les géographes modélisateurs à utiliser la puissance mise à disposition par le HPC.




----

Un point déjà évoqué au début de notre exposé, les géographes modélisateurs ayant déjà expérimenté certains de ces challenges guidés par les évolutions technologiques. 

Dirigées vers l'usage du HPC pour l'exploration de modèle de simulation, des tentatives de calibration du modèle de Peter Allen se font dès les années 1980, guidées de façon automatique par un algorithme de minimisation basé sur une fonction objectif minimisant les écarts entre données simulées et données empiriques. Preuve que dans l'activité de modélisation, l'usage du HPC a aussi pu être à un moment donné motivé par un autre besoin plus méthodologique, que cette seule absence effective de micro-ordinateur pour exécuter les modèles.

Ce qui nous amène à un deuxième argument pouvant également expliquer la durabilité des relations avec certains centres de calculs. En effet, dans certains cas, les challenges scientifique d'hier sont aussi restés pour certains des challenges scientifiques d'aujourd'hui. La nécessité d'accès à une ressource informatique de puissance supérieure à un micro, reste comme dans le cas d'exploration des modèles de simulation, toujours une réalité.



En ce qui concerne la seconde question questionnant les usages HPC en SHS et en géographie vis à vis de leur \enquote{apparente} absence sur ce terrain, il faut bien noter que cet état de fait n'a pas toujours été vrai. Se posera donc ensuite la question suivante, que s'est il donc passé pour que l'on constate aujourd'hui une telle absence des géographes sur ce terrain ?

Avant de revenir plus en détail sur l'évolution des pratiques vers le HPC à Géographie-Cités, il me paraissait important de questionner cette absence d'intérét général pour le HPC, en les mettant en perspective d'une certaine actualité questionnant les pratiques l'enseignement de l'informatique.

Car si ce n'est pas vraiment l'absence de challenges, ou de géographes trop aventureux qui a pu faire oublier à la majeure partie des géographes cette existence de ressources informatiques inespérées au-delà du simple micro-ordinateur, alors quelles pistes de réflexion nous reste-t-il  ?

% transition à refaire.
Cette première tentative sera aussi l'occasion pour nous de faire le lien (section \ref{ssec:hist_pratiques}) entre cette première période d'accès à l'informatique, l'expression d'un premier besoin latent, la transformation du paysage dans le HPC, et l'installation puis la transformation des pratiques dans ce laboratoire Géographie-Cités, marquant un retour, espérons cette fois-ci définitif, vers l'usage du HPC pour l'exploration de nos modèles de simulation.

----

%Oui c'est tout à fait vrai, mais il faut aussi garder en tête ce témoignage d'Alexandre  Kych, qui conforte largement cette analyse de Pierre Mounier-Kuhn, lorsqu'il fait état d'une école mathématique francaise ayant vu dès le départ d'un très mauvais oeil ce qui est devenu par la suite la discipline informatique : \enquote{Pour travailler sur des fichiers numériques, nous n'avions pas le choix. C'était un centre de calcul ou bien la règle à calcul ou les tables de Barlow ou de Bouvart et Ratinet. Au début des années 70, les calculettes commençaient tout juste à apparaître. Je me souviens avoir assisté à la réunion annuelle des professeurs de mathématiques du secondaire à Rennes dans la 1ère moitié des années 70. Il y avait plusieurs centaines de personnes. Les organisateurs avaient convié la société Commodore à présenter sa dernière calculatrice. Cette calculatrice aurait fait pale figure 20 ans après. Il n'empêche qu'une partie des mathématiciens présents était hostiles à ces premières calculatrices et ils ont même fait appel à un calculateur prodige qui a montré qu'il calculait plus rapidement.}

%Produire une meilleur exploration des modèles, dont on a vu qu'elle était une autre façon plus modeste, mais aussi plus honnete, de parler de la validation (évaluation) \autocite{Amblard2006}

%Si il ne peut donner réponse à lui seul au problème de la validation, celui-ci peut au moins nous donner les bases d'une discussion honnete, comme support préalable à cette part d'expression collective de la validation.

%Personne ne s'étonne de pouvoir lancer des simulation d'automates cellulaires sur des matrices 2D disposant de millions d'éléments, et pourtant, on a vu dans le chapitre 1 que les pionniers comme Marble et Pitts, programmant sur des superordinateurs de l'époque, ont du malgré tout se resigner à redimensionner leur problèmes dans des proportions informatiquement acceptables (données et complexité), au détriment du raisonnement scientifique entrepris au départ ?


%\textit{Vrai ?} ou \textit{Faux ?}, cet état des lieux que l'on imagine volontairement provocateur, est le reflet d'une réalité loin d'être aussi simple à trancher qu'il n'y parait, l'outil informatique étant définitivement à double tranchant; un argument que n'hésiterons pas d'ailleurs à mobiliser et remobiliser sous son plus mauvais jour les critiques récurrents d'une science géographique quantitativiste soit disant aliéné, stérilisé par ces choix technologiques.  L'idéologie ne vient qu'aprés coup, dans la construction et la manipulation des hypothèses constitutives des modèles, ou encore dans l'interprétation qu'on veut bien en faire. Si il y'a inévitablement eu des abus et des erreurs de jeunesse (dont quelqu'une sont exposé dans le chapitre 1), l'examen de certains projets scientifiques inscrit dans le long terme est là pour montrer qu'un tel constat est heureusement très loin d'être généralisable à l'ensemble des géographes modélisateurs.

%Si on se concentre en france sur l'exemple d'une autre technologie ayant impacté la géographie, le SIG. D’adoption plus ancienne et donc logiquement largement plus diffusé que le HPC, on retrouve pourtant dans l'HDR de Thierry Joliveau une forme d'écho à l'argumentaire d'Openshaw quant celui-ci pointe le danger qu'il y a délaisser ainsi les apports technologiques.

%Ce détachement progressif des géographes avec \enquote{la programmation} autre que seulement statistique n’est-il pas en train de devenir un contresens dans une société occidentale ou au contraire, la courbe va en s'accélérant ? A tel point qu'il devient de plus en plus difficile d'imaginer, même à court terme, quel forme sociétale viendra s'appuyer sur un paysage technologique aussi variable ? Après tout, ce qui révélait encore de chimère il y'a à peine 20 ans, comme l'ordinateur quantique, les drones personnels, sont touchés par des percées technologiques répétés qui font de ces objets autrefois de science fiction des objets sur le point d'intégrer à tout moment notre réalité sociétale. Une intrusion d'autant plus violente que certaines se font presque en silence, laissant au final très peu de temps aux acteurs publics pour intégrer ces changements; on pensera notamment à la banalisation et la diffusion de l'usages des drones comme outils personnels à tout faire, une révolution silencieuse dont les pouvoirs publics peine encore à mesurer l'ampleur. Pour Turton et Openshaw \Anote{openshaw_revolution}, le HPC rentre aussi dans cette catégorie des technologies ayant infiltré, presque sans bruit, la réalité d'un certain nombres de disciplines scientifique de façon quasi-continue depuis 1980, jusqu'à apparaitre tout à coup comme d'un intérét scientifique évident pour les quelque géographes encore ouverts à l'innovation informatique \Anote{note_equipe}.

%Si il n'est pas possible de statuer sur les sciences humaines en général, la voie nouvelle offerte par la simulation chez les géographe francais ne fera pourtant que révèler aux yeux des premiers modélisateurs l'existence de certains challenges qu'il faudra bien résoudre en tenant compte de \enquote{tous} les progrès de l'informatique, logiciels, matériels, paradigmatique, ou algorithmiques.

%On ne saurait toutefois ignorer les efforts toujours investi par les géographes dans une forme de programmation orienté vers les statistiques (SPSS, SAS, R) ou la cartographie (R, Flash, et Webmapping en général), ainsi que le combat toujours en cours de certains pour engager les modélisateur dans \enquote{des pratiques de modélisation et de simulation libérées} \autocite{Banos2013}. Les plateformes de modélisation agents plus \textit{user-friendly} apparu dans les années 1990 ayant permis une approche à la fois plus ludique, et plus pragmatique de la programmation : Netlogo, Gama, Repast, etc. C'est bien dans le prolongement de ce qui constitue encore un petit monde que se place notre tentative pour démocratiser le HPC dans la simulation.

%Avant d'engager cette discussion dans le cadre d'une évolution des pratiques au laboratoire géographie-cités, il était important il me semble de donner quelques élements d'histoire sur les pratiques des premiers géographes quantitatifs au contact des centres de calculs. En effet, ces structures qui ont pour la plupart disparu ou sont devenus inacccessible aux géographes d'aujourd'hui, semble pourtant représenter auprès des témoins interviewé un esprit qui n'est pas sans évoquer un acteur majeur nous ayant permis de concrétiser, et de prolonger ce projet d'une plateforme pour la construction et l'évaluation de modèle de simulation en géographie.

%Note : Montre le fait que HPC est un probleme qui n'est pas nouveau chez les géographes, et que certains avait déjà préssenti son utilité, pas que pour la modélisation : openshaw HPC

------------------------------------------------------------------
------------------------------------------------------------------
------------------------------------------------------------------
------------------------------------------------------------------


On a pu observer dans le chapitre 1 et au travers de la section précédente que la validation recoupe tout à la fois des niveaux de discours différents, mais également une multiplicité de problématiques, dont les racines remontent à l'invention de la simulation sur ordinateur. Il est intéressant de montrer par la suite que ces questions ne s'attachent en réalité que très partiellement aux supports informatiques permettant la construction des modèles de simulation. 

\hl{Il semblerait qu'on puissent isoler quelques personnes qui à cet époque semble agir aux travers de leurs actions comme catalyseur d'une tendance plus profonde probablement entamé de façon symétrique en Europe et aux Etats-Unis.}


En effet, après avoir rapelé l’avènement au début des années 1990 du méta-formalisme agent comme support préférentiel de modélisation dans les sciences humaines et sociales, nous verrons que bon nombre de questions persistent, et se cristallise encore aujourd'hui dans des revendications pour l'émergence d'un protocole standard. Un objectif difficile à atteindre, celui mettant à la fois en tension la nécessité d'une contextualisation propre à la validation, et la nécessité d'une généricité propre à la standardisation.

%Le fait que des auteurs continue de développer ces aspects par eux meme.
%cf introduction de Squazzoni2009 par exemple

%Travaux de Steels, Deneubourg, etc dans le giron de l'université libre de Bruxelle et des travaux appliqués à la biologie de  (a voir si ca va la ou dans la parenthèse, j'aurais tendance à dire plutot là)


% Percolation dans la géographie ... avec force de Netlogo/Starlogo aujourd'hui, et rapport avec Seymourt, Minsky la decentralisation évoqué au debut avec le connexionisme. *

Les débats évoqués par la suite autour de cette problématique sont mis en parallèle des innovations apparu dans les années 1990, avec l'apparition et la diffusion des systèmes multi-agents (SMA) et des Automates Cellulaires (AC) comme nouveaux outils pour la représentation de dynamiques spatialisés complexes en sciences humaines et sociales (la quatrième vague d'innovation selon \autocite{Banos2013a}). Si cette technologie a indégnablement permis la levée de certaines barrières théorique et techniques en permettant l'intégration de l'hétérogène dans les modèles, tant du point de vue des échelles que des formalismes mobilisable pour la représentation des hypothèses \Anote{lena_bottomUp}, elle a aussi de fait participé à la complexification de cette question de la validation. \autocite[38-41]{Varenne2013} \hl{A détailler plus si j'ai le temps ... }

De cette variation dans la formalisation des hypothèses découlent des différences importantes dans les résultats, comme le prouve de nombreux travaux et publications étudiant ces transferts d'un formalisme à un autre tout en minimisant l'écart aux hypothèses. C'est une question qui s'est rapidement posé comme importante dans le cadre de la validation, l'\enquote{alignement de modèle} visant à établir quelle variabilité pouvait être imputable non pas aux hypothèses, mais à leur différence de support informatique. \hl{ref epstein}

Malgré cela, il me semble que les questions opérés en amont de la selection, de l'introduction et de l'organisation des hypothèses dans un réseau de causalité en partie support de l'explication reste quand à elles relativement indépendante de la technologie sous jacente. Ainsi le mode opératoire décrit par les pionniers réalisant le modèle A.M.O.R.A.L basé sur l'utilisation des systèmes dynamiques sont confrontés au même dilemme quand à la selection des hypothèses représentative qu'un modélisateur qui voudrait réaliser ce même modèle usant du méta-formalisme agent. \hl{A voir pour le muscler avec la boucle données -> modele -> données)}


 %La typologie de Varenne est intéressante car elle sous entend une grande partie des sous débats ou raffinements qui peuvent exister sur ce thème, \autocite{Eckhart2010}

%Et c'est vrai que des propriétés intéressantes développés par Hacking comme l'autonomie des modèles et de ce fait l'autonomie des résultats, est un concept intéressant lorsqu'on le rattache à la vie des modèles de simulations tels que nous les construisons.


 %On pourra également arguer que c'est bien là le problème des sciences de la complexité, c'est qu'il est difficile sinon impossible de rendre compte du fonctionnement global d'un système en étudiant seulement les éléments qui le constitue, coupés de tout ou partie de leur interactions%, avec pour effet l'intrication des causes et des effets.


%++ Innovation en géographie des ABÙ, par rapport aux système dynamique outre la flexibilité exposé, c'est l'apport de la pluriformalisation et la possibilité de formuler (ou pas) un rapprochement entre entité virtuelle et réelle (dénotation interne / externe de Varenne); avec tout les dangers qu'un tel rapprochement suppose... cf les individu micro pour les sociologues, les villes pour les géographe, etc. Mais les modèles restent des modèles causaux, ou ce qui est dans le modèle compte plus pour l'explication que le modèle en lui meme en tant qu'instantané ++

%Mais j'aimerais revenir à présent sur l'apport historique d'Hermann à ces débats, un acteur important dans l'histoire de la V\&V, et dont il me semble on mesure encore l'actualité des questionnements qu'il souleve en 1967.



%D'une part l'individu-centré doit être dissocié de l'entité-centré, car le premier lève une ambiguité qui renvoie en science sociale à l'individualisme méthodologique, alors même que l'entité centré reste neutre sur le niveau de représentation associé. Cette notion peut s'exprimer d'un point de vue mathématique au sens classique, à différents niveau d'échelles, 

%Ce que permet l'Agent, et le paradigme Objet sur lequel la majorité des plateformes s'appuient tient dans sa capacité holonique, et sa capacité à intégrer l'hétérogène. % FERBER ET DESCRIPTIONS L1 --) L5

%Les deux foyers évoqués s'appuient sur la maturation d'une \enquote{vision} individu-centré qui ne s'appuie pas sur des méthodes classiques mathématiques, et par la mise en oeuvre d'un registre explicatif différent, la flexibilité de représentation et la capacité d'intégration qu'elle permet. 

%Si on prend l'exemple de la simulation des systèmes urbains, \autocite{Sanders2013}

%\autocite{[18]Grimm2004}

%L'usage de l'ordinateur comme machine à manipuler des symboles autres que mathématique, déjà observé au travers des études d'Hagerstrand, de Tobler, de Allen, permet d'une part de découpler 

%L'individu-centré doit être découplé du formalisme informatique sous jacent

%l'usage tranché des formalismes informatiques pour la simulation tel que donnés par les catégories de Varenne. 

%Les tentatives de mise en place d'un point de vue individu-centré dans la simulation arrivent très vite, en géographie \autocite{Hagerstrand1952}, en économie \autocite{Orcutt1957}, en archéologie \autocite{Doran1970}, en écologie, en sociologie, etc.

%L'invention en informatique du paradigme Acteur, puis du paradigme Objet apporte courant des années 1980 le support adéquat qui permet non seulement le développement de plateformes plus abordables, supportant les briques de bases du méta-formalisme agent.

%Preuve que cette limite est flou, certains modèles réalisés en géographie courant des années 1980, comme ceux de Peter Allen en Fortran mélangeant équation différentielle et spatialisation d'objet géographique à base de règles, se situent déjà dans un intermédiaire délicat entre catégories de simulation. L'apparition du paradigme Objet permet sans aucun souci l'expression d'une individuation sans que soit fait une quelconque référence au concept agent. Mais dans la continuité des travaux de Ferber, Drogoul, il semble que cela soit finalement plus les apports conceptuels dérivé de l'écologie virtuelle qui retiennent au départ l'attention des géographes.

%\autocite{Louail2010}

%L'acquisition rapide durant les années 1990 du formalisme agent par les différentes disciplines des sciences sociales semblent se faire dans un certain flou autour de la notion d'Agent \autocite{Drogoul2003} dont l'acceptation conceptuelle n'implique pas forcément une traduction informatique correspondante. Un constat rapporté par différents porteurs dans la communautés agent lors de bilan faisant état de dix ans de pratiques, \autocite{Grimm1999}, \autocite{Doran1999,Doran2000}. 

%Le modèle agent tel qu'il est intégré par les disciplines ne tient pas d'un formalisme unique, mais d'un socle commun d'élements que l'on organise à loisir en fonction de sa problématique. Un \enquote{méta-formalisme agent} qui capture une diversité d'usage très bien résumé dans les multiples instances de modèles de simulation de l'ouvrage de Jean-Pierre Treuil, Alexis Drogoul et Jean-Daniel Zucker \enquote{modélisation et simulation à base d'agents}.



% Aller retour problématique entre l'évidente nécessité d'un protocole par les tenants de la validation, et l'évidence sans cesse rapellé de son absence ! 


T : Ce qui nous intéresse c'est de montrer que l'accès à de meilleure outils  n'offre une visibilité que sur une petite partie du problème de la validation. 

%Qu'en est il des problématiques de la validation déjà opérant dans les années 1970 ? Il n'y a aucune raison pour qu'ils disparaissent, au contraire ?

%\paragraph{Explosion des usages, flou du formalisme}

%L'inception de cette technique dans les différentes disciplines des sciences sociales a donné lieu à une explosion en volume du nombre de travaux courant des années 1990, ce qui ne nous permet pas de détailler par la suite le cheminement de chacune des disciplines dans l'appropriation des technologies sous jacente à l'individu centré. 


% Roughgarden2013 > Grimm and Railsback (2005) detail seven “challenges” that IBMs have faced in ecology: long time needed to develop the model, difficulty in analyzing results,lack of common language to communicate model and results, requirement for too much data, uncertainty and error propagation, lack of generality, lack of standards. Ecological IBM modelers have faced these challenges head on.

%Evolutionary Individual Based Models (EIBE) ou Individual Based Ecology (IBE) 

%With the Across-Trophic-Level System Simulation (ATLSS) DeAngelis initiated a large research program ained at implementing IBMs to the Everglades.
%http://tuvalu.santafe.edu/projects/echo/
%http://books.google.fr/books?id=K2M6VDCQ5mMC&pg=PA231&lpg=PA231&dq=echo+forrest+holland&source=bl&ots=K-hbENMZ9Q&sig=gCGsGqlgwNN-axPbqvN46W0_uwg&hl=fr&sa=X&ei=CbQVVJOGD8vTaOb3gtAK&ved=0CCwQ6AEwAQ#v=onepage&q=echo%20forrest%20holland&f=false

% THOMAS LOUAIL : Histoire ABM, méta formalismes plus que formalismes, etc.

%% A voir si ca va pas dans la partie construction = trajectoire ... mais je pense pas, c'est la qu'il faut appuyer le fait que la partie technique, si elle apporte bien des nouvelles choses, elle ne permet pas tout.

INVARIANT DANS LA CONSTRUCTION DES MODÈLES = parmis les exemples, il ya celui de la construction,evaluation de modèle, qui reste similaire dans l'usage. Cf exemple avec AMORAL, pratique similaire à ce que l'on fait encore actuellement en terme d'incrément. Je propose une hypothèse, je l'évalue , et voilà ...

\paragraph{La validation face à ces nouveaux outils ?}
\label{p:validation_nouveaux_outils}

Dans la continuité de ce qui a été dit dans le chapitre 1, la famille de questions relative à la validation des modèles de simulation se posent très tôt au début des années 1970, bien avant que n'arrive l'usage du méta-formalisme Agent dans les sciences sociales.

Plus que l'évolution des techniques, dont il est toujours temps de mesurer l'influence dans un deuxième temps, il est important de soulever avec quelle constance un certaines nombre de problématiques propre à la validation traversent les années. 

La question des protocoles pour la construction de modèles de simulation est il me semble un bon exemple. Dès premières évocations dans les années 1970 jusqu'à 2008, la recette n'a guère vu que sa forme évolué.



\hl{Reprendre partie écrite dans une précédente version, et l'adapté à la problématique de ce paragraphe}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% EN COURS DINTEGRATION + LIAISON AVEC POM ? 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

Heureusement, les capacités de calcul informatique et les méthodes de modélisation multi-échelles des informations spatio-temporelles se sont bien développées, en parallèle et en réponse à ces nouveaux défis. Parmi les sciences humaines et sociales, les géographes sont aujourd'hui particulièrement présents dans l’expérimentation et la mise au point méthodologique, qu’il s’agisse des les appliquer à des données environnementales (Kanevski, Delahaye, Douvinet) ou à des problèmes de géographie urbaine (Batty, Banos, …Ruas) ou régionale (Wilson, Dauphiné, White, Engelen, Daudé) ou encore à l’analyse de réseaux (Rozenblat). 

Les systèmes complexes sont bien entendu aujourd'hui au cœur des interrogations en sciences humaines et sociales, dans la mesure où il est connu depuis très longtemps dans ces disciplines que les interactions en jeu ne sont pas linéaires \todo{ (Morin, 200 ) }  (Morin, 200 ), que des individus engagés dans des processus collectifs sont susceptibles de donner lieu à des phénomènes originaux à l’échelon macroscopique (Schelling, 1978) \todo{ (Schelling, 1978) }, et que les situations initiales ou les effets de contexte sont parfois aussi déterminants que les modalités qui règlent les échanges entre les individus, suscitant des évolutions où la \foreignquote{english}{path dependence} \todo{ (Arthur, 1994) } (Arthur, 1994) ou \enquote{enchaînement historique} joue un rôle très important, en limitant les expressions des dynamiques possibles.

Suivant une remarque emprunté à Gould \autocite[31]{Gould2004} \enquote{[...] il faut bien se rendre compte que la formulation de questions et les avancées méthodologiques sont les deux faces d'une meme piece}. Banos \autocite{Banos2013} compte jusqu'à quatre étapes majeures ou progression technologique rime avec évolution et remise en question des questionnements théoriques. Ainsi, tout comme la systémique et la formalisation mathématique avait permis la découverte des trajectoire complexes mais cohérentes des objets géographiques, soumises non plus à des rencontres locales mais à des contingence génératrice de bifurcations dans la dynamique des villes \autocite[137]{Pumain2002}, l'apport de la modélisation multi-agent des années 1990 pousse elle encore un peu plus loin la transformation. \autocite{Sanders2007}

%Avec l'apparition des modèles agents comm en géographies les nouvelle approches ouvrent de nouvelles voies à la pensée et au questionnement littéralement impensables auparavent, on peut citer par exemple la programmation déjà linéaire, qui souleve (cf Leslie Curry, aussi cité ailleur par Hagget je crois) + introduction de Gould1970 "The intellectual revolution in geography since the middle and late fifties rests upon two main supporting pillars" 

Les modèles d’agents employés en géographie ont la particularité d’avoir été développés non seulement pour des simulations d’agents individuels, par exemple pour simuler la diffusion d’épidémies dans un territoire urbain ou régional ou sur un réseau (Banos SRAS, Badariotti, Laperrière, Eubank…) ou la diffusion d’innovations (Daudé), la propagation d’incendies (Langlois), ou encore pour éclairer les choix résidentiels (Bonnefoy…) mais ont démontré l’intérêt de modélisations fondées sur des entités plus vastes (villes, régions, voire états du monde) pour examiner les possibilités d’émergence à partir de leurs interactions à des niveaux géographiques plus larges. 

%Le laboratoire Géographie-cités a été pionnier dans l’application à la géographie des systèmes multi-agents, notamment avec la série des modèles Simpop réalisés en collaboration avec des informaticiens (Bura et al., 1996, Sanders et al., 2007, Pumain, 2012) mais aussi avec des modèles d’agents développés directement avec des logiciels simplifiés comme Netlogo (Banos…). L’évolution des modèles ne se limite pas à l’emploi de logiciels mais participe plus généralement des réflexions de la communauté scientifique intéressée par les systèmes complexes. Le GDRE S4 (Simulation Spatiale pour les Sciences Sociales) a par exemple inscrit son action parmi les premiers projets de l’Institut des Systèmes Complexes de Paris-Ile-de-France (ISC-PIF) fondé par Paul Bourgine \todo{ (ref) }. L’UMR  Géographie-cités est devenue en 2011 l’antenne  de l’Institut des systèmes complexes à l’Université Paris I et acquiert du matériel pour sa propre autonomie de fonctionnement.

\Anotecontent{grimqualite}{On soulignera à ce titre la qualité de l'ouvrage de Railsback et Grimm \autocite{Railsback2012}, les deux auteurs prenant le temps de développer ces aspects en détail avec des exemples}

\hl{footnote a compléter}


A décharge de l'auteur, une des difficultés, qui sera détaillé dans la section \ref{sec:validation}, est probablement la nature tout à fait contextuelle de cette étape. Pour reprendre les mot de \textcite{Amblard2006} \enquote{[...] les questions pour  la validation des modèles ne devraient jamais être abordées en dehors des questions relatives à leurs usages.}

Il ne serait alors pas outrancier de de tenter ici une comparaison entre la désillusion vécu courant des années 1970, et la situation tel qu'elle apparait en 2014. Car sur ce point, un regard sur l'histoire passée de la simulation dans les sciences sociales n'est pas fait pour nous rassurer, la plupart des problèmes cités comme facteurs limitants dès les années 1970 recoupent encore aujourd'hui tout ou partie de nos problèmes actuels. On pourra arguer que certaines problématiques, comme les problématiques techniques d'accès aux ordinateurs, ou aux programmes ont quasiments disparu. Certes oui, mais de nouvelles problématiques, de même nature, sont apparus, et s'avère tout autant limitantes dans le but poursuivi. La problématique de l'accès et de l'utilisation d'une puissance de calcul nécessaire et suffisantes pour l'exploration des modèles n'est ainsi qu'une translation à un niveau supérieurs de problématiques plus anciennes...

%En forte interaction, ceux ci peuvent être rapportés à au moins trois dimensions explicatives, une dimension technique, une dimension méthodologique et une dimension institutionnelle, ce qui peut être explique pourquoi ceux ci n'ont jamais pu être totalement résolus dans le cadre d'une seule politique, d'une seule projet, ou d'une seule publication : faible nombre de modèles publiés et reproductibles, absence de publication décrivant des protocoles d'évaluation de modèles et des mises en application de ces protocoles, difficulté d'accès à l'information et à la ressource technique nécessaire pour l'exploration des modèles, stratégie de publication misant sur la publication de modèles déjà finalisés mais jamais appliqués de nouveau, manque de formations adaptées ou dédiées, confrontation avec des courants disciplinaires \textit{mainstream} ignorant l'activité modélisante, etc.

%La plupart du temps, ces papiers méthodologique pointent des arguments faisant état des limitations techniques, mais il apparaît de façon assez nette que la problématique de la validation des simulations et la lutte régulière pour reconnaître le caractère « scientifique » de la simulation et son intérêt pour dégager de la connaissance semble être un argument transerval dans les différentes disciplines et dans le temps.

%En reprenant notre argumentaire pour y exposer non plus le modèle non plus dans sa dimension uniquement technique, mais dans sa dimension de construit, de nouveaux objet d'étude apparaissent.

Ce qui semble être décrit comme une immaturité dans la méthode Agents \autocite{Heath2009} se rapporte en réalité à des pratiques qui dépasse largement l'utilisation spécifique de cette technique spécifique. La simulation agents en apportant cette nouvelle et importante flexibilité dans l'expression des problématiques de recherche, a pu certainement provoquer un effet de mode menant à des dérives, un état de fait qui ne nous fera pas oublier l'enracinement d'une bonne partie des critiques qui lui sont adressés, à tort ou à raison.

Compte tenu de cette persistance des problématiques dans le temps, au delà de l'évolution des techniques, il est fait ici un choix radical. En objectivant cette \enquote{démarche scientifique de construction de modèle} pour en faire un objet d'étude à part entière, et en laissant dans un premier temps de coté les supports techniques (modèles équationels, modèles agents, etc.), il est proposé dans cette étude de se concentrer sur la problématique de validation/vérification des modèles, une étape que l'on juge déterminante au cœur de la construction du modèle. 


% glissement vers ABM?

%Ces problématiques sont depuis lors des incontournables pour qui veut entamer une discussion sur la \enquote{validation}, la sous-détermination des Théorie par les données de Quine, ou l'équifinalité de Bertalanffy étant des contraintes indépassables avec lesquels notre raisonnement doit s'adapter pour constuire des modèles explicatifs \enquote{malgré tout}. 

Bien que ce niveau de discours épistémologique paraisse décalé face à des pratiques qui s'organise dans une logique tout à fait indépendante, les débats récents ont montrés qu'une assise épistémologique plus explicite pouvait dans le cadre de nos construction de modèle être non plus vu comme un défaut, mais comme un atout pour mieux faire face au critiques. Les sociologues \autocite{Hedstrom2010, Elsenbroich2012, Squazzoni2010, Manzo2007} et les économistes \autocite{Epstein1996} semblent être ces dernières années les principaux acteurs de ce type de rapprochements dans la communauté agents 

Mais ce débat n'est qu'un premier filtre, une autre épistémologie vient se greffer sur la notion de \enquote{laboratoire virtuel} et questionne cette fois ci, non plus tant le résultat obtenu via l'expérimentation, que la condition de sa transférabilité au réel.

Les décennies qui vont suivre vont voir s'affronter plusieurs débats sur la validation, tant sur le point philosophique que méthodologique.

%La section \ref{ssec:crise_mutation} traitait du renouveau de la simulation de modèle en énumérant simplement les évolutions techniques et méthodologiques sans forcément pointer les débats qui accompagne ces transformations et qui touche à la problématique de la \enquote{vérification}, une problématique qui sera plus développé dans cette partie de la thèse.

Il sera donc important de revenir sur les changements opérés depuis les années 1970 et la prise de conscience de l'équifinalité et des problématiques de validation qu'elle entraîne dans l'évaluation des modèles sur les systèmes sociaux.

%, et nous verrons que si il existe effectivement une différence dans la validation entre science sociale et science naturelle, celle ci n'empeche en rien l'emploi de techniques quantitatives sont neutre politiquement et peuvent être employé dans un but tout à fait non-positiviste pour l'explication !

%ce n'est pas parce que Hagget (voir citation de gregory p.) pense que la prévision peut être un moteur, voir un idéal a atteindre dans la construction des modèles en géographie qu'il renie les briques d'explication mobilisé dans ce but !)

%Hors, à ne percevoir la modélisation que par le prisme de son application politique comme le fait Gregory, on 

%et affiche quand à l'opérationalisation des notions de systèmes un septicisme dont l'argumentation s'avère avec le recul très peu convaincante, la biologie tout comme la physique qui ont vu ces dernières années la remise en cause de théories soit disant universelles, et la multiplication de modèles concurents candidats à l'explication des mêmes loi phénomènales. Le paradigme systémique ou la théorie de la complexité ayant fini de faire tomber de son piédestal la biologie et même la physique, qui peuvent aujourd'hui être confronté à des problématiques de validation certes de nature différente (les objets ne sont pas les même, encore que dans le cadre des simulation le substrat des modèles est en partie partagé), mais dont la complexité d'évaluation s'avère similaire.

%qui a le mérite de réaffirmer dans sa relecture des préceptes (simulé!) d'Hagerstrand de nouveaux moyens pour l'explication, parfois tout à fait compatible avec une vision quantitative (la théorie de la structuration dont la version spatialisé proposé par Giddens est tout à la fois mobilisé par Gregory que Pumain, dont l'expression opérationelle ne peut qu'être que très différente) mais qui peine une fois la vague marxiste évaporé dans certains pays, comme la France, à s'établir comme une véritable alternative. 

%difficilement se passer des nouveaux outils quantitatifs, dont fait partie la simulation. A ce titre, il me semble que le développement de la systémique, se place dans une continuité de réflexion sur les objets géographiques qui amène dans cette phase de transformation critique de la discipline à dérouler des modèles capable eux aussi de rendre compte de façon complémentaire de cette complexité des interactions entre l'homme et son milieu.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 







%\hl{=> Transition apport de Varenne par rapport à tout ce bazar}

%A faire remonter ? 

C'est là que le travail de Varenne réalisé au cours des années 2000 \autocites{Varenne2008, Varenne2013} apparait assez audacieux, en proposant une typologie de fonctions épistémiques flexible et cumulable, il propose une grille de lecture permettant d'intégrer à la fois la diversité des approches dans les disciplines (inter) mais également l'évolution de ces même approches à l'intérieur d'une discipline (intra). Un découplage qui permet également une définition plus fine des rapports que peuvent entretenir les disciplines entre le modèle et la simulation.

%\hl{ Varenne, la simulation comme expérience de second genre, la possibilité d'un rapport à l'empirie ... (a voir si je rentre la dedans maintenant ou si je garde ça pour plus tard dans la partie construction de modèle de simulation}

%Q : Peut on se contenter d'une lecture statique de la validation ? 

%Doran et cie en UK reprennent le flambeau
%En france : Amblard , Phan, et cie


% PROBLEMATIQUE validation SIMILAIRE FINALEMENT, 
% Cf permanence des problématique évoqué par Doran1999, Doran2000