



article Pumain D., Saint-Julien T., Sanders L., 1986, Urban dynamics of some French cities. European Journal of Operational Research, 25, 3-10. Ainsi que Pumain D., Saint-Julien T., Sanders L., 1987, Application of a dynamic urban model. Geographical Analysis, n°1-2, 152-168, dans la thèse de Lena Sanders (Sanders1984), et dans un ouvrage collectif en 1989 “Ville et auto-organisation” (Pumain1989 p 112).


Replication supports model validation because validation is a process that determines a correspondence between the outputs from an implemented model and real-world measures. If the replicated model produces different outputs than the original model then that raises questions as to which outputs correspond more to real world data. If the replicated model's outputs are closer to the real world data that lends support to the validity of the replicated model as compared with the original model. More importantly, model replication raises questions about the details of the original modeling decisions and how they correspond to the real world. These questions help clarify whether there is sufficient correspondence between the original model and the real world. Replication forces the model replicater to examine the face validity of the original model by re-evaluating the original mapping between the real world and the conceptual model, since the replicater must re-implement those same concepts.


--

%Il semble bien donc que cela soit par la construction d’une coopération entre géographes et disciplines plus rompu à l’informatique, comme les statistiques, mathématiques ou la physique que se construisent majoritairement en France les nouveaux modèles de simulations.

%Un autre marqueur intéressant peut être soulevé, révélateur d'une époque ou le programme informatique n'a pas encore acquis de valeur patrimoniale, est celui d'une absence totale de stratégie pour la sauvegarde des modèles de simulations ainsi réalisés. Pour un modèle tel que Simpop 1 réalisé au début des années 1990, il est étonnant de retrouver les compte rendu de réunion semaine par semaine parfaitement conservé sous leur formes papiers et éléctroniques, mais aucune version éléctronique archivé, la gestion de cet aspect étant délégué de façon implicite au travail des informaticiens. Comme si la seule vrai valeur du modèle résidait plus dans sa fonction explicative, formalisatrice, permise par sa construction, plutôt que dans sa capacité à produire et reproduire un résultat.

Il n’y a pas je crois de travail de synthèse existant permettant d'acter en géographie la “possible” expression de ce désengagement des géographes dans la formation en “programmation”, et la façon dont elle pourrait se traduire à la fois dans les enseignements et son impact sur les projets dans les laboratoires de recherche plus spécialisé dans la modélisation. Car cette démocratisation de l’outil, si elle permet le passage  plutot \enquote{heureux} de certaine de ces techniques dans le vocabulaire courant du géographe (AFC; ACP; SIG; etc.) \autocite{Pumain2002} elle dessert également une autre vision de l'informatique, celle de l'informatique \enquote{boite-noire}. En effet qu’advient t il de la programmation comme activité “créative” dès lors que son autre fonction principale définissant l’apprentissage de celle-ci comme un \enquote{passage obligé vers d’autres applications} disparait au profit de logiciel plus simple à utiliser ? \textcite[4]{LeBerre1987} cite ainsi à propos de sa formation à l’informatique opéré dans un contrat entre la DGRST et le groupe Dupont, \enquote{il m’a fait refuser l’utilisation de l’informatique presse bouton, dangereuse pour le travail scientifique, et qui malheureusement se répand avec la diffusion des micro-ordinateurs}

Ce mouvement est peut être en train de s'inverser avec les efforts de la génération de modélisateurs précédentes, et l'avénement de support plus accessible pour la modélisation. Si ce combat n'est pas encore gagné, un autre nous attend déjà, il s'agit maintenant de ré-engager les géographes modélisateurs à utiliser la puissance mise à disposition par le HPC.




----

Un point déjà évoqué au début de notre exposé, les géographes modélisateurs ayant déjà expérimenté certains de ces challenges guidés par les évolutions technologiques. 

Dirigées vers l'usage du HPC pour l'exploration de modèle de simulation, des tentatives de calibration du modèle de Peter Allen se font dès les années 1980, guidées de façon automatique par un algorithme de minimisation basé sur une fonction objectif minimisant les écarts entre données simulées et données empiriques. Preuve que dans l'activité de modélisation, l'usage du HPC a aussi pu être à un moment donné motivé par un autre besoin plus méthodologique, que cette seule absence effective de micro-ordinateur pour exécuter les modèles.

Ce qui nous amène à un deuxième argument pouvant également expliquer la durabilité des relations avec certains centres de calculs. En effet, dans certains cas, les challenges scientifique d'hier sont aussi restés pour certains des challenges scientifiques d'aujourd'hui. La nécessité d'accès à une ressource informatique de puissance supérieure à un micro, reste comme dans le cas d'exploration des modèles de simulation, toujours une réalité.



En ce qui concerne la seconde question questionnant les usages HPC en SHS et en géographie vis à vis de leur \enquote{apparente} absence sur ce terrain, il faut bien noter que cet état de fait n'a pas toujours été vrai. Se posera donc ensuite la question suivante, que s'est il donc passé pour que l'on constate aujourd'hui une telle absence des géographes sur ce terrain ?

Avant de revenir plus en détail sur l'évolution des pratiques vers le HPC à Géographie-Cités, il me paraissait important de questionner cette absence d'intérét général pour le HPC, en les mettant en perspective d'une certaine actualité questionnant les pratiques l'enseignement de l'informatique.

Car si ce n'est pas vraiment l'absence de challenges, ou de géographes trop aventureux qui a pu faire oublier à la majeure partie des géographes cette existence de ressources informatiques inespérées au-delà du simple micro-ordinateur, alors quelles pistes de réflexion nous reste-t-il  ?

% transition à refaire.
Cette première tentative sera aussi l'occasion pour nous de faire le lien (section \ref{ssec:hist_pratiques}) entre cette première période d'accès à l'informatique, l'expression d'un premier besoin latent, la transformation du paysage dans le HPC, et l'installation puis la transformation des pratiques dans ce laboratoire Géographie-Cités, marquant un retour, espérons cette fois-ci définitif, vers l'usage du HPC pour l'exploration de nos modèles de simulation.

----

%Oui c'est tout à fait vrai, mais il faut aussi garder en tête ce témoignage d'Alexandre  Kych, qui conforte largement cette analyse de Pierre Mounier-Kuhn, lorsqu'il fait état d'une école mathématique francaise ayant vu dès le départ d'un très mauvais oeil ce qui est devenu par la suite la discipline informatique : \enquote{Pour travailler sur des fichiers numériques, nous n'avions pas le choix. C'était un centre de calcul ou bien la règle à calcul ou les tables de Barlow ou de Bouvart et Ratinet. Au début des années 70, les calculettes commençaient tout juste à apparaître. Je me souviens avoir assisté à la réunion annuelle des professeurs de mathématiques du secondaire à Rennes dans la 1ère moitié des années 70. Il y avait plusieurs centaines de personnes. Les organisateurs avaient convié la société Commodore à présenter sa dernière calculatrice. Cette calculatrice aurait fait pale figure 20 ans après. Il n'empêche qu'une partie des mathématiciens présents était hostiles à ces premières calculatrices et ils ont même fait appel à un calculateur prodige qui a montré qu'il calculait plus rapidement.}

%Produire une meilleur exploration des modèles, dont on a vu qu'elle était une autre façon plus modeste, mais aussi plus honnete, de parler de la validation (évaluation) \autocite{Amblard2006}

%Si il ne peut donner réponse à lui seul au problème de la validation, celui-ci peut au moins nous donner les bases d'une discussion honnete, comme support préalable à cette part d'expression collective de la validation.

%Personne ne s'étonne de pouvoir lancer des simulation d'automates cellulaires sur des matrices 2D disposant de millions d'éléments, et pourtant, on a vu dans le chapitre 1 que les pionniers comme Marble et Pitts, programmant sur des superordinateurs de l'époque, ont du malgré tout se resigner à redimensionner leur problèmes dans des proportions informatiquement acceptables (données et complexité), au détriment du raisonnement scientifique entrepris au départ ?


%\textit{Vrai ?} ou \textit{Faux ?}, cet état des lieux que l'on imagine volontairement provocateur, est le reflet d'une réalité loin d'être aussi simple à trancher qu'il n'y parait, l'outil informatique étant définitivement à double tranchant; un argument que n'hésiterons pas d'ailleurs à mobiliser et remobiliser sous son plus mauvais jour les critiques récurrents d'une science géographique quantitativiste soit disant aliéné, stérilisé par ces choix technologiques.  L'idéologie ne vient qu'aprés coup, dans la construction et la manipulation des hypothèses constitutives des modèles, ou encore dans l'interprétation qu'on veut bien en faire. Si il y'a inévitablement eu des abus et des erreurs de jeunesse (dont quelqu'une sont exposé dans le chapitre 1), l'examen de certains projets scientifiques inscrit dans le long terme est là pour montrer qu'un tel constat est heureusement très loin d'être généralisable à l'ensemble des géographes modélisateurs.

%Si on se concentre en france sur l'exemple d'une autre technologie ayant impacté la géographie, le SIG. D’adoption plus ancienne et donc logiquement largement plus diffusé que le HPC, on retrouve pourtant dans l'HDR de Thierry Joliveau une forme d'écho à l'argumentaire d'Openshaw quant celui-ci pointe le danger qu'il y a délaisser ainsi les apports technologiques.

%Ce détachement progressif des géographes avec \enquote{la programmation} autre que seulement statistique n’est-il pas en train de devenir un contresens dans une société occidentale ou au contraire, la courbe va en s'accélérant ? A tel point qu'il devient de plus en plus difficile d'imaginer, même à court terme, quel forme sociétale viendra s'appuyer sur un paysage technologique aussi variable ? Après tout, ce qui révélait encore de chimère il y'a à peine 20 ans, comme l'ordinateur quantique, les drones personnels, sont touchés par des percées technologiques répétés qui font de ces objets autrefois de science fiction des objets sur le point d'intégrer à tout moment notre réalité sociétale. Une intrusion d'autant plus violente que certaines se font presque en silence, laissant au final très peu de temps aux acteurs publics pour intégrer ces changements; on pensera notamment à la banalisation et la diffusion de l'usages des drones comme outils personnels à tout faire, une révolution silencieuse dont les pouvoirs publics peine encore à mesurer l'ampleur. Pour Turton et Openshaw \Anote{openshaw_revolution}, le HPC rentre aussi dans cette catégorie des technologies ayant infiltré, presque sans bruit, la réalité d'un certain nombres de disciplines scientifique de façon quasi-continue depuis 1980, jusqu'à apparaitre tout à coup comme d'un intérét scientifique évident pour les quelque géographes encore ouverts à l'innovation informatique \Anote{note_equipe}.

%Si il n'est pas possible de statuer sur les sciences humaines en général, la voie nouvelle offerte par la simulation chez les géographe francais ne fera pourtant que révèler aux yeux des premiers modélisateurs l'existence de certains challenges qu'il faudra bien résoudre en tenant compte de \enquote{tous} les progrès de l'informatique, logiciels, matériels, paradigmatique, ou algorithmiques.

%On ne saurait toutefois ignorer les efforts toujours investi par les géographes dans une forme de programmation orienté vers les statistiques (SPSS, SAS, R) ou la cartographie (R, Flash, et Webmapping en général), ainsi que le combat toujours en cours de certains pour engager les modélisateur dans \enquote{des pratiques de modélisation et de simulation libérées} \autocite{Banos2013}. Les plateformes de modélisation agents plus \textit{user-friendly} apparu dans les années 1990 ayant permis une approche à la fois plus ludique, et plus pragmatique de la programmation : Netlogo, Gama, Repast, etc. C'est bien dans le prolongement de ce qui constitue encore un petit monde que se place notre tentative pour démocratiser le HPC dans la simulation.

%Avant d'engager cette discussion dans le cadre d'une évolution des pratiques au laboratoire géographie-cités, il était important il me semble de donner quelques élements d'histoire sur les pratiques des premiers géographes quantitatifs au contact des centres de calculs. En effet, ces structures qui ont pour la plupart disparu ou sont devenus inacccessible aux géographes d'aujourd'hui, semble pourtant représenter auprès des témoins interviewé un esprit qui n'est pas sans évoquer un acteur majeur nous ayant permis de concrétiser, et de prolonger ce projet d'une plateforme pour la construction et l'évaluation de modèle de simulation en géographie.

%Note : Montre le fait que HPC est un probleme qui n'est pas nouveau chez les géographes, et que certains avait déjà préssenti son utilité, pas que pour la modélisation : openshaw HPC

------------------------------------------------------------------
------------------------------------------------------------------
------------------------------------------------------------------
------------------------------------------------------------------


On a pu observer dans le chapitre 1 et au travers de la section précédente que la validation recoupe tout à la fois des niveaux de discours différents, mais également une multiplicité de problématiques, dont les racines remontent à l'invention de la simulation sur ordinateur. Il est intéressant de montrer par la suite que ces questions ne s'attachent en réalité que très partiellement aux supports informatiques permettant la construction des modèles de simulation. 

\hl{Il semblerait qu'on puissent isoler quelques personnes qui à cet époque semble agir aux travers de leurs actions comme catalyseur d'une tendance plus profonde probablement entamé de façon symétrique en Europe et aux Etats-Unis.}


En effet, après avoir rapelé l’avènement au début des années 1990 du méta-formalisme agent comme support préférentiel de modélisation dans les sciences humaines et sociales, nous verrons que bon nombre de questions persistent, et se cristallise encore aujourd'hui dans des revendications pour l'émergence d'un protocole standard. Un objectif difficile à atteindre, celui mettant à la fois en tension la nécessité d'une contextualisation propre à la validation, et la nécessité d'une généricité propre à la standardisation.

%Le fait que des auteurs continue de développer ces aspects par eux meme.
%cf introduction de Squazzoni2009 par exemple

%Travaux de Steels, Deneubourg, etc dans le giron de l'université libre de Bruxelle et des travaux appliqués à la biologie de  (a voir si ca va la ou dans la parenthèse, j'aurais tendance à dire plutot là)


% Percolation dans la géographie ... avec force de Netlogo/Starlogo aujourd'hui, et rapport avec Seymourt, Minsky la decentralisation évoqué au debut avec le connexionisme. *

Les débats évoqués par la suite autour de cette problématique sont mis en parallèle des innovations apparu dans les années 1990, avec l'apparition et la diffusion des systèmes multi-agents (SMA) et des Automates Cellulaires (AC) comme nouveaux outils pour la représentation de dynamiques spatialisés complexes en sciences humaines et sociales (la quatrième vague d'innovation selon \autocite{Banos2013a}). Si cette technologie a indégnablement permis la levée de certaines barrières théorique et techniques en permettant l'intégration de l'hétérogène dans les modèles, tant du point de vue des échelles que des formalismes mobilisable pour la représentation des hypothèses \Anote{lena_bottomUp}, elle a aussi de fait participé à la complexification de cette question de la validation. \autocite[38-41]{Varenne2013} \hl{A détailler plus si j'ai le temps ... }

De cette variation dans la formalisation des hypothèses découlent des différences importantes dans les résultats, comme le prouve de nombreux travaux et publications étudiant ces transferts d'un formalisme à un autre tout en minimisant l'écart aux hypothèses. C'est une question qui s'est rapidement posé comme importante dans le cadre de la validation, l'\enquote{alignement de modèle} visant à établir quelle variabilité pouvait être imputable non pas aux hypothèses, mais à leur différence de support informatique. \hl{ref epstein}

Malgré cela, il me semble que les questions opérés en amont de la selection, de l'introduction et de l'organisation des hypothèses dans un réseau de causalité en partie support de l'explication reste quand à elles relativement indépendante de la technologie sous jacente. Ainsi le mode opératoire décrit par les pionniers réalisant le modèle A.M.O.R.A.L basé sur l'utilisation des systèmes dynamiques sont confrontés au même dilemme quand à la selection des hypothèses représentative qu'un modélisateur qui voudrait réaliser ce même modèle usant du méta-formalisme agent. \hl{A voir pour le muscler avec la boucle données -> modele -> données)}


 %La typologie de Varenne est intéressante car elle sous entend une grande partie des sous débats ou raffinements qui peuvent exister sur ce thème, \autocite{Eckhart2010}

%Et c'est vrai que des propriétés intéressantes développés par Hacking comme l'autonomie des modèles et de ce fait l'autonomie des résultats, est un concept intéressant lorsqu'on le rattache à la vie des modèles de simulations tels que nous les construisons.


 %On pourra également arguer que c'est bien là le problème des sciences de la complexité, c'est qu'il est difficile sinon impossible de rendre compte du fonctionnement global d'un système en étudiant seulement les éléments qui le constitue, coupés de tout ou partie de leur interactions%, avec pour effet l'intrication des causes et des effets.


%++ Innovation en géographie des ABÙ, par rapport aux système dynamique outre la flexibilité exposé, c'est l'apport de la pluriformalisation et la possibilité de formuler (ou pas) un rapprochement entre entité virtuelle et réelle (dénotation interne / externe de Varenne); avec tout les dangers qu'un tel rapprochement suppose... cf les individu micro pour les sociologues, les villes pour les géographe, etc. Mais les modèles restent des modèles causaux, ou ce qui est dans le modèle compte plus pour l'explication que le modèle en lui meme en tant qu'instantané ++

%Mais j'aimerais revenir à présent sur l'apport historique d'Hermann à ces débats, un acteur important dans l'histoire de la V\&V, et dont il me semble on mesure encore l'actualité des questionnements qu'il souleve en 1967.



%D'une part l'individu-centré doit être dissocié de l'entité-centré, car le premier lève une ambiguité qui renvoie en science sociale à l'individualisme méthodologique, alors même que l'entité centré reste neutre sur le niveau de représentation associé. Cette notion peut s'exprimer d'un point de vue mathématique au sens classique, à différents niveau d'échelles, 

%Ce que permet l'Agent, et le paradigme Objet sur lequel la majorité des plateformes s'appuient tient dans sa capacité holonique, et sa capacité à intégrer l'hétérogène. % FERBER ET DESCRIPTIONS L1 --) L5

%Les deux foyers évoqués s'appuient sur la maturation d'une \enquote{vision} individu-centré qui ne s'appuie pas sur des méthodes classiques mathématiques, et par la mise en oeuvre d'un registre explicatif différent, la flexibilité de représentation et la capacité d'intégration qu'elle permet. 

%Si on prend l'exemple de la simulation des systèmes urbains, \autocite{Sanders2013}

%\autocite{[18]Grimm2004}

%L'usage de l'ordinateur comme machine à manipuler des symboles autres que mathématique, déjà observé au travers des études d'Hagerstrand, de Tobler, de Allen, permet d'une part de découpler 

%L'individu-centré doit être découplé du formalisme informatique sous jacent

%l'usage tranché des formalismes informatiques pour la simulation tel que donnés par les catégories de Varenne. 

%Les tentatives de mise en place d'un point de vue individu-centré dans la simulation arrivent très vite, en géographie \autocite{Hagerstrand1952}, en économie \autocite{Orcutt1957}, en archéologie \autocite{Doran1970}, en écologie, en sociologie, etc.

%L'invention en informatique du paradigme Acteur, puis du paradigme Objet apporte courant des années 1980 le support adéquat qui permet non seulement le développement de plateformes plus abordables, supportant les briques de bases du méta-formalisme agent.

%Preuve que cette limite est flou, certains modèles réalisés en géographie courant des années 1980, comme ceux de Peter Allen en Fortran mélangeant équation différentielle et spatialisation d'objet géographique à base de règles, se situent déjà dans un intermédiaire délicat entre catégories de simulation. L'apparition du paradigme Objet permet sans aucun souci l'expression d'une individuation sans que soit fait une quelconque référence au concept agent. Mais dans la continuité des travaux de Ferber, Drogoul, il semble que cela soit finalement plus les apports conceptuels dérivé de l'écologie virtuelle qui retiennent au départ l'attention des géographes.

%\autocite{Louail2010}

%L'acquisition rapide durant les années 1990 du formalisme agent par les différentes disciplines des sciences sociales semblent se faire dans un certain flou autour de la notion d'Agent \autocite{Drogoul2003} dont l'acceptation conceptuelle n'implique pas forcément une traduction informatique correspondante. Un constat rapporté par différents porteurs dans la communautés agent lors de bilan faisant état de dix ans de pratiques, \autocite{Grimm1999}, \autocite{Doran1999,Doran2000}. 

%Le modèle agent tel qu'il est intégré par les disciplines ne tient pas d'un formalisme unique, mais d'un socle commun d'élements que l'on organise à loisir en fonction de sa problématique. Un \enquote{méta-formalisme agent} qui capture une diversité d'usage très bien résumé dans les multiples instances de modèles de simulation de l'ouvrage de Jean-Pierre Treuil, Alexis Drogoul et Jean-Daniel Zucker \enquote{modélisation et simulation à base d'agents}.



% Aller retour problématique entre l'évidente nécessité d'un protocole par les tenants de la validation, et l'évidence sans cesse rapellé de son absence ! 


T : Ce qui nous intéresse c'est de montrer que l'accès à de meilleure outils  n'offre une visibilité que sur une petite partie du problème de la validation. 

%Qu'en est il des problématiques de la validation déjà opérant dans les années 1970 ? Il n'y a aucune raison pour qu'ils disparaissent, au contraire ?

%\paragraph{Explosion des usages, flou du formalisme}

%L'inception de cette technique dans les différentes disciplines des sciences sociales a donné lieu à une explosion en volume du nombre de travaux courant des années 1990, ce qui ne nous permet pas de détailler par la suite le cheminement de chacune des disciplines dans l'appropriation des technologies sous jacente à l'individu centré. 


% Roughgarden2013 > Grimm and Railsback (2005) detail seven “challenges” that IBMs have faced in ecology: long time needed to develop the model, difficulty in analyzing results,lack of common language to communicate model and results, requirement for too much data, uncertainty and error propagation, lack of generality, lack of standards. Ecological IBM modelers have faced these challenges head on.

%Evolutionary Individual Based Models (EIBE) ou Individual Based Ecology (IBE) 

%With the Across-Trophic-Level System Simulation (ATLSS) DeAngelis initiated a large research program ained at implementing IBMs to the Everglades.
%http://tuvalu.santafe.edu/projects/echo/
%http://books.google.fr/books?id=K2M6VDCQ5mMC&pg=PA231&lpg=PA231&dq=echo+forrest+holland&source=bl&ots=K-hbENMZ9Q&sig=gCGsGqlgwNN-axPbqvN46W0_uwg&hl=fr&sa=X&ei=CbQVVJOGD8vTaOb3gtAK&ved=0CCwQ6AEwAQ#v=onepage&q=echo%20forrest%20holland&f=false

% THOMAS LOUAIL : Histoire ABM, méta formalismes plus que formalismes, etc.

%% A voir si ca va pas dans la partie construction = trajectoire ... mais je pense pas, c'est la qu'il faut appuyer le fait que la partie technique, si elle apporte bien des nouvelles choses, elle ne permet pas tout.

INVARIANT DANS LA CONSTRUCTION DES MODÈLES = parmis les exemples, il ya celui de la construction,evaluation de modèle, qui reste similaire dans l'usage. Cf exemple avec AMORAL, pratique similaire à ce que l'on fait encore actuellement en terme d'incrément. Je propose une hypothèse, je l'évalue , et voilà ...

\paragraph{La validation face à ces nouveaux outils ?}
\label{p:validation_nouveaux_outils}

Dans la continuité de ce qui a été dit dans le chapitre 1, la famille de questions relative à la validation des modèles de simulation se posent très tôt au début des années 1970, bien avant que n'arrive l'usage du méta-formalisme Agent dans les sciences sociales.

Plus que l'évolution des techniques, dont il est toujours temps de mesurer l'influence dans un deuxième temps, il est important de soulever avec quelle constance un certaines nombre de problématiques propre à la validation traversent les années. 

La question des protocoles pour la construction de modèles de simulation est il me semble un bon exemple. Dès premières évocations dans les années 1970 jusqu'à 2008, la recette n'a guère vu que sa forme évolué.



\hl{Reprendre partie écrite dans une précédente version, et l'adapté à la problématique de ce paragraphe}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% EN COURS DINTEGRATION + LIAISON AVEC POM ? 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

Heureusement, les capacités de calcul informatique et les méthodes de modélisation multi-échelles des informations spatio-temporelles se sont bien développées, en parallèle et en réponse à ces nouveaux défis. Parmi les sciences humaines et sociales, les géographes sont aujourd'hui particulièrement présents dans l’expérimentation et la mise au point méthodologique, qu’il s’agisse des les appliquer à des données environnementales (Kanevski, Delahaye, Douvinet) ou à des problèmes de géographie urbaine (Batty, Banos, …Ruas) ou régionale (Wilson, Dauphiné, White, Engelen, Daudé) ou encore à l’analyse de réseaux (Rozenblat). 

Les systèmes complexes sont bien entendu aujourd'hui au cœur des interrogations en sciences humaines et sociales, dans la mesure où il est connu depuis très longtemps dans ces disciplines que les interactions en jeu ne sont pas linéaires \todo{ (Morin, 200 ) }  (Morin, 200 ), que des individus engagés dans des processus collectifs sont susceptibles de donner lieu à des phénomènes originaux à l’échelon macroscopique (Schelling, 1978) \todo{ (Schelling, 1978) }, et que les situations initiales ou les effets de contexte sont parfois aussi déterminants que les modalités qui règlent les échanges entre les individus, suscitant des évolutions où la \foreignquote{english}{path dependence} \todo{ (Arthur, 1994) } (Arthur, 1994) ou \enquote{enchaînement historique} joue un rôle très important, en limitant les expressions des dynamiques possibles.

Suivant une remarque emprunté à Gould \autocite[31]{Gould2004} \enquote{[...] il faut bien se rendre compte que la formulation de questions et les avancées méthodologiques sont les deux faces d'une meme piece}. Banos \autocite{Banos2013} compte jusqu'à quatre étapes majeures ou progression technologique rime avec évolution et remise en question des questionnements théoriques. Ainsi, tout comme la systémique et la formalisation mathématique avait permis la découverte des trajectoire complexes mais cohérentes des objets géographiques, soumises non plus à des rencontres locales mais à des contingence génératrice de bifurcations dans la dynamique des villes \autocite[137]{Pumain2002}, l'apport de la modélisation multi-agent des années 1990 pousse elle encore un peu plus loin la transformation. \autocite{Sanders2007}

%Avec l'apparition des modèles agents comm en géographies les nouvelle approches ouvrent de nouvelles voies à la pensée et au questionnement littéralement impensables auparavent, on peut citer par exemple la programmation déjà linéaire, qui souleve (cf Leslie Curry, aussi cité ailleur par Hagget je crois) + introduction de Gould1970 "The intellectual revolution in geography since the middle and late fifties rests upon two main supporting pillars" 

Les modèles d’agents employés en géographie ont la particularité d’avoir été développés non seulement pour des simulations d’agents individuels, par exemple pour simuler la diffusion d’épidémies dans un territoire urbain ou régional ou sur un réseau (Banos SRAS, Badariotti, Laperrière, Eubank…) ou la diffusion d’innovations (Daudé), la propagation d’incendies (Langlois), ou encore pour éclairer les choix résidentiels (Bonnefoy…) mais ont démontré l’intérêt de modélisations fondées sur des entités plus vastes (villes, régions, voire états du monde) pour examiner les possibilités d’émergence à partir de leurs interactions à des niveaux géographiques plus larges. 

%Le laboratoire Géographie-cités a été pionnier dans l’application à la géographie des systèmes multi-agents, notamment avec la série des modèles Simpop réalisés en collaboration avec des informaticiens (Bura et al., 1996, Sanders et al., 2007, Pumain, 2012) mais aussi avec des modèles d’agents développés directement avec des logiciels simplifiés comme Netlogo (Banos…). L’évolution des modèles ne se limite pas à l’emploi de logiciels mais participe plus généralement des réflexions de la communauté scientifique intéressée par les systèmes complexes. Le GDRE S4 (Simulation Spatiale pour les Sciences Sociales) a par exemple inscrit son action parmi les premiers projets de l’Institut des Systèmes Complexes de Paris-Ile-de-France (ISC-PIF) fondé par Paul Bourgine \todo{ (ref) }. L’UMR  Géographie-cités est devenue en 2011 l’antenne  de l’Institut des systèmes complexes à l’Université Paris I et acquiert du matériel pour sa propre autonomie de fonctionnement.

\Anotecontent{grimqualite}{On soulignera à ce titre la qualité de l'ouvrage de Railsback et Grimm \autocite{Railsback2012}, les deux auteurs prenant le temps de développer ces aspects en détail avec des exemples}

\hl{footnote a compléter}


A décharge de l'auteur, une des difficultés, qui sera détaillé dans la section \ref{sec:validation}, est probablement la nature tout à fait contextuelle de cette étape. Pour reprendre les mot de \textcite{Amblard2006} \enquote{[...] les questions pour  la validation des modèles ne devraient jamais être abordées en dehors des questions relatives à leurs usages.}

Il ne serait alors pas outrancier de de tenter ici une comparaison entre la désillusion vécu courant des années 1970, et la situation tel qu'elle apparait en 2014. Car sur ce point, un regard sur l'histoire passée de la simulation dans les sciences sociales n'est pas fait pour nous rassurer, la plupart des problèmes cités comme facteurs limitants dès les années 1970 recoupent encore aujourd'hui tout ou partie de nos problèmes actuels. On pourra arguer que certaines problématiques, comme les problématiques techniques d'accès aux ordinateurs, ou aux programmes ont quasiments disparu. Certes oui, mais de nouvelles problématiques, de même nature, sont apparus, et s'avère tout autant limitantes dans le but poursuivi. La problématique de l'accès et de l'utilisation d'une puissance de calcul nécessaire et suffisantes pour l'exploration des modèles n'est ainsi qu'une translation à un niveau supérieurs de problématiques plus anciennes...

%En forte interaction, ceux ci peuvent être rapportés à au moins trois dimensions explicatives, une dimension technique, une dimension méthodologique et une dimension institutionnelle, ce qui peut être explique pourquoi ceux ci n'ont jamais pu être totalement résolus dans le cadre d'une seule politique, d'une seule projet, ou d'une seule publication : faible nombre de modèles publiés et reproductibles, absence de publication décrivant des protocoles d'évaluation de modèles et des mises en application de ces protocoles, difficulté d'accès à l'information et à la ressource technique nécessaire pour l'exploration des modèles, stratégie de publication misant sur la publication de modèles déjà finalisés mais jamais appliqués de nouveau, manque de formations adaptées ou dédiées, confrontation avec des courants disciplinaires \textit{mainstream} ignorant l'activité modélisante, etc.

%La plupart du temps, ces papiers méthodologique pointent des arguments faisant état des limitations techniques, mais il apparaît de façon assez nette que la problématique de la validation des simulations et la lutte régulière pour reconnaître le caractère « scientifique » de la simulation et son intérêt pour dégager de la connaissance semble être un argument transerval dans les différentes disciplines et dans le temps.

%En reprenant notre argumentaire pour y exposer non plus le modèle non plus dans sa dimension uniquement technique, mais dans sa dimension de construit, de nouveaux objet d'étude apparaissent.

Ce qui semble être décrit comme une immaturité dans la méthode Agents \autocite{Heath2009} se rapporte en réalité à des pratiques qui dépasse largement l'utilisation spécifique de cette technique spécifique. La simulation agents en apportant cette nouvelle et importante flexibilité dans l'expression des problématiques de recherche, a pu certainement provoquer un effet de mode menant à des dérives, un état de fait qui ne nous fera pas oublier l'enracinement d'une bonne partie des critiques qui lui sont adressés, à tort ou à raison.

Compte tenu de cette persistance des problématiques dans le temps, au delà de l'évolution des techniques, il est fait ici un choix radical. En objectivant cette \enquote{démarche scientifique de construction de modèle} pour en faire un objet d'étude à part entière, et en laissant dans un premier temps de coté les supports techniques (modèles équationels, modèles agents, etc.), il est proposé dans cette étude de se concentrer sur la problématique de validation/vérification des modèles, une étape que l'on juge déterminante au cœur de la construction du modèle. 


% glissement vers ABM?

%Ces problématiques sont depuis lors des incontournables pour qui veut entamer une discussion sur la \enquote{validation}, la sous-détermination des Théorie par les données de Quine, ou l'équifinalité de Bertalanffy étant des contraintes indépassables avec lesquels notre raisonnement doit s'adapter pour constuire des modèles explicatifs \enquote{malgré tout}. 



Mais ce débat n'est qu'un premier filtre, une autre épistémologie vient se greffer sur la notion de \enquote{laboratoire virtuel} et questionne cette fois ci, non plus tant le résultat obtenu via l'expérimentation, que la condition de sa transférabilité au réel.

Les décennies qui vont suivre vont voir s'affronter plusieurs débats sur la validation, tant sur le point philosophique que méthodologique.

%La section \ref{ssec:crise_mutation} traitait du renouveau de la simulation de modèle en énumérant simplement les évolutions techniques et méthodologiques sans forcément pointer les débats qui accompagne ces transformations et qui touche à la problématique de la \enquote{vérification}, une problématique qui sera plus développé dans cette partie de la thèse.

Il sera donc important de revenir sur les changements opérés depuis les années 1970 et la prise de conscience de l'équifinalité et des problématiques de validation qu'elle entraîne dans l'évaluation des modèles sur les systèmes sociaux.

%, et nous verrons que si il existe effectivement une différence dans la validation entre science sociale et science naturelle, celle ci n'empeche en rien l'emploi de techniques quantitatives sont neutre politiquement et peuvent être employé dans un but tout à fait non-positiviste pour l'explication !

%ce n'est pas parce que Hagget (voir citation de gregory p.) pense que la prévision peut être un moteur, voir un idéal a atteindre dans la construction des modèles en géographie qu'il renie les briques d'explication mobilisé dans ce but !)

%Hors, à ne percevoir la modélisation que par le prisme de son application politique comme le fait Gregory, on 

%et affiche quand à l'opérationalisation des notions de systèmes un septicisme dont l'argumentation s'avère avec le recul très peu convaincante, la biologie tout comme la physique qui ont vu ces dernières années la remise en cause de théories soit disant universelles, et la multiplication de modèles concurents candidats à l'explication des mêmes loi phénomènales. Le paradigme systémique ou la théorie de la complexité ayant fini de faire tomber de son piédestal la biologie et même la physique, qui peuvent aujourd'hui être confronté à des problématiques de validation certes de nature différente (les objets ne sont pas les même, encore que dans le cadre des simulation le substrat des modèles est en partie partagé), mais dont la complexité d'évaluation s'avère similaire.

%qui a le mérite de réaffirmer dans sa relecture des préceptes (simulé!) d'Hagerstrand de nouveaux moyens pour l'explication, parfois tout à fait compatible avec une vision quantitative (la théorie de la structuration dont la version spatialisé proposé par Giddens est tout à la fois mobilisé par Gregory que Pumain, dont l'expression opérationelle ne peut qu'être que très différente) mais qui peine une fois la vague marxiste évaporé dans certains pays, comme la France, à s'établir comme une véritable alternative. 

%difficilement se passer des nouveaux outils quantitatifs, dont fait partie la simulation. A ce titre, il me semble que le développement de la systémique, se place dans une continuité de réflexion sur les objets géographiques qui amène dans cette phase de transformation critique de la discipline à dérouler des modèles capable eux aussi de rendre compte de façon complémentaire de cette complexité des interactions entre l'homme et son milieu.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 







%\hl{=> Transition apport de Varenne par rapport à tout ce bazar}

%A faire remonter ? 


%\hl{ Varenne, la simulation comme expérience de second genre, la possibilité d'un rapport à l'empirie ... (a voir si je rentre la dedans maintenant ou si je garde ça pour plus tard dans la partie construction de modèle de simulation}

%Q : Peut on se contenter d'une lecture statique de la validation ? 

%Doran et cie en UK reprennent le flambeau
%En france : Amblard , Phan, et cie


% PROBLEMATIQUE validation SIMILAIRE FINALEMENT, 
% Cf permanence des problématique évoqué par Doran1999, Doran2000


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

Lors de l'implémentation une deuxième vague de paramètres est à prendre en compte lors des expérimentations. En effet lorsque vient la nécessité d'implémenter les hypothèses dans un programme informatique, le modélisateur doit faire face à la fois aux ambiguités du langage naturel, mais doit également prendre en compte des externalités qui interfere avec l'image du modèle conceptuel.

Les informaticiens sont habitués à cette exercice de décomposition, recomposition qui permet de passer d'une description très abstraite, à une implémentation fonctionelle, par exemple dans la manipulation de base de données. Seulement ici, ce n'est pas tant la qualité fonctionelle qui guide l'implémentation du modèle mais bien la qualité de similarité avec le modèle conceptuel. L'implémentation . C'est cette fameuse étape de vérification qui est censé nous garantir que le modèle dit bien ce que l'on veut qu'il dise.

Toute nouvelle variable exogène à ce modèle conceptuel rentre donc soit dans la catégorie des omissions involontaire, jalon naturel d'une bonne progression dans un raisonnement scientifique, soit dans la catégorie des paramètre a-thématique informatique, dont on ne peut s'empecher de penser qu'ils interferent et dégradent forcément ce bel édifice scientifique en cours de construction.


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\begin{framewithtitle}[bla]{bla}

Pour le fourrage, il existe trois fonctions objectifs. Elle se base chacune sur le temps écoulé entre le début de la simulation $t_0$ et le temps indiqué pour la disparition complète d'un tas de nourriture $tn$.

\begin{enumerate}
\item $f_1$ stocke le pas de temps à laquel le tas 1 disparait.
\item $f_2$ stocke le pas de temps à laquel le tas 2 disparait.
\item $f_3$ stocke le pas de temps à laquel le tas 3 disparait.
\end{enumerate}

Compte tenu de la configuration des différents tas de nourriture, il correspond un ensemble de jeu de valeurs de paramètres permettant de résoudre.

antnumbers.png

\end{framewithtitle}


\subsection{L'évolution contrainte de la ressource informatique, seule verrou historique limitant le développement de la méthode simulation ?}
\label{sec:verrou_informatique}

L'amélioration de la formation des géographes européens, et notamment des géographes français dans les années 1970, permet à ceux-ci d'intégrer plus facilement les évolutions de l'informatique durant les années 1970-1990, leur garantissant ainsi une certaine autonomie de développement qui va donner lieu à plusieurs collaborations fructueuses avec les physiciens \autocite{Pumain1984}; 

Concernant l'accès à la ressource informatique pour construire et explorer les modèles de simulation, même si les conditions se sont améliorées avec la démocratisation de l'ordinateur, celle-ci reste un élément bloquant pour l'exécution et l'exploration des modèles de simulation.

Tout d'abord, il y a ce témoignage\footnote{Ce témoignage est issu d'un échange par mail en 2013} précieux de Duane Marble, un des pionniers modélisateurs américains, qui lorsqu'il est interrogé sur la suite des problématiques de validation des modèles de simulation détaillées dans son article de 1972 \autocite{Marble1972}, conforte d'une certain façon notre point de vue : \foreignquote{english}{As I recall, the situation in the 1980's had not changed very much. Simulation in human geography did not last long. Much of this was the result of a lack of computer capacity. Simply replicating Hagerstrand's diffusion model proved difficult and our attempt to inject a more explicit temporal element just would not work due to the computational load.}

Malgré les apports heuristiques indéniables qui vont avec l'utilisation de l'outil, on retrouve l'expression de difficultés concernant le calibrage des modèles plus complexes chez de nombreux auteurs pionniers modélisateurs \autocites{Batty1976, Pumain1998a}[400]{Sanders1984}, notamment pour ce qui concerne le calibrage des modèles, souvent difficile pour ces modèles dynamiques non linéaires soumis à de tels fluctuations dans leur comportements. Voici comment \autocite{Pumain1998a} résume les difficultés opérationnelles résultats de plusieurs années de travaux menés autour des modèles de simulation dynamiques non-linéaires opérant dans le cadre de la théorie de l'auto-organisation : \enquote{Les difficultés de calibrage, associées à la capacité élevée de bifurcation des modèles, ont été maintes fois décrites, de même que l’impossibilité de valider comme \enquote{meilleur ajustement} une configuration donnée de paramètres.}

Batty est probablement un des premiers géographes à faire ce travail d'état de l'art des techniques de calibrations disponibles et applicables à cette nouvelle classe de modèles urbains. Des méthodes de calibration basées sur des méta-heuristiques de type descente de gradient, ou \textit{hill-climbing} sont déjà utilisées par les géographes comme \textcite[159-160]{Batty1976} pour résoudre des problèmes d'optimisations utilisant les sorties de modèles. Toutefois ces méthodes sont encore trop souvent limitées à des modèles à 1 ou 2 paramètres, s'avèrent peu robustes face à des problèmes acceptant des minima locaux, et se limite à l'optimisation de fonctions uniquements unimodales. 

Une chose est sûre pour ce qui est de la recherche de paramètres, on perçoit très tôt chez certains géographes la nécessité d'optimiser cette étape, rendue improductive et dangereuse du fait de la non-linéarité des modèles \enquote{The trial and error method of searching for best-parameter values by running the model exhaustively through a range of parameter values or combinations thereof represents a somewhat blunt approach to model calibration. [...] Moreoever, as each run of the model can be expensive or take a large amount of time, few applications have attempted to find systematically the optimum parameter values [...] Clearly, then, there is a need for the introduction of methodes suitable for calibrating intrinsically non-linear models of spatial interaction.} \autocite[155]{Batty1976}

L'appel à l'utilisation de nouvelles méthodes pour l'exploration des modèles déjà lancés par \textcite{Batty1976}, est par la suite repris de façon implicite par \textcite{Openshaw1996}. Celui-ci publie en 1996 avec un collègue de Leeds un article sur les algorithmes génétiques, la méthode la plus efficace disponible alors pour explorer des espaces de paramètres de façon efficace. La conclusion est explicite :

\foreignquote{english}{The results demonstrate that even GA en ES can provide very good solutions for spatial interaction model calibration, albeit sometimes at the expense of considerable extra compute times. [...] It would also be worth considering the use of other forms of global optimization method; [....] As computer hardware becomes faster, the attraction of simple, relatively assumption-free, and highly robust approaches to global parameter estimation can only grow and allow the geographical model builder to worry less about the problems of parameter estimation and focus more on the task of model design.}\autocite{Openshaw1996}

La course à la puissance informatique nécessaire pour explorer et calibrer les modèles ne fait en réalité que commencer. Les méthodes sont encore en cours de développement, et leur usage s'avère extrêmement coûteux sur le plan informatique.

Se pose alors la question suivante, l'incapacité à calibrer un modèle de simulation n'est-elle pas un problème qui limite de facto l'évolution en crédibilité de l'outil simulation ? 

Concernant ce problème plus large de la validation, dont le calibrage n'est qu'une facette, le changement de paradigme explicatif et l'ouverture sur la complexité a soulevé un débat qui dépasse en réalité la seule problématique technique. Il ne suffit plus de garantir un résultat pour que le modèle soit considéré comme valide, sa structure causale est elle aussi considérée comme le résultat d'un processus social, et dont la contenance doit normalement être validée terme à terme avec le domaine empirique; or c'est celle-là même qu'on ne peut observer dans le cadre d'un système complexe. (cf \textit{observational dilemna} de \textcite[296]{Batty1976} :

\foreignquote{english}{Perhaps the major problem concerns the ability to observe or monitor the urban system. Unlike the physical sciences in which the effect of critical variables on the system of interest can be isolated in the laboratory, such a search for cause and effect is practically impossible in social systems. Thus, there are many instances when it is difficult, if not impossible, to disentangle one cause from another in the changing behaviour of such systems. This is a fundamental limitation which is referred to here as the observational dilemma.}

La dernière phrase d'Openshaw prend alors tout son sens, et en nous rappelant que la construction de modèle est un processus incrémental, il fait indirectement écho à l'évaluation elle aussi incrémentale d'une structure causale où chaque mécanisme lorsqu'il est ajouté/enlevé, remet en cause l'exploration précédente. Dès lors, la systématisation de cette calibration devient \textbf{le seul moyen de garantir une construction} qui serait faite en tout connaissance de cause, en mesurant, et donc en discutant l'apport de chacune des hypothèses durant le processus de calibration.

La dépendance à la ressource informatique se renforce en réalité encore un peu plus avec la nécessité d'explorer les modèles, non plus lorsqu'ils sont terminés, mais dès que la première brique est posée. 

\hl{Introduire la section suivante}

%%FIXME : A MODIFIER POUR COLLER AVEC LE PARAGRAPHE PRÉCÉDENT %%
%Paradoxalement il donne aussi à voir les limites des approches proposées pour létude de l'homme dans son environnement, et offre ainsi le matériel idéal pour appuyer la formulation critique des géographes radicaux marxistes, un mouvement qui s'amplifie dès le début des années 1970 en parallèle avec la conjoncture politique nationale et mondiale. \autocite{Golledge2006}


%Ainsi les progrès fulgurants de l'informatique, l'apparition de nouveaux langages exclusivement orientés pour la simulation comme Dynamo, la prise de conscience tout au long des années 1960-70 des défauts de cette première génération de modèles, et les changements d'objectifs de la discipline \autocite[12]{Batty1994} \autocite{Boyce1988} autorisent (voire recommandent) la formation de nouveaux modèles. Ceux-ci sont conçus comme plus parcimonieux, autorisant les démonstrations plus abstraites \autocite{Forrester1969}, plus orientées vers la compréhension des mécanismes à l’œuvre que sur la prédiction (un retour sur les modèles théoriques est opéré), intégrant plus facilement l'hétérogénéité dans la nature des dynamiques (rétro-action, non linéarité) des processus \autocite{Forrester1969, Wilson1970, Allen1978}, et ouvert à l'intégration d'autres dimensions explicatives à l'oeuvre dans la formation des processus, comme ceux déjà explorés l'individu et le temps \autocite{Hagerstrand1967a,Orcutt1957,Forrester1961}. En lisant les articles de Pred, d'Olsson \autocite{Olsson1969,Olsson1970}, de Curry, on percoit chez les nouveaux économistes spatiaux cette volonté de changement, avec la reintroduction de la stochasticité et des modèles probabilistes, l'intégration du temps dans les modèles, mais aussi les causalités multiples.

% PLUSIEURS points développement méthodologiques accompagnant renouvellement théoriques accompagnant nouvelle géographie : Hagerstrand , Orcutt -> causalité + individualisme méthodologique,  Forrester -> complexité
% Hagerstrand premiere utilisation montecarlo en science sociale, vient a Washington et rencontre Morril... qui pour Benko Stromayer marque troisieme theme dominant le bouleversement quantitatif) Gould2004

% simulation permet de développer cette causalité ...
% Systeme dynamique, non linéarité, permet avancée fondamentale dans les questionnements, révélateur aussi de l'apport des techniques / méthodologies...
% Basculement vers explicatif !


% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % 


Activité d'exploration semble être le pivot central par lequel évolue notre outillage, et notre activité de modélisation.

1 - Les centaines d'heures accumulés pendant toute la durée du projet avec Romain et Mathieu autour de la construction de workflows pour le modèle permettent de tester, debugger, améliorer la syntaxe utilisé pour les workflow, mais aussi de tester la robustesse du logiciels sur un cas d'utilisation concret sur temps long.

2 - Les différentes campagnes d'explorations menées vont avoir des retombées sur la construction MGO, et le couplage entre entre openMOLE et MGO, ce qui en retour améliore les explorations.

	a) Intégration et Amélioration des algorithmes génétiques pour openMOLE
		- EA Steady State, Ilots permettent de mieux utiliser le parallélisme des méta-heuristique et donc de rendre plus efficiente les explorations appuyés sur cette utilisation (Aout 2012) 

	b) Simplification des Workflows pour l'exploration utilisant des meta-heuristique (à partir de Mars 2012)

3 - Boucle reliant l'activité d'exploration et l'activité de construction des modèles. 

	a ) L'exploration des résultats a posteriori s'avère incertaine et trop couteuse (durée d'execution, volume données), ce qui nous renvoie à un questionnement a priori des modèles en utilisant des méta-heuristiques ( Exploration <-> Modèlisation )
		- prototype de calibration par inversion \autocite{Schmitt2015}

	b) L'exploration permet d'avoir un retour sur les hypothèses et la façon dont elles sont implémentées ( Exploration > Modèlisation ), mais la construction modifie aussi la façon dont on va penser l'exploration
		- construction par incrément EBIMM
		

4 - Boucle liant l'activité de construction MGO, Activité de construction du modèle, Exploration des modèles

	a) Amène le développement ou l'amélioration de méta-heuristique pour l'exploration, dont l'application impacte en retour le développement des modèle (Exploration -> Mgo -> Modèle )
	- Hypervolume / SMSMOEA (Juillet 2012) \autocite{Schmitt2015} 
	- Cp-Profile \autocite{Reuillon2015} 
	- PSE \autocite{Cherel2015}
	- Epsilon Dominance (Decembre 2012)
	- Re-Evaluation de l'archive (Mars 2013)

	b) Amène l'amélioration MGO et l'utilisation de SimPuzzle pour construire et d'explorer des familles de modèles
		- Création de SimFamilly (mai 2014)

	c) Les modèles peuvent être amené à être réécrit pour satisfaire des raisons de performance, ou de modularité
		- Réécriture SimpopLocal (mars-avril 2012)
		- Intégration SimpopLocal dans SimPuzzle (Avril 2013)
		- Intégration Marius dans Simpuzzle (Mai 2013)

	d) Les modèles sont modifiés pour être utilisé par les métaheuristiques, et celle-ci mettent à l'epreuve en retour l'implémentation du modèle vis à vis des objectifs fixés. Ce processus intervient dans la validation interne des modèles en soulevant les erreurs, les comportement contre-intuitif, les zones de valeurs de paramètres bloquant, ou les conditions d'arrêt trop ou pas assez stricte pour l'exploration. L'introduction d'objectifs sous forme de critère multiples pour évaluer les sorties de la simulation de façon automatique participe également de la formalisation et de la construction des connaissances, entrainant souvent pour les modélisateurs un retour sur l'empirie.
		- SimpopLocal \autocites{Schmitt2014,Schmitt2015}
		- Marius \autocite{Cottineau2014b}

5 - Boucle liant OpenMole et Mgo, le framework est repensé pour être plus beaucoup modulaire, et donc plus facilement utilisable sous forme de tâche dans openMOLE. 
	- Mai 2011, redeveloppement MGO

6 - Construction d'outils d'AD intégré à openMOLE pour la visualisation de sorties d'exploration (Janvier - Mai 2013)

7 - Construction d'outils plus d'AD pour l'exploration de sortie de simulation de la famille SIMPOP 
	- Varius (début 2015)
	- TrajPOP (septembre 2011)


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\paragraph{Persistance des guides méthodologiques et absence d'outils}


 et forme des évocations sur le problème de la validation


\foreignquote{english}{This time we believe Hume would divorce from Epstein: whereas for the Santa Fe scientist growing is finding the sufficient conditions for a given explanandum to occur, for the British philosopher remote causes are not explanations until all the intermediate factors from remote causes to the effects are reconstructed.} \autocite{Conte2007}

\foreignquote{english}{In any event, the first point is that the motto is a criterion for explanatory candidacy. There may be multiple candidates and, as in any other science, selection among them will involve further considerations.} \textcite{Epstein2006}


Validation interne / externe F. Amblard ? 


aussi les critiques de \textcite{Conte2007} vis à vis de cette phrase si souvent partagé

Cette pratique d'une épistémologie \textit{bottom-up} venant des modélisateurs, et parfois des épistémologues assumant volontier cette démarche \autocite{Varenne2014}, même si elle peut apparaitre décalé face aux pratiques, permet toutefois de se défendre en cas d'attaque.

C'est aussi là d'ailleurs que le travail de Varenne réalisé au cours des années 2000 \autocites{Varenne2008, Varenne2013} apparait assez audacieux, en proposant une typologie de fonctions épistémiques flexible et cumulable, il propose une grille de lecture permettant d'intégrer à la fois la diversité des approches dans les disciplines (inter) mais également l'évolution de ces même approches à l'intérieur d'une discipline (intra). Un découplage qui permet une définition plus fine des rapports que peuvent entretenir chacune des disciplines entre le modèle et la simulation.

Une erreur dans laquelle est malheureusement tombé \textcite{Yanoff2008}, qui a jugé ce travail sans être au courant des codes, des discussions en cours à l'oeuvre dans cette communauté. Or d'une part il existe une méthodologie implicte à la construction de tel modèles, et d'autres part si on veut parler de la validation en SHS, on est obliger de traiter la question du collectif de part le rôle important qu'il joue cette fois-ci dans la Validation, non pas dans l'absolu et détaché de tout contexte, mais par les pairs en SHS \autocite{Rouchier2013, Ahrweiler2005}. \hl{on en parle aussi plus loin ! } \autocite{}



Une des force des \enquote{mécanismes générateurs}, c'est qu'il introduisent de façon beaucoup plus détaillé ce rapport liant hypothèses du modèle et données. Un travail que l'on retrouve de façon équivalente chez les géographes.

% A deplacer plutot dans la partie .
Le challenge véritable ce n'est pas tant la Validation et la Verification, de toute façon insoluble, mais la façon explicite et positive dont on va gérer l'équifinalité et la sous-détermination qui caractérise les modèles de simulations en sciences humaines et sociales.


Si Chattoe reconnait bien que l'existence d'une sortie unique peut effectivement être un problème, l'attaque mené par Grüne-Yanoff, qui suppose l'absence de méthode ( et c'est là surement le plus grand tort de ce modèle, de ne pas avoir assez mis en avant leur méthodologie de construction) ne peux en aucun cas être généralisé à l'ensemble des autres modèles agents en SHS. 

It is good, in scientific terms, that those \enquote{outside} a particular field should be free to criticise it. However, it is reasonable that potential critics should know enough about what they are criticising to make relevant objections.

As well as disregarding the social science context within which AS research occurs,
Grüne-Yanoff also fails to acknowledge that AS already has an explanatory methodology laid out succinctly in the standard textbook for the field (Gilbert and Troitzsch 2005, 15-18). What distinguishes AS from statistics, in particular, is that a model is not simply \enquote{fitted} to data but that the individual level assumptions that generate the data to be compared with  \enquote{reality} are  \enquote{more or less} validated independently.

This is what researchers were trying to do in the Anasazi study (Grüne-Yanoff, 544) when they drew on behavioural data from “comparable” traditional studies (in the absence of the Anasazi themselves) and although Grüne-Yanoff discusses this as a weakness, he does not give it credit as part of a recognised methodology. Of course, as Grüne-Yanoff points out, in the context of an extinct society, this independent validation may itself be problematic (we do not know how to choose modern societies which are “like” the Anasazi because the whole point is that we don’t know enough about the Anasazi) but that remains a point against the example and not the methodology per se.

Cette équifinalité, Epstein n'a jamais voulu la cacher

seule n'intervient qu'en partie dans cette question, car il s'intègre, au titre de nombreux autres outils, dans une démarche méthodologique plus générale et englobante \autocite{}. Aborder la validation du seul point de vue de l'outil est à mon sens une erreur, car comme l'ont déjà noté Hélène Mathian et Léna Sanders \autocite{Mathian2014}, les géographes concoivent la démarche, ou plutot les démarches de construction de connaissance sur les objets spatiaux en mobilisant non pas un, mais des modèles parmis un ensembles à leur dispositions. Tous sont construction, et donc tous sont faux à des degrés divers. La question de la validation prise hors de son contexte thématique n'a pas de sens dans un cadre pratique de construction de connaissances, mais tout cela à déjà été dit, et c'est pour cela que nous préférons dans la pratique nous concentrer en tant que modélisateur, sur l'évaluation des modèles de simulations, plutot que sur leur validation. \autocite{Amblard2006}



La modélisation comme processus abductif renvoie en alternance aux données et aux modèle de données, qui elle même renvoie aux hypothèses et aux implémentations de ces hypothèses, aux indicateurs et à la façon dont on les a construit, etc. A l'image des autres outils, il peut être mobilisé de façon hypothético-déductive, ou de façon 

Les hypothèses mobilisés existe sur un autre temporalités, en dehors de cette sphère de la validation par les modèles.

Pour Openshaw, 

, une preuve qui vient s'ajouter à celle déjà évoqué dans le chapitre 1 pour justifier de l'enracinnement de cette problématique dans l'histoire de la simulation.


cette critique récurrente de l'outil sur le plan de la scientificité, une faiblesse qui constitue toujours un danger pour la pérénité des pratiques inter-disciplinaire autour de la simulation agents \autocite[220]{Squazzoni2010}, a tel point que la communauté se dote de guides de survie pour se protéger des sceptiques. \autocite{Waldherr2013} Est ce là le seul fait d'une mauvaise communication autour de notre discipline comme le laisserait penser la lecture de ces travaux ? 





L'établissement de guide ne suffit plus.


Ne pas oublier la partie technique


Cette seule connaissances des dynamiques à l'oeuvre dans les modèles ne fait pas tout, mais elle offre déjà une base de discussion marqué par l'honneteté de cette démarche.

A l'image de certain auteurs, on pourra reprocher l'absence d'un protocole plus standard encadrant cette problématique. 

Cette discussion questionne aussi l'absence d'un protocole, d'un standard pour l'évaluation des modèles. %est cité comme un des nombreux écueils avec lequel se bat toujours la discipline, comme en témoigne les discussions réccurentes de nombreux auteurs sur un sujet dont la complexité touche à une dimension technique, que méthodologique ou philosophique. 

Autant d'auteurs \autocite{Richiardi2006} \autocite[198]{Fagiolo2007} \autocite{Moss2008} \autocite{Windrum2007} \autocite{Barlas1996} \autocite{Amblard2003} \autocite{OSullivan2004} \autocite{Doran2000} \autocite{Crooks2012} \autocite{Rouchier2013} dont il faudra par la suite développer les discussions.  


%validation des modèles de simulation agents, on se rend compte qu'un certain nombre de problématiques persistent et limitent toujours la diffusion des modèles en dehors du cercle bienveillant de l'inter-disciplinarité \autocite{Richiardi2006}. Ce problème, loin d'être un isolat touchant uniquement les sciences humaines et sociales, existe également dans d'autres disciplines, comme en écologie, où même lorsque les modèles sont publiés, l'absence de protocole pour répliquer, évaluer le modèle est courant \autocite{Grimm1999}. \hl {ref à vérifier}

A : Les approches appliqués les plus sérieuse nous paraissent celle de Grimm, et celle de Behavior search, elles seront commenté en conclusion.

= Les géographes interviennent dans cette communautée, exprime forme de détachement

A vouloir mettre en évidence un protocole de validation générique axé autour du seul modèle de simulation, on applanit sans le vouloir une démarche de construction des connaissances mobilisant un ensemble de modèle et de méthode. Il est important de rapeller sans cesse le poids de cet héritage disciplinaire à l'interface de questionnement plus génériques, comme le font régulièrement dans notre discipline Hélène Mathian, Léna Sanders, et plus récemment Cottineau2014b.

Les géographes modélisateurs, même si ils interviennent également au contact de cette communauté, continuent pour certains à développer une approche ou le SMA n'est qu'un outil parmis d'autres hérités d'une longue histoire de la géographie avec la simulation. La question de la validation prise sous la forme de \enquote{construction de connaissances} s'inscrit dans une démarche beaucoup plus globale ou interviennent des multiplicité de modèles et de méthodes.

En ce sens l'équifinalité n'est pas un problème.

= Geographes \textbf{centré sur la démarche} et non sur l'outil

Pour raccrocher cette remarque avec les dernières analyses portant sur l'usage des techniques de simulation en géographie, les observateurs (Varenne) tout autant que les acteurs \autocite{Sanders2013}\Anote{sanders_couplage_spirale} de ces pratiques tendent à mettre en avant une tendance croissante à la pluri-formalisation croissante des modèles agents, preuve que cette multiplicité des formalismes et des techniques est de plus en plus considérée comme une richesse dans l'approche de systèmes complexes.

Dans son analyse sur la place de l'explication en analyse spatiale, \textcite{Sanders2000} voit plus dans le rapport de l'outil à la démarche adoptée (exploratoire ou hypothético-deductif) une question d'interprétation, ce qui contextualise encore un peu plus la place de l'outil \enquote{Ce sont en effet la manière dont l'outil est inséré dans une chaîne de réflexion et de traitement et l'interprétation des informations figurant en entrée et en sortie qui permettent de donner le statut descriptif ou explicatif d'une démarche.} Avant de conclure un peu plus loin à la fin de son analyse \enquote{[...]il n'y a pas de relation simple et fixe, ni entre les niveaux d'observation et la nature des explications, ni entre les outils de l'analyse spatiale et l'explication. La variété des approches permet de diversifier les éclairages sur les phénomènes que l'on cherche à expliquer. Nos démarches en analyse spatiale consistent ainsi davantage à \enquote{éclairer} qu'à \enquote{démontrer} et identifier des jeux de causalités bien stricts.}

Pour Maryvonne le Berre encore, \enquote{Il n’y a donc pas d’adaptation de la géographie à la technique mais recherche d’une technique adaptée à chaque objet d’étude géographique}. \textcite{Sanders2013} expose une idée assez similaire de pré-existance des concepts sur l'outils, car pour elle \enquote{La comparaison entre famille de modèles montre qu'il existe des décalages temporels entre la construction conceptuelle d'un champ et la mise au point d'outils appropriés pour la tester. Dans une certaine mesure on peut avancer que les outils \enquote{ont rattrapé} les concepts dans ce champ de recherche.}

= Reste qu'il faut quand meme un support à ce developpement

















