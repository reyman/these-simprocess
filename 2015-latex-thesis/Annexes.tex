% -*- root: These.tex -*-

\input{p5_notes}

\graphicspath{{FigureAnnexe/}}

\renewcommand{\appendixpagename}{Annexes}
\renewcommand{\appendixtocname}{Annexes}



\appendix
\nopartblankpage

\appendixpage

\pagenumbering{arabic}

\addtocontents{toc}{\cftpagenumbersoff{chapter}}

\chapter{Historique du paradigme systémique}

\section{Retour sur la fondation et les apports du \enquote{paradigme systémique} au début du XXème siècle}
\label{ssec:systemique}

De la même façon que les épistémologues des sciences comme ici Olivier Orain \autocite{Orain2001}, l'auteur ne détaillera pas ici une approche interdisciplinaire de la notion \footnote{Au sens donné par Piaget, voir note de bas de page \autocite {Orain2001}} de \enquote{système}, difficile à envisager dans un cadre global car sa diversité d'acceptation est fonction, d'une part de la rapide évolution de cette notion depuis les années 1940, et d'autre part la règle définissant l'acceptation de cette \textit{notion} dépend non seulement de la variabilité interdisciplinaire, mais aussi intradisciplinaire. Le terme \enquote{approche systémique} est proposé par \autocite{Orain2001} pour incarner cette diversité d'intégration par les disciplines des sciences sociales de la \enquote{théorie systémique} ou de la \enquote{systémique}.

La complexité d'approche caractéristique de cette notion est pour Jean Louis Lemoigne grandement liée à la reconstruction épistémologique \textit{a posteriori} de ce qu'il appelle \enquote{paradigme systémique}. Une acceptation qui paraît d'autant plus justifiée tant l'étude exhaustive de la ramification qui découle du concept est impossible, et sans rentrer dans les détails de querelles entre les différentes \enquote{chapelles}, il est acceptable de voir cette construction comme un processus de raffinement cumulatif. 

\subsection{La Cybernétique}
\label{ssubsec:cybernetic}

\subsubsection{Des outils pour penser une nouvelle causalité}

Une des branches communément admise comme fondatrice du mouvement tient dans l'organisation des conférences de Macy entre 1942 à 1953. Celles-ci sont considérées comme un des tous premiers regroupements interdisciplinaires et marquent une période de changement profond dans l'histoire des sciences en général, et particulièrement en sciences sociales. Celles-ci vont réunir pendant plusieurs années autour d'une même table des acteurs majeurs des sciences physiques et sociales pour discuter autour de régularités communément observées, avec pour idée la construction d'un savoir commun que l'on pourra alors qualifier de transdisciplinaire.

Les conférences naissent suite à la rencontre entre un mathématicien réputé au MIT N. Wiener, un neurobiologiste A. Rosenbluch, et un ingénieur électronicien J.Bigelow qui vont opérer un rapprochement entre l'homme et la machine entre 1942 et 1946 (pour rappel le premier ordinateur ENIAC est opérationel en 1946) par le biais de groupes interdisciplinaires chargés d'explorer ce \textit{no man's land} à l'interface des deux disciplines.

Plusieurs \enquote{outils} dérivent de ces premiers séminaires organisés dès 1942 à la Josiah Macy, Jr. Foundation : la notion de \enquote{boîte noire} ou système téléologique fonctionnel, et la notion de \textit{feedback} ou causalité circulaire, avec pour objectif principal l'étude de l'homéostasie introduite auparavant par les travaux pionniers du physiologiste Walter Cannon en 1926.

Si la notion d'homéostasie pour des organismes vivants apparaît pour la première fois citée par Claude Bernard 1865, celle ci est reprise et étendue par Walter Cannon en 1932 dans le livre \textit{The Wisdom of the Body} \autocite{Cannon1932} comme \enquote{ l’ensemble des processus organiques qui agissent pour maintenir l’état stationnaire de l’organisme, dans sa morphologie et dans ses conditions intérieures, en dépit de perturbations extérieures }. Ainsi dans le cadre de son application biologique, cette rétro-action permet de décrire un certain nombre de mécanisme à l'oeuvre dans une cellule en interaction avec son environnement qui tente de maintenir de façon stable dans son milieu la concentration d'éléments comme les ions, la glycémie, etc.

L'attention des discutants dans ces premiers séminaires porte donc avant tout sur l'ubiquité du concept et la pertinence de son transfert hors des systèmes biologiques. Wiener fait alors un rapprochement décisif entre les problématiques de calcul de trajectoire en balistique et des maladies nerveuses ayant pour symptôme l'ataxie. De ces discussions émergent alors un même schéma explicatif qui convenir à ces deux problématiques : la \enquote{causalité circulaire} \autocite[774]{Pouvreau2013, Rosnay1975}.

L'approche néo-béhavioriste retenue par les discutants \enquote{consiste à étudier un objet comme une \enquote{boîte noire}, par l'examen de l'extrant de l'objet [i.e tout changement produit dans son environnement] et des relations entre cet extrant et l'intrant [i.e tout événement externe qui modifie l'objet]} \autocite{Pouvreau2013}. En adoptant cette approche, le \enquote{comportement} d'une entité est perçu \enquote{comme tout changement extérieur détectable de cette entité par rapport à son environnement} , et par téléologique il faut entendre un comportement \enquote{finalisé} c'est-à-dire déterminé par un mécanisme de \enquote{rétroaction} négative. De la connaissance de ces entrants et de ces sortants, on peut en déduire qu'il existe une retro-action négative ou positive, ou \textit{feedback} permettant de décrire progressivement le système de commande de la boîte noire.

L'introduction de cette \enquote{causalité circulaire} est pour l'époque loin d'être anodine car elle remet en cause le schéma classique linéaire \enquote {cause \textrightarrow conséquence}, qui se traduit dans le temps par la relation \enquote {avant \textrightarrow après}, la cause étant irrémédiablement suivie d'une conséquence. La possibilité de causalité circulaire, positive ou négative, brise ce schéma, et ne permet plus d'isoler un ordre entre cause et conséquence, c'est le problème de \enquote{la poule et de l'oeuf}. En réintroduisant la poursuite d'un but, on injecte une autonomie, une spontanéité, une dynamique entre objets qui était jusque là absente de la causalité linéaire déterministe.

Si l'on l'applique à un système servo-mécanique, la stabilité de celui-ci suppose la capacité à anticiper et à annuler les agressions extérieures par une capacité de régulation (flexibilité) qui repose plus alors sur la dynamique des interactions que sur la structure physique en place (rigidité), un mode de fonctionnement impossible si l'on se place dans le cadre de la \enquote{pensée classique} de l'époque.

%Dans "Behavior, Purpose and Teleology", le terme téléologie est à ce titre utilisé comme un synonyme de "l'objectif controllé par la rétroaction".\footnote{wikipedia}

\subsubsection{La réintroduction du concept de \enquote{téléologie}}

Avec la mise en place d'une classification de ces comportements, et en prenant distance du concept de \enquote{causalité finale} qui lui était rattaché, il est possible de revaloriser le concept de téléologie, renouant ainsi avec la reconnaissance de l'\enquote{importance du but} qui avait disparu avec la mise au ban de ce concept. Sur ce point, \textcite[776]{Pouvreau2013} cite \textcite[23-24]{Rosenblueth1943} \enquote{[...] Puisque nous considérons la finalisation comme un concept nécessaire afin de comprendre certains modes de comportement, nous suggérons qu'une étude téléologique est utile si elle évite les problèmes de causalité et se limite à s'attacher à l'étude du but [...] Le comportement téléologique devient synonyme de comportement contrôlé par une rétroaction négative et gagne donc en précision par une connotation suffisamment restreinte.} La finalité est réintroduite via le concept de \enquote{téléologie}, mais elle est libérée de la notion de \enquote{causalité} qui lui était autrefois associée. Elle redevient l'étude des comportements associés à un but, dont l'importance ne peut plus être niée, et redevient compatible avec le concept autrefois opposé de déterminisme.\footnote{Pour donner un exemple peut-être plus parlant, l'étude en biologie des comportements oeuvrant dans la formation d'un organisme par une méthode téléologique n'empêche pas l'usage d'un cadre de pensée déterministe  correspondant à la formation d'un même organisme à partir d'un même code initial (un déterminisme largement remis en cause depuis, voir par exemple cet \href{http://www.nytimes.com/2014/01/21/science/seeing-x-chromosomes-in-a-new-light.html?ref=science&_r=0}{@article} du New York Times)}

De ces discussions, deux articles fondateurs à la fois des sciences cognitives \autocite[23]{Dupuy2000} et de la cybernétique vont être publiés : \textit{Behavior, Purpose and Teleology} ou Rosenblueth, Wiener, et Bigelow \enquote{ propose[nt] de déconstruire la distinction entre action volontaire et acte réflexe, en assimilant la volonté à un mécanisme de rétro-action (\textit{feedback})}; et \textit{A logical calculus of the ideas immanent in nervous activity} où McMulloch et Pitts donnent \enquote{une base purement neuroanatomique et neurophysiologique au jugement synthétique \textit{a priori}, et de donner ainsi une neurologie de l'esprit}.

\subsubsection{ Les limites du transfert des concepts aux sciences sociales}

\paragraph{Introduction aux sciences sociales}
Parmi les auteurs de ces premiers séminaires organisés entre 1942 et 1944 figurent deux représentants des sciences sociales, Gregory Bateson et Margaret Mead. Enthousiastes, il vont rapidement trouver dans l'étude des concepts développés dans ce premier séminaire (1942) un écho à leurs propres travaux sur la dynamique sociale, la notion d'homéostasie n'étant qu'un nouveau mot permettant de rassembler des travaux existants déjà au fait de ces phénomènes. Cette mise au jour de problématiques communes entre le biologique et le mécanique permet d'envisager la construction d'un référentiel lui aussi commun; une prise de conscience qui va amener les auteurs du cercle de réflexion initial à envisager rapidement l'élargissement de celui-ci à l'ensemble des acteurs des sciences sociales.

La suite des conférences de Macy (1946-1952) sera organisée par Arturo Rosenbluch et son ami Warren McCulloch, un autre neurobiologiste. Cette ouverture vers les sciences sociales est timide dans un premier temps, et ce n'est qu'à la deuxième conférence en octobre 1946 sur une suggestion de Lazarsfeld que les conférences concrétisent cette ouverture dans le cadre d'un sous séminaire intitulé \textit{Téléogical Mechanisms in Society}. La quatrième conférence acte cette ouverture et introduit pour la troisième fois de suite une modification de l'intitulé, avec cette fois-ci l'adjonction d'une dimension sociale à un objet d'étude, qui apparaît encore à cette date difficile à définir : \enquote{la causalité circulaire et des mécanismes de \textit{feedback} dans les systèmes biologiques et sociaux}. Le terme \textit{Cybernetics} est pour la première fois introduit dans les séminaires par Wiener en 1946. Il faut toutefois attendre 1949 et la septième conférence pour que sous l'influence d'un nouveau participant nommé H. Von Foerster, ce terme chapeaute de façon définitive les prochains intitulés de séminaires. Au final, ces dix séminaires vont participer de l'émergence de la \enquote{science cybernétique} en \enquote{permettant l'échange effectif de savoir et d'experiences, tant entre les disciplines qu'entre les sciences et la société}, réalisant par là un des objectifs annoncés par Wiener et Rosenbluch dans leur classification, faisant de la cybernétique une \enquote{[...] science générale des systèmes à comportement finalisé ayant principalement pour objet ceux dont le comportement est \enquote{téléologique} } \autocite{Pouvreau2013}.

\paragraph{Des biais mécanicistes mettent en échec ce premier transfert}

Wiener mais aussi d'autres acteurs de cette première cybernétique ont vu assez tôt tout l'intérêt que pourrait apporter l'utilisation et le transfert d'outils comme cette \enquote{boîte noire}, ou le principe de régulation par \enquote{rétro-action} une fois appliqué à l'étude des interactions dans les systèmes sociaux. Mais les difficultés d'application et les critiques ont rapidement mis à mal cet objectif transdisciplinaire, pour plusieurs raisons qui tiennent : d'une part à l'existence de restrictions mathématiques remettant en cause la scientificité des résultats obtenus : (a) les statistiques sur le long terme sont difficiles à obtenir (b) il est  difficile de minimiser la distance entre observateur et phénomènes observés, ce qui induit un biais des données; et d'autre part au réductionnisme et le biais mécaniciste touchant la vision de certains acteurs des conférences de Macy  : \enquote{[...] la vie était pensée comme un dispositif de réduction d'entropie ; les organismes et leur associations, en particulier les hommes et leurs sociétés, l'étaient comme des servomécanismes ; et le cerveau comme un ordinateur} \autocite[784]{Pouvreau2013}

\autocite[782]{Pouvreau2013} explique très bien les limitations qui font de l'extension de la cybernétique aux sciences humaines une simple \enquote{[...] ressemblance superficielle au niveau du formalisme. Ne serait-ce que parce que dans un système tel que conçu par la \enquote{première} cybernétique, par définition fermé à l'information, la téléologie ne peut qu'être confinée au cercle d'un but déterminé; et que pour cette raison, ce modèle ne permet pas de comprendre de quelle manière un système peut être amené à redéfinir ses buts à partir de ses interactions avec son environnement, la pertinence d'une téléologie relative à des buts \textit{intentionels} restant donc intacte en sciences humaines}.

\subsection{La GST ou la théorie des \enquote{systèmes ouverts}}
\label{subsec:gst}

Cette incapacité de la première cybernétique à se rapprocher des problématique des systèmes sociaux va trouver un écho plus positif dans un courant qui se développe en parallèle du mouvement cybernétique. Ce mouvement fondé par le biologiste Ludwig Von Bertalanffy en 1937 peut être considéré comme la deuxième branche venant enrichir le paradigme systémique. Tout en apportant de nouveaux concepts, celui-ci va se positionner de façon critique par rapport à la \enquote{première cybernétique} tout en englobant par la suite les autres innovations qui proviendront de ce courant, Asbhy jouant le rôle important de médiateur entre ces deux courants.\autocite{Pouvreau2013}. De cette prise de position va peu à peu découler la construction d'une théorie établissant une méthodologie logico-mathématique à vocation unifiante, accessible à n'importe quel champ disciplinaire pour décrire des lois de structures similaire (isomorphes) \autocite{LeMoigne2006a}.

Ainsi rapporté par LeMoigne en 1977, cette \enquote{vision stupéfiante est celle d'une une théorie générale de l'univers, du système universel} \autocite[59]{Lemoigne1977}. Le mot \enquote{Vision} est ici quasi synonyme de \enquote{Révélation}, car elle amène à voir une tout autre approche du réel pour qui s'en rapporte. Ainsi selon les mots même de Bertalanffy, \enquote{De tout ce qui précède se dégage une vision stupéfiante, la perspective d'une conception unitaire du monde jusque-là insoupçonnée. Que l'on ait affaire aux objets inanimés, aux organismes, aux processus mentaux ou aux groupes sociaux, partout des principes généraux semblables émergent} \autocites[59]{Lemoigne1977}{Bertalanffy1961}. Une idée déjà existante dans la maxime célèbre de Claude Bernard en 1885, remise au goût du jour par \textcite{Lemoigne1977}, celle-ci résume toute la souplesse offerte par cette notion d'un point de vue de la modélisation : \enquote{Les systèmes ne sont pas dans la nature mais dans l'esprit des hommes}.

Cette théorie nommée \textit{General System Theory} (GST) est évoqué pour la première fois en public en 1937-38 par Bertalanffy. Il s'ensuit la rédaction d'une première ébauche en 1950, et il faudra attendre 1968 pour qu'un ouvrage titré \textit{General System theory: Foundations, Development, Applications} propose une synthèse de toutes les avancées. La durée de développement de cette théorie n'est pas anodine, car si on en croit Pouvreau \autocite{Pouvreau2013} qui a analysé en détail la très vaste littérature associée à cette thématique, cette théorie n'en est pas vraiment une (de thématique) en réalité. En effet l'état inachevé du projet de Bertanlanfy laisse plus à penser qu'il s'agit là d'un \enquote{projet}, et c'est à ce titre que Pouvreau préfère employer le terme de \enquote{systémologie générale} pour désigner ce qu'il définit alors comme \enquote{le \textit{projet} d'une \textit{science de l'interprétation systémique} du \enquote{réel} } \autocite[9]{Pouvreau2013}. L'hypothèse défendu par Pouvreau étant que cette \enquote{[...]science de l'interprétation systémique du \enquote{réel} se caractérise en fin de compte comme une herméneutique, au sens où elle a pour vocation d'élaborer à la fois les moyens de construire des interprétations systémiques d'aspects particulier du \enquote{réel} sous la forme de modèles théoriques spécifiques et les moyens d'interpréter à leur tour de tels modèles comme des déclinaisons de modèles systémiques théoriques d'un degré de généralité supérieur.}\autocite[9-10]{Pouvreau2013}

Mais avant de même de fonder ce projet unifiant qui par la suite va rayonner et être absorbé (non pas sans déformation ...) dans un grand nombre de disciplines, dont la géographie, il est intéressant de rappeler comment la théorie biologique de Bertalanffy a participé de la formation de grandes notions comme l'\enquote{équifinalité} ou l'\enquote{auto-organisation}, des notions aujourd'hui communément admises comme fondatrice du paradigme actuel de la \enquote{complexité}.

Bertalanffy poursuit depuis 1937 cet objectif de dépasser une compréhension des systèmes biologiques engluées dans une dualité opposant les \enquote{vitalistes} et \enquote{mécanistes}. La synthèse de ces travaux est organisé dans une \enquote{biologie organismique} qui fonde une troisième voie visant d'une certaine manière la réconciliation entre les deux approches \autocites[55-56]{Lemoigne1977}[258]{Bertalanffy1961}. Avec cette nouvelle biologie théorique il s'agissait d'incarner \enquote{l'avenir de la biologie} en établissant, via la mobilisation de moyens scientifiques (analyse et analogies physico-chimique et mathématique du vivant) écartant la métaphysique/psychiques, un programme de recherche des \enquote{loi systémiques ou d'organisation à tous les niveaux de la nature vivante} entendues comme \enquote{l'explication de l'harmonie et de la coordination des processus à partir de la dynamiques des forces qui leur sont immanentes} \autocite[456]{Pouvreau2013}. Principalement \enquote{ordonnées en direction de la conservation de la totalité}\autocite[440-458]{Pouvreau2013} dans une \enquote{tendance à une complication croissante}, cette \enquote{Gestalt organique} de la théorie \enquote{organismique} de Bertalanffy place \enquote{l'Organisation} des processus comme une véritable problématique de recherche, et met de côté la question de la \enquote{finalité} du vivant \autocite[455-457]{Pouvreau2013}.

Déjà tout à fait conscient que \enquote{le tout est plus que la somme des parties} Bertalanffy admet que l'étude des mécanismes physico-chimiques des processus vitaux tient plus d'une heuristique de recherche, une \enquote{méthode téléologique qui permet \enquote{d'examiner jusqu’à quel point le caractère de conservation de la totalité se manifeste dans les processus qui se déroulent en eux}} sans jamais arriver à en donner une complète description \autocite[464]{Pouvreau2013}.

Cette \enquote{biologie théorique organismique} (également appelé de façon synonyme par Bertalanffy \enquote{théorie systémique du vivant}) montre en bien des points toutes les prémisses d'une pensée systémiste et non réductionniste qui dépassent déjà largement le cadre seul de la biologie, et cela même avant 1937 et l'introduction de \enquote{systèmes ouvert} \autocite[499]{Pouvreau2013} qui ont fait la renommée de l'auteur. Cette \enquote{biologie organismique} de Bertalanffy, bien évidemment construite sur les acquis et l'aide de bien d'autres de ces contemporains (voir \autocite{Pouvreau2013}, arrive à maturité en 1937 \autocite[14]{Pouvreau2013}, et présente déjà à ce stade tous les traits d'une première \enquote{systémologie restreinte}, qui va servir d'\enquote{antichambre} à la formation de la future \enquote{systémologie générale} \autocite[670]{Pouvreau2013} de Bertalanffy (la première évocation publique date de 1945, mais des traces indirectes de ses premiers discours semblent remonter à 1937).

% D'abord on fait le point sur les principes (ce qui suppose de faire une grosse parenthèse avec tout ce que l'on a décrit sur la thermodynamique) et ensuite on peut passer à la critique, évoquant l'équifinalité et la hierarchisation de processus qui permet de recentrer aussi l'étude des boites noires.

Le projet de Bertalanffy est articulé autour de deux \enquote{principes organismiques} qui fondent sa théorie, et apparaissent de façon très claire dans une première définition du vivant en 1932, raffinée par Bertalanffy en 1937.

%Définition des deux principes organismiques !?

Le premier principe théorique \enquote{organismique} de Bertalanffy s'appuie sur le principe biologique fondamental qu'il a énoncé dès 1929 avec la \enquote{conservation du système organique en équilibre dynamique}. Un équilibre qui paraît statique d'un point de vue extérieur, mais qui est en réalité dynamique car son existence même est basée sur la remise en jeu permanente d'une partie du travail effectué par la cellule pour maintenir le système organique loin de l'équilibre \enquote{vrai} (physique, c'est à dire celui qui correspond à une mort thermique, ou chimique qui ne peut pas produire non plus de travail à l'équilibre). Un \enquote{équilibre de flux} qui ne peut être réalisé que parce que l'organisme n'est ni un système fermé, ni un système statique, mais un système dont l'ordre et l'organisation sont fondées sur un travail issu d'un \enquote{flux} de matière et d'énergie résultat d'une transaction à double sens avec son environnement \autocite[472]{Pouvreau2013}. Je me permettrai de citer ici Morin, qui reprenant Héraclite, évoque très bien cet antagonisme à l'oeuvre dans les systèmes organiques, mais aussi par extension sociaux : \enquote{Vivre de mort, mourir de vie} \enquote{ [...] ne vivons-nous pas de la mort de nos cellules qui vieillissent et se décomposent pour laisser la place à des cellules jeunes ? [...] La vie et la mort sont certes deux ennemies fondamentales, mais la vie lutte contre la mort en utilisant la mort. Néanmoins, il est tuant de se régénérer en permanence. C’est épuisant. Finalement, on meurt à force de rajeunir. On meurt de vie. } 

% Critique cybernétique
Le principe d'\enquote{équilibre des flux}, même si il peut être rapproché du concept d'\enquote{homéostasie} définit par les tenants de la \enquote{première Cybernétique} (en analogie avec les systèmes mécaniques) comme la \enquote{conjonction des processus par lesquels, nous autres, être vivants, résistons au courant général de corruption et de dégénérescence} est trop généraliste pour application en tant que tel à toutes les notions de régulations organiques \autocite[194]{Morin1977}. L'\enquote{homéostasie} tel que définie par Wiener dans le cadre de la Cybernétique s'avère en réalité être un mécanisme de régulation organique parmi tant d'autres, tous n'étant pas basé sur le schème de rétro-action. A ce titre, la notion d'\enquote{homéostasie} pourtant quasi semblable dans sa définition à l'équilibre de flux dans un système ouvert, mobilise en réalité un tout autre fonctionnement que le schème de rétro-action Cybernétique, et tient plus de l'extension aux systèmes ouverts du principe dit de \enquote{Le Chatelier}. De la même façon la régulation intervenant dans le processus de croissance des organismes qui nécessite la régénération, et l'évolution des structures dans le temps n'est pas compatible avec l'ordre structural pré-établi des machines et le scheme de rétro-action promis par la Cybernétique. La vision \enquote{machinaliste} limitée/biaisée des premiers cybernéticiens n'est donc pas satisfaisante pour une application aux systèmes organiques, dès lors qu'il faut accepter la constance non pas des structures mais des interactions entre les structures. Bertalanffy développe une classification plus complète de ces régulations qu'il considère selon le type de leur téléologie, et introduit le concept d'\enquote{équifinalité} comme téléologie dynamique moteur dans la construction et le maintien des systèmes organiques. Dans ce contexte, le principe d'équifinalité \autocite[131]{Pouvreau2013}, est ainsi évoqué pour la première fois comme la possibilité d'atteindre le même état finalisé à partir de trajectoires quelconques, un processus impossible dans le cadre de système fermé où les condition initiales définissent par avance l'état final. Ce faisant, Bertalanffy introduit la primauté de l'ordre dynamique sur l'ordre structurel et fait de l'équifinalité un concept qui dérive de l'ouverture des systèmes \autocite[489,647]{Pouvreau2013}. Un exemple illustrant les effets de l'équifinalité dans les organismes vivants peut être montré avec le processus de division embryonnaire. Ainsi un organisme à qui on impose la fragmentation, la régénération, ou des blessures d'unités biologiques élémentaires comme les gènes ou les chromosomes va de façon constante s'organiser suivant un plan pré-établi menant à la \enquote{constitution d'un tout}, autrement dit un organisme complet.

Le deuxième \enquote{principe organismique} est celui du respect d'un \enquote{ordre hiérarchique}, comme le résume Pouvreau : 

\blockquote[{\cite[476-477]{Pouvreau2013}}]{Le \enquote{ second principe } de Bertalanffy était le schéma suivant de développement épigénétique d’un système, dont on peut remarquer qu’il correspond bien à ce que Chauvet a récemment posé comme une \enquote{ caractéristique de la vie dans la matière }. Dans une étape \enquote{ primaire }, le système est \enquote{ unitaire } : il forme une \enquote{ totalité équipotentielle } ayant des capacités maximales de régulation. Aucune de ses parties n’y est encore investie d’une fonction spécifique. Dans un second temps survient un processus de \enquote{ ségrégation } au cours duquel le système se \enquote{ scinde } en sous-systèmes dont le développement spécifique ultérieur se prédétermine. Un processus de \enquote{ différenciation progressive } engage alors chaque sous-système dans la voie de développement qui lui a été ainsi assignée. Il se caractérise par l’attribution de fonctions déterminées à ces sous-systèmes et la constitution de structures plus ou moins rigides. C’est un processus d’autonomisation relative et de spécialisation des parties et des processus, qui implique pour le système dans son ensemble une \enquote{ perte de régulabilité } (ou de \enquote{ plasticité }) et que Bertalanffy appela à partir de 1937 la \enquote{ mécanisation progressive }.}

%Il nécessite un autre mode d'explication de processus téléologique, celui de la cybernétique s'avérant incompétent au regard du principe d'équifinalité observé dans les systèmes organiques.

% Bertalanffy s'appuie dans sa critique à raffiner sa classification des téléologies, ce qui lui permet d'introduire le concept d'équifinalité comme sous-type de téléologie dynamique, un type de processus de régulation qui selon lui ne peut pas être expliqué par les schèmes cybernétique initiaux, seulement capable de mobiliser le concept de finalité en regard d'une explication basé sur un arrangement structural pré-établi (une machine faites de composants) et non pas l'ordre  dynamique propres au système en équilibre de flux.

La combinaison des deux principes \enquote{organismique} mène à la théorie des \enquote{système ouvert en équilibre de flux} \autocite[481]{Pouvreau2013}:
\begin{itemize}
\item La subordination du \enquote{principe de hierarchisation} à celui du \enquote{système ouvert en équilibre de flux}, autrement dit la genèse et le maintien de l’ordre hiérarchique d’un \enquote{système organique} est conditionné par l'existence d'un \enquote{système ouvert en équilibre de flux}.
\item  La relation précédente est un principe ubiquitaire s’appliquant à tous ses niveaux.
\end{itemize}

Cette idée sera particulièrement fructueuse une fois articulée avec le principe d'un enboîtement des systèmes, l'accroissement du degré de liberté dans un système résultant de l'équifinalité \autocites[38]{Bertalanffy1973}[786-788]{Pouvreau2013}.

%Developpement rendu possible uniquement par l'apport des théories de la thermodynamique ... l'expression d'une trajectoire indépendamment de l'état final, celui ci n'est qu'un processus de régulation parmis d'autres, car ce même système organique est non seulement capable de maintenir son état mais choses plus importante, il permet surtout de produire de l'organisation, de la complexification.

% Relation avec science sociale ??
% => entéléchie /

Telle que définie, cette notion d'équilibre dynamique de Bertalanffy est bien différente de celle produite en physique et en chimie, qui se caractérise justement par l'absence de travail disponible, l'énergie disponible étant minimale. Pour que la permanence d'un ordre puisse être effective dans la théorie organismique, il faut qu'il y ait un échange, un flux d'énergie mais aussi de matière possible avec l'environnement; une différenciation qui amène Bertalanffy à développer dès 1937 une théorie des \enquote{systèmes ouverts}, la seule capable de s'appliquer également à des systèmes sociaux par la suite.

% Sur l'ouverture des systèmes
Pour mieux comprendre en quoi cette ouverture est importante pour l'application du paradigme systémique aux sciences sociales, il faut revenir quelques décennies en arrière pour définir les limitations des premiers systèmes issues de la thermodynamique, limitations qui par la suite ont irriguées les réflexions initiales des cybernéticiens tout autant que les motivations de Bertalanffy pour les dépasser dans le cadre de sa théorie \enquote{organismique}

La seconde loi de la thermodynamique esquissée par Carnot et formulée par Clausius en 1850 montre que l'energie calorifique ne peut se reconvertir, elle se \textit{dégrade} et perd son aptitude à effectuer un \textit{travail}. Clausius nomme \enquote{entropie} cette diminution irréversible de l'aptitude à se transformer et à effectuer un travail, propre à la chaleur.\autocite[35]{Morin1977} Prigogine dans la \textit{fin des certitudes} écrit à propos de l'entropie qu'elle \enquote{[...] est l’élément essentiel introduit par la thermodynamique, la science des processus irréversibles, c’est-à-dire orientés dans le temps.}

Ce sont Boltzmann, Gibbs et Planck qui vont par la suite faire le lien entre le niveau micro des particules et la notion de chaleur. Parce que la chaleur est caractérisée par l'agitation désordonnée des molécules dans un système, l'entropie devient plus qu'une simple réduction du travail, c'est aussi l'ordre et le désordre des molécules qui en est la cause. Cette transformation s'effectue avec création d'entropie, une \enquote{quantité de désordre} qui ne peut que croître dans le temps et cela jusqu'à atteindre une valeur maximale équivalente à ce nouvel état d'équilibre. De ce fait et de façon générale celle-ci définit comme évolution irréversible toute transformation réelle dans un système isolé (Système où la frontière est totalement imperméable : l'Univers est par définition un tout englobant) ou fermé (Système ou la frontière est perméable aux flux entrant ou sortant d'énergie mais imperméable aux échanges de matière : La Terre reçoit de l'énergie du soleil) partant d'un état non stable et se dirigeant vers un nouvel état stable. Ainsi si on considère l'univers comme un méta-système isolé englobant tous les autres, alors ce second principe a pour corollaire que l'entropie de l'univers augmente vers un état de désordre maximal qui se traduit en définitive par une mort thermique.

L'intuition de cette possible analogie entre loi gouvernant systèmes physiques et biologiques est issue des réflexions menées par Boltzman, qui comme ces contemporains du XIX siècle est admiratif pour la récente théorie évolutive de Darwin \autocite[27]{Prigogine1996}. Celui-ci tente alors un parallèle avec ses propres travaux sur la seconde loi de thermodynamique, que l'on retrouve dans une des fameuses citations présente dans son livre \textit{second law of thermodynamic} : \foreignquote{english}{ The general struggle for existence of living beings is therefore not a struggle for raw materials — the raw materials of all organisms in the air, water and soil are in abundance there — nor about energy, which in the form of heat, unfortunately, is contained abundantly [but unfortunately] [in]convertible in each body, but a struggle for entropy, which is available [disposable] by the transfer of energy from the hot sun to the cold earth.}

% Le sys ouvert/fermé , de la thermodynamique à la biologie ?
Le point de vue de Boltzmann est repris et théorisé par Alfred J. Lotka, un mathématicien, chimiste et statisticien. Celui-ci va largement influencé par la suite Bertalanffy dans la formation de sa \enquote{systèmologie générale} \autocite[178]{Pouvreau2013} par ces études de la démographie des populations et des flux de matières dans le monde biologique \autocite[545-546]{Pouvreau2013}, toutes deux usant largement des équations différentielles (un premisse d'isomorphisme mathématique applicable à diverses disciplines, visible dans le formalisme de Lotka, et par la suite de Lotka et Volterra \autocite[550]{Pouvreau2013}). De la même façon que Bertalanffy un peu plus tard, celui-ci ignore sciemment les débats entre \enquote{vitalistes} et \enquote{mécanicistes}, et adopte un point de vue unificateur qui vise la réconciliation entre système physique et système biologique, et part à la recherche d'isomorphisme en s'appuyant sur le processus d'irréversibilité commun aux deux paradigmes : \foreignquote{english}{[...] the law of evolution is the law of irreversible transformation; that the \textit{direction} of evolution [...] is the direction of irreversible transformations. And this direction the physicist can define or describe in exact terms. For an isolated system, it is the direction of increasing entropy.  The law of evolution is, in this sense, the second law of thermodynamics} \autocite[26]{Lotka1925}.

Dès 1922 Lotka \autocites{Lotka1922a, Lotka1922b} développe une nouvelle théorie qui acte la capacité de capturer de l'énergie comme un optimum à atteindre guidant la sélection telle qu'elle est décrite par la théorie Darwinienne. Il est également l'un des premiers à percevoir les limites des lois actuelles de la thermodynamiques pour expliquer les processus du vivant : \enquote{Tenant pour légitime de traiter les êtres vivants et leurs associations comme des systèmes physiques, Lotka insistait toutefois sur le fait qu’il s’agit de « systèmes ouverts » aux flux de matière et d’énergie (ainsi que Raymond Defay (en 1929) et Bertalanffy (en 1932) les qualifièrent plus tard), capables d’échapper à l’équilibre thermodynamique défini par un maximum d’entropie promis aux systèmes fermés par le Second Principe, et d’évoluer vers une structuration croissante} \autocite[179]{Pouvreau2013}.

En effet pour un système vivant, l'état d'équilibre tel que décrit pour des systèmes clos ou isolés, correspond à un état de mort cellulaire. Or, il est prouvé empiriquement à cette période que les systèmes vivants évoluent dans un environnement chimique en perpétuel évolution loin de l'équilibre, et sont de fait capables de maintenir un haut niveau d'organisation par l'échange d'énergie et de matière avec l'environnement. Autrement dit, il n'est pas possible de concevoir l'équilibration permanente des systèmes vivants comme le résultat d'une évolution entropique croissante \autocite[248]{Lemoigne1977}. Des résultats énoncés sous forme de loi en 1929 par Bertallanfy, qui fait de \enquote{la conservation de système organique en équilibre dynamique} un \enquote{principe biologique fondamental}, et qui deviendra plus tard dans sa théorie \enquote{organismique}, le premier principe de  \enquote{système ouvert} en \enquote{équilibre de flux}. \autocite[492]{Pouvreau2013}

Mais en voulant faire l'analogie entre ces deux systèmes, une question va rapidement se poser aux scientifiques. \enquote{Comment la progression irréversible du désordre pouvait-elle être compatible avec le développement organisateur de l'univers matériel, puis de la vie, qui conduit à homo sapiens ?}, une question qui va engendrer la problématisation et un changement de point de vue radical. Comme le résume bien Morin dans son premier tome de \textit{La Méthode}, \enquote{A partir du moment où il est posé que les états d'ordre et d'organisation sont non seulement dégradables, mais improbables, l'évidence ontologique de l'ordre et de l'organisation se trouve renversée. Le problème n'est plus : pourquoi y a-t-il du désordre dans l'univers bien qu'il y règne l'ordre universel ? C'est : pourquoi y a-t-il de l'ordre et de l'organisation dans l'univers ? } \autocite[37]{Morin1977}

Avec de tels propos se pose alors rapidement la question des mécanismes à l'oeuvre dans le vivant qui permettraient de rétablir l'universalité de la seconde loi thermodynamique. Bien qu'intuitées par de nombreux chercheurs comme Lotka ou Bertalanffy, il faudra attendre les années 1940 pour que se développent de nouvelles hypothèses. Celles-ci apparaissent dans le rapprochement entre paradigme évolutionniste et domaine de la thermodynamique, avec le partage des théories entre biologistes et physiciens.

Reprenant l'acceptation d'un système ouvert, c'est le livre \textit{What is Life} de Schrödinger (1944) qui va marquer le plus les esprits, et soulève le mieux ce paradoxe à la croisée des deux théories. Deux choses au moins fascinent celui-ci \autocite{Foerster1959}, d'une part l'existence d'un code héréditaire qui définit au niveau micro la formation, l'organisation d'organisme au niveau macro (le principe \enquote{order-from-order}), d'autre part l'étonnante stabilité de ce code héréditaire immergé à 310 Kelvin \autocite[47]{Schrodinger1944}, et qui ne répond donc pas au fameux principe statistique \enquote{order-from-disorder} établit précédemment par Boltzmann.

En inscrivant comme nécessaire l'existence d'un code génétique comme un plan guidant l'évolution (tout comme Bertalanffy qui développe des théories similaires à la même époque), il introduit avec son concept de d'\enquote{entropie négative} un principe qui rend de nouveau compatible la seconde loi de thermodynamique avec l'évolution des systèmes biologiques : \enquote{le physicien attribuait le maintien de l’organisme dans un état \enquote{ stationnaire } éloigné de l’équilibre vrai à sa capacité de se \enquote{ nourrir } d’\enquote{ entropie négative } grâce à son ouverture sur son environnement. Une \enquote{ néguentropie } interprétée comme une \enquote{ création d’ordre à partir d’ordre } -- l’organisme créant un ordre spécifique à partir de la matière déjà ordonnée, structurée d’une manière déterminée mais devant être transformée pour ses besoins énergétiques, qu’il trouve dans son environnement} \autocite[502]{Pouvreau2013}. Autrement dit, le maintien de l'organisation est un équilibre dynamique, un jeu à somme nulle où la création d'entropie est annulée par la capacité des organismes à transformer l'énergie, l'ordre puisé dans l'environnement pour maintenir ce degré d'organisation, un processus qualifié de néguentropique. Ce concept, déjà difficile à accepter tel quel dans sa généralité \autocite[225]{Lemoigne1977} va par la suite être raccroché à théorie de l'information de Shannon après son introduction en 1948 dans le microcosme Cybernétique. L'introduction de cette théorie étant un autre moment fort (avec la thermodynamique) ayant inspiré de nombreux développements dans la cybernétique. Mais les tentatives d'unification entre les deux théories débouchent sur deux rapprochement possible, avec d'une part la qualification d'une \enquote{information pensée comme quantité physique} ou d'autre part l'expression des \enquote{quantités physique pensées comme de l'information}, selon que l'on adopte le point de vue de Wiener ou de Brillouin 1956 \autocite{Brillouin1956} (auteur de la \enquote{néguentropie} qui associe \enquote{information} et principe de \enquote{néguentropie} ). Ces points de vue font encore à l'heure actuelle l'objet de nombreux débats, certains voyant la physique de l'information comme un point de départ à creuser pour appeler une théorie de l'\enquote{organisation} \autocite[37-38]{Morin1990}, alors que d'autres n'y voient qu'un concept flou seulement basé sur la similitude des deux formules. Autant de ramifications naissent de ces positions, et leur présentation dépassent de loin le seul cadre d'étude de cette thèse, mais le lecteur pourra se référer au travail de \autocite{Triclot2007} pour mieux comprendre le point de départ d'un malentendu qui dure toujours\Anote{ton_difference}. 

Mais finalement plus que les idées développés par Shrödinger, la plupart étant déjà largement sous entendu dans les travaux des biologistes de l'époque, il semblerait plutôt que cela soit la diffusion du petit livre dans le grand public, ce nouvel éclairage physiciste apporté à la biologie \autocite[482]{Pouvreau2013} et l'espoir de trouver de nouvelles lois physique à l'oeuvre dans la construction du vivant, qui amèna peut être de nombreux physiciens à ne plus ignorer les avancées dans ce domaine, notamment durant les années 1940 / 50, tel que Prigogine \autocite[77]{Prigogine1996}, Von Foerster, \autocite[73]{Lemoigne1977}, etc. 

Bertalanffy est conscient des manquements et des reproches faits à son approche, alors incomplète, car focalisée uniquement sur la cinétique. Celle-ci n'est pas relié à une théorie plus explicative sur les mécanismes énergétiques à l'oeuvre susceptible de justifier l'existence de telles propriétés chez les systèmes vivants dans le cadre des systèmes ouverts. Ce sont les récents développements sur la \enquote{Thermodynamique des processus irréversibles} qui vont introduire \textit{a posteriori} la possibilité d'une thermodynamique des systèmes ouverts compatible avec l'approche de Bertalanffy. Des physiciens ayant participé à ces travaux sur la thermodynamique des systèmes ouverts loin de l'équilibre (Osanger, etc.) ce sont les travaux de Prigogine en 1946 \autocite{Prigogine1946} qui vont le plus attirer l'attention de Bertalanffy. Lorsque celui-ci découvre vers 1948 ces récentes avancées qui semblent faire parfaitement écho à ces travaux ( Prigogine n'hésitant pas à citer Bertalanffy comme un de ses modèles d'inspiration \autocite{Prigogine1996}), le rapprochement se fait assez rapidement et Bertalanffy n'hésite pas à promouvoir cette nouvelle thermodynamique comme le parfait support physique justifiant des principes qu'il a établis dans sa propre théorie des systèmes ouverts en équilibre des flux ! \autocite[653-658]{Pouvreau2013}

Malheureusement le \enquote{théorème de Prigogine} de \enquote{minimum de production d'entropie} ne s'exprime que dans des conditions très drastiques \autocite[53]{Lebon2008} et se limite à des systèmes très proches d'un état d'équilibre, comme le prouve les travaux de Denbigh : \foreignblockquote[{\cite{Denbigh1952}}]{ It is possible that certain reactions in biological systems may be sufficiently close to equilibrium for the rate of entropy production due to them to be very small. But in general it seems that the notion of minimum entropy production has no real significance as applied to chemical reaction in open systems [...] it is incorrect to regard the tendency of an open system to approach a stationary state as being determined by thermodynamic factors. The stationary state may or may not coincide with a state of minimum entropy production, according to whether the rates of the individual processes are linear functions of thermodynamic variables. In the above we have assumed this to be the case for diffusion (eqn. (ll)), but it is known not to be true for chemical reaction.} 

Hors l'état des systèmes biologiques est semble-t-il très loin d'être proche d'un quelconque état d'équilibre thermodynamique... Bertalanffy qui jusqu'à présent se contentait de relier les résultats à son programme organismique ne cache alors plus sa déception lorsque en 1953 il écrit \enquote{Un minimum de production d'entropie ne caractérise donc pas l'équilibre des flux dans les systèmes ouverts [...]}; autrement dit \enquote{la thermodynamique [...] ne nous dit jamais ce qui peut se passer dans un système, ce qui est permis [...] Et le problème de l'organisation progressive, la tendance néguentropique de l'évolution des organismes simples aux organismes compliqués, reste à présent non résolu.} \autocite[661]{Pouvreau2013}. Bien qu'il n'abandonne pas l'idée de voir expliquer un jour sa théorie organismique par une théorie thermodynamique adaptée, il abandonne en 1953 l'étude de la biophysique des systèmes ouverts et se consacre par la suite uniquement à la construction de sa théorie du système général.

Le fait est qu'il y a réduction d'entropie dans les systèmes en équilibre de flux, et qu'il y a également maintien et augmentation du niveau d'organisation, sans que l'on sache pourquoi pour le moment dans le monde du vivant. Si l'analogie et le pont tissé entre physique et biologie semble à cette période soumis à questionnement, les travaux de Prigogine sur la thermodynamique des systèmes ouverts va continuer quant à elle à ouvrir bien d'autres perspectives, notamment dans les systèmes sociaux.

%paragraphe dimension reflexive auto-orga ...
%Elle dépasser largement ce cadre, et appuie sur des bases physiques le concept d'"auto-organisation", une notion déjà introduite dans le mouvement cybernétique par Ashby, un homme clef dans la convergence des idées entre Cybernétique et GST.

%Inspiré par Von Foerster, vont alors introduire un autre concept \enquote{d'order from noise}, totalement différent du \enquote{order-from-disorder} de Schrodinger.

Un concept important est introduit par le psychiatre et ingénieur anglais William Ross Ashby dans le mouvement Cybernétique. Le concept d'auto-organisation, et l'introduction du mot \enquote{auto} amorcent ainsi un virage réflexif qui annonce la seconde Cybernétique, piloté principalement par Von Foerster.

\subsection{Seconde cybernétique et auto-organisation, quelques éléments historiques}
\label{sssec:heritage_complexe}

Une première influence est d'abord à chercher dans l'émergence de ce que l'on appelle aujourd'hui \enquote{Cybernétique de Second Ordre}; et dont on trouve les premières traces à la charnière des années 40-50, avec l'introduction par l'influent McCulloch du physicien Viennois Von Foerster comme orateur en 1949 puis secrétaire jusqu'en 1953 des conférences interdisciplinaires de Macy.

L'homme qui nous intéresse ici, McCulloch, est donc d'autant plus influent par ses travaux qu'il figure également comme participant et organisateur dès les toutes premières et importantes conférences de Macy (1942). Si on peut encore discuter sur la part d'influence qu'il convient d'attribuer à McCulloch ou à Wiener sur la structuration des idées dans ce groupe Macy, il n'y a aucun doute sur l'importance des travaux menés par ce dernier avec Pitts et Von Neumann \enquote{ sur la logique mathématique comme instrument d'une théorie unifiée liant fonctionnement du cerveau et des ordinateurs } \autocite[777]{Pouvreau2013}. 
Malgré le biais mécaniciste réductionniste \textcite[783-784]{Pouvreau2013} induit par le discours de ces derniers autour de leur modèle de réseau de neurones formels, \enquote{ ce fut surtout parce qu’il contribuait à l’extension du domaine de la science \enquote{ exacte } à la neurophysiologie, parce qu’il permettait dans un même mouvement de connecter celle-ci à la théorie des automates, et parce qu’il nourrissait le consensus autour de l’idée que la pensée a pour structure physique sous-jacente des réseaux de neurones biologiques analogues aux réseaux constitutifs des automates de calcul} que \autocite[777]{Pouvreau2013} considère les travaux de McCulloch comme un des quatre moments clef dans la construction de la cybernétique. Si le ton des conférences de Macy porte une vision réductionniste\Anote{reductionisme_pouvreau_macy}, le point de vue de Von Neumann et McCulloch se différencie toutefois des positions plus modérées de Wiener ou Rosenblueth. Personnage complexe, on pourra se rapporter aux écrits de \textcite{Dupuy2005} et \textcite{Levy1985} afin de mieux comprendre et replacer historiquement l'immense héritage laissé par McCulloch, notamment par rapport à l'intelligence artificielle, dont il est un éminent précurseur. Car selon \textcite{Dupuy2005} bien que celui-ci se range plus souvent dans sa carrière du côté des biologistes que des ingénieurs, ce fut paradoxalement par les psychologues et les embryologistes qu'il fut plus particulièrement rejeté. Un point de vue partagé par \textcite[778]{Pouvreau2013}, sa théorie ayant eu une bien plus grande influence dans le domaine des automates que dans le domaine biologique\Anote{influence_turing}.

Influent, McCulloch l'est également par le vaste réseau de relations internationale qu'il est amené à mobiliser pour découvrir des travaux originaux \autocites{Dupuy2005, Husbands2012, Levy1985}. Ainsi tout comme le soutien important qu'il a pu apporté aux travaux du jeune Von Foerster, c'est également McCulloch qui recrute a plusieurs reprises des \enquote{cybernéticiens avant l'heure} membres du \textit{Ratio Club} anglais\Anote{mcculloch_ratioClub}. C'est le cas par exemple du psychiatre et ingénieur anglais Ashby, qu'il invite pour participer aux 9ème conférences de Macy en 1952. Une inflexion scientifique qu'il maintient également dans le projet du \textit{Biological Computer Laboratory} (BCL) de Von Foerster, où il place Günther en 1967 comme scientifique titulaire, et que l'on peut entrevoir lorsque Ashby est lui aussi titularisé dans ce centre par Foerster en 1961, où il restera 9 années.

Von Foerster est reconnu comme le chef de file d'une transformation de la pensée cybernétique. La fin des conférences de Macy en 1953, et l'absence de véritable lieu physique pour discuter de cette problématique sous un angle véritablement biologique et interdisciplinaire, semblent être deux arguments moteurs dans le projet du BCL initié par Von Foerster. Soutenu et initié à la biologie par McCulloch et le mexicain Rosenblueth pendant cette période d'entre-deux, Von Foerster semble plus intéressé pour poursuivre l'investigation de la \enquote{computation au sens biologique} déjà incarnée dans la figure de McCulloch que par les problématiques purement cybernétique\Anote{foerster_interview}. En effet, la causalité circulaire dans sa spécificité biologique n'ayant été que très peu traitée par la cybernétique\Anote{dupuy_causalite}.

Ce sont probablement ces éléments qui vont pousser Von Foerster a fonder en 1958 le \textit{Biological Computer Laboratory} (BCL) au cœur de l'université de l'Illinois. Un foyer interdisciplinaire initié et dirigé par ce dernier jusqu'à son départ et la fermeture qui s'ensuit au milieu des années 1970. \autocite{Proulx2003}.

C'est donc dans ce creuset accueillant du BCL où sont invités à défiler un certain nombre de chercheurs, de façon permanente ou temporaire, que vont être amenés à discuter de nombreuses et très différentes problématiques dont la notion aujourd'hui bien connue d'\enquote{auto-organisation}. Nous ne rentrerons pas ici dans les détails d'une généalogie du concept dont \textcite{Stengers1985}\Anote{livret_CREA} a montré qu'elle était d'un point de vue épistémologique un puzzle de lecture extrêmement complexe, mais nous pouvons d'ores et déjà rappeler quelques éléments saillants, évoquant par le biais des influences de certains acteurs majeurs de cette réflexion le différentiel de points de vue pouvant animer les débats sur cette question.

Les principales discussions du BCL sur cette notion sont données à voir par le biais de \textit{proceedings}, résultat de trois conférences voulues par Von Foerster : \autocite{Yovits1960}, \autocite{Yovits1962} et \autocite{Foerster1962}. Dans un tel cadre, l'intérêt biologique est également amené à croiser l'intérêt informatique. Le BCL côtoie ainsi dans ces conférences les contributions de ce qui est en train de devenir depuis 1956 et la conférence de Darmouth la toute jeune discipline de l'Intelligence Artificielle. Il n'est pas anodin alors de citer l'influence de McCulloch qui opère depuis 1952 justement dans la division électronique du MIT, et travaille avec les pionniers Minsky (projet SNARC), etc. Il est ainsi intéressant de voir réuni dans ces conférences sur l'auto-organisation de 1960 tous les précurseurs de ce domaine, réunis autour d'une cause commune, alors même que les tensions entre partisans du \enquote{symbolisme} et \enquote {connexionisme} ne semble pas encore avoir éclatés\Anote{connexionisme_symbolisme}. Sont ainsi présent lors des conférences, Herbert Simon, Allen Newell, John Shaw, Marvin Minsky, John McCarthy ainsi que le pionnier des réseaux neuronaux Frank Rosenblatt, et les cyberneticiens Warren McCulloch, Gordon Pask, et évidemment Von Foerster \autocites[256]{Asaro2007}{Yovits1960}.

Bien que ces conférences attirent des cybernéticiens brillants, \textcite[87]{Stengers1985} fait état d'un bilan en demi-teinte, ces \textit{proceedings} faisant plus penser à un catalogue de points de vue hétérogènes qu'à une réelle volonté de synthèse. Ainsi à l'instar de Stengers, l'histoire retiendra principalement de ces publications les auteurs des points de vue alors déjà célèbres (homeostat en 1952, loi de la variété requise en 1956) du psychiatre et ingénieur \autocites{Ashby1947, Ashby1962}, et ceux plus contemporains de cette époque du physicien \textcite{Foerster1959} \autocites{Muller2007a}[55-56]{Stengers1985}. Si le sens du concept d'auto-organisation semble nous filer entre les doigts tant il est polymorphe, il n'en représente pas moins pour \textcite[106-110]{Livet1985} un mot d'ordre que l'on aurait tort de négliger dans l'analyse des travaux au BCL, car il constitue un drapeau de ralliement qui marque par un horizon de pensée, la spécificité de ces questionnements par rapport à la première cybernétique.

Selon Umpleby \autocite{Umpleby2003}, pour Von Foerster la première cybernétique est définitivement effacée par la seconde, la seule qui devient acceptable d'un point de vue scientifique. Comment alors le réductionisme fervent de McCulloch se transforme-t-il dans la filiation de questions opérées au travers des positions de Foerster et des projets menés au BCL ? Selon \textcite[120-122]{Livet1985} \enquote{la cybernétique de \enquote{second ordre} du BCL a conservé l'hypothèse de Mac Culloch d'une computation universelle, mais elle a aussi accentué les aspects non-réductionnistes, et tout d'abord le refus du behaviorisme [...]} Ni totalement réductionniste, ni holiste au sens le plus simple, Levy y voit une certaine parenté avec l'\enquote{organicisme} sans toutefois pouvoir l'y rattacher, car si les cybernéticiens semble bien admettre des différences entre l'organique et l'inorganique, l'organique est quand même étudié ici comme \enquote{machine biologique} capable de \enquote{computation}, à la différence des embryologistes organicistes.

La rencontre de Foerster avec le biologiste Maturana (à Leiden en 1962) et de son disciple Varela (1965) donnera naissance à la notion d'auto-poièse. Pour ces deux scientifiques cette notion n'a rien à voir avec le concept d'auto-organisation tel qu'il est abordé lors de leur passage au BCL, et cela même rétrospectivement lorsque ceux-ci découvrent en 1976-1977 l'autre sens thermodynamique prise par la notion. Si l'article de référence sur l'auto-poeïse date de 1974 \autocite{Varela1974}, la notion se cristallise certes dans l'historique des pratiques expérimentales des deux biologistes mais également surtout par la pratique de cet environnement fécond qu'est le BCL. Ainsi c'est au détour d'une publication interne du BCL (1970) qu’apparaît pour la première fois ce terme; une preuve de cette synergie féconde orchestrée par et autour de Von Foerster, le seul en réalité capable de discuter ces idées et d'opérer une synthèse au travers des différentes approches - parfois opposées -  qui traversent son laboratoire. \autocites[283-287]{CREA1985}{Muller2007b, Varela1995}. 
 
Ainsi par exemple, à la lecture des interviews de \textcite{Varela1995}, Maturana \autocite{Muller2007b}, ou Von Foerster \autocite{Franchi1995} on comprend que les relations déjà complexes de certains membres avec le \textit{MIT AI group} fondé en 1958 par Minsky et McCarthy vont se renforcer avec la disparition des financements supportant le BCL. En désaccord avec la vision du cerveau comme machine de traitement symbolique \autocite[43]{Muller2007b}, ces derniers expriment également toute leur méfiance envers un certain nombre de mots clefs de la cybernétique. 

Toutefois, il existe probablement une piste à explorer entre la direction prise par Von Foerster dans le courant des années 1960 sous l'influence réciproque de Maturana et Varela, et le concept d'auto-organisation tel qu'évoqué pour la constitution du vivant en biologie théorique. 

Cette filiation pour la notion d'auto-organisation est explorée par Stengers. Elle note que dans son sens physico-chimique le terme n’apparaît chez Prigogine que tardivement (1969). Or pour \textcite[64]{Stengers1985}, une explication pour justifier l'apparition spontanée de ce terme dans les textes de Prigogine tient de sa familiarité originelle avec la biologie, où le terme est utilisé depuis longtemps, notamment en embryologie. Mais on ne peut aller plus loin sans évoquer la part d'héritage que doivent ces réflexions aux travaux antérieurs de Von Bertalanffy. La construction de sa théorie organiciste  entamé dans les années 1930 fait de lui un des acteurs incontournables dans l'établissement d'une biologie théorique. Or on sait que sur les réflexions théoriques sur les systèmes ouverts éloignés de l'équilibre extrait des travaux de Von Bertalanffy sont entrés très tôt en résonance étroite \autocite[653-661]{Pouvreau2013} avec les réflexions de Prigogine \autocite{Prigogine1996}, ce dernier ne cachant pas son inspiration pour la biologie comme tendent à le montrer plusieurs de ses collaborations et publications \autocites[59-67]{Stengers1985}{Prigogine1946}.

D'un tout autre côté, dans l'interview donnée pour le \textcite[255]{CREA1985}, Von Foerster indique bien ne pas avoir pensé, lorsqu'il était au BCL, à appliquer les mathématiques des systèmes non linéaires à la problématique de l'auto-organisation. Des mathématiques dont il connaît pourtant l'existence de par sa formation.

% Voir page Dupuy : https://books.google.fr/books?id=bwlm7kVy5WoC&pg=PP53&lpg=PP53&dq=mcculloch+foerster&source=bl&ots=lD2chp1gL5&sig=QRk4AgrqRe7jmCgI7_ERrqVdyPo&hl=fr&sa=X&ei=jyqPVPr5Bo3SaKbAgagD&ved=0CGwQ6AEwCg#v=onepage&q=mcculloch%20foerster&f=false

Un ensemble d'observations qui tendent à avaliser cette hypothèse forte donnée par Stengers, pour qui cette branche de réflexion double abordant la notion sous l'angle biologique et thermodynamique évoluent dans une relative indépendance par rapport à la réflexion menée au BCL. \textit{Pourquoi relative ?} Car il faut prendre en compte l'existence tout à fait plausible d'une forme de recoupement entre ces deux voies de réflexions. Mais avant d'aborder la possibilité d'une telle piste, il faut donner un aperçu de la spécificité de cette seconde réflexion sur l'auto-organisation.

La question de l'auto-organisation s'inscrit en biologie dans une tradition beaucoup plus ancienne que celle évoquée dans la tradition cybernétique ou physico-chimique. On pourra ainsi retrouver dans les débats des biologistes de multiples références à la philosophie, comme par exemple celle de Kant, qui critiquait déjà en 1789 l'hypothèse mécaniste pour justifier de la vie et \enquote{considérait déjà l'auto-organisation comme principe distinctif du vivant} \autocites[76]{Pouvreau2013}[275]{Mossio2010}[6]{Mossio2014}. Ce concept d'auto-organisation \autocite[68]{Stengers1985} est rediscuté à la lumière des débats opérant à la charnière des années 1920-1930, dans l'émergence d'un courant de biologie théorique dont la volonté nomothétique se fait l'écho conjoncturel d'une discipline biologique en crise \autocites[421-434]{Pouvreau2013}. C'est appuyé par la pensée pionnière de quelques scientifiques opérant principalement dans le monde germanique (Allemagne, Autriche) \autocite{Drack2007b}, en Grande-Bretagne et aux Etats-Unis que va se constituer un mouvement de chercheurs porteurs de perspectives holistiques capables de caractériser et de prendre le contre-pied des dérives et des débats jusque là stériles (vitaliste/mécaniciste, darwinisme/lamarckisme, etc.) qui décrédibilisent la biologie à cette période \autocite[153-154]{Pouvreau2013}. Un héritage qui va influencer les travaux de Von Bertalanffy tant sur les aspects philosophiques, que mathématiques. Une discipline mathématique dont il va questionner sa relation avec la biologie \autocite{Pouvreau2005} jusqu'en 1932, date à laquelle il finit par accepter sa nécessité dans l'établissement de son projet de systèmologie générale. Les travaux biomathématiques de cette période sont alors assimilés de façon tout à fait sélective et congruente à son programme organismique, comme tâche de le montrer \textcite[515]{Pouvreau2013} dans sa thèse. Il est intéressant de garder en mémoire pour la suite de notre exploration qu'une partie de cette prise de contact avec les biomathématiques se soit faite par les préoccupations communes, l'amitié et le travail de Woodger avec Bertalanffy \autocite[347,433]{Pouvreau2013}. Woodger est un embryologiste et philosophe anglais qu'il a rencontré en 1926, et avec qui il correspond de façon intense entre 1930 et 1932 \autocite[165]{Pouvreau2013}.

Parmi les différents foyers intégrant ce courant holistique, on s'intéressera donc plus à celui représenté par les membres du \textit{Theoretical Biological Club} (TCL) (connu aussi sous le nom de \textit{Biotheoretical gathering}) opérant de 1932 à 1938, et continuant ensuite après guerre jusqu'en 1952. Co-fondateur de ce club interdisciplinaire, le biologiste philosophe Joseph Henri Woodger a constitué et défendu (avec d'autres, comme Bertalanffy) pour l'époque des anti-thèses importantes dans la constitution d'une biologie théorique. Une importance, qui après de nombreuses critiques, lui est aujourd'hui justement restituée \autocite{Nicholson2013}. Le club regroupe initialement les biochimistes Dorothy et Joseph Needham, la mathématicienne et philosophe Dorothy Wrinch, le physicien cristallographe John Desmond Bernal, l'embryologiste fondateur de l'epigénétique Conrad Hal Waddington; des scientifiques dont l'originalité et la portée des réflexions va rapidement attirer d'autres personnalités importantes, comme Karl Popper, Alfred Tarski et bien d'autres \autocite[14-43]{Niemann2014}. Si le club est effectivement amené à couvrir un large panel de sujets, celui-ci vise collectivement le \foreignquote{english}{[...] development of a ‘mathematico-physico-chemical morphology’ that would enable an interdisciplinary engagement with the problem of biological organization at the supracellular, cellular, and subcellular levels.} \autocite [277]{Nicholson2013}. Toutefois, si l'influence de ce courant anglo-saxon dans le projet de Bertalanffy est notable, celle-ci ne constitue pas la voie unique de ses influences, et sa vision des choses puise dans l'ensemble du champ des biomathématiques de l'époque (Rachevsky, Lotka, etc.) \autocite[574-585]{Pouvreau2013}.

Autre moment important de la biologie théorique, représentative des points de vue hétérodoxe de la biologie moléculaire, est celle de l'embryologiste et généticien Waddington. Dans une forme de continuité de réflexion par rapport au TCL celui-ci organise au début des années 1970 un ensemble de symposium intitulé \enquote{Towards a Theoretical Biology} tous les ans de 1966 à 1969 en Italie à Bellagio. Pour \textcite[512-513]{Nanjundiah2010}, les problématiques soulevées dans les deux premières conférences telles que résumées par \textcite{Waddington1968} sont triples : \foreignquote{english}{There was the high level of complexity of biological systems in terms of both the number of variables that had to be taken into account for describing them and the number of interactions among those variables. Next, the prevailing gene-centred view failed to take into account the fact that genes were as much responders as actors. Third, evolution had to be integrated into any theory of development. One needed to understand organisms and their development by including the workings of genes and the environment in one conceptual whole.}

%Ainsi la notion d'attracteur réaparait sous des formes différentes, tout autant dans la notion d'équifinalité repris et développé par bertalanffy et dont on trouve écho dans les premier travaux de Prigogine (voir Annexe), que dans la métaphore de paysage génétique de Waddington dont la traduction en système dynamique démarre avec Réné Thom (participant des premieres conférences), et se poursuit encore aujourd'hui au travers de nombreux projets.

% Notion d'auto organisation, on la retrouve par exemple dans le cadre de l'embryogenese.

Les conceptions épigénétiques de la morphogenèse de Bertalanffy vues au travers de son second principe organismique\Anote{Pouvreau_secondprincipe}, couplées à celles développées par d'autres embryologistes comme Weiss, Woodger, Waddington - dont on doit entre autre l'origine du mot épigénétique -  forment un cadre de réflexion historique où la trajectoire de la notion d'auto-organisation, bien que partageant dès le départ certaines similarités avec l'angle de vue physico-chimique \autocite{Prigogine1946}, sont d'emblée amenées à être dépassées.

Il ne s'agit pas de nier ici l'importance de ces dernières, car elles fourniront le matériel conceptuel et mathématique nécessaires à l'engagement d'une toute nouvelle réflexion dans d'innombrables disciplines, notamment en géographie (voir \ref{sssec:progressive_systemique}). Il s'agit plutôt ici de traduire leur insuffisance à fournir à elles seules une explication universelle en biologie. Chez Von Bertalanffy, c'est finalement dans l’acceptation \autocite[657-661]{Pouvreau2013} de cette faiblesse dans la partie thermodynamique de sa théorie organismique que se révèle toute la richesse d'une théorie dont les problématiques sous-jacentes à l'articulation des concepts dépassent le seul questionnement de son opérationalisation physico-chimique.

%Ainsi il parait impossible de négliger l'émergence dans les débats sur l'embryogénèse des années 1930 d'un point de vue intégrant tout autant l'importance d'un présuposé matériel génétique, que son interaction avec l'environnement (phenotype). \hl{Ref}

Une acceptation largement partagée par la communauté des biologistes, d'autant plus lorsqu'elle est appuyée par les dires d'un des collaborateurs les plus proches de Prigogine. Le physicien et biologiste Jean-Louis Deneubourg affirme ainsi dans le livre \textit{Self-Organization in Biological Systems} \foreignblockquote{english}[{\cite[12-13]{Camazine2003}}]{The mechanisms of self-organization in biological systems differ from those in physical systems in two basic ways. The first is the greater complexity of the subunits in biological systems. [...] The second difference concerns the nature of the rules governing interactions among system components. In chemical and physical systems, pattern is created through interactions based solely on physical laws. [...] Of course, biological systems obey the laws of physics, but in addition to these laws the physiological and behavioral interactions among the living components are influenced by the genetically controlled properties of the components. In particular, the subunits in biological systems acquire information about the local properties of the system, and behave according to particular genetic programs that have been subjected to natural selection.} 

C'est également le point de vue capturé par \textcite{Mossio2014}. Celui-ci appelle la notion supplémentaire de \enquote{clôture organisationnelle} développé par Piaget pour faire une lecture originale des spécificités du vivant. Biologiste de formation, Piaget s'appuie de façon précoce sur les idées de Waddington, comme on peut le constater dans le chapitre d'ouverture de \textit{Biologie de la connaissance} (1967). Piaget ayant également eu des interactions fortes avec Von Bertalanffy dès 1953 \autocite[310-311]{Pouvreau2013} - notamment dans le cadre plus général du transfert fructueux de sa théorie organismique à la psychologie \autocite[945-951]{Pouvreau2013} - il n'est pas étonnnant de voir Mossio inscrire ce concept comme complémentaire de l'ouverture thermodynamique de Von Bertalanffy.

Tel qu'utilisé par \textcite{Mossio2014} ce concept\Anote{piaget_mossio} fonde un support conceptuel spécifique au vivant sur lequel peuvent se greffer les contributions de Maturana et Varela, ou encore celle de Pattee et Rosen dont les réflexions s'inspirent en partie des travaux de Waddington.

Si on ne peut qu'être d'accord avec la lecture de Mossio établissant l'insuffisance du concept d'auto-organisation au sens thermodynamique des structures dissipatives, la frontière entre ces deux notions de l'auto-organisation apparemment distinctes (auto-poeïse et auto-organisation) apparaissent beaucoup plus floue dès lors qu'on envisage leur mise en perspective avec les discussions opérant depuis plusieurs décennies chez les biologistes organicistes, inspirées en partie du concept d'auto-organisation au sens Kantien \autocites[76-78]{Pouvreau2013} (voir un peu plus bas dans le texte la note de bas de page citant l'échange avec Pouvreau sur cette question)

En ce qui concerne l'influence de Bertalanffy sur les discussions de la notion au BCL, on peut considérer que malgré l'absence de communication lors de la conférence sur l'auto-organisation en 1960, sa présence suffit en quelque sorte à établir l'importance de son point de vue sur cette notion. Une autre influence de ce dernier, plus indirecte, passe par la présence d'Ashby au BCL. En effet pour \autocite[791]{Pouvreau2013} le cybernéticien Ashby est un homme singulier non seulement par la nature précoce de ses questionnements (1940) et des réalisations mises en œuvre (1948) pour étudier les comportements adaptatifs, mais également par les échanges et la médiation que ces travaux ont permis d'enclencher entre le point de vue cybernétique et l'évolution du projet systémique tel qu'entamé par Bertalanffy depuis sa théorie organismique. Alors même que les premiers contacts de celui-ci avec les écrits de Bertalanffy datent au moins de 1952, \textcite[793]{Pouvreau2013} tend à montrer que malgré des désaccords de façade, il existe dans la comparaison de leur travaux d'étonnantes accointances. Sachant cela, l'\enquote{impossibilité d'une auto-organisation} telle qu'évoquée par \textcite{Ashby1962}\Anote{ordre_desordre} dans le cadre des conférences du BCL est alors d'autant plus évocatrice de l'influence implicite des travaux de Von Bertalanffy que cette réflexion d'Ashby va être considérée par Foerster au sein du BCL. A ce constat, il ne faut pas oublier d'ajouter que pendant une large partie de sa présence au BCL Ashby est également président (1962-1965) de la \textit{Society for General Systems Research} (SGSR) entre autre fondée par Von Bertalanffy \autocite[826]{Pouvreau2013}.

Sachant l'accrochage dès 1948 de Weiss et Culloch à Hixon, la proximité de Maturana avec McCulloch puis Foerster, la présence de von Bertalanffy à la conférence de 1960 du BCL, et la présence que l'on suppose marquante d'Ashby au BCL entre 1961 et 1970, il semble légitime de questionner quels transferts peuvent être établis entre les principes au cœur de la théorie \enquote{organismique} représentée ici par Von Bertalanffy et la formalisation \textit{a posteriori} du concept d'auto-poeïse. Or en dehors des influences fortes et réciproques constatées entre Von Foerster et Maturana \autocites{Muller2007b}[255-273]{CREA1985}, ce dernier reste relativement discret sur les références qui ont pu guider sa réflexion en tant que biologiste (un fait largement reconnu par ailleurs \autocite[161]{Pangaro2007}). Maturana et Varela font également face dans la formation initiale du concept à une difficulté de formalisation assumée (Von Foerster leur viendra en aide) \autocite[258-263]{CREA1985} dont on trouve encore trace dans les contours difficiles à cerner qu'est la notion d'auto-poeïse\Anote{etude_pouvreau_mossio}\Anote{piquant_weiss}.

Ces hypothèses et ces analyses sont plus le résultat d'une lecture de \enquote{seconde main} que d'un travail historiographique dédié. Le lecteur pourra sur ce point se référer aux publications passionantes de Pouvreau, Drack, Mossio, Moreno Bergareche \autocites{Pouvreau2006, Pouvreau2013, Drack2015, Mossio2010, Mossio2014, Bergareche2015} mais également au livret édité en 1985 par le CREA, déjà plusieurs fois cité. (voir également l'annexe \ref{ssubsec:cybernetic})

% A retravailler avec les remarques de Pouvreau...
% Retour de la biologie systémiste Braillard2008
%On y retrouve également l'influence de concept propre au paradigme systémique partant de la seconde cybernétique, dont on peut ancré tout ou partie des concepts initiaux dans l'étude du vivant tel que ceux mené dans les années 1950 par Von Bertalanffy (théorie organismique \autocite{Pouvreau2013}) ayant inspiré par la suite les travaux de Varela (auto-poeise \autocite{Varela1979?})\Anote{varela_modele_ca}, que l'influence des multiples travaux informatiques mimant les processus évolutif décrit par la théorie darwiniste.

\printbibliography[heading=subbibliography]



\chapter{Le double foyer d'apparition des SMA en SHS}
\label{chap:double_foyer_sma}

\section{Les principaux initiateurs de la simulation Agent pour les SHS en Europe}
\label{s:communautes_europe}

%http://books.google.fr/books?id=2YJTAQAAQBAJ&pg=PT326&lpg=PT326&dq=james+doran+1982+archaeology&source=bl&ots=04tyzJ0HoM&sig=T_OpaK1gtQVjlJv-R4qPG0GHUmk&hl=fr&sa=X&ei=aNARVOaVOMSWauXwgeAO&ved=0CCwQ6AEwAQ#v=onepage&q&f=false

% SMALLTALK premier SIMPOP, deuxième grand moments pour les sciences urbains (Sanders2013); trouve une réponse encore plus adapté au concept

En Europe, l'ingénieur et sociologue Nigel Gilbert fait partie de ces personnalités qui ont oeuvré très largement pour la diffusion et la vulgarisation de la modélisation multi-agents (\textit{Agent Based Model}) en sociologie, mais également en sciences sociales dans la communauté internationale\Anote{gilbert_date_clef}.

En 1985, il participe et édite le recueil de papier tiré de la conférence \foreignquote{english}{Social Actions and Artificial Intelligence} qui s'est tenue à Surrey en 1984 \autocite{Gilbert1985}. De cette confrontation de points de vue entre chercheurs en intelligence artificielle et sociologues, on retiendra surtout l'article \textit{The computational approach to knowledge, communication and structure in multi-actor systems} de James Doran \autocite{Doran1985}\Anote{doran_85_DAI}, un informaticien de l'université ESSEX (depuis 1973) formé par Donald Mitchie, déjà très actif dans la communauté des archéologues\Anote{doran_archeologie} durant les années 1970 (voir la section \ref{ssec:engouement_sciencesociale}). 

Suite à cette rencontre\Anote{doran_note} une collaboration s'établira sur le long terme entre Doran et Gilbert; une façon aussi de rappeler que Gilbert s'est par la suite largement appuyé pour ses développements théoriques sur l'émergence du projet EOS (\textit{Emergence of Organised Society}) dirigé James Doran et Mike Palmer, un autre informaticien spécialisé en archéologie \autocite{Doran1994a, Gilbert1995a}. Car si Nigel Gilbert se dit impliqué dans ce projet depuis sa création, il avoue lui même ne pas être le principal réalisateur du projet\Anote{gilbert_EOS} \autocite[122-131]{Gilbert1995a}. La première publication évoquant de façon implicite le futur projet \foreignquote{english}{EOS} date de 1982 \autocite{Doran1982} et paraît dans l'ouvrage collectif publié par \textcite{Renfrew1982}.

Comme on a déjà pu le constater pour les géographes dans la section \ref{sssec:progressive_systemique}, les archéologues sont sensibilisés depuis les années 1970 aux possibilités de formalisations offertes par la systémique (section \ref{ssec:engouement_sciencesociale}). Les années 1980 concèdent l'accès à de nouveaux concepts pour penser et explorer la complexité, au travers d'une mise en application de la dynamique des systèmes commencée avec Forrester \autocite{Forrester1961}, et étendue depuis aux regards de nouvelles découvertes, redécouvertes et applications nouvelles des concepts relatifs aux mathématiques des systèmes dynamiques. La publication côte à côte de \textcite{Doran1982} et \textcite{Allen1982} dans l'ouvrage déjà cité de \textcite{Renfrew1982}\Anote{rencontre_renfrew} introduisant ces concepts aux archéologues montre que cette petite communauté d'archéologues modélisateurs ne se contente pas d'explorer la seule voie mathématique de la dynamique des systèmes pour construire des modèles dynamiques, mais abordent également les prémisses prometteuses\Anote{renfrew_futur_archeology} offertes par un futur usage des DAI (\textit{Distributed Artificial Intelligence}), comme en témoignent certains passages de \textcite{Doran1982}\Anote{doran_82_DAI} et \textcite{Doran1986b}\Anote{doran_86_DAI}.

Ainsi, presque douze ans après sa publication de 1970 \autocite{Doran1970}, déjà visionnaire par les descriptions de simulations qui y sont imaginées\Anote{description_imagine_simulation}, Doran se retrouve une deuxième fois avec ses collègues en position de pionnier avec la mise en œuvre des toutes dernières techniques de l'intelligence artificielle distribuée\Anote{doran_DAI} pour l'archéologie\Anote{doran1982_reclamation}, mais également en sociologie \autocite{Doran1985}\Anote{note_bond_liens}. 

James Doran tient depuis longtemps son inspiration principale de la simulation (au sens algorithmique et pas mathématique), de l'intelligence artificielle, et des DAI (\textit{Distributed Artificial Intelligence}) dès lors que cette sous-branche de l'IA émerge à la fin des années 1970. S'il n'est pas possible de fournir ici un éclairage exhaustif sur les enracinements multiples de cette discipline, on propose dans le paragraphe suivant quelques notes autour des travaux et de la figure importante de Carl Hewitt. Pour les experts du domaine comme Wooldridge\Anote{inspiration_wooldridge} et Ferber\Anote{inspiration_ferber} les travaux de Carl Hewitt semblent en effet jouer un grand rôle dans l'histoire dans la formation du paradigme multi-agent.


%\hl{Reintroduire rapidement la référence à la simulation en sociologie, et le rapprochement initial qui peut être fait avec l'héritage systémique opéré en sociologie, voir \ref{sssec:progressive_systemique}}

% Comme le dit Sanders2013 il est fort probable que comme en géographie, les outils ne fassent que rejoindre des concepts déjà bien intégrés.

% L'objectif affiché ici par Doran et son équipe est très clair, il s'agit de tester si les théories développés en inteligence artificielle distribué peuvent être transferable à un modèle archéologique au préalable déjà formalisé par Paul Mellars en 1985 {Mellars1985}.

% Si l'on se tient aux définitions donnés par Jacques Ferber quand à la nature des agents, soit «cognitifs», soit «réactifs» il semblerait que se découpe déjé une délimitation nette dans les modèles apparaissant dans ce premier et ce deuxième ouvrage. Nigel Gilbert et James Doran utilise par exemple des agents cognitifs pour leur plateforme EOS, alors que MANTA est un modèle qui tente de reproduires le fonctionnement d'une fourmillière en utilisant des agents réactifs.

Carl Hewitt et plusieurs de ces collègues développent dès les années 1970 des travaux innovants qui vont inspirer par la suite les futures recherches en DAI et sur les systèmes multi-agents \autocite{Ferber1995}.

Dès le départ les initiateurs de l'intelligence artificielle distribuée se sont tournés vers l'analyse des phénomènes sociaux existants pour formuler une forme d'intelligence distribuée à même de résoudre des problèmes complexes\Anote{hewitt_metaphore_sociale}. Les premiers systèmes nommés \textit{blackboard system}\Anote{blackboard} souffrent très vite de plusieurs limitations importantes. La présence d'une ressource partagée centralisée, le tableau, est un goulot d'étranglement qui pousse les chercheurs à envisager une autre forme de parallélisme \autocite{Wooldridge2009}.

En 1971 Carl Hewitt obtient son doctorat pour son implication dans la construction du système de démonstration de théorèmes \textit{PLANNER}. Ce langage est largement inspiré des méthodes dites de \textit{blackboard system}, qui s'appuient sur une analogie avec une société d'experts pour l'analyse et la résolution de problèmes complexes. Mais c'est à la suite de son travail au MIT sur SMALLTALK\Anote{inspiration_double_small} que naît le formalisme \enquote{Acteur}\Anote{acteur_definition_ferber} qui va être repris et opérationnalisé par la suite dans de nombreux autres travaux\Anote{futur_histoire_acteur}. Les chercheurs œuvrant dans le cadre des systèmes multi-agents, une des branches composantes des DAI (\textit{Distributed Artificial Intelligence}), s’appuieront ensuite largement sur cette frontière très mince entre les notions d'acteurs et d'agents pour appliquer des versions plus ou moins dérivées de ces protocoles d'échanges de messages dans le cadre de plateforme (ou \textit{Testbeds}). Pour ces dernières on retiendra les très connues et influentes plateformes MACE (\textit{Multi-Agent Computing Environment}) développée à l'\textit{University of Southern California}\Anote{mace_systeme}, ou DVMT (\textit{Distributed Vehicle Monitoring Testbed}) développée à l'\textit{University of Massachusetts}, intégrant un système formalisé pour l'échange d'information structurées entre entités expertes autonomes.

On trouve un historique et une description des influences sur l'IAD beaucoup plus complète dans l'article de \textcite{Bond1988} couvrant la période de recherche jusqu'au année 1990, et de façon plus générale dans les ouvrages de \textcite{Wooldridge2009} et \textcite{Ferber1995}. 

L'approche multi-agents s'inspire également d'une autre branche de recherches, la Vie Artificielle (ou \textit{Artificial Life}) \autocite[28]{Ferber1995}. Cette méta-discipline qui regroupe une multiplicité de domaines de recherche apparaît comme centrale dans la constitution d'un second foyer important pour les SMA en SHS.

% LAPPROCHE MULTI AGENT ACTUELLE SE NOURRIT A LA FOIS DE L'IAD ET DE LA VA (voir page 28 de Ferber) Il est intéressant de voir au travers des deux foyers initiaux américains et européen l'influence plus ou moins prononcé de l'une ou de l'autre approche, tout en suivant le meme objectif, l'émergence. Alors que le pole gilbert, conte, doran est plus orienté vers la mise en oeuvre d'agent cognitif traditionel en IAD,  Epstein et Axtell qui s'inspirent avant tout de ce qui est fait au Santa Fe Institute en terme de vie artificielle.

% Evidemment dans les fait, les deux approches cognitiviste et réactive, sont représentés dans les ouvrages, et partage finalement ce socle commun.

%L'approche KISS a tendance à favoriser l'émergence de modèle agent plutot reactif, les approches cognitivistes mobilisant d'emblée beaucoup plus d'expertise. Le débat de façon générale dans les SMA s'est transmis à la modélisation orienté agent.

% PROFITER APRES CETTE INTRODUCTION POUR INTRODUIRE LE FAIT QUE LA MISE EN OEUVRE (PAR QUI? , COMMENT ? ) DU PROTOCOLE DE CONSTRUCTION JOUE DANS LA VALIDATION, NE SERAIT CE QUE PAR LA PERCEPTION QUI EST FAIT DES OBJETS MANIPULÉ. UNE REVELATION FAITE ÉGALEMENT PAR DROGOUL2003 QUI SOULEVE LA PROBLEMATIQUE DE LA MODELISATION AGENT, entre concept et implémentation. MAIS DONT ON TROUVE ÉGALEMENT EN GEOGRAPHIE LE TEMOIGNAGE DE GLISSE.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Le deuxième foyer américain, et l'inspiration majeure d'une nouvelle discipline, la \enquote{vie artificielle}}
\label{sec:vie_artificielle}

Le terme d'\foreignquote{english}{Artificial Societies} qui consacre les usages alors naissant des modèles individu centré dans la discipline aurait été selon \textcite{Gilbert2000a} plus ou moins inventé en même temps en Europe et aux Etats-Unis, cela de façon indépendante à la fois par Epstein en 1996 et Gilbert and Conte en 1995.\Anote{gilbert_confidence}. Mais il est intéressant de voir que derrière un terme et des objectifs finalement similaires (produire des expériences \textit{in silico} , mettre en oeuvre le concept d'émergence), les motivations et les sources d'inspirations mises en avant diffèrent légèrement. Là où l'expertise de Doran et de Gilbert s'appuie sur cette triple compétence mêlant intelligence artificielle, question théorique en sociologie et ancrage archéologique,  \textcite[17-19]{Epstein1996} révèle une approche plus abstraite de ces notions (émergence, auto-organisation, etc.) au travers de \textit{Sugarscape}, un modèle de simulation inspiré par le domaine de l'\textit{Artificial Life} ou Vie Artificielle (VA), un domaine de recherche alors très actif au \textit{Santa Fe Institute} (SFI).

%Doran and Gilbert (1994) argue that computer simulation is an appropriate methodology whenever a social phenomenon is not directly accessible, either because it no longer exists (as in archaeological studies) or because its structure or the effects of its structure, i.e. its behaviour, are so complex that the observer cannot directly attain a clear picture ofwhat is going on (as in some studies of world politics). The simulation is based on a model constructed by the researcher that is more observable than the target phenomenon itself. This raises issues immediately about which aspects of the target ought to be modelled, how the model might be validated and so on. However, these issues are not so much of an epistemological stumbling block as they might appear. Once the process of modelling has been accomplished, the model achieves a substantial degree of autonomy. It is an entity in the world and, as much as any other entity, it is worthy of investigation. Models are not only necessary instruments for research, they are themselves also legitimate objects of enquiry. Such “artificial societies” and their value in theorizing will be the concern of the first part of this chapter.

Joshua Epstein et Robert Axtell se sont rencontrés au \textit{think tank} de \textit{Brookings} en 1992. C'est peu de temps après, lors d'une conférence sur la \enquote{Vie Artificielle} (VA) au \foreignquote{english}{Santa-Fe Institute} (SFI), qu'ils trouvent l'inspiration pour la réalisation du modèle de simulation \textit{SugarScape}\Anote{histoire_sugarscape}. Un travail qui donne lieu à l'ouvrage \textit{Growing Artificial Societies: Social Science from the Bottom Up}, ce dernier réunissant différentes expérimentations autour de variations du modèle de simulation original.

% Artificial Social Life (ASL) Epstein / Axtell

Le SFI est centre de recherche interdisciplinaire indépendant ouvert en 1984 au Nouveau Mexique, principalement dédié à l'étude de la complexité au travers des \textit{Complex Adaptative System} (CAS) sous toutes leurs formes : physiques, biologiques, sociaux, etc. Un des axes de développement important à SFI durant la fin des années 1980 tient dans l'émergence (en réalité la ré-émergence) du concept de Vie Artificielle sous l'impulsion principale de Christopher Langton, l'inventeur du terme. Cette acceptation permet tout à la fois de regrouper et de rendre visible sous une bannière identifiable les travaux de plusieurs décennies de recherches dans différentes disciplines (mathématique, informatique, robotique, biologie, écologie, etc.)  \autocite{Taylor1999}. Il en ressort également une forme de questionnement commun autour du concept de \enquote{Vie} lorsqu'il est appliqué à un environnement \enquote{informatique}.

Attention toutefois à ne pas voir le Santa Fe institute comme le lieu de création \textit{ex-nihilo} des concepts sous-jacents aux \textit{Complex Adaptative System} et de la nouvelle discipline de l'\textit{Artificial Life} de Langton. En effet, les problématiques et les discussions abordées dans ces \enquote{nouvelles disciplines} puisent matière dans les riches échanges interdisciplinaires datant du début et milieu du XXième siècle, cela avant bien avant que le SFI ne sorte de terre au Nouveau Mexique en 1984.


%\hl{Années d'or 1977}

En France par exemple, les travaux sur la Cybernétique sont déjà observés de près depuis les années 1950 par le polytechnicien Robert Vallée et ses collègues dans le cadre du \enquote{Cercle d’études cybernétiques} \autocites{Bricage1990,LeRoux2015}. \textcite[404]{Mounier2010} parle même d'une \enquote{vague de création associatives}, dont il reste malheureusement peu de traces.

Le livre de Wiener \textit{Cybernetics or Control and Communication in the Animal and the Machine} est publié en Français en 1948 et l'ouvrage \enquote{Les problèmes de la vie}\Anote{pouvreau_livre1949} qui consacre le travail de Bertalanffy démarré dans les années 1930 paraît en allemand en 1949, en anglais en 1952, et la première traduction francaise date de 1961 \autocite{Vallee2005}. 

% Marois1971 et Marois1969
Autre événément important dans l'histoire du rapprochement entre discipline, c'est à l'Institut de la Vie fondé en 1960 à Versailles et voulu par Maurice Marois \autocites{Marois1969,Marois1971} que se réunissent en 1967 des chercheurs de tous horizons pour une première grande conférence internationale de physique théorique et de biologie. Première d'une longue lignée, celle-ci est ouverte par le zoologiste et président de l'académie des sciences Pierre-Paul Grassé (inventeur entre autre du terme \textit{stigmergie} \autocite{Theraulaz1999}), alors entouré d'un comité scientifique non moins prestigieux : P.Auger, A. Fessard, H.Frolich, A.Lichnérowicz, I.Prigogine, L.Rosenfeld. 

Des conférences qui vont se poursuivre à Versailles jusqu'en 1973, puis à Edimbourg par la suite, avec cette volonté toujours renouvelée de défricher toutes les passerelles plausibles qui constituent le lien entre physique et biologie autour de cette thématique universelle \enquote{Qu'est-ce-que la vie ?}. Parmi les participants réguliers on retrouve Ilya Prigogine, mais également Hermann Haken. Ce dernier, déjà présent lors des premières conférences en 1967, sera amené dans un futur proche à porter le concept de \enquote{Synergétique} en tant qu'orateur en 1971 \autocites{Kroger2012, Kroger2015}. De son coté,  Prigogine est amené à introduire le concept des \enquote{structures dissipatives} bien plus tôt, dès les premières conférences \autocite[60]{Stengers1985}.

Une inspiration qui se poursuit dans les 1970-80 avec l'introduction de ces nouveaux concepts dans une communauté qui revient avec enthousiasme sur les fondements de ce projet cybernétique, puis systémique : \textcite{Morin1974}, Le Moigne \textcite{Lemoigne1977}, \textcites{Dumouchel1983,CREA1985b}, etc. L'année 1977 est souvent qualifiée d'\textit{Annus mirabilis} par ces auteurs, car elle est marquée par la sortie de nombreux ouvrages majeurs. Le mouvement est également structuré autour de réseau comme l'AFCET (devenu depuis 1999 AFSCET) qui coordonne depuis sa création en 1968 \autocite{Hoffsaes1990} les réflexions de centaines de chercheurs et ingénieurs autour de groupes de travail interdisciplinaires, de publications, de conférences internationales. En réalité, comme nous le disent \textcites{Hoffsaes1990}[404-420]{Mounier2010} ce réseau est d'inscription plus ancienne, l'AFCET est déjà en 1968 une réunification de différentes associations créées en 1956-1957 (AFCAL, SoFro, AFRA, etc.). Plusieurs événements majeurs ont lieu sous l'égide de ces réseaux autour de l'auto-organisation (congrès de Versailles par exemple : \autocite{Prigogine1977}) au début des années 1980, comme le colloque de Cerisy organisé en 1981 intitulé \enquote{L'auto-organisation: De la physique au politique} \autocite{Dumouchel1983}, ou la conférence de 1982 à Bruxelle sponsorisé par l'AFCET-SOGESCI et organisé par Bernard Paulré, le point culminant d'une série de conférences démarrées à Toulouse en 1975 sur les Systèmes Dynamiques \autocite{Andersen2007}.

%\hl{retravail AVEC CITATION DE l'annexe A}
% Présence de Deneubourg à LOS ALAMOS... retrouver la ref
%\hl{Travail de Deneubourg (sur les deux plans), Brooks (retour au subsymbolisme) à intégrer ici ?!}

On comprendra avec ce bref éclairage sur quelques événements Français (on trouve aussi plus de détails sur l'auto-organisation dans ses ramifications européennes dans l'annexe \ref{sssec:heritage_complexe}) les quelques grincements de dents des Européens \autocites{Varela1995} lorsqu'il s'agit d'évoquer l'origine des CAS et de la notion (trop?) computationalisée de \textit{ALife}, qui bénéficie d'une couverture médiatique et institutionelle importante, dans la pure tradition des financements américains\Anote{helmreich_IA}. Des concepts ou variantes de l'auto-organisation ont en effet été introduits et opérationalisés, en CA (\textit{Cellular Automata}) ou SD (\textit{Systems Dynamics}) assez tôt dans un grand nombre de disciplines, que cela soit en biologie \autocite{Varela1974, Pattee2001, Hogeweg1978}\Anote{varela_modele_ca}\Anote{biologie_pattee_ca}, en écologie \autocites{Hogeweg1979, Hogeweg1981, Hogeweg1983}, en organisation des sociétés animales \autocites{Prigogine1977, Deneubourg1976, Deneubourg1977}, en géographie (\autocite{Pumain2003}), en archéologie (\autocite{Renfrew1982}) %, en anthropologie (\autocite{Adams1988}), etc.

%Citation du livre Handbook of archeological method
%Edited by Herbert D.G.Maschner et Christopher Chippindale
%2005
%One of the key insights claimed for CAS structures is their ability to self-organize (Holland 1992 / Kauffman 1993)
%Despite the implication from Americanist litterature that self-organized phenomena are a recent product of CAS research at Santa-Fe (Gumerman and Gell-Man 1994, Kauffman 1995) , it need to be remenbered that the paradigm of self-organization  has a somewhat longer history mainly because of the work of Ilya Prigogine on nonlinear dynamics and dissipative structures (Nicolis And Prygogine 1977, Prygogyne 1978 1980)
%In fact the paradigme was first introduced to an archeological audience a decade ago by Prigogine's colleague Peter Allen(1982a 1982b) and to Anthropology by Adams (1988) !

Conscient maintenant du recul historique nécessaire pour évaluer à leur juste valeur les travaux initiés au SFI dans les années 1980, on peut évoquer les racines historiques de l'outil qui a servi de support principal à ces principaux développements : l'automate cellulaire AC (ou \textit{Cellular automata} CA\Anote{ancien_survey}). Ainsi à ce titre, et en parallèle des développements mathématiques abordant l'auto-organisation sous l'angle de la thermodynamique\Anote{liaison_prigogine_foerster}, l'automate cellulaire s'est avéré très tôt comme l'outil idéal capable d'intégrer ces influences multiples, grâce notamment aux très nombreuses propriétés que ce type de formalisme continue d'exposer \autocite{Ganguly2003}. %classification des automates cellulaires de Wolfram, Temps discret etat de Zeigler 1976

Parmi les différentes propriétés qu'il est possible d'étudier dans les automates cellulaires, on retiendra pour l'étude de la VA\Anote{publications_va} la réplication et la reproduction\Anote{taylor_reproduction} d'entités autonomes évoluant dans un environnement ouvert. Une branche de recherche sur l'évolution que l'on retrouve dans la VA sous le terme d'\textit{Open-Ended Evolution (OEE)} définis par Taylor\Anote{taylor_openended}.

Sans rentrer plus en avant dans les subtilités qu'amène une telle définition, on observe pour ces différents concepts des publications marquantes, inspirées le plus souvent des travaux initiaux de Von Neumann et Ulmman, mais aussi : les travaux très concrets et souvent oubliés \autocites[111-130]{Dyson1997}{Fogel1998, Taylor1999, Hackett2014} du mathématicien et biologiste Italo-Norvégien Nils Aall Barricelli (1957) (la notion de \foreignquote{english}{symbioorganism}), la proposition d'automate cellulaire évolutionnaire pour l'auto-organisation du cybernéticien du BCL Gordon Pask \autocite{Pask1961}, le jeu de la vie de Conway, les travaux en biologie de \textcites{Pattee2001,Pattee2002, Conrad1970} et de l'équipe du CCS de Burks (proche de Von Neuman) \autocite{Weinberg1969}, les $\alpha$-Universes développées par Holland \autocites{Holland1976, Mcmullin1991}, les travaux sur la morphogénèse de \textcite{Hogeweg1978}, la boucle reproductible contenant du matériel génétique de \textcite{Langton1984}, les automates cellulaires illustrant la théorie de l'auto-poièse au croisement des travaux de Maturana, Varela et Von Foerster \autocites{Varela1974, McMullin1997b, McMullin1997, McMullin2004, CREA1985}, etc.

De façon encore plus générale, la VA va s'appuyer sur cette large classe d'algorithmes inspirée par la biologie (\textit{Biological computing}). Ainsi et dans la continuité des travaux évoqués au dessus, la VA va utiliser pour la mise en œuvre des aspects évolutionnaires de ces programmes des travaux regroupés sous le terme générique de \textit{Evolutionary Computation} (EC) \autocites{Back1997, Fogel1998, Fogel2006a}. Une sous classe de techniques issues de l'Intelligence Arficielle principalement inspirées des mécanismes d'évolution biologique, eux-même subdivisés en différentes familles (\textit{Genetic Algorithm (GA), Genetic Programming (GP), Particle Swarm Optimization (PSO), Ant Colony Optimization (ACO)}, etc.) parfois difficiles à distinguer. Ils peuvent être appliqués à différentes classes de problèmes, et ne relèvent pas forcément d'une fonction fitness explicite : évolution de comportement, évolution de forme, évolution artistique, etc. Tout dépend donc à quelle échelle\Anote{echelle_optimization} ont considère le problème d'optimisation; l'individu amené à être évalué peut tout à la fois dénoter une entité virtuelle autonome évoluant dans un environnement comme un robot dans une simulation, ou les paramètres d'un modèle de simulation, ou encore un sous ensemble de fonctions mathématiques dans un polynôme.

%Ecologie + GA Hamblin2013

Mais elle s'inspire également de ce que l'on peut considérer comme le chemin inverse (Computational Biology) qui constitue à simuler le vivant en s'appuyant sur l'informatique \autocite{Ermentrout1993}, ce qui peut inclure l'emploi de technique évolutionnaire, les aller retour entre les deux approches (\textit{Biological computing} et \textit{Computational Biology}) étant bien établis \autocites{Giavitto2002, Hogeweg1992, Hogeweg2011}\Anote{terme_bioinformatique}

Ainsi on constate par exemple en biologie la similarité \autocites{ Hogeweg1974, Stauffer1998} des automates cellulaires (issus au départ d'une analogie avec le vivant) et des \textit{L-System} \autocite{Prusinkiewicz1999} de Lindenmayer (1971), également étudiés et mis en application dans des simulations utilisant des automates cellulaires \autocites{Hogeweg1978}.

En effet, Arthur Walter Burks (mathématicien, physicien, philosophe), bien connu pour son travail sur ENIAC ( \textit{Electronic Numerical Integrator and Computer} ) et sa collaboration fructueuse avec Von Neumann sur de nombreux sujets. C'est lui qui va achevé en 1966 l'ouvrage de Von Neumann \textit{Theory of Self-Reproducing Automata}, soit presque 10 ans après sa mort. Etabli à l'université du Michigan, il crée en 1949 le \textit{Logic of Computers Group} rattaché au département de philosophie, un fait pas si étonnant car Burks a disserté en 1941 sur les fondations logiques du scientifique et philosophe Charles Sanders Peirce (\textit{The Logical Foundations of the Philosophy of Charles Sanders Peirce}). Après rapprochement avec le département de linguistique de Peterson, un comité est formé et devient capable de délivrer des diplôme dès 1957. Fait d'enseignement interdisciplinaire délivré dans chacun des départements respectifs, le \textit{Computer \& Communications Sciences}\Anote{nature_ccs} passe de programme à département en 1965. Dédié à l'étude du \textit{computing} et d'orientation interdisciplinaire, ce département va former de nombreuses figures connues de l'informatique et de la simulation. Si on en croit la base de données de \textit{Mathematics Genealogy Project} Burks n'a eu que peu d'élèves, John Holland en 1959, premier professeur du CCS (qui a encadré par la suite plus de deux cents chercheurs), probablement un des premiers \textit{phd} en informatique, et Christopher Langton en 1991. Dans les années 1980, Burks fera partie du groupe interdisciplinaire nommé \href{http://www.lsa.umich.edu/cscs/aboutus/bachgroup}{@BACH}, réunissant Arthur Burks, Bob Axelrod, Michael Cohen et John Holland. Un groupe qui préfigure le futur CSCS de Michan en 1999. Ce département et ce groupe d'universitaires en pointe sur les CA n'est pas ignorant des dernières recherches sur l'auto-organisation. Burks par exemple participe et publie aux conférences \textit{Self Organizing Systems} du BCL dès 1959 \autocite{Yovits1960}. Le département interdisciplinaire accueille également tout autant des travaux misant sur le transfert de la biologie à l'informatique \autocite{Holland1976}, que l'inverse, avec par exemple les travaux de \textcites{Weinberg1969, Weinberg1970, Weinberg1971} pour simuler la croissance dans un environnement chimique réaliste de bactéries unicellulaires.

Le travail du biophysicien Howard H. Pattee est également assez intéressant à étudier, car il se situe à la croisée de multiples influences : en physique, en biologie, en informatique. Il est très fortement influencé par les réflexions biologiques/informatiques de Von Neumann\Anote{neumann_biologie} et biologiques/embryologiques de Waddington \autocite{Pattee2001, Umerez2001}. Comme on va pouvoir le constater au travers des différentes citations, les réflexions de Pattee sont encore aujourd'hui largement entretenues et discutées tout autant par les philosophes biologistes que les informaticiens développant des programmes de Vie Artificielle. 

Pendant cette décennie 1970 Pattee construit à la fois des programmes informatiques avec Conrad \autocites{Conrad1970,Pattee2002} et développe de nouvelles réflexions en biologie physique \autocite{Pattee2001}. En effet il participe à plusieurs conférences en biologie, dont les quatre conférences \autocite{Umerez2001} organisées par l'embryologiste Waddington\Anote{ouvrage_waddington}.
\textcites{Mossio2009, Mossio2010} par exemple le positionnent comme un des pionniers (avec Varela, Rosen, etc.) ayant abordé cette problématique de la \enquote{clôture organisationnelle} en biologie.

Pour la VA, mais aussi pour les écologistes, il est plus connu comme étant l'initiateur avec son disciple informaticien et biophysicien Conrad d'une première expérience de simulation d'un d'écosystème biologique évolutif artificiel (sans fitness explicite)\Anote{conrad_explanation}. Nommé EVOLVE, ce programme\Anote{conrad_abstract} voit sa première version datée de 1970 \autocites{Conrad1970, Pattee2002}. Il apparaît également comme un penseur critique indispensable dans ce puzzle interdisciplinaire réuni en 1987 par Langton, en posant déjà un certain nombre de questions essentielles qu'il tire d'une réflexion qu'il a lui même démarrée comme plusieurs de ses collègues physiciens dans le courant des années 1960 \autocites{Pattee1987, Pattee1995, Pattee2001}. Les résultats des premières expériences\Anote{conrad_model} l'amènent à évoquer sous un jour à peine déguisé, des problématiques classiques se rapportant à la validation\Anote{patte_deception}, évoquant au travers du substrat support de la simulation la question délicate du rapport entre univers simulé et réalité. Les formes décevantes de comportements chaotiques observées dans ses premières simulations avec Conrad l'ont amené à penser qu'il était nécessaire de pousser non pas tant le réalisme que la cohérence de l'univers physique simulé, condition \textit{sine qua non} pour dépasser cette limitation.

Sa vision de l'\textit{Artificial Life} \autocite{Pattee1995} et sa réflexion ouverte sur les problématiques biologiques en fait avec Von Neumann\Anote{vonneuman_openended} un des pionniers de l'\textit{Open-Ended evolution} telle que définie ainsi par \autocites{Taylor1999,Taylor2012}. Tout un groupe de philosophes, et d'informaticiens analyse ou se retrouve actuellement dans ses multiples travaux (Mossio, Umerez, Taylor, Moreno Bergareche, etc.)\Anote{numero_special} Tim Taylor par exemple a la volonté d'analyser et de construire des simulateurs capables de répondre aux problématiques posées en amont par Pattee et Waddington \autocite{Taylor1999}.

%C'est dans ce cadre qu'apparait un lien de filiation faisant écho avec les questionnements ultérieurs posés par la Vie Artificielle, la question de la détermination d'un organisme vivant par la seule reproduction, replication de code génétique en dehors de tout modèle physique ou chimique comme cela a été le cas dans un certain nombre de modèle de simulation n'abordant en réalité que la moitié du problème, laissant en partie de coté les problématiques d'autodétermination, et de l'évolution caractéristique du vivant \autocite{Mossio}.

Avec le retour de la biologie systémique sur le devant de la scène \autocites{Mossio2014b, Braillard2008}, son principal biographe \textcite{Umerez2009} révèle ainsi comment la toute nouvelle discipline biologique de la \enquote{biosémiotique} trouve écho pour ses problématiques dans les travaux de Pattee. Un point étonnant, car lui-même avoue dans une réponse à Umerez \autocite{Pattee2009} qu'il n'avait jusqu'alors que peu de connaissances de ce domaine et de sa trajectoire historique, dont il fait partie maintenant. 

Pourrait-on dire que Pattee se situe au croisement de l'auto-organisation vue par les embryologistes, les physiciens, et les informaticiens ? Même si ce travail de réflexion est encore en cours, des auteurs comme \textcite{Bergareche2015} développe de premières réponses. Ils considèrent en effet les travaux de Pattee et Rosen comme des piliers importants dans la construction du concept d'autonomie en biologie, inscrite dans la lignée \enquote{organiciste kantienne}\Anote{autonomie}. Des recherches qu'il faut aussi mettre en relation des travaux en cours sur l'étude du BCL et de Bertalanffy \autocite{Pouvreau2013} sur des concepts proches de la \enquote{clôture organisationnelle}, comme décrite dans l'annexe \ref{sssec:heritage_complexe}.



%1957
%Nils Aall Barricelli.
%Symbiogenetic evolution processes realized by artificial methods.
%Methodos, 9(35-36), 1957.
%
%Dyson1998
% “symbioorganism” defined as a “self-reproducing structure constructed
%by symbiotic association of several self-reproducing entities of any kind”


% 1976 AFCET RECENSEMENT DS : J.F Le Maitre( 2 projets sur ibm 360),P. Uvietta dès juin 1975 (probablement amoral, ibm 360), Ch. alexandre (démarre en 73)

% ANNUS MIRABILIS pour la self organization avec la publication de multiples ouvrages (voir ce que dit jean louis lemoigne + Pelster)
% Prigogine1977 AFCET versailles => Urbain est présenté par prigogine, allen ,etc. A mettre en parallele avec la liste donné pour SD par Karsky

% symbioorganism self-reproducing structure constructed by symbiotic association of several self-reproducing entities of any kind

%VENUS : http://www.cs.cmu.edu/Groups/AI/html/faqs/ai/genetic/part3/faq-doc-4.html
%rasmussen : http://www.scoop.co.nz/stories/HL1212/S00060/steen-rasmussen-the-flag-bearer-of-artificial-life.htm
% rasmussen bio : http://flint.sdu.dk/index.php?page=steen-rasmussen

%TIERRA : https://www.cs.cmu.edu/afs/cs/project/ai-repository/ai/areas/alife/systems/tierra/
% http://life.ou.edu/
%http://books.google.fr/books?id=DGdwAwAAQBAJ&pg=PA284&lpg=PA284&dq=tierra++%2B+VENUS+Rasmussen&source=bl&ots=mZtaVfMpUN&sig=2ZAUtx-yQoET3jPUP7pxudewbzo&hl=fr&sa=X&ei=jiwXVOavCciUavbLgKgI&ved=0CDgQ6AEwAg#v=onepage&q=tierra%20%20%2B%20VENUS%20Rasmussen&f=false

Plus informés des concepts sur lequel s'appuie la VA lorsqu'elle émerge comme nouvelle discipline au SFI, on peut développer quelques applications développés dans ce centre. Plusieurs chercheurs internes ou externes se concentrent au début des années 1990 sur le développement de supports logiciels innovants, au-delà des pratiques courantes d'utilisation d'automates cellulaires, comme le démontrent ces trois différents projets :

\textbf{a)} la famille de logiciels ECHO tient d'une commande faite par Murray Gell-Mann, un des fondateurs du SFI, à John Holland pour développer un logiciel d'écologie virtuelle pour les CAS. L'initiateur des \textit{Genetic Algorithms} (GA) reprend avec ses collègues Mitchell et Forrest des travaux usant des GA dans une perspective écologique et individus-centré compatible avec ce que l'on peut attendre de la Vie Artificielle. \autocites{Holland1993, Mitchell1993, Smith2000}

\textbf{b)} Le logiciel TIERRA de Thomas Ray, un biologiste tropical converti à la VA au début des années 1990. Inspiré entre autres par le développement des programmes CoreWar (Dewdney1984) et CoreWorld/VENUS (Virtual Evolution in a Nonstochastic Universe Simulator) du chimiste danois Rasmussen \autocite{Rasmussen1990}, Thomas Ray développe un écosystème virtuel sous forme de métaphore de l'ordinateur. Des morceaux de codes vivent, luttent, mutent et se reproduisent dans dans un espace mémoire virtuel limité en utilisant de l'énergie tirée d'un CPU lui aussi virtuel. Par les surprenantes formes de vies artificielle qu'il a mis à jour, le travail toujours actif de Ray a inspiré d'autres recherches et d'autres logiciels similaires comme Avida (1998) ou Cosmos (\autocite{Taylor1999})

\textbf{c)} enfin, le logiciel SWARM \autocite{Minar1996} est initié puis supervisé par Langton au début des années 1990. Créateur du terme fédérateur de \enquote{Vie Artificielle}, c'est lui qui organise au SFI en 1987 les premières conférences sur ce vaste sujet. La plateforme SWARM, initiée et développée par Langton, est une idée dont l'origine prend racine dans les expérimentations de Langton courant des années 1980. Cette fois-ci, le SFI met à disposition de Langton une équipe de développeurs dédiés à la création d'une plateforme Agents spécifique aux CAS. Mais celle-ci repose sur des bases différentes des deux autres, dans le sens où elle ne s'attache pas spécifiquement aux aspects évolutionnaires des CAS pour fournir dans une librairie développée au dessus du langage Objective-C des objets de plus haut niveau d'abstraction permettant aux scientifiques de manipuler plus rapidement des agents, quelle que soit leur nature.



%Kaiser 1979, Lomnicki 1978,1988 mais Neither the work of Kaiser nor that of ?Lomnicki had a strong impact on the early development of the IBM approach.; cf Grimm2004

%Sarkar2000
%L-Systems et A-Life : hogeweg, smith

L'écologie est également par de nombreux aspects une discipline intéressante à évoquer de notre point de vue. D'une part car elle est également pionnière sur la mise en oeuvre de ces techniques individu-centré (le terme consacré dans la discipline étant \textit{Individual Based Modelling} ou IBM ) usant très tôt des automates cellulaires, et d'autre part car elle tient une place et une influence particulière dans la géographie à la fois passée et probablement à venir. Enfin, il faut souligner que des écologues comme Volker Grimm ont produit des ouvrages qui se sont avérés fondateurs pour l'évolution et la formalisation des IBM en écologie \autocites{Grimm2004, DeAngelis2014}, ouvrages dont ils ont tiré des publications méthodologiques et techniques \autocite{Railsback2012} tout à fait remarquables par la pédagogie (Netlogo) et la pertinence des solutions proposées, tout à fait applicables dans d'autres communautés utilisant ce méta-formalisme agent (ODD \autocite{Grimm2010}, POM \autocite{Grimm2005,Grimm2011}, Analyse de sensibilité avec R et Netlogo \autocite{Thiele2011,Thiele2014a}). % Thiele2014b ?

Dans un article de \textcite{DeAngelis2005} les deux auteurs réalisent un état de l'art des usages de l'IBM sur 900 références. Un travail conséquent permettant d'établir une première grille de lecture en cinq axes pour qualifier la nature des variations individuelles : \textit{(a) spatial variability, local interactions, and movement; (b) life cycles and ontogenetic development; (c) phenotypic variability, plasticity, and behavior; (d) differences in experience and learning; and (e) genetic variability and evolution.} Des variations qui peuvent être mobilisées pour mettre en lumière sept types de processus écologiques et évolutionnaires différents : \textit{movement through space, formation of patterns among invidivual, foraging and bioenergetics to population dynamics, exploitative species interaction, local competition and community dynamics, evolutionary process, management-related processes}. A partir de ces deux caractérisations, on ne peut que constater l'existence d'un lien forcément étroit \autocite{Dorin2008} et ancien \autocites{Hogeweg1988, Hogeweg1990, DeAngelis2014} entre certaines pratiques mises en oeuvre dans les études de Vie Artificielle et celles des écologues pionniers dans l'\enquote{écologie virtuelle}, ne serait-ce que sur l'axe des processus évolutionnaires.

Si l'approche individualisée en écologie ne devient vraiment significative qu'à partir des années 1990, c'est grâce notamment à la publication d'articles théoriques fondateurs \autocite{Huston1988} et de plusieurs reviews faisant état de formalismes \autocite{Hogeweg1988} et de modèles pionniers \autocites{Hogeweg1990, DeAngelis1992, Judson1994} dans diverses branches de l'écologie. Ces travaux pionniers sont réalisés dans plusieurs branches de l'écologie, de la biologie, usant d'écosystèmes virtuels pour mener leurs expérimentations. Pour n'en citer que quelqu'uns, on trouve une longue suite de modèles de simulations innovants dans l'étude des dynamiques forestières \autocite{Bugmann2001} tel que JABOWA (1970) \autocite{Botkin1972}, FORET (1977), FORTNITE (1982); des pionniers que l'on trouve également dans l'éthologie (étude des comportements animaux) avec les travaux sur l'auto-organisation et le multi-niveau de Hogeweg et Hesper appuyé par leur système de simulation MIRROR \autocites{Hogeweg1979, Hogeweg1981, Hogeweg1983}; enfin il faut également noter la famille de simulateurs EVOLVE initiée en 1970 et améliorée au fil des années par Michael Conrad et Howard Pattee \autocites{Conrad1970, Pattee2002}.

%smith_bio

Toutefois, comme semble le souligner \textcite{Dorin2008}, bien que ces simulations de VA possèdent une utilité de par les questions génériques qu'elles abordent (un peu à la façon du modèle de Schelling), il n'est pas raisonnable à l'heure actuelle d'en faire un usage comparé avec l'écologie réelle. Ce dernier invite à un rapprochement mutuel qu'il estime à terme bénéfique pour les deux parties, l'utilisation de méthodologies adaptées (POM) permettant d'assurer l'approche réelle de ces questions d'évolution, jusqu'alors relativement peu prisent en compte avec l'approche multi-agents actuelle. Un constat étonnant quand on connaît l'ancienneté des solutions utilisant ces techniques pour simuler les processus d'évolution du vivant.

Cette écologie virtuelle qui coexiste de façon relativement proche à la VA fournit une inspiration importante aux informaticiens qui vont par la suite évoluer au contact des scientifiques en sciences sociales, notamment en France. Si Doran fait partie des scientifiques s'appuyant sur la branche des agents \enquote{cognitif}, la VA vient de façon complémentaire nourrir les réflexions d'une branche divergente en DAI, celle des agents dits \enquote{réactifs}. Comme le disent deux acteurs importants dans la formalisation et la diffusion de cette branche \autocite[31-32]{Ferber1995} et \textcite[7-10]{Drogoul1993}, cette résurgence de la VA coincide avec l'approche réactive dans le contre-pied pris face à l'approche \enquote{cognitiviste}; le concept d'auto-organisation est évoqué plus simplement au travers des concepts d'autonomie, de viabilité et d'une intelligence plus simple de type stimulus/réponse montrant qu'il est déjà possible d'obtenir des comportements complexes à partir de mécanismes simples.

\printbibliography[heading=subbibliography]

\chapter{Exploration du modèle SimpopLocal}

\begin{framewithtitle}{Notes}

Le titre orginal de l'article paru en 2015 dans \textit{Environment and Planning B (EPB)} est \foreignquote{english}{Half a billion simulations: evolutionary algorithms and distributed computing for calibrating the SimpopLocal geographical model}
\\ \\
Le modèle de simulation a été développé et expérimenté par Sébastien Rey-Coyrehourcq et Clara Schmitt entre 2010 et 2013. Sur le plan thématique, les principales hypothèses du modèle ne bougeront quasiment plus à partir de 2012 avec les résultats des premières expérimentations. Romain Reuillon rejoint le développement en avril 2012 après une première réécriture du modèle original de Netlogo vers Scala par Sébastien Rey-Coyrehourcq. Le modèle a ensuite subi un troisième redéveloppement en avril 2013 avec son introduction par Romain Reuillon dans la plateforme de développement générique de modèle SimPuzzle.

\end{framewithtitle}

\Anotecontent{foot1}{Computed with $\alpha = 1.36$.}

\Anotecontent{foot2}{Nonlinear effects make this estimation tricky: the expected value would have been $\num{10000}$ inhabitants, according to the objective of population, but in order to reach this value, the maximum carrying capacity of the landscape must be slightly higher.}

\Anotecontent{foot3}{The nonrounded parameter setting is: $R_{max} = \num{10259.331894632433}$ ; $DistanceDecay= \num{0.6882107473716844}$ ; $P_{creation} = \SI{1.2022185310640896e-6}{}$ ; $P_{diffusion} = \SI{7.405303653131592e-7}{}$ ; $InnovationImpact = \num{0.007879556611500305}$. The seed used for the simulation depicted in figure \ref{fig:S_ranksize} is $\num{-6863419716327549772}$.}

\Anotecontent{foot4}{\href{http://www.openmole.org/current/Documentation_Console\%20DSL_Tasks_NetLogo.html}{@Tutorial}}

\Anotecontent{foot5}{\href{http://www.openmole.org/current/Documentation_Tutorials_GA\%20with\%20NetLogo.html}{@Tutorial}}

\section{The challenge of calibrating geographical multiagent models}
\label{sec:challenge}

Geographical simulation models of systems of cities are based on the assumption that the microgeographical interactions are likely to generate the emergence of stylized dynamics on the macrogeographical scale, which constitute one of the recurring universal characteristics of these complex systems (Pumain2013). The heterogeneity and the large number of possible scenarios of implementation to represent these processes, the individual-based description of dynamics, the nonlinearity of interactions, the diversity of forms of spatial relationships, and the importance of the historical context are so many reasons for geographers to regularly use agent-based modelling as a support for reflection and experimentation \autocites{Batty2008, Crooks2008, Heppenstall2011, Sanders2007}. We introduce here a simplified model called SimpopLocal \autocites{Rey-Coyrehourcq2015,Schmitt2014} that extends the Simpop family of models \autocites{Pumain2011,Pumain2009}. It describes the emergence of a system of settlements where the process of innovation that generates the growth dynamics of the settlements is made endogenous. What is required for a successful agent-based model in geography is to assemble a set of individual-based mechanisms adapted to the level of resolution of the problem and to evaluate its ability to answer this problem with a high level of confidence \autocite{Sargent2005}. The evaluation usually requires a calibration stage \autocite{Balci1998} during which the ability of the model to reproduce a specific dynamic or structure is assessed. The calibration phase is generally conducted using a trial and error method. When applied to Simpop2 this resulted in a fairly satisfactory calibration being obtained using a hundred simulations \autocite{Bretagnolle2010}. It was not possible to determine if the corresponding values found for the parameters, which were tedious to estimate through this manual calibration procedure (each new increment in the parameter values can disturb the dynamics), were the best possible estimation or simply denoted the existence of a local minimum in the phase space of the dynamic model. Thus, it is crucial to automate the process of calibration and to substitute temporarily the human expertise of the model with an automated expertise, which requires a quantitative transposition of the expert evaluation. To carry out this automation, we developed and implemented methods and tools within the community of research on complex systems \autocite{Bourgine2009} and, more precisely, in the modelling platform called SimProcess \autocite{Rey-Coyrehourcq2015}. After having described the geographical model SimpopLocal on which the method is tested, we present an automated procedure of calibration that allows a massive exploration of the value space of the parameters, which was until now impossible to handle in the usual way. This exploration is guided by specific and relevant objectives as for the issues dealt with by SimpopLocal. After extraction and study of the results, we identify a set of coherent parameter settings that give particularly interesting simulation outputs in compliance with the model calibration objectives. The main contribution of the paper is to provide a reusable method for calibrating a real-world agent-based model.

\section{The SimpopLocal model}
\label{sec:simpoplocal}

\subsection{A model to simulate the emergence of a structured urban settlement system}
\label{subsec:simulate_structure}

SimpopLocal is a stylized model describing an agrarian society in the Neolithic period, during the primary ‘urban transition’ manifested by the appearance of the first cities \autocite{Schmitt2014}. It is designed to study the emergence of a structured and hierarchical urban settlement system by simulating the growth dynamics of a system of settlements whose development remains hampered by strong environmental constraints. This exploratory model seeks to reproduce a particular structure of the rank–size distribution of settlements well defined in the literature as a generalized stylized fact: for any given settlement system throughout time and the continents, the distribution of sizes is strongly differentiated, exhibiting a very large number of small settlements and a much smaller number of large settlements \autocites{Archaeomedes1998,Berry1964a,Fletcher1986,Liu1996}. This distribution can be modeled by a power law when small settlements are not considered or a log–normal law when small settlements are considered \autocites{Favaro2011, Robston1973}. Such distributions are easily simulated by rather simple and nonspatial statistical stochastic models \autocites{Gibrat1931,Simon1955}. However, for theoretical considerations and to specify the model in a variety of geographical frames, we think it necessary to make explicit the spatial interaction processes that generate the evolution of city systems. According to the evolutionary theory of cities \autocite{Pumain2009}, the growth dynamics of each settlement are controlled by its ability to generate interurban inter­actions. The multiagent system modelling framework enables us to include mechanisms, derived from this theory, that govern the interactions between settlements \autocites{Bretagnolle2006, Sanders2013}. The application of this concept resulted in several Simpop models \autocites{Bretagnolle2010, Bura1996,Pumain2009, Sanders1997} in which the expected macrostructure of the log-normal distribution of sizes emerges from the differentiated settlement growth dynamics induced by the heterogeneous ability of interurban interactions. Therefore, the aim of SimpopLocal is to simulate the hierarchy process via the explicit modelling of a growth distribution that is not entirely stochastic as in the Gibrat model \autocite{Gibrat1931} but that emerges from the spatial interactions between microlevel entities. Compared with the previous Simpop models, the originality of SimpopLocal is twofold. First, it is a simplified version which no longer qualitatively distinguishes the successive urban functions simulated during the evolution of urban systems but transposes them into an abstract innovation process having over time less and less impact in terms of wealth and population growth. (This rule is in accordance with the concept of the decreasing efficiency of the improvement of an existing socioeconomic system and prevents introducing any a priori amplification effect.) Second, the appearance of these innovations is an endogenous process that is linked to the size of the settlements.


\subsection{Agents, attributes, and mechanisms of the model}
\label{subsec:mechanisms}

The SimpopLocal model is a multiagent model developed with the Scala programming language. It simulates the growth dynamics of agrarian settlements and their possible evolution towards urban settlements under strong environmental constraints that are progressively overcome by successive innovations. The landscape of the simulation space is composed of hundreds of settlements. Each settlement is considered as a fixed agent and is described by three attributes: the location of its permanent habitat, the size of its population, and the available resources in its local environment. The amount of available resources is quantified in units of inhabitants and can be understood as the carrying capacity of the local environment for sustaining a population which depends on the resource exploitation skills that the local population has acquired from inventing or acquiring innovation. Each new innovation acquired by a settlement develops its exploitation skills. This resource exploitation is done locally and sharing or trade is not represented explicitly in the model. The growth dynamics of a settlement are modelled according to the assumption that its size is dependent on the amount of available resources in the local environment and is inspired by the Verhulst model \autocite{Verhulst1845} or logistic growth. For this experiment, we assume a continuous general growth trend for population—this may be different in another application of the model. The growth factor $r$ is expressed on an annual basis; thus, one iteration or step of the model simulates one year of demographic growth. The limiting factor of growth $R^{i}_{M}$ is the amount of available resource that depends on the number $M$ of innovations the settlement $i$ has acquired by the end of the simulation step $t$. $P^{i}_{t}$ is the population of the settlement $i$ at the time $t$ :

\begin{equation}
P^{i}_{t+1} = P^{i}_{t} \left[ 1 + r  \left( 1 -  \frac{P^{i}_{t}} { R^{i}_{m} }\right)\right]
\end{equation}

The acquisition of a new innovation by a settlement allows it to overtake its previous growth limitation by allowing a more efficient extraction of resources and thus a gain in population-size sustainability. With the acquisition of innovations the amount of available resources tends to the maximal carrying capacity $R_{max}$ of the simulation environment:

\begin{equation}
R^{i}_{M} \xrightarrow{innovations acquisition} R_{max}
\end{equation}

The mechanism of this impact follows the Ricardo model of diminishing returns [also a logistic model \autocite{Turchin2003}]. The $InnovationImpact$ represents the impact of the acquisition of an innovation and has a decreasing effect on the amount of available resources $R^{i}_{M+1}$ with the acquisition of innovations:

\begin{equation}
R^{i}_{M+1} = R^{i}_{M} \left[ 1 + InnovationImpact \left( 1 - \frac{R^{i}_{M}}{R_{max}}\right)\right]
\end{equation}

Acquisition of innovations can occur in two ways, either by the emergence of innovation within a settlement or by its diffusion through the settlement system. In both cases, interaction between people inside a settlement or between settlements is the driving force of the dynamics of the settlement system. It is a probabilistic mechanism, depending on the size of the settlement. Indeed, innovation scales superlinearly: the greater the number of innovations acquired, the bigger the settlement and the higher the probability of innovation \autocites{Arthur2009, Diamond1997, Lane2009}. To model the superlinearity of the emergence of innovation within a settlement, we model its probability by a binomial law. If $P_{creation}$ is the probability that the interaction between two individuals of the same settlement is fruitful, that is, leads to the creation of an innovation, and $N$ the number of possible interactions, then, by the binomial law, the probability of the emergence of at least one innovation $P(m_{creation} > 0)$ can be calculated and then used in a random drawing:

\begin{equation}
\begin{split}
P(m_{creation} > 0) & =  1 - P\left(m_{creation = 0}\right),  \\
& = 1 - \left[   \frac{N!}{0!(N - 0)!} * P^{0}_{creation} * (1 - P_{creation})^{N - 0}  \right], \\
& = 1 - \left( 1 - P_{creation} \right)^{N}
\end{split}
\end{equation}

If the size of the settlement is $P^{i}_{t}$ then the number $N$ of possible interactions between individuals of that settlement is :
 \begin{equation}
N = \frac{P^{i}_{t} \left(P^{i}_{t}  - 1 \right)}{2}
\end{equation}

The diffusion of an innovation between two settlements depends on both the size of populations and the distance between them. If $P_{diffusion}$ is the probability that the interaction of two individuals of two different settlements is fruitful—that is, leads to the transmission of the innovation—and $K$ is the number of possible interactions, then, by the binomial law, the probability of diffusion of at least one innovation $P ( m_{diffusion} > 0)$ can be calculated and used in a random drawing:

\begin{equation}
P (m_{diffusion} > 0 ) = 1 - ( 1 - P_{diffusion})^{K}
\end{equation}

But in this case, the size $K$ of the total population interacting is a fraction of the population of the two settlements $i$ and $j$ which is decreasing by a factor $DistanceDecay$ with the distance $D_{ij}$ between the settlements, as in the gravity model \autocite{Wilson1971}:

\begin{equation}
K = \frac{P^{i}_{t} P^{j}_{t}}{ 2 D^{DistanceDecay}_{ij}}
\end{equation}

The process of population growth and the process of innovation creation and diffusion are reiterated throughout the simulation. Because of the two positive feedbacks that operate on resource and population growth through the creation of innovation, the model is able to generate a very rapid expansion of settlements: that is, an escalation of settlement growth. The simplest way to avoid situations where too many innovations are created, which would lead to huge time-consuming simulations, is to decide to stop the simulation when it reaches an arbitrary number of, say, $\num{10000}$ innovations. In order to ensure the replicability of the model, the source code of SimpopLocal is filed in a public repository ( \href{http://iscpif.github.io/ simpoplocal-epb/}{http://iscpif.github.io/ simpoplocal-epb/} ).


\subsection{Parameters to calibrate}
\label{subsec:parameters}

SimpopLocal has many parameters that have to be estimated in order to calibrate the model. Some can be evaluated empirically with the help of historical data and knowledge, while it is very difficult to give values to others. Those representing the initial spatial distribution and organisation of settlements in the landscape can be approximated. The log-normal distribution of the settlement sizes and the central place theory \autocite{Christaller1933} for the geographical distribution of locations are models that are widely used by archaeologists to describe their spatial data \autocites{Archaeomedes1998, Johnson1977, Sanders2012}, including Neolithic archaeological sites \autocite{Liu1996}. In SimpopLocal the mean density of the landscape and the average size of each settlement are representative of the usual orders of magnitude presented in these works. A hundred settlements are distributed according to these two theories and each settlement is initially composed of 80–400 inhabitants. Several scholars agree that an average annual growth of 0.02\% is representative of the growth of agrarian settlements in Neolithic times \autocites{Bairoch1985,Renfrew1979}. The length of time required for a transition of settlements from agrarian to urban is estimated according to \textcite{Bairoch1985} and \textcite{Marcus2008} to a couple of thousand years. We choose to operate our simulations on a four-thousand-year time period for settlements ranging from one hundred inhabitants up to about ten thousand inhabitants.

Because of a lack of empirical data, five parameters cannot be approximated empirically: $P_{creation}$ , the probability that an innovation emerges from the interaction between two indi­ viduals of a same settlement. $P_{diffusion}$ , the probability that an innovation is transmitted between two individuals of different settlements. In this model we consider that the probability of diffusion is greater than the probability of creation, which means that copying is easier than inventing \autocite{Pennisi2010}. InnovationImpact, the impact of the acquisition of innovation on the growth of settlements. DistanceDecay, the deterrent effect of distance on diffusion. $R_{max}$ , the maximum carrying capacity of the landscape of each settlement (measured in number of inhabitants).

The model has been simplified as far as possible to retain only five parameters that cannot be empirically estimated. This number may seem rather low compared with the $40$ – $50$ parameters that were activated in the other versions of Simpop models, but this simplification allows a global exploration of the capabilities of SimpopLocal. Indeed, by means of intensive exploration, a calibrated state of the model can produce an estimation of the value of some parameters that could not be deduced from the empirical literature on archaeological settlements. These estimations will be useful later for making predictions about the possible evolution of concrete settlement systems or comparing the evolutions of early urban systems in different regions. SimpopLocal is intended to provide an open evolution that may lead to any type of size distribution of settlements depending on the parameter values. In order to estimate parameter values that could generate plausible size distributions, we designed an automated calibration procedure.

\section{Designing an automated calibration procedure}
\label{sec:automated_calibration}

\subsection{Calibration as an optimization problem}
\label{subsec:calibration}

Model calibration is a procedure which seeks to minimize the difference between the behaviour simulated by the model and a behaviour defined according to expert knowledge and/or data. In most multiagent systems the calibration is done manually by introducing values for the parameters and visually verifying that the output of the model corresponds to the expected results. But this method raises numerous problems highlighted by \textcite{Stonedahl2011a}: relationships between parameters are often nonlinear; expertise during behaviour exploration may be biased by erroneous assumption; some parameters cannot be compared with empirical values; and manual exploration is tedious. In our first attempt to manually calibrate the model, we could not reach a stage where the calibration could be considered reasonably satisfying. Because of the number of parameters and the continuous scale of variation of their fields of variation, any exhaustive search strategy is not tractable either. First, this strategy generates a number of experiments that grows exponentially with the number of parameters under study. For SimpopLocal this would imply too gigantic an amount of computation. Moreover, that strategy produces a large quantity of data, which then has to be processed and visualized. The search for patterns in the space of the dynamics of the model is thus replaced with the problem of a search for patterns in a database. Postprocessing of such a large quantity of data is long and tiresome. Instead of resorting to exhaustive search methods, we choose to consider calibration as an optimisation exercise. The a posteriori exploration of results then becomes an a priori question: is there a parameter setting that would match our expectation? Evolutionary algorithms have been established as suitable solutions to this way of considering calibration \autocites{Calvez2006a, Stonedahl2011a}. They have been used to calibrate multiagent systems in several fields, such as medicine \autocite{Castiglione2007}, ecology \autocite{Duboz2010}, economics \autocites{Espinosa2012, Stonedahl2010a}, and hydrology \autocite{Solomatine1999}. Despite the wide use of multiagent systems in social sciences, this method has not been applied very often. To our knowledge only a few real- world applications have been carried out \autocites{Heppenstall2007, Stonedahl2010}. Indeed this kind of numerical experiment remains a real challenge: \begin{enumerate}[label=(\arabic*),labelindent=0pt, leftmargin=*]
 \item It requires the definition of quantitative goals that evaluate the simulation outputs in a coherent manner which correspond to the experts’ expectations. \item It generates a massive computation load that requires technical skills and adapted infrastructures, \item It seeks to optimize a noisy (stochastic) fitness function which is in itself a challenging exercise \autocite{Pietro2004}. To overcome these obstacles, cutting-edge knowledge and tools in several fields of expertise have to be coupled within highly transdisciplinary teams involving social scientists, statisticians, optimization method specialists, and distributed-computing experts.\end{enumerate}

\subsection{Exploring the parameter space given a set of objectives}
\label{subsec:exploring}

Evolutionary algorithms are heuristics that scan the search space using strategies inspired by natural processes to solve optimisation problems. In order to use evolutionary algorithms to calibrate a model, the first step is to formalize what the expected result of a ‘good’ simulation is, which, in our case, is a suitable configuration of the settlement system. This suitable configuration reflects stylized facts that were established over the years by many scholars, thanks to the exploration and processing of large amounts of empirical data. We have summarized this suitable configuration using three objective functions extracted as relevant stylised facts from the existing literature: a log-normal distribution of settlement sizes \autocites{Archaeomedes1998,Johnson1977,Liu1996,Sanders2012}; a maximum size of ten thousand inhabitants \autocites{Bairoch1985,Marcus2008}; and a four-thousand- year period for achieving this distribution [in any region of the world where agriculture was invented, the emergence of the first cities occurred a few thousand years later \autocites{Bairoch1985,Marcus2008}]. The automatic technique of calibration that we propose is based on an evolutionary algorithm that explores the space of the parameter settings. This exploration is guided by the three objectives above \autocites{Rey-Coyrehourcq2015,Schmitt2014}. Each parameter setting (ie, a set of values for the five unknown parameters of the model) is evaluated according to the simulation output it produces. This evaluation measures the proximity between the outputs of simulation and the three objective functions defined for the model and thus assesses the ability of the parameter settings to reproduce the stylized facts that we seek to simulate. The parameter settings receiving the best evaluations are then used as a basis for generating new parameter settings which will then be tested. Since the SimpopLocal model is stochastic, the simulation outputs do vary for a given parameter setting from one simulation to the next. Therefore, evaluation of the parameter setting on the three objectives must take into account this variability. We checked if $100$ simulations for each set of parameters was sufficient to capture this variability and found that it was a suitable compromise between capturing the variability and not increasing the computation duration too much. We test each set of parameters according to the three objective functions as follows:

\begin{enumerate}[label=(\arabic*),labelindent=0pt, leftmargin=*]
\item The \textit{objective of distribution} quantifies the ability of the model to produce settlement size distributions that fit a log–normal distribution. First we evaluate the outcome of each subset of $100$ simulations corresponding to one parameter setting by computing, according to a two-sample Kolmogorov–Smirnov test, the deviation between the simulated distribution and a theoretical log–normal distribution having the same mean and standard deviation. Two criteria are reported, with value $1$ if the test is rejected and $0$ otherwise: the likelihood of the distribution (the test returns $0$ if $p-value > 5\%$) and the distance between the two distributions (the test returns $0$ if $D-value < D_{\alpha}$\Anote{foot1} ). In order to summarize those tests in a single quantified evaluation, we add the results of the two tests for the $100$ simulations. The best possible score on this objective is thus $0$ (all tests returning $0$) and the worst $200$ (all tests returning 1). By dividing this score by the worst possible value ($200$), we get a normalized error that facilitates comparisons between parameter settings according to their scores for the three objectives.

\item The \textit{objective of population} quantifies the ability of the model to generate large settlements that have an expected size. The outcome of one simulation is tested by computing the deviation between the size of the largest settlement and the expected value of $\num{10000}$ inhabitants: $|(population of largest settlement −\num{10000})/\num{10000}|$. The evaluation of the parameter setting reports the median of this test on the $100$ simulations. This value represents the normalized error produced by the parameter setting being evaluated on the calibration (the closer to $0$, the smaller the error). Computing this normalized error is not necessary but it facilitates comparisons between parameter settings according to their scores for the three objectives.

\item The \textit{objective of simulation duration} quantifies the ability of the model to generate an expected configuration in a suitable length of time (measured in simulation steps). The duration of one simulation is tested by computing the deviation between the number of iterations of the simulation and the expected value of $\num{4000}$ simulation steps: $|(simulation duration − \num{4000})/\num{4000}|$. The evaluation of the parameter setting reports the median of this test on the $100$ simulations. This value represents the normalized error produced by the parameter setting being evaluated for the calibration. This normalized error (the closer to $0$, the smaller the error) facilitates comparisons between parameter settings according to their scores for the three objectives.
\end{enumerate}

The three objective functions constitute a fitness function of our optimisation problem which is therefore multiobjective. To solve such problems, it is sometimes possible to aggregate the vector values in a single scalar value reflecting an absolute quality of the solution. In our case it is not possible to provide a meaningful aggregated function. Thus, we rely on a multiobjective optimization algorithm. This type of algorithm computes compromise solutions such that among those solutions none dominates the others by presenting better scores for all three objectives at the same time. Such solutions are called a Pareto compromise and the entire set of \textit{Pareto compromise} solutions is called the \textit{Pareto front}.


\subsection{Executing a large-scale distributed evolutionary algorithm with a noisy time-consuming fitness function}
\label{subsec:executing}

The use of global methods of search such as evolutionary algorithms for the calibration of a multiagent model (and especially a stochastic multiagent model) entails a high computational cost \autocite{Sharma2006}. This kind of load is too large to be executed on local computers. Supercomputers are very expensive and not easily available in most laboratories, therefore we decided to develop a calibration framework that could run on both expensive supercomputers and on computing grids, which make computing power more accessible by federating it on a worldwide scale. For the experiment presented in this paper we used the European Grid Infrastructure (EGI). Computing grids offer a solution for the resolution of such computationally intensive problems. However, computing at such a large scale is a true challenge. It supposes orchestrating the execution of tens of thousands of instances of the model on computers distributed all over the world. The cumulative probability of local breakdowns and the impossibility of distributing the workload optimally on the grid system makes its effective use very difficult. To overcome these difficulties we used the OpenMOLE platform (Open Model Exploration, \href{http://www.openmole.org}{http://www.openmole.org} ) \autocites{Reuillon2010,Reuillon2013}, which provides a bridge that helps modellers cross the technical and methodological gap which separates them from high-performance computing. OpenMOLE is a dedicated, textual, and graphic language, exposing coherent bricks at the right level of abstraction to conceive reusable experiments in order to solve inverse problems using models. The methods of resolution are described independently of a particular model and thus support the reproducibility and reuse of the experimental numerical methods across the modellers. For calibrating SimpopLocal we implemented a classical multiobjective evolutionary algorithm called SMS-EMOA \autocite{Emmerich2005} in a distributed manner, on top of OpenMOLE. The parameters of this algorithm are displayed in table \ref{t:parametreslocal}. The code of this workflow and the code for the fitness computation are available at \href{http://iscpif.github.io/simpoplocal-epb/}{http://iscpif.github.io/simpoplocal-epb/}.

\begin{table}[!htbp]
\begin{sidecaption}[Domaine d'exploration des paramètres à calibrer]{Global domain of exploration and calibrated parameter value domain of variation.}[t:parametreslocal]
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{ll}
\toprule
Genome  & \begin{tabular}[c]{@{}l@{}}$P_{creation} [0;1]$,  \\ $P_{diffusion} [0;1]$,\\ $InnovationImpact [0;2],$\\ $DistanceDecay [0;4],$\\ $R_{max} [1;40000]$\end{tabular}   \\
Objective function & Multiobjective: distribution, population, and simulation duration  \\
Crossover  & \begin{tabular}[c]{@{}l@{}}SBX RGA operator with bounded variable modification,\\ {[}see Appendix A of Deb (2000){]}\\ → with a distribution index equal to 2.0 and a crossover rate equal to 0.5\end{tabular} \\
Mutation & Gaussian mutation and self adaptation (Hinterding, 1995)   \\
Dominance & \begin{tabular}[c]{@{}l@{}}epsilon dominance,\\ → epsilons = distribution 0.0, population 10.0, simulation duration 10.0\end{tabular}  \\
Nadir  & \begin{tabular}[c]{@{}l@{}}distribution 500,\\ population 100 000,\\ simulation duration 10 000\end{tabular}             \\
Selection  & Binary tournament                       \\
Population   &  200                                 \\ \bottomrule
\end{tabular}%
}
\end{sidecaption}
\end{table}

This algorithm evaluates millions of parameter settings to approximate the Pareto front. In its most efficient implementation one execution of SimpopLocal lasts $1.5$ seconds on a current processor. The automatic calibration of SimpopLocal requires the simulation of almost $500$ million executions of the model, which would represent nearly $20$ consecutive years on a single computer. To achieve this huge computation load, the SMS-EMOA algorithm was distributed to the numerous computers of the EGI using the technique known as the ‘island model’ \autocites{Emmerich2005,Whitley1997}. The classical island model consist of instantiating permanent islands (isolated instances of an evolutionary algorithm) on many computers and organising the migration of solutions between those islands. The EGI grid is a worldwide batch system on which organizing direct communications between islands running on multiple execution nodes is very challenging. Thus the classical island model has been adapted to the EGI architecture.

During the computation a central population of $200$ parameter settings (here called individuals) is maintained on the computer that orchestrates the submission of the computing jobs on the grid. Each job computes the evolution of the population of an island, which is an independent instance of SMS-EMOA seeded with $50$ individuals randomly sampled among the central population of $200$ individuals. The ‘island job’ life cycle is managed by the EGI. Each job is submitted to the EGI, then to the queue of a cluster, and finally starts running when a slot becomes available on the cluster. When it starts running it is configured to run for 1 hour. $5000$ concurrent jobs are maintained on the grid at any time.

Once an island job is finished its final population of individuals (ie, parameter settings) is transferred back to the submission computer and merged into the global population using the elitism algorithm of the SMS-EMOA (based on the contribution to the hypervolume of the Pareto). A new island job is then submitted to the grid. This algorithm is run until $\num{190000}$ island jobs have been completed.

As previously stated, the fitness function of this experiment is stochastic, so we execute the model a hundred times for each parameter-setting evaluation. Despite this precaution, it is well known that among the millions of executions computed by the algorithms, some solutions might ‘get lucky’ and be overestimated, creating a bias in the evolution process \autocite{Pietro2004}. According to Pietro et al, who reviewed various method to tackle this problem, we choose to reevaluate already evaluated solutions on a regular basis to prevent them “from retaining incorrect fitness” \autocite[2]{Pietro2004}. A specific strategy was implemented: over a hundred parameter setting evaluations were used to reevaluate already evaluated parameter settings instead of evaluating a new parameter setting. A parameter setting which has been overestimated the first time and thus kept among the optimal individuals by mistake, will probably be correctly evaluated the second time and eliminated from the optimal selection.


\section{Results and discussion}
\label{sec:results}

Evolutionary algorithms are good solutions when no exhaustive method is available. However they do not ensure the optimality of the computed solution. Because of their heuristic nature, they are procedures that can run indefinitely. In this work we have used the number of evaluated islands as a stopping criterion given the computing power that was at our disposal. That way we preconditioned the experience so that the total execution would last for three to four days, which means that the evolutionary optimization algorithm was stopped after the execution of $\num{190000}$ islands, which represents about $\num{200000}$ computation hours [$\num{200000}$ CPU (central processing unit) hours]. An evolutionary algorithm is declared ‘converged’ when it makes no further improvements in the search for good solutions. One of the best metrics for measuring the convergence of the multiobjective optimization algorithm that is currently available is the stagnation of the hypervolume \autocites{Fonseca2006,Naujoks2005,Zitzler1998}. The hypervolume measures the volume of the dominated portion of the objective space and its stagnation indicates that the algorithm has converged. Figure \ref{fig:S_hypervolume} depicts the evolution of the hypervolume as a function of the number of evaluated islands, which can also be quantified in terms of computation time or CPU hours. After the execution of $\num{120000}$ CPU hours, the hypervolume stabilizes. This stability in the long term ($\num{70000}$ CPU hours) indicates that the algorithm has converged. The Pareto front probably would not have improved much more with further computation time. This result shows that despite the stopping criterion that we have chosen due to technical limitations, the proposed parameter settings of the Pareto front are seemingly the best possible results that can be obtained with this method and they most likely correspond to the global optimum.

\begin{figure}[!htbp]
\begin{sidecaption}[Représentation de l'Hypervolume dans le temps]{Hypervolume enclosed by the Pareto front and the converging evolution of the evolutionary algorithm under the constraint of three objectives In order to smooth the local variability of the raw
hypervolume value, it has been averaged using a $\num{1000}$ island sliding window.}[fig:S_hypervolume]
  \centering
 \includegraphics[width=1.0\linewidth]{half_billion_simulation_fig1_hypervolume.png}
  \end{sidecaption}
\end{figure}

After the convergence of the algorithm, $200$ different parameter settings were proposed as possible calibrated sets of parameter values. Because a set of $100$ executions can only provide an approximation of the objective scores of a parameter setting and because we want to make sure that they are well evaluated, $\num{10000}$ more executions of each proposed parameter setting were conducted. After recomputing the dominance selection, the dominated parameter set­ tings were excluded and $62$ parameter settings remained in the Pareto front. As stated above, a multiobjective exploration does not lead to the selection of a single optimized solution but leads to a set of possible candidates for the calibration: each set of parameter settings selected by the procedure represents a specific compromise on the three objectives. Thus, it is possible to distinguish among the parameter settings those satisfying only two objectives out of three from those which offer a better compromise on the three objectives without, however, reaching the best possible values on each objective. This configuration confirms that these three objectives are not redundant and that the procedure vacillates between them to find well-fitted parameter settings. Among the $62$ parameter settings of the Pareto front, some are less satisfying than others. We consider as unsuitable the parameter settings where at least one of the scores of the three objectives has a normalized error over $0.1$ (ie, over $10\%$ of error). Only $29$ parameter settings satisfy this new condition. The values estimated for each parameter of the $29$ selected sets are presented in table \ref{t:calibrated}.


The analysis of this new subset shows interesting aspects. First, all of them correctly identify the order of magnitude of the only parameter of the model which can be roughly deduced from the initial conditions and the calibration objectives: the maximum resource parameter ($R_{max}$ ) which is used to limit the logistic growth of the population of each settlement\Anote{foot2}. Second, the four parameters whose values are a priori unpredictable (DistanceDecay, the parameter of dissuasion from the interactions by the distance; $P_{creation}$ , the probability of appearance of an innovation; $P_{diffusion}$ , the probability of its diffusion; and finally InnovationImpact, the impact of the innovation on the growth of the population) are estimated in a very small domain of variation compared with their possible domain of variation (table \ref{t:domain}). The ratio between the estimated and theoretical volume defined by these five dimension domains is of \~ $\SI{4.7e-16}{} / \num{320000}{} = \SI{1.5e-21}{}$ ! What is remarkable is how the values taken by each parameter are all in a small neighbourhood, which suggests high reliability considering that they must be given those orders of magnitude to obtain plausible results while using the model for simulation. However it should be noted that the exact values estimated for each parameter do not have any absolute meaning. They only make sense all together, according to their interrelationships in the mechanisms of the model.

\begin{table}[H]
\begin{sidecaption}[Comparaison du domaine de variation pour les paramètres avant et après calibrage]{Global domain of exploration and calibrated parameter value domain of variation.}[t:domain]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
                   & Explored Domain & Solutions Domain          \\ \midrule
$P_{creation}$     & {[}0,1{]}       & ~{[} \SI{1.1e-06} ; \SI{1.3e-06} {]} \\
$P_{diffusion}$    & {[}0,1{]}       & ~{[} \SI{6.7e-07} ; \SI{6.9e-07} {]} \\
$InnovationImpact$ & {[}0,2{]}       & ~{[} \SI{7.7e-03} ; \SI{8.4e-03} {]} \\
$DistanceDecay$    & {[}0,4{]}       & ~{[} 0.66; 0.75 {]}       \\
$R_{max}$          & {[}1,40000{]}   & ~{[} 10090; 10465 {]}     \\
Volume             & 320000          & ~ \SI{4.7e-16}{}               \\ \bottomrule
\end{tabular}
\end{sidecaption}
\end{table}

Within the subset of $29$ parameter settings there is no way to prefer one setting over the others: the multiple executions of each setting lead to good results and acceptable variability of the outputs. Figures \ref{fig:S_ranksize} and \ref{fig:S_hypervolumemedian} give an example of such outputs. They depict the results produced by one of the parameter settings (in bold characters\Anote{foot3} in table \ref{t:calibrated}). Figure \ref{fig:S_ranksize} shows the evolution of the rank–size distribution of settlement sizes during a simulation of the parameter setting. This evolution corresponds to what is expected of the model: a progressive and continuous process of hierarchical organisation of the settlement system (the slope of the linear fit of the rank–size distribution shifts from $0.2$ to $0.9$ in $4000$ years for a maximum reached size reached of about $\num{10000}$ inhabitants). This result is quite robust if we consider the low variability of the recorded final state from one simulation to another (figure \ref{fig:S_hypervolumemedian}).


\begin{table}[!htbp]
\begin{sidecaption}[Valeurs des solutions dans le front de Pareto]{Calibrated parameter settings (rounded values).}[t:calibrated]
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}llllllll@{}}
\multicolumn{5}{l}{Parameter setting}                                               & \multicolumn{3}{l}{\% error for each objective score}                                                                                                                                           \\ \midrule
$R_{max}$ & \begin{tabular}[c]{@{}l@{}}$Distance$ \\ $Decay$ \end{tabular}  & $P_{creation}$ & $P_{diffusion}$ &  \begin{tabular}[c]{@{}l@{}}$Innovation$ \\ $Impact$ \end{tabular} & \begin{tabular}[c]{@{}l@{}}distribution \\ objective\end{tabular} & \begin{tabular}[c]{@{}l@{}}population \\ objective\end{tabular} & \begin{tabular}[c]{@{}l@{}}time \\ objective\end{tabular} \\ \midrule
$\num{10464}$     & $\num{0.691}$           & \SI{1.11e-06}{}       & \SI{7.90e-07}{}        & $\num{0.0077}$                        & $\num{2.02}$                                                 & $\num{6.59}$                                                            & $\num{1.00}$                                                      \\
$\num{10465}$     & $\num{0.709}$           & \SI{1.14e-06}{}       & \SI{7.88e-07}{}        & $\num{0.0078}$                        & $\num{2.04}$                                                 & $\num{7.31}$                                                            & $\num{0.10}$                                                      \\
$\num{10459}$     & $\num{0.705}$           & \SI{1.15e-06}{}       & \SI{7.92e-07}{}        & $\num{0.0077}$                        & $\num{2.22}$                                                 & $\num{6.98}$                                                            & $\num{0.48}$                                                      \\
$\num{10465}$     & $\num{0.708}$           & \SI{1.15e-06}{}       & \SI{7.86e-07}{}        & $\num{0.0078}$                        & $\num{2.51}$                                                 & $\num{6.00}$                                                            & $\num{0.40}$                                                      \\
$\num{10261}$     & $\num{0.679}$           & \SI{1.17e-06}{}       & \SI{7.78e-07}{}        & $\num{0.0078}$                        & $\num{2.79}$                                                 & $\num{5.29}$                                                            & $\num{4.55}$                                                      \\
$\num{10262}$     & $\num{0.679}$           & \SI{1.15e-06}{}       & \SI{7.52e-07}{}        & $\num{0.0078}$                        & $\num{2.95}$                                                 & $\num{5.00}$                                                            & $\num{2.15}$                                                      \\
$\num{10262}$     & $\num{0.665}$           & \SI{1.14e-06}{}       & \SI{7.24e-07}{}        & $\num{0.0078}$                        & $\num{2.99}$                                                 & $\num{5.53}$                                                            & $\num{1.23}$                                                      \\
$\num{10261}$     & $\num{0.683}$           & \SI{1.12e-06}{}       & \SI{7.38e-07}{}        & $\num{0.0080}$                        & $\num{3.1 }$                                                 & $\num{4.34}$                                                            & $\num{1.15}$                                                      \\
$\num{10260}$     & $\num{0.699}$           & \SI{1.17e-06}{}       & \SI{7.38e-07}{}        & $\num{0.0079}$                        & $\num{3.62}$                                                 & $\num{5.51}$                                                            & $\num{0.20}$                                                      \\
$\num{10287}$     & $\num{0.690}$           & \SI{1.23e-06}{}       & \SI{7.56e-07}{}        & $\num{0.0078}$                        & $\num{3.63}$                                                 & $\num{3.74}$                                                            & $\num{3.46}$                                                      \\
$\mathbf{\num{10259}}$     & $\mathbf{\num{0.688}}$           & $\mathbf{\SI{1.20e-06}{}}$       & $\mathbf{\SI{7.41e-07}{}}$        & $\mathbf{\num{0.0079}}$                        & $\mathbf{\num{3.74}}$                                                 & $\mathbf{\num{3.55}}$                                                            & $\mathbf{\num{2.48}}$                                                      \\
$\num{10169}$     & $\num{0.736}$           & \SI{1.29e-06}{}       & \SI{7.39e-07}{}        & $\num{0.0079}$                        & $\num{4.90}$                                                 & $\num{5.20}$                                                            & $\num{0.03}$                                                      \\
$\num{10205}$     & $\num{0.683}$           & \SI{1.19e-06}{}       & \SI{7.42e-07}{}        & $\num{0.0082}$                        & $\num{5.10}$                                                 & $\num{2.60}$                                                            & $\num{6.03}$                                                      \\
$\num{10126}$     & $\num{0.738}$           & \SI{1.22e-06}{}       & \SI{7.61e-07}{}        & $\num{0.0082}$                        & $\num{5.69}$                                                 & $\num{2.88}$                                                            & $\num{1.50}$                                                      \\
$\num{10126}$     & $\num{0.738}$           & \SI{1.24e-06}{}       & \SI{7.39e-07}{}        & $\num{0.0082}$                        & $\num{6.02}$                                                 & $\num{3.08}$                                                            & $\num{0.55}$                                                      \\
$\num{10096}$     & $\num{0.701}$           & \SI{1.14e-06}{}       & \SI{7.14e-07}{}        & $\num{0.0084}$                        & $\num{6.12}$                                                 & $\num{2.58}$                                                            & $\num{1.55}$                                                      \\
$\num{10169}$     & $\num{0.736}$           & \SI{1.29e-06}{}       & \SI{7.39e-07}{}        & $\num{0.0080}$                        & $\num{6.25}$                                                 & $\num{2.46}$                                                            & $\num{1.20}$                                                      \\
$\num{10165}$     & $\num{0.734}$           & \SI{1.29e-06}{}       & \SI{7.24e-07}{}        & $\num{0.0080}$                        & $\num{6.31}$                                                 & $\num{2.91}$                                                            & $\num{0.30}$                                                      \\
$\num{10121}$     & $\num{0.732}$           & \SI{1.28e-06}{}       & \SI{7.41e-07}{}        & $\num{0.0081}$                        & $\num{6.41}$                                                 & $\num{2.36}$                                                            & $\num{1.90}$                                                      \\
$\num{10164}$     & $\num{0.735}$           & \SI{1.29e-06}{}       & \SI{7.27e-07}{}        & $\num{0.0080}$                        & $\num{6.45}$                                                 & $\num{2.74}$                                                            & $\num{0.45}$                                                      \\
$\num{10103}$     & $\num{0.733}$           & \SI{1.24e-06}{}       & \SI{7.42e-07}{}        & $\num{0.0084}$                        & $\num{7.67}$                                                 & $\num{1.90}$                                                            & $\num{3.10}$                                                      \\
$\num{10092}$     & $\num{0.736}$           & \SI{1.29e-06}{}       & \SI{7.14e-07}{}        & $\num{0.0082}$                        & $\num{7.81}$                                                 & $\num{2.22}$                                                            & $\num{1.10}$                                                      \\
$\num{10098}$     & $\num{0.737}$           & \SI{1.29e-06}{}       & \SI{7.12e-07}{}        & $\num{0.0082}$                        & $\num{7.84}$                                                 & $\num{2.55}$                                                            & $\num{0.58}$                                                      \\
$\num{10094}$     & $\num{0.741}$           & \SI{1.28e-06}{}       & \SI{7.12e-07}{}        & $\num{0.0083}$                        & $\num{8.46}$                                                 & $\num{1.99}$                                                            & $\num{1.00}$                                                      \\
$\num{10129}$     & $\num{0.737}$           & \SI{1.29e-06}{}       & \SI{7.07e-07}{}        & $\num{0.0082}$                        & $\num{8.64}$                                                 & $\num{1.97}$                                                            & $\num{0.68}$                                                      \\
$\num{10110}$     & $\num{0.735}$           & \SI{1.28e-06}{}       & \SI{6.77e-07}{}        & $\num{0.0083}$                        & $\num{9.04}$                                                 & $\num{2.48}$                                                            & $\num{0.03}$                                                      \\
$\num{10091}$     & $\num{0.744}$           & \SI{1.31e-06}{}       & \SI{7.25e-07}{}        & $\num{0.0083}$                        & $\num{9.22}$                                                 & $\num{1.51}$                                                            & $\num{2.68}$                                                      \\
$\num{10091}$     & $\num{0.741}$           & \SI{1.31e-06}{}       & \SI{7.12e-07}{}        & $\num{0.0083}$                        & $\num{9.61}$                                                 & $\num{1.49}$                                                            & $\num{2.15}$                                                      \\
$\num{10109}$     & $\num{0.734}$           & \SI{1.28e-06}{}       & \SI{6.79e-07}{}        & $\num{0.0084}$                        & $\num{9.64}$                                                 & $\num{1.77}$                                                            & $\num{0.18}$                                                      \\ \bottomrule
\end{tabular}%
}
\end{sidecaption}
\end{table}

\begin{figure}[!htbp]
\begin{sidecaption}[Evolution de la distribution rang-taille pour le meilleur jeu de valeurs de paramètres]{Evolution of the rank–size distribution during a simulation of one of the best calibrated parameter settings.}[fig:S_ranksize]
  \centering
 \includegraphics[width=1.0\linewidth]{RTbleu.png}
  \end{sidecaption}
\end{figure}

\begin{figure}[!htbp]
\begin{sidecaption}[Variabilité de la rang-taille pour 100 réplications]{Variability of the rank–size distribution at the final simulation step in 100 simulations of one of the best calibrated parameter settings.}[fig:S_hypervolumemedian]
  \centering
 \includegraphics[width=1.0\linewidth]{varRTrouge.png}
  \end{sidecaption}
\end{figure}

\section{Conclusion}
\label{sec:conlusion}

An automatic calibration procedure was applied for the first time on a multiagent model, SimpopLocal, which was developed to simulate the emergence of a system of urban settlements. It provides convincing results to solve the calibration problem for multiagent models. With five parameters whose value could not be estimated from empirical data, it would have been quite difficult to find an estimation of a calibrated parameter setting ‘manually’. By reversing this calibration problem into an optimisation problem, our automated calibration procedure generates parameter settings which reproduce the stylized facts embodied in three objective functions very well. This enables us to confirm a decisive advance in the validation of this model: we proved that the SimpopLocal model, as conceived in its simplicity, is able to satisfactorily generate a plausible evolution including the emergence of an urban hierarchy. and quantifying criteria for evaluating the model; a methodology for exploring the parameter space with an automated process using evolutionary algorithms; a technical protocol for reducing the exploration time by massive distributed computing. The application of these principles to modelling not only improves the quality of communication about the model, but also ensures the repeatability of its experimentation and contributes to creating accessible modelling tools that can be shared via the free and open-source software tool for model experiments. OpenMOLE ( \href{http://www.openmole.org}{http://www.openmole.org}) . In order to help the dissemination of these techniques, we published several tutorials on the developed tools. For example, on how to design a grid exploration of a NetLogo model\Anote{foot4} and how to explore a NetLogo model with evolutionary algorithms.\Anote{foot5} This work forges a path for other modellers, who could use a similar data-intensive grid-based approach in their modelling experiments.

\textbf{Acknowledgments.} This work is funded by the ERC Advanced Grant Geodivercity, the Agence de l’Environnement et la Maitrise de l’Energie, the network Réseau de Recherche sur le Développement Soutenable, and the Institut des Systèmes Complexes Paris Île-de-France. Results obtained in this paper were computed on the biomed and the vo.complex-system.eu virtual organization of the European Grid Infrastructure ( http://www.egi.eu ). We thank the European Grid Infrastructure and its supporting National Grid Initiatives (France-Grilles in particular) for providing the technical support and infrastructure. We also thank an anonymous reviewer for help in improving the first version of the paper.

\section{Annexe}
\label{sec:annexe}

\begin{figure}[!htbp]
\begin{sidecaption}[Diagramme de classe]{Diagramme de classe}[fig:S_classe]
  \centering
 \includegraphics[width=1.0\linewidth]{slocal_uml.pdf}
  \end{sidecaption}
\end{figure}

\begin{figure}[!htbp]
\begin{sidecaption}[Diagramme d'activité principal]{Diagramme d'activité principal}[fig:S_activite]
  \centering
 \includegraphics[width=0.9\linewidth]{slocal_state.pdf}
  \end{sidecaption}
\end{figure}

\begin{figure}[!htbp]
\begin{sidecaption}[Diagramme d'activité pour la gestion des innovations]{Diagramme d'activité pour la gestion des innovations}[fig:S_activite]
  \centering
 \includegraphics[width=1.0\linewidth]{slocal_groupeinov.pdf}
  \end{sidecaption}
\end{figure}

\begin{figure}[!htbp]
\begin{sidecaption}[Diagramme d'activité pour la diffusion des innovations]{Diagramme d'activité pour la diffusion des innovations}[fig:S_activite]
  \centering
 \includegraphics[width=0.6\linewidth]{slocal_diffuse.pdf}
  \end{sidecaption}
\end{figure}

\printbibliography[heading=subbibliography]

\input{p5_entretiens}
