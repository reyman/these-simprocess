% -*- root: These.tex -*-

\section{Une plateforme \textit{High Performance Computing} (HPC) pour l'exploration de modèles de simulation en géographie : OpenMOLE}
\label{sec:retourgeoHPCopenmole}

\epigraph {Fou de joie, le géographe s'appuya de tout son long sur le clavier pour en enfoncer toutes les touches; et de la multitude des composants, circuits, transistors, condensateurs et rupteurs, des disques et des disquettes, des fils et des interfaces, des périphériques, sphériques et féériques, des matériels, des logiciels, des progiciels et de près du Ciel, monta une prodigieuse clameur.

C'était le chant du monde.

Alors le géographe s'endormit, apaisé, car il était comblé.}{ --- \textup{Henry Chamussy - Songe d'une nuit de calcul.. - 1984}}

\epigraph {I know I've made some very poor decisions recently, but I can give you my complete assurance that my work will be back to normal. I've still got the greatest enthusiasm and confidence in the mission. And I want to help you.}{ --- \textup{HAL9000 - 2001, l'odysée de l'espace  - 1968}}

\subsection{Engager \enquote{un retour} vers le HPC en géographie}
\label{ssec:retourHPC}

Si le terme de \textit{High Performance Computing} HPC semble d'origine récente lorsqu'il est entrepris par les institutions \autocite{HPCHorizon2020}, il suffit d'oser jeter un regard en arrière pour trouver son emploi chez les géographes à la fin des années 1990. A la lecture de ces ouvrages \autocites{Turton1998, Openshaw2000, Openshaw2000b}, on s'aperçoit que cet acronyme n'a vraiment de sens que si on le contextualise. Ainsi, pour \textcite{Openshaw2000} \foreignquote{english}{The term \enquote{HPC} is very easy: it stands for high-performance computing (or \enquote{computer} depending on context), but the definition of what is \enquote{high-performance} is vague, relative and almost constantly changing as hardware continues to improve. It is a characteristic feature that today's workstations now offer levels of performance (or better) than only three-five years ago required extremely expensive HPC hardware in the form of vector supercomputers.}

En ce sens, la source brute de calcul impliquée par le terme HPC sous sa forme actuelle correspondra probablement d'ici 20 ans à la puissance d'un micro-ordinateur standard, comme on déjà pu le voir dans le passé sous le coup de la loi de Moore, dont on a fêté il y a peu les 50 ans (1965 - 2015); une loi quasiment devenue pour certains observateurs une prophétie autoréalisatrice, de par la pression économique et la course à l'innovation qu'elle maintient dans l'industrie et la recherche. Créer et utiliser de super-ordinateurs (\textit{supercomputers}) pour réaliser des prouesses en calculs impossible autrement, et cela avec ou sans parallélisme des applications, est une idée qui trouve des racines évidentes dans les tout débuts de l'informatique, comme on le verra plus loin.

Le HPC ne peut donc que difficilement être vanté comme provenant d'une quelconque révolution matérielle ou logicielle récente, et ne se substitue qu'à l'expression logique d'un mouvement entamé il y a bien longtemps, celui-ci s'étant largement diversifié par la suite. Se sont ainsi succédés depuis les années 1930 (Z1 mécanique) / 1950 (ENIAC électronique), une longue série d'innovations dans le matériel, le logiciel, et plus généralement dans les paradigmes informatiques qui peuvent les motiver. Le HPC est donc un terme utilisé pour pointer tout autant les anciens (le mythique et \enquote{monstrueux} Cray-1 de 1976) que les plus récents super-ordinateurs (ADA ou TURING du laboratoire CNRS IDRIS, 230 et 1258 TFlops), mais également depuis les années 1990 la mise en réseau local d'ordinateurs standards à moindre coût \Anote{projet_beowulf}, ou encore une puissance répartie sur la base d'une mise en réseau de cartes micro-pc \Anote{projet_parallela}, ou de milliers d'ordinateurs de particuliers bénévoles (le 13 mai 2015 l'initiative BOINC rassemble 720000 ordinateurs et totalise 6,7 PFlops \Anote{puissance_collective}). Tout cela sans compter enfin cette infrastructure scientifique nommée grille de calcul qui supporte à une maille locale, nationale, ou internationale la mise en réseau de tout ce qu'il convient d'appeler aujourd'hui de façon générique des noeuds de calculs tant leur nature est hétérogène.

Il y a évidemment des différences dans l'organisation et la gestion de ces différentes ressources, dans les motivations guidant leur construction et leur bonne utilisation; mais en soi, la finalité du projet semble assez similaire, avec la mise à disposition au plus grand nombre d'une ressource de calcul permettant la résolution la plus rapide possible, et/ou à moindre coût, de problèmes scientifiques ou industriels de nature plus ou moins complexe.

Le terme HPC ne semble être au final qu'une réminiscence d'une course à la performance d'origine bien plus ancienne; un terme parapluie qui accueille dans son ombre une diversité toujours plus grande de matériels et de logiciels.

L'\textit{European Technology Platform for HPC} (ETP4HPC) créé en 2011, reconnu comme la plateforme de référence pour l'établissement de partenariat public privé et la réalisation des objectifs du projet HPC \enquote{Horizon2020} , définit le HPC ainsi :

\foreignblockquote{english}[\cite{ETP4HPC2013}]{
High-Performance Computing (HPC) refers to any form of computing where the density of processing or the size of the problems addressed require more than a standard or commodity computing system in order to achieve the expected result under the given constraints, and the application of advanced techniques such as the use of multiple processors (tens, hundreds, thousands or even more) connected together by some kind of network to achieve a performance well above that of a single processor. Both projects began in 1993, with significant systems deployed in 1994, and both had strong
impact on the community, essentially defining the range
of capabilities and techniques to be incorporated to this
day.

Two traditional HPC categories are distinguished:

\begin{itemize}
\item Capability computing refers to the use of a large and high-performing computing infrastructure to solve a single, highly complex problem in the shortest possible time. Systems used for this purpose are called supercomputers and consist of many tightly coupled compute nodes with distributed memory, all controlled by a single unit.

\item Capacity computing refers to optimising the efficiency of using a compute system to solve as many mid-sized or smaller problems as possible at the same time at the lowest possible cost.
\end{itemize}
}

Même si cette définition synthétique comporte du vrai, l'approche contextualisée du terme telle que choisie par Openshaw est non seulement plus éclairante sur la diversité des approches aujourd'hui masquée par l'acronyme, mais elle permet aussi de rendre hommage à ce qui s'est fait auparavant en des termes équivalents. Les années 1993 et 1994 citées par l'ETP4HPC ne représentent qu'une des étapes (avec l'accélération et la généralisation des moyens pour effectuer du calcul scientifique) dans une histoire des usages du calcul \enquote{intensif} par les scientifiques d'origine beaucoup plus ancienne. En résumé, ce que l'on appelle \textit{supercomputers} ne tient pas seulement d'une histoire qui a démarré en 1993. Par contre, là où cette définition rejoint celle d'Openshaw, c'est sur cette impressionnante montée en intensité permise par l'apparition d'architectures massivement parallèle au tournant 1980-1990, décrite un peu plus loin dans la section \ref{p:Tournant1980}.

Toutefois, il faut bien reconnaitre de façon positive l'intérêt récent porté à ces ressources les institutions européennes, et la considération de celles-ci à juste titre, comme un levier aujourd'hui indispensable pour définir l'innovation. Comme en témoignent les intitulés des projets européens \enquote{Horizon2020}, il s'agit aujourd'hui de se positionner dans une course qui n'est pas sans rappeler celle que l'on a connue pour l'espace, où la mutualisation est devenue nécessaire pour \enquote{tenir la distance} face à certaines puissances (la Chine possède le numéro 1 du top 500 depuis 2013, avec Tianhe-2 à 33,86 PFlops). C'est du moins ce que laissent entendre les intitulés des projets pour \enquote{Horizon2020} \foreignquote{english}{High-Performance Computing: Europe's place in a global race} \autocite{HPCHorizon2020}, encore repris sous une autre forme dans le programme de l'ETP4HC \autocites{ETP4HPC2012, ETP4HPC2013} \foreignquote{english}{Today, \enquote{to Out-Compute is to Out-Compete} best describes the role of HPC.}

Qu'en est-il aujourd'hui de la place des géographes et de la géographie dans ces grands défis scientifiques imposés par l'Union européenne, dont le HPC et ses usages scientifiques font partie intégrante ?

Comme le fait remarquer Arnaud Banos dans son HDR \autocite[63]{Banos2013}, puis dans l'introduction de ce qui peut être considéré comme la seule conférence récente sur le calcul intensif en SHS, les sciences humaines et sociales sont les grandes absentes du nouveau livre blanc du CNRS sur le calcul intensif \autocite{COCIN2012}. Si on regarde également du côté des derniers rapports d'activités de la société civile \enquote{Grand Équipement National du Calcul Intensif} (GENCI\Anote{genci}) \autocite{GENCI2014} qui représente la France en Europe (projet \textit{Partnership For Advanced Computing in Europe} (PRACE) européen), et pilote depuis sa création en 2007 le centre de calcul du CNRS IDRIS, les centres nationaux comme le CINES, et les équipements HPC des centres régionaux (EquipMeso), là aussi les sciences humaines sont encore une fois totalement absentes du côté des usages.

Si ce constat ne saurait suffire à prouver l'absence de géographes sur ces plateformes de calculs HPC, on extrait dans un premier temps quelques questions nous permettant à défaut de pouvoir répondre entièrement à cette affirmation, d'y voir au moins un peu plus clair :

\begin{figure}[!h]
\begin{sidecaption}[Un placement historique des trois questions]{Placement historique des trois questions}[fig:S_historiquePlacement]
  \centering
 \includegraphics[width=.9\linewidth]{plan_HPC.pdf}
  \end{sidecaption}
\end{figure}


\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item Une première partie chercher à démontrer quels sont les enjeux actuels du HPC pour la géographie. Pour cet argumentaire on mobilise les textes des  géographes de l'école de Leeds, pionniers dans les usages du HPC chez les géographes \textcite{Openshaw2000b}. Ecrit dans la décennie 1990-2000, les arguments développés restent d'actualité et s'avèrent toujours très convaincants pour démontrer en quoi il est problématique encore aujourd'hui pour la géographie et les géographes de ne pas s'intéresser aux usages possibles du HPC.
\item Ce constat nous permettra d'évoquer ensuite les possibles causes à ce désintérêt actuel (zone verte), en évoquant notamment les aspects techniques difficiles du HPC, et l'enseignement informatique souvent absent des formations en géographie.
\item La troisième partie (zone rouge) questionnera les pratiques passées, en mobilisant parfois des entretien dans une démarche plus descriptive. L'objectif est de montrer qu'il y a bien eu des usages du HPC dans le passé, dans des centres de calcul en connections avec des \textit{supercomputers}, pour la computation (statistiques, analyses de données, cartographie, télédetection, etc.) mais également pour la simulation (exécution, calibrage). On découvrira également qu'il existe une filiation historique dans les usages passés et présents du HPC, car certains laboratoires mobilisent encore ce type de ressource dans certains domaines de la géographie (télédétection, simulation). Conscient de cette réalité historique, et malgré ces cas particuliers, les problématiques soulevées dans la partie (a) et (b) en ressortiront tout de même renforcé, car si ces pratiques du HPC étaient difficiles mais faisables \enquote{hier}, pourquoi se sont elles plus ou moins perdues \enquote{aujourd'hui }?
\end{enumerate}

Une synthèse sera proposée en fin de section pour faire le lien entre ces différentes sous-sections.

%\item Pourquoi cette activité a disparu ? Est ce l'absence de  challenges à la hauteur en géographie ?

%En faisant le point de façon plus détaillé sur les pratiques informatiques de certaines équipes de géographes dans les centres de calcul, nous verrons d'une part que les challenges au delà de la micro-informatique continue d'exister bien après sa démocratisation, et que des tentatives d'explorations des modèles par l'usage d'algorithmes piochés dans les outils interdisciplinaire  comme l'exploration de modèle intervient immédiatement ou presque après  des retour vers le HPC entamé depuis 2010 au laboratoire Géographie-cités des pratiques des modélisateurs de Géographie-cités une pratique resté latente jusqu'à la redécouverte  on mettra en valeur les pratiques certains des usages ayant émerger dans les années 1980 au laboratoire Géographie-cités, en explicitant
% Le point de vue d'Openshaw

\subsubsection{GéoComputation et HPC, quels enjeux pour la modélisation ? }
\label{sssec:enjeuxHPC}

Le mieux pour tenter de répondre, au moins partiellement, à cette première question, est de repartir des réflexions menées sur le HPC ces 25 dernières années par Stan Openshaw, Ian Turton et leurs collègues de l'école de Leeds. Ce qui n'était au départ qu'une intuition en 1983\Anote{openshaw_intuition} devient très vite une réalité applicative en 1986 \autocite{Openshaw1988}. Basé sur l'usage du HPC et des techniques issues de l'IA (\textit{simulated annealing},\textit{genetic algorithm}, \textit{genetic programming}), Openshaw introduit son article \textit{Building an Automated Modelling System to explore a universe of spatial interaction models} (AMS) une rupture dans la façon de construire, évaluer et de façon plus générale penser l'exploration de modèles\Anote{modele_interactionspatiale} en géographie; rupture qui fait d'ailleurs à cette période parfaitement écho aux nouveautés technologiques annoncées par les différents articles de Couclelis : Automates Cellulaires \autocite{Couclelis1985}, Intelligence Artificielle, etc. \autocite{Couclelis1986}

Le saut technologique et méthodologique permis par ce projet, ainsi que les autres projets développés en parallèle, finit probablement de persuader Openshaw et son équipe de l'étendue des possibilités bientôt offertes par l'accélération et la transformation que subit le HPC entre 1980 et 2000 (voir section suivante \ref{p:Tournant1980}).

Cette réflexion, l'école de Leeds finit en 1996 par l'intégrer dans un nouveau paradigme pour la géographie, la \foreignquote{english}{GeoComputation}

\foreignblockquote{english}[{\cite[4]{Openshaw2000b}}]{GeoComputation is not just the application of computers in geography. Nor is it just about computation for its own sake. It is meant to imply the adoption of a large-scale computationally intensive scientific paradigm as a tool for doing all manner of geographical research. Some will now claim they have been doing GC for 10 or 30 years or more. This is certainly possible, but if they were then, until 1996, it was certainly called something else; terms such as mathematical modelling, simulation, statistical modelling all spring to mind. There are three aspects which makes GC special. Firstly, there is an emphasis on the ‘geo’ subject [...] Secondly, the computation subphrase in GC is also special. It is the intensity of the computation that is especially distinctive [...] Thirdly, just as important and maybe even more significant is the underlying mindset. Computation implies a very particular paradigm based on numerical approximation rather than analytical precision. It can be based on data-driven high-performance computer-powered inductive tools rather than data free, analytically based, deductive methods. It involves trying to compute solutions to problems that could not previously be solved at all. It is based on substituting vast amounts of  computation as a substitute for missing knowledge or theory and even to augment intelligence. It could be data driven in a data mining sense, or it could be entirely data free with large-scale computer experimentation being used as a purely theoretical tool for understanding how complex systems work via modelling and simulation of their dynamics and behaviours. }

Toute une partie de la mise en oeuvre de ce paradigme est apportée par l'usage démocratisé du HPC.

\foreignblockquote{english}[{\cite[17]{Openshaw2000}}]{GeoComputation is a relatively new term invented(or first used in its current form) in 1996. It is defined as the adoption of a large-scale computationally intensive approach to the problems of doing research in all areas of geography, including many GIS applications, although the principles are more generally applicable to other social and physical sciences; [...] It involves porting current computationally intensive activities on to HPC platforms as well as the development of new computational techniques, algorithms and paradigms that can take particular advantages of HPC hardware and the increasing availability of spatial information.}

Déjà critiqué pour ses positions techniques un peu trop \enquote{innovantes}, Openshaw prend les devants en explicitant point par point dans son manifeste \textbf{ce que n'est pas} la GéoComputation.

\foreignblockquote{english}[{\cite[11]{Openshaw2000}}]{To summarize, GeoComputation is :
\begin{itemize}
\item Not another name for GIS
\item Not quantitative geography
\item Not extreme inductivism
\item Not devoid of theory
\item Not lacking of philosophy
\item Not a grab-bag set of tools
\end{itemize}}

De par son aspect paradigmatique, et l'ouverture disciplinaire qu'elle défend, cette réflexion n'est pas limitée à une application en particulier, comme en témoigne cette liste d'opportunités très génériques supportées par le HPC :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item \textit{To speed up existing computer-bound activities so that more extensive theory-related experimentation can be performed or to enable real-time analysis of geoinformation;}
\item \textit{To improve the quality of the results by using computing-intensive methods to reduce the number fo assumptions and remove shortcuts and simplifications forced by computational constraints that are no longer relevant;}
\item \textit{To permit larger databases to be analysed and/or to obtain better results by being able to proces finer-resolution data and make good use of very large computer memory sizes, and finally;
\item To develop new approaches and new methods based on computational technologies to provide new analytical tools and models, both of which are going to be highly important in the geoinformation-rich world of the future.}
\end{enumerate}

Comme il le précise déjà lui-même, les gains supposés ne sont pas forcément immédiats, et certains nécessitent le développement, ou le redéveloppement, de modèles ou d'algorithmes pour exploiter différents niveaux de parallélisme\Anote{dependent_application}. Dans le cadre plus restreint des modèles de simulations, nous nous intéressons principalement aux gains immédiats permis par \begin{enumerate}[label=(\alph*)]  \item la réutilisation des modèles ou algorithmes originaux libérés des contraintes techniques d'autrefois (mémoire, processeur)  \item l'utilisation d'algorithmes récents pour résoudre des problématiques autrefois insolubles \item la désimplification des modèles ou protocoles d'exploration \end{enumerate}

D'une certaine façon, la communauté des modélisateurs agents emprunte déjà dans certaines de ces pratiques des améliorations telles qu'elles sont décrites par Openshaw (a). C'est le cas par exemple lorsque \autocite{Brearcliffe2014} tente de reproduire (\textit{reproductibility} et non \textit{replicability}\Anote{reproduire_repliquer}) un des premiers modèles individu-centré développé en 1983 par Hogeweg et Hesper \autocite{Hogeweg1983}. Il est évident qu'une toute nouvelle exploration réalisée avec un micro-ordinateur d'un modèle autrefois développé sur un \textit{mainframe} doté d'un processeur à 25Mhz et 4 Mb de mémoire ne peut qu'apporter de nouveaux éclairages sur le fonctionnement du modèle. Il est en effet aisé de faire beaucoup plus d'exécution du modèle sur la même durée, et cela sans compter tous les bénéfices apportés par un tel changement de plateforme \autocite{Wilensky2007a}.

Faire le pari de la GeoComputation, c'est être plus audacieux, et ne pas seulement se contenter du HPC pour gagner du temps ou des exécutions, mais intégrer dans nos pratiques un style de pensée qui valide l'utilisation pour nos modèles de calculs difficiles ou impossibles autrement qu'en utilisant du HPC (b)(Openshaw parle de \textit{HPC-dependence} ). C'est encore dans ce cadre de liberté, en théorie illimité (il y aura toujours plus de processeurs, toujours plus puissants), que peut s'exprimer au mieux une certaine forme de créativité et d'innovation algorithmique, différente de celle plus contrainte de la micro-informatique \autocite[26-28]{Openshaw2000}. C'est le cas par exemple de la méthode \textit{Pattern Search Exploration} (PSE) développé par \textcite{Cherel2015}, qui peut nous donner une bien meilleure compréhension de la palette des dynamiques exprimables par un modèle de simulation, et cela sans avoir à le re-développer, ou à lui ajouter des objectifs à atteindre susceptibles de contraindre trop fortement cette exploration. Pour Openshaw de nombreuses connaissances sont en réalité déjà sous nos yeux, et n'attendent pour être découvertes que l'exécution de nouvelles explorations\Anote{against_oblivion}. En effet, certains modèles de simulation (modèles de Wilson ? modèles de Peter Allen ? modèle de Weidlich et Haag ? ) n'ont jamais pu être analysés dans le détail, en partie du fait de l'absence de méthodes, ou de moyens de calcul limités.

Concernant le dernier point (c), la simplification intervient déjà dans l'activité de modélisation KIDS ou KISS, via le retrait d'hypothèses inutiles, ou dans le choix du niveau d'abstraction considéré comme judicieux/suffisant vis-à-vis de la problématique donnée. En dehors du fait que la simplification soit déjà au coeur du processus général de modélisation, celle-ci est également bénéfique lorsqu'elle est motivée par une substitution, par exemple lorsqu'il s'agit de remplacer l'expression complexe d'une dynamique micro par une dynamique agrégée macro quasi équivalente (on parle de \textit{surrogate model} lorsque la substitution est par exemple motivée par un gain en performance lors de l'exécution du modèle). La simplification des explorations du modèle est plus dangereuse, car elle met en tension l'interprétation faite des résultats avec des dynamiques dont on ne connait l'expression que de façon incomplète, ou incertaine.

Un exemple récent est donné dans l'exploration du modèle de simulation LUTI MobiSim, développé depuis plusieurs années maintenant par l'équipe ThéMA. Déjà familière du mésocentre de calcul de Franche-Comté \autocite{Asch2012}, il est pourtant indiqué dans la thèse de \textcite{Hirtzel2015} soutenue en février 2015 une limite dans l'utilisation possible des infrastructures disponibles, les simulations s'exécutant seulement sur des machines de $12$ coeurs (5h30 par simulation) ou $16$ coeurs (6h30 par simulation). $8$ machines (x$12$ ou x$16$ donc) permettaient de faire des simulations en parallèle pour un total de $25$ jours de calcul, $790$ simulations exécutées équivalent à $\num{60000}$ heures de calculs cumulés pour l'analyse de sensibilité présentée. Hirtzel évoque dans sa thèse l'impact important qu'une telle contrainte a eu dans la préparation, la planification, et la simplification de cette analyse de sensibilité assez complexe devant évaluer l'interaction entre $79$ paramètres.\Anote{hirtzel}

L'effort est certes déjà impressionnant, et il ne s'agit pas ici de critiquer une expérience aussi importante techniquement et méthodologiquement pour notre discipline; toutefois, au vu de la contrainte technique régulièrement citée comme un goulot d'étranglement important pour l'exécution de ce modèle, il me semble que cela soulève une question d'accès aux ressources informatiques de ce type en SHS en général. En effet, pourquoi se contenter de cette infrastructure limitée et ne pas directement \enquote{taper à la porte} des physiciens ou des biologistes disposant d'équipements nationalisés coûtant plusieurs dizaines de millions d'euros à l'achat (20 millions d'euros pour le lot de calculateurs Ada et Turing), qui seront très probablement remplacés dans un futur proche, et cela hors coût de fonctionnement ?\Anote{naive_eject_shs}

Une autre façon de prendre conscience de l'impact possible de cette ressource dans notre quotidien de modélisateur est de prendre le problème à l'envers, comme le propose \textcite{Openshaw2000}. \foreignblockquote{english}[\cite{Openshaw2000}]{ [...] how would do you research if that PC on your desk was suddenly 10000 times faster and more powerful. It is likely that some researchers would not know what to do with it, some would not want it, but some would spot major new possibilities for using the computer power to do geography (and geo-related science) differently.}

Avec l'effacement brutal d'un grand nombre de contraintes techniques s'ouvre la possibilité d'un tout nouvel horizon de pratiques. Cette idée d'une exploration des modèles de simulation plus systématique, intervenant quasiment sans délai entre chaque remaniement conceptuel ou technique du modèle, et cela durant toute la construction, devient possible.

La possibilité d'une parallélisation à moindre coût rend également possible l'exécution et donc la confrontation de modèles de simulation en parallèle. Ce n'est plus un modèle consacrant une seule série d'hypothèses que l'on évalue, mais un ensemble d'hypothèses regroupées dans une famille de modèles dont on explore les combinaisons de façon concurrente et simultanée, fonction de la question et des données présentées. Une idée qu'avait déjà explorée Openshaw dans son générateur de modèles AMS en 1988 \autocite{Openshaw1988}, et que nous avons reprise sans le savoir à notre compte dans la progression opérée par notre équipe de modélisateurs dans GeoDiverCity \autocite{Cottineau2014b}.

Même sans avoir à réécrire nos modèles pour tirer meilleur parti d'un parallélisme potentiel et plus implicite (1 ville = 1 processeur par exemple), les gains sont déjà évidents si on se contente du niveau de parallélisme le plus courant en géographie aujourd'hui (une simulation = 1 processeur). Un modèle de simulation qui s’exécute en $5$ minutes sur $100$ villes, pourra une fois ces mécanismes validés à cette échelle, être appliqué à une toute autre échelle géographique, en $5$ heures par exemple sur les milliers de villes françaises. Sur un micro-ordinateur disposant de seulement quelques processeurs opérant en parallèle, cette \textit{scaling capacity} du modèle est inenvisageable. Par contre, sur un environnement HPC composé de milliers de processeurs, cette durée de simulation est largement compensée par le nombre de simulations réalisables en parallèle, avec cet avantage qu'une application écrite pour tirer parti de ce parallélisme d'assez haut niveau reste valide quelque soit le matériel qui l'exécute. L'achat régulier de machines supplémentaires ou le remplacement du matériel existant par du matériel plus rapide dans les parcs informatique provoque immédiatement un bond dans la durée d'exécution et le nombre de simulations qu'il est possible d'exécuter simultanément\Anote{plus_ordinateur}.

Avec la démultiplication des capacités de stockage mémoire des machines, et l'avènement de collectes de données à des échelles toujours plus fines, d'autres opportunités s'offrent au modélisateur à la fois d'un point de vue de la modélisation (la portée des axes KIDS/KISS, Stylisé/Particulier \autocite{Banos2013a} est étendue par les capacités techniques ainsi augmentées), mais aussi de la méthodologie. Il est en effet possible d'inclure dans la démarche explicative habituelle des modèles de simulation intégrant des données empiriques de résolution spatiale beaucoup plus fine qu'actuellement. La possibilité de passer d'un niveau de résolution spatiale à un autre est un moyen de ré-évaluer et de confronter la pertinence des mécanismes implémentés et des valeurs de paramètres choisies pour questionner nos connaissances théoriques supposées de la réalité, à différentes échelles d'observations.

On voit également très vite l'intérêt que peuvent avoir ces deux arguments ( \textit{scaling capacity}, résolution des données toujours plus fine) dans le cadre de la micro-simulation qui s'attache à reproduire des phénomènes observés dans des sociétés reproduites grandeur nature \autocite{Sanders2006}, souvent très difficiles à explorer du fait de temps d'exécution importants.

Des scénarios exposés ci-dessus, il faut encore ajouter cette possibilité, déjà exposée, d'utiliser d'anciens (plus rapides) ou de tout nouveaux (impossibles avant) algorithmes tirant parti eux aussi de cette parallélisation des simulations pour les explorations. On pense par exemple aux analyses de sensibilités, aux algorithmes évolutionnaires, etc.

En plus d'offrir une solution relativement pérenne, cet état d'esprit n'impose pas forcément le remplacement de pratiques de modélisation déjà existantes sur micro-ordinateur, nous verrons comment plus tard. En résumé, pour \textcite{Openshaw2000b} \foreignquote{english}{You start by tackling small and simple versions of a more complex problem and then scaling up the science as and when either the HPC systems catch-up or knowledge of algorithms, models, and theory start to show signs of being able to cope [...] \textbf{The message is smart small but think big}}

Cette façon de penser, les géographes modélisateurs en ont déjà saisi depuis quelques temps certains points, avec le développement d'une réflexion géographique appuyée par la computation, et l'adhésion à cette plus grande famille des sciences dites de la complexité. Pour ne citer que l'évolution la plus récente, le passage au paradigme informatique Agent dans les années 1990 est clairement un apport technologique externe à la géographie quantitative, appelant un renouvellement des réflexions méthodologiques et théoriques \autocite{Sanders2006} qui va bien au-delà du simple constat technique. Un apport technologique qui a redonné corps à des voies déjà explorées, tout en offrant la possibilité de résoudre des problèmes techniques ou conceptuels autrefois insolubles en les abordant sous un nouveau jour (le paradigme objet/agent est - entre autre - bien plus flexible pour la représentation des interactions entre entités). Certes la révolution HPC d'après 1990 n'entre pas (encore) en ligne de compte, mais l'idée d'un dépassement par l'apport technologique est déjà là.

L'accession successive à de nouvelles technologies a toujours servi à développer un projet scientifique qui se nourrit de cette multiplicité d'approches. Il faut y voir un processus d'accumulation, et non pas une succession poussée par une mode ou une autre.

Si l'on prend l'exemple de la théorie évolutive des villes de Denise Pumain \autocite{Pumain1997}, force est de constater que pratiquement toutes les nouveautés, technologiques et/ou paradigmatiques susceptibles d'un apport théorique ou méthodologique servant cette théorie (et par extension les objets géographiques concernés), ont été mises en perspective de façon critique dans chaque publication : les outils statistiques unis et multi-variés, les analyses factorielles, les systèmes dynamiques non linéaires, les réseaux de neurones, les automates cellulaires, la modélisation agent, tout a été testé avant d'être intégré dans la boîte des outils potentiellement remobilisables. Car il n'y a pas une technique meilleure qu'une autre, mais bien une combinatoire de techniques constitutives d'un raisonnement scientifique, dans laquelle chacune peut exposer tour à tour de multiples facettes \autocite{Sanders2000, Mathian2014}.

Il n'est pas interdit donc de penser que les progrès de l'informatique, si l'on cherche bien, apportent toujours au modélisateur de son époque les moyens de devenir un meilleur aventurier, pour reprendre l'image de l'explorateur donné par \textcite[22]{Banos2013} ou encore en citant la vision \textcite[1]{Openshaw2000b} \foreignquote{english}{Computation permits the investigator to test theory by simulation, to create new theory by experimentation, to obtain a view of the previously invisible, to explore the previously unexplorable, and to model the previously unmodellable.}

En un sens, cette seule phrase me semble suffisante pour imager tous les espoirs que nous avons placés dans ces usages renouvelés du HPC pour l'exploration des modèles de simulations en géographie.

Pour \textcite{Openshaw2000}, tous ces nouveaux usages ont acquis une forme de matérialité dès que la démocratisation du HPC s'est enclenchée. Il suffisait déjà dans les années 2000 de quelques efforts pour saisir cette chance. C'est d'ailleurs tout l'enjeu de \textcite{Turton1998} à cette période, lorsqu'ils proposent, forts d'une expérience enracinée dans les années 1980 et de multiples prototypes fonctionnels innovants, de former les géographes aux usages simplifiés du HPC en moins de 200 pages. Presque 15 ans plus tard, et en dehors du colloque annuel de \textit{GeoComputation}, pourquoi cet argumentaire, pourtant brillant, développé par les supporters de la \textit{GeoComputation}, n'a pas été entendu et mis en pratique outre-Atlantique ?

%Pour de multiples raisons, dont certaines déjà évoqués par Openshaw en 2000 - et sur lesquels nous reviendrons indirectement en étudiant le cas francais - il semble malheureusement que ces appels soient restés vains en dehors du cercle des géographes de Leeds, et des participants majoritairement anglo-saxon (plus géomaticiens ? plus géographes ?) régulièrement présent à la conférence \enquote{GeoComputation}.


\subsubsection{Quelles explications possibles pour ce désintérêt envers le HPC après 1990 ? }
\label{sssec:desertionHPC}

\textcite[20-22]{Openshaw2000} citent un certain nombre de critères qui peuvent selon eux expliquer ce retard chez les Anglo-saxons.

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item Jusqu’à récemment (1998 du point de vue de l’auteur), cette ressource n’était pas satisfaisante pour envisager une utilisation plus généralisée en géographie. Il est à noter selon lui que la plupart des usages du HPC en géographie ne permet pas un usage incrémental, et il existe un véritable seuil à partir duquel les ressources à disposition deviennent intéressantes. Il est vrai que la mémoire mise à disposition des géographes dans ce type de machine est par exemple un facteur important d’utilisabilité, car que cela soit pour l’analyse spatiale ou la simulation, il est important de pouvoir utiliser et stocker un maillage spatial suffisamment représentatif de la situation étudiée.
\item  La critique de fond adressée aux quantitativistes en géographie depuis les désillusions des années 1970, et la montée en puissance de géographie(s) plus qualitatives.
\item L’absence de tradition dans l’utilisation de ressources de calculs importantes, et le peu d’applications sont en partie la conséquence des deux points suivants.
\item  Les objections philosophiques héritées des critiques du néo-positivisme ou positivisme logique, et la peur de ne pas arriver à concevoir l’outil technologique comme un support neutre : \foreigntextquote{english}[{\cite[21]{Openshaw2000}}]{Computation is just a tool, and how the tool is used and what it is used for and the context in which it is used depend on the interests, skills, and value systems of the user, which are themselves grounded in contemporary society.}
\item La complexité \enquote{apparente} de la programmation et la nécessité d'acquisition de nouvelles capacités qualifiées de difficiles par beaucoup de géographes ne programmant plus depuis longtemps.
\item Le fait qu’on puisse imaginer cette activité computationnelle comme un renoncement automatique à toute pensée critique.
\item Il n’y a aucun agenda de recherche et aucune ressource informatique mise à disposition des géographes pour s’initier à l’usage des HPC.

\end{enumerate}

L'histoire et le contexte anglo-saxon sont évidemment très différents du contexte français, mais les points évoqués ici par Openshaw forment toutefois une première liste à minima de pistes à explorer depuis notre position actuelle. Les limitations techniques (a), et le peu de projets (c) ont pu être un facteur limitatif évident jusqu'à la fin des années 1990, mais aujourd'hui ces arguments sont plus difficiles à mobiliser. C'est également le cas du dernier point (g): des appels à projets et des infrastructures sans précédent sont en train d'émerger ces dernières années à un niveau européen, la motivation des géographes pour intégrer ce type de projet devrait logiquement être beaucoup plus audible. Restent donc les critiques classiques (b)(d)(f) dont on s'apercevra par la suite qu'elles continuent d'être mobilisées pour freiner le retour pourtant nécessaire d'un débat sur les rapports de la géographie à l'informatique, un frein visible aussi dans les enseignements de cette discipline (e).

Pour comprendre de quelles limitations techniques il s'agit, il faut faire le point sur ce qu'implique cette transformation touchant le HPC dans les années 1990. Une fois mise en perspective toute la difficulté pour un géographe d'accéder seul à ce type de ressources, il faudra bien se poser la question plus générale de l'autonomie que l'on veut donner aux géographes sur le plan des enseignements informatiques, avant d'imaginer dans la continuité de nos pratiques, une solution plus concrète.

\paragraph{Tournant des années 1990 dans le HPC}
\label{p:Tournant1980}

La plupart des informations historiques et techniques proposées ici sont tirées des ouvrages suivants \autocites{Fox1994, Fox1988, Seitz1985, CM2-1990, Lerman1993, Padua2011, Dietrich1984}[81-84]{Culler1998, Steele2011} ainsi que des \textit{Working Report} suivant \autocites{Athas1987, Su1987, Seitz1983, Seitz1984a, Seitz1984b} dont la liste plus complète est disponible sur le \href{http://authors.library.caltech.edu/view/person-az/Seitz-C-L.html}{@site} des archives de Caltech.

Entre les années 1960-1980 on peut encore parler d'une phase de recherche et d'expérimentations, avec la création et l'amélioration des premiers \textit{supercomputers} à processeurs vectoriels, particulièrement adaptés au calcul scientifique rattaché généralement à cette époque à l'architecture SIMD\Anote{simd_def} qui va ensuite dominer le marché du HPC jusqu'en 1990\Anote{liste_hpc_simd}. Sur la base de ces premiers super ordinateurs, très coûteux, des années pré-1980, le domaine naissant du HPC va ensuite connaître un certain nombre de révolutions entre 1980 et 1990.

%Sur une autre branche, les MIMD, de nombreuses expérimentations concernant les architectures multi-processeurs, et la gestion du parallélisme ont également lieu : D825 (1962), CDC 6500 (1966), Multics system (1969)

Entre autres innovations technologiques, l'introduction progressive des microprocesseurs depuis les années 1970 va permettre d'aller beaucoup plus loin dans l'invention fin 1970, début des années 1980 de nouveaux paradigmes de construction pour les ordinateurs. %Le parallèlisme peut être pensé autrement, et s'appuyé sur la baisse des couts qui frappe l'industrie informatique dans les années 1980.

Danny Hillis publie dès 1981 un mémo au MIT donnant les concepts \autocite{Hillis1981} d'une nouvelle machine massivement parallèle nommée \textit{Connection Machine} (CM), qui va amener un certain renouveau dans la façon de penser ce type d'architecture SIMD.

Ce dernier travaille d'abord avec Minsky et Papert au laboratoire MIT sur le langage LOGO, avant de créer la société \textit{Thinking Machine} en 1983. Celle-ci commercialise une série de CM (\textit{Connection Machine}) d’architecture SIMD (puis MIMD\Anote{mimd_def}), dont le premier prototype est présenté à la DARPA en 1985, et la première livraison commerciale est réalisée en 1986. Ces machines constituent une référence historique initiale importante dans la construction de machines composées de plusieurs milliers de processeurs.

Sachant que David Hillis s'est largement inspiré des travaux de Minsky et de son livre \textit{Society of Mind}\Anote{mind_minsky}, cette architecture n'est pas sans évoquer l'image idéalisée du fonctionnement d'un cerveau\Anote{note_cm} tel qu'on se la représente à l'époque en IA symbolique, les CM's ciblant d'ailleurs résolument ce type d'application avec l'implémentation machine d'une version spécialisée de lisp, le *lisp (star lisp). Ainsi même si ce type d’architecture SIMD (dominante sur le marché des HPC jusqu’aux années 1990) est généralement plus complexe à appréhender, la CM a par exemple été utilisée très tôt pour développer des applications rattachées aux systèmes complexes dans le cadre de l’\textit{Artificial Life} et des \textit{Complex Adaptative System} (CAS), car elle s’appuie dans sa construction (voir la participation de Feynamn, et de Wolfram sur l'architecture de la CM) sur la propriété de parallélismes propres aux automates cellulaires (1 processeur = 1 cellule/patch)\Anote{CA_physical}. Les CM proposent certes une alternative intéressante par cette possibilité augmentée de parallélismes, mais elles brillent aussi par leur performance, la CM-1 (prototype) de 1985-1986 , la CM-2 (commercialisation) de 1986-1987 (2.5 à 6 GFlops), la CM-5 (qui passe d'une architecture SIMD à MIMD, avec 1024 processeurs) est numéro 1 du premier \href{http://www.top500.org/featured/systems/cm-5-los-alamos-national-lab/}{@Top500} de 1993 avec 59.7 GFlops sur le Linpack test. En comparaison, à l'IDRIS en 1993, le Cray C98 Axis avec 8 processeurs dépasse à peine les 7 GFlops.

Des travaux similaires ont également lieu depuis les années 1970 dans l’équipe de physiciens de Tommaso Toffoli (en contact aussi avec les travaux du français Yves Pomeau \autocites{Toffoli2005,Toffoli1987}) au MIT, qui finissent par concevoir, dans le courant des années 1980, un \textit{hardware} spécifique (CAM-6, CAM-8) pour paralléliser les CA en s’appuyant sur leurs propriétés fondamentales \autocite{Toffoli1987}\Anote{ca_simd_avantage}.

%Epez1993

En France, il existe également des développements similaires en physique, avec la famille de machines R.A.P (Réseau d’Automates Programmables) démarrée à l'ENS en 1986 à Paris, OUPPI (Ordinateur Utilisant des Processeurs Parallèles Intégrés) à Marseille \autocites{Manneville1989, Hiebeler1990}. Ces deux architectures différentes (voir \autocite{Hillis1981}) inspirées par les CA \autocites{Hillis1984, Hillis1989, Toffoli1987} et issues de travaux démarrés au MIT, sont également utilisés pour des applications scientifiques similaires \autocite{Toffoli2005}. L’équipe de l’UCLA de David Jefferson et ses modélisations de fourmilières (\textit{Tracker}, \textit{Genesys}, \textit{AntFarm}) utilisent une CM contenant pour la première fois plusieurs milliers de fourmis, un préalable pour observer des comportements auto-organisationnels. Ou encore la première version parallélisée du langage LOGO qui sera développé sur ce type de machine CM avant de passer plus tard sur Macintosh (Starlogo 1991). Au Los Alamos National Lab (LANL), Langton recrute Hiebeler\Anote{hiebeler_parcours} en 1989 pour travailler sur le logiciel \textit{CellSim} initié en 1988 et mis à jour jusqu’en 1990. Hiebeler possède une expertise sur les CAM-6, mais également sur les CM car il a travaillé à \textit{Thinking Machine} entre 1991 et 1992, ce qui lui permet de développer une interface sur ce logiciel vers des machines de type CM. Ces deux types de \textit{hardware} seront voués à un échec commercial, pour des raisons diverses qui ne touchent pas forcément au seul aspect scientifique, car ces outils ont constitué de réelles avancées d’un point de vue de la computation scientifique aux moments où ils étaient disponibles.

Dans l'avalanche d'innovations ayant lieu dans le courant des années 1970-80 au niveau du \textit{software} et du \textit{hardware} (topologie hypercube, vlsi et microprocesseur, baisse des coûts de production, langage acteur \autocite{Hewitt1973}, Unix, etc. ), le paradigme de construction basé sur l'architecture MIMD\Anote{exemple_simd_mimd} connaît lui aussi un certain rafraîchissement au tout début des années 1980, bien avant la livraison de la première Connection Machine CM-1.

C'est le cas par exemple du prototype \textit{Caltech Cosmic Cube} dont la première version fonctionnelle est finalisée en 1983\Anote{caltech}. Ce dernier est une architecture MIMD à mémoire distribuée, et s'appuie sur une gestion logicielle pour l'envoi des messages entre processeurs. Plusieurs logiciels et plusieurs versions de cette machine vont être commercialisés\Anote{caltech_logiciel}. Évidemment incompatibles entre eux, cette variété de matériels et de logiciels préfigure l'expression chez les utilisateurs d'un besoin rapide pour la constitution d'un standard/norme d'échange de messages implantés de façon transparente sur chaque machine : la norme \textit{Message Passing Interface} (MPI), dont on parle plus ensuite. Cette machine universitaire plusieurs fois améliorée de façon interne au laboratoire de recherche (Mark II (1984)/ Mark III (1986)) agrège autour d'elle plusieurs dizaines de programmes de recherches en tous genres, pour son utilisation ou son amélioration. Cette machine va également être déclinée dans des versions commerciales plus ou moins différentes (meilleurs processeurs, mémoire augmentée, ajout de composants pour réaliser des traitements spécifiques, utilisation d'Ethernet pour les échanges de messages, nombre de noeuds constitutifs, logiciels pour la gestion des messages différents, etc.) : nCUBE, Ametek, Intel iPSC, FPS T-Series, Paralex Gemini.

Autre avantage de cette machine, pour 1\% du prix du Cray-1, elle produit 10\% de ses performance, ce qui prouve pour la première fois la faisabilité et les bons résultats d'une architecture parallèle utilisant des composants standards à moindre coûts. L'intel 8086 étant le premier micro-processeur de la famille x86 que l'on va retrouver dans une variante sur les IBM PC vendus dès 1981 à plusieurs millions d'exemplaires. %Sur la base d'une architecture MIMD à mémoire distribué la communication entre processeurs se fait sur la base d'une topologie en hypercube, avec un protocole d'échange de messages entre processeurs qui n'est plus piloté de façon physique par des routeurs, largement inspiré par la logique acteur de Hewitt.

On pourrait également citer par exemple les travaux sur le NYU Ultracomputer, de David E. Shaw sur la machine \enquote{Non-Von} (pour \enquote{Non Von Neumann}) de la Colombia University, le \textit{Goodyear Massively Parallel Processor} (MPP), et probablement d'autres, moins connus. Toujours est-il que ces différentes initiatives sont toutes reconnues au début des années 1980 comme étant des pionnières dans la construction d’architectures \textbf{massivement parallèles}, c'est-à-dire mettant en oeuvre au moins plusieurs centaines de processeurs.

C'est avec l'apparition ou la maturation durant les années 1980-1992 d'une véritable grappe de nouvelles technologies qu'émerge la notion moderne de \textit{cluster} : apparition des protocoles d'échanges de messages PVM puis MPI, mais également création d'Ethernet, développement d'Internet,  miniaturisation constante et augmentation des fréquences d'horloges des processeurs, apparition de logiciels et de matériels libres, etc. Ainsi cet esprit de réutilisation de \enquote{matériel standard} déjà expérimenté par l'équipe de Berkeley sur le \textit{Cosmic Cube} trouve son apogée dans l'invention plus ou moins simultanée de projets de mise en grappes de micro-ordinateurs dans les années 1990, en s'appuyant non plus cette fois-ci sur une architecture (\textit{hardware}) spécialisée reliant les différents composants (processeurs), mais sur leur simple mise en réseau appuyée par des applications (\textit{software}) de partage et d'échange de données efficients entre les machines.

%Cluster p292 / le projet NOW de Berkeley (NOW emphasized high-end workstations, high performance networks, and proprietary software) / et le projet Beowulf de la Nasa (Beowulf emphasized low cost personal computers, mass market networks (Ethernet), and the new trend in open source software including the Gnu editors and compilers and the Linux operating system)

Mais en dehors de ces applications et de ces architectures pionnières, d’un point de vue utilisateur la révolution tient surtout de l’émergence, et de la mise à disposition d’un plus large public, de protocoles de communication permettant de s’abstraire complètement de l’infrastructure qui supporte physiquement le parallélisme, que celui-ci opère sur un \textit{cluster}, ou sur des ordinateurs distants reliés par Internet, ou un mélange des deux. Ce type de norme a ainsi permis le développement plus aisé de programmes distribués de façon massivement parallèle, sans avoir à se soucier du langage à adopter, et de l’ensemble des contraintes techniques propres à chacune des technologies supportant ce parallélisme. Par exemple, la norme MPI (\textit{Message Passing Interface}) est devenue depuis sa création en 1992-1994 le modèle quasi-dominant implémenté (encore aujourd’hui) sur l’ensemble du matériel dédié à une utilisation partagée.

Ainsi, ces différentes visions d'une infrastructure support du parallélisme peuvent cohabiter et soulager l'utilisateur de la contrainte liée au \textit{hardware}. Il est possible aujourd'hui de programmer un logiciel qui marche tout à la fois sur son propre \textit{cluster} personnel construit à base d'Arduino/Raspberry/Edison, que sur des architectures techniques plus complexes mélangeant différentes technologies ou paradigmes choisis pour une utilisation adéquate. Ainsi, les supercalculateurs d'aujourd'hui sont souvent eux-mêmes des formes de \textit{clusters}, à la fois héritiers des premiers prototypes expérimentant des combinaisons de \textit{hardwares} et \textit{softwares} innovants, mais également héritiers de toutes les avancées liées à la démocratisation qui touchent l'ensemble des aspects matériels et logiciels dans l'informatique.

Le calculateur Turing de l'IDRIS\Anote{idris} est construit selon une architecture massivement parallèle (MPP de type \textit{MIMD distributed memory} \autocite{Snir2011}) cumulant de très nombreux noeuds de calcul ($\num{6144}$ noeuds de $16$ coeurs à $1$ Go de mémoire par coeur) pouvant être connectés selon de multiples topologies. Celui-ci peut être accédé comme une seule et même machine (jusqu'à $\num{65536}$ coeurs simultanés autorisés en exécution). En comparaison, le \textit{cluster} ADA dispose d'une mise en réseau locale très rapide de plusieurs centaines de noeuds de calculs séparés : $332$ noeuds d'architecture type \textit{Symmetric MultiProcessor} SMP) qui mélange deux types d'architectures : \textit{MIMD shared memory} sur le SMP et \textit{MIMD distributed memory} pour assurer la communication entre les SMP. Chaque noeud dispose d'un ensemble de processeurs multi-coeurs ($32$ coeurs) avec une mémoire partagée plus importante ($4$ à $8$ Go par coeur), ce qui fait de ce calculateur ADA une architecture multi-processeurs destinée à des calculs plus communs (jusqu'à $\num{2048}$ coeurs autorisés en exploitation), tout en restant très facilement extensible via l'ajout de nouveaux noeuds.

Comme on peut le voir dans les quelques paragraphes ci-dessus, le jargon technique (SIMD, MIMD-SH, MIMD-DM, DMM, SMP, etc.) qui accompagne l'environnement du HPC est un préalable à l'utilisation probablement rédhibitoire pour qui s'intéresse au HPC sans être du domaine.

\medskip

\textbf{Mais, a-t-on besoin de tous ces détails techniques et de ce jargon ?}

\medskip
Comme le disent si bien \textcite{Openshaw2000} : \foreignquote{english}{We really do not need a whole lot of unnecessary details from computer science and computer engineering. Why not simply take it for granted and concentrate on using the technology? [...] Geographers should be viewing parallel computing as a tool that can be used on their problems; there is absolutely no need to know more than about 0.01 percent of the technical details of how the hardware actually works}

Ce qui nous amène à l'utilisation de ces différents types de ressources groupées sous le terme très générique d'HPC. Pour ce qui est des ressources pilotées par GENCI, toutes les demandes passent par un dossier administratif indiquant la nature du projet scientifique et la demande d'attribution d'heures correspondantes. Les projets doivent être déposés plusieurs mois à l'avance, avant d'être approuvés par un conseil scientifique réuni une à deux fois par an \autocite{GENCI2015}. Sur le \href{https://www.edari.fr/}{@site} de Demande d'Attribution de Ressources Informatiques (DARI) qui accueille les dépôts de dossier GENCI, on trouve la liste des 11 comités d'évaluation existant en 2015, et comme on peut le constater, il n'y a tout simplement pas de comité réservé pour les sciences humaines et sociales : environnement, écoulements non réactifs, écoulements réactifs ou/et multiphasiques, biologie et santé, astronomie et géophysique, physique théorique et physique des plasmas, informatique algorithmique et mathématique, dynamique moléculaire appliquée à la biologie, chimie quantique et modélisation moléculaire, physique chimie et propriétés des matériaux, nouvelles applications et applications transversales du calcul.

Là encore, nous sommes très éloignés des pratiques, mais également des besoins pour la modélisation en SHS. Si on regarde à l'inverse du côté des sciences physiques, ce type d'accès est tout à fait normalisé, l'attribution du temps de calcul faisant souvent partie intégrante du projet de recherche déposé par les étudiants. Il existe aussi des encadrements dédiés pour les scientifiques ou les étudiants voulant apprendre à manipuler ces ressources. La \enquote{Maison de la Simulation} dispose par exemple pour les travaux pratiques de ses formations d'un accès dédié aux ressources HPC disponibles sur le campus du plateau de Saclay. C'est le cas aussi de l'IDRIS qui met à disposition des usagers du CNRS des formations dédiées.\Anote{formation_idris}

Si on regarde du côté des services offerts par un autre type de ressources HPC, les grilles de calculs (\textit{Grid Computing}) offrent une flexibilité d'emploi qui s'avère en revanche très intéressante, et cela malgré des limites évidentes pour certains usages.
% A reintegrer.
Elles offrent le plus haut niveau de parallélisme envisageable, et permettent de partager des exécutions de calculs sur des ressources complètement hétérogènes distribuées n'importe où sur la planète, en communiquant avec elles via Internet. Cette ressource ne fournit pas de mémoire partagée et/ou de mécanisme de partage de données entre les différents noeuds, considérés comme autonomes, ce qui limite leurs utilisations dans le cadre de certaines applications nécessitant à la fois beaucoup de mémoire partagé, une communication forte entre les différents noeuds de calculs, et/ou un niveau de parallélisme interne au programme : c'est le cas par exemple des astrophysiciens travaillant sur des modèles de simulation de l'univers composés de plusieurs milliards d'étoiles en interaction.

Il existe effectivement différents niveaux de granularité envisageables quand on parle de paralléliser des applications :
\begin{itemize}[label=\textbullet]
\item le parallélisme peut être envisagé de façon interne au niveau des primitives : des traitements opérants habituellement de façon séquentielle sur les objets contenus dans une liste vont être parallélisés sur l'appel d'une primitive dédié.
\item le parallélisme peut être envisagé toujours de façon interne au programme, mais au niveau des objets cette fois-ci : c'est le cas par exemple d'une simulation multi-agents où les agents s'exécutent sur un environnement distribué.
\item le parallélisme peut être externe au programme : si on reprend l'exemple d'une simulation, alors ce niveau de parallélisme permet l'exécution simultanée de plusieurs simulations, en s'appuyant par exemple sur un environnement de calcul distribué.
\end{itemize}

Les niveaux ne sont pas exclusifs, et une application peut très bien cumuler ces différents niveaux. Toutefois le parallélisme le plus simple et le plus immédiatement disponible consiste pour le modélisateur à attribuer un noeud de calcul par simulation, car il n'y a théoriquement aucune modification à apporter au programme. Dans ce cas-là, il s'agit d'un niveau de parallélisme externe au programme.

De multiples équipements informatiques implantés et accessibles à des mailles géographiques différentes, d'utilisation pour la production ou la recherche, sont réunis et accessibles aux chercheurs par le biais de divers portails administratifs. Appelées Organisation Virtuelle (\textit{Virtual Organization}), ces entités administratives virtuelles régissent l'attribution des ressources de la grille de calcul en fonction des différents accords passés entre les acteurs intervenant dans la constitution de la grille. Divers critères rentrent en jeu dans ce découpage, ceux-ci peuvent être de nature thématique, technique,selon des zones géographiques, politique, etc.

En France,depuis 2010 c'est le GIS France-Grilles (vo.france-grille.fr) qui mène cette mission de mutualisation entre les différents acteurs publics (Laboratoires, Méso-Centres, Centres nationaux) et privés au niveau national (Initiative Nationale de Grille ou NGI), mais aussi européens en étant le partenaire et représentant francais de l'EGI (European Grid Infrastructure). Piloté par des institutions scientifiques majeures (MESR, CNRS, CEA, INRA, INRIA, INSERM, CPU, RENATER), ce Groupe d'Intérêt Scientifique (GIS) coordonne la mise en place d'une grille de calcul nationale a priori accessible à toutes les disciplines scientifiques par le biais de différentes VO. D'après les statistiques 2012, sur $700$ utilisateurs regroupés en $89$ VO rattachées à la VO France-Grilles, seulement $12$ utilisateurs sont enregistrés dans la catégorie systèmes complexes, et $0$ pour les sciences sociales. Sur $\num{1450}$ référencements entre 2010 et 2013, la collection HAL maintenue par France-Grilles pointant les publications utilisateurs de cette grille nationale, il y avait une seule publication référencée en sciences de l'homme et de la société. Aujourd'hui il y a environ une dizaine de publications référencées, provenant en majorité de géographes.

Dans le cadre des SHS en France, il y a également la mise à disposition de ressources informatique de ce type, notamment par la collaboration au CNRS entre le \href{http://cc.in2p3.fr/}{@CC-IN2P3} et l'ancien TGE Adonis devenu TGIR \href{http://www.huma-num.fr/}{@Huma-Num}. Human-Num offre une liste de services à disposition des laboratoires, dans lequel figure un accès à des ressources informatiques de type grille de calcul, traitées par le biais de l'IN2P3, et accessibles via la technologie \textit{Grid-Engine}.

Pour utiliser tout ou partie de ce type de ressources HPC, il faut que le laboratoire d'accueil soit rattaché à une de ces entités administratives virtuelles, qui pourra ensuite délivrer un certificat d'accès unique au futur utilisateur (fichier crypté contenant une clef unique personnalisée reconnue par les machines de la grille), renouvelable tous les ans. Le laboratoire doit être enregistré auprès de l'autorité de certification du CNRS GRID2-FR pour pouvoir récupérer et valider des certificats. La ressource est ensuite disponible de façon illimitée et relativement immédiate dès lors qu'on possède l'un de ces certificats. La ressource rattachée à chaque VO est généralement partagée avec l'ensemble des différents utilisateurs de celle-ci, même s'il peut exister dans certains cas des systèmes de quota. Une fois inscrit dans une VO, le laboratoire désigne une personne de confiance capable de signer et de délivrer des certificats de façon locale au laboratoire. Le processus pour délivrer les certificats est complexe, mais reste en définitive beaucoup simple qu'un dépôt de dossier administratif et scientifique ensuite jugé par un conseil scientifique thématique, comme c'est le cas via GENCI \autocites{GENCI2014,GENCI2015}. Intégrée par une seule personne, la procédure paraît d'un point de vue utilisateur moins coûteuse (pas de dépôt de dossier scientifique), et très simple (la demande d'attribution et la réception du certificat se font directement par Internet).

Ces modalités d'accès sont donc beaucoup plus intéressantes du point de vue d'un modélisateur dont l'activité de construction nécessite d'évaluer très régulièrement ses modèles de simulation par l'usage de techniques diverses, toutes relativement exigentes sur les aspects computationnels : analyse de sensibilité, plan d'expérience divers, usage d'algorithmes d'optimisation pour la calibration ou l'exploration des comportements du modèle, etc.

En résumé, nous nous intéresserons principalement par la suite aux \textbf{grilles de calculs}, car :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item La finesse de parallélisme engagée dans cette architecture est largement suffisante pour engager une première étape dans la voie du HPC immédiatement bénéfique pour la simulation en géographie,
\item On a accès à une ressource globale masquant l'hétérogénéité du parc de machines qu'elle représente, ce qui en fait potentiellement une ressource en constante évolution/amélioration
\item Les modalités d'accès à cette ressource sont plus compatibles avec les pratiques réelles des modélisateurs
\end{enumerate}

Mais il y a un hic dans tout cela. Si l'émergence, l'amélioration successive, et le maintien ces 20 dernières années de la norme MPI comme un standard indétrônable dans l'industrie du HPC a certes beaucoup simplifié les prérequis nécessaires pour user de ces ressources, le 0.01\% d'Openshaw censé représenter le seuil minimal d'informations techniques qu'un débutant devrait avoir à apprendre pour manier ce type de ressource HPC inclut forcément des bases en informatique. Il faut en effet pouvoir comprendre et utiliser les fonctions MPI implémentées dans un langage de programmation, quelqu'il soit.

Des formations à la programmation, à l'architecture technique spécifique du HPC, sont évidemment ouvertes dans ces différents instituts (GENCI, IDRIS, Maison de la Simulation, etc.). Seulement ces organisations ne sont pas connues de la plupart des géographes, et le niveau technique nécessaire à la compréhension de ces formations est bien trop complexe et trop éloigné du domaine de compétence des géographes pour que ceux-ci s'y présentent spontanément.

Autrement dit, et sans cette formation, une fois l'utilisateur connecté à une ressource HPC, quelle que soit sa forme, que se passe-t-il lorsque le curseur du terminal informatique clignote dans le vide? Quels sont ensuite les manipulations, les commandes, les fonctions à connaître, à appeler, à intégrer dans nos programmes pour exécuter nos calculs, nos simulations en parallèle sur ces infrastructures ?

Même dans le cadre d'une mise à disposition pour les SHS, la ressource est fournie sous sa forme la plus brute\Anote{human_num_note}. On imagine qu'il y a peu de monde disposant, sans projet ou structure interdisciplinaire associée, du niveau suffisant pour mettre en oeuvre l'amorce technique nécessaire à la bonne utilisation de cette ressource informatique.

En définitive, et malgré des efforts conséquents des géographes de l'école de Leeds \autocite{Openshaw2000} pour introduire le HPC et la norme MPI aux géographes dans les années 1990-2000, on ne peut pas vraiment dire que cette pratique se soit diffusée outre-Manche ces 15 dernières années (et peut-être même en dehors d'un cercle très restreint des géographes informaticiens de Leeds).

Faut-il donc persévérer dans cette voie, ou existe-t-il aujourd'hui des solutions alternatives pour amener le HPC jusqu'au bout des doigts des géographes modélisateurs ?


%\item Des logiciels permettent de s'abstraire de la couche de programmation MPI normalement nécessaire pour accéder à ce type de ressources HPC,

%https://hal.archives-ouvertes.fr/FRANCE-GRILLES
%https://hal.archives-ouvertes.fr/FRANCE-GRILLES/search/index/q/%2A/domain_t/shs/
%GRID2-FR

%MPP p574

%Distributed Memory = Chaque processeur a sa mémoire propre, et peux accéder à la mémoire des autres via une forme de connection (réseau local, réseau spécialisé, bus spécifique, etc.)

%Blue Gene L = Distributed Memory MIMD computer

\paragraph{Quelle place pour les débats sur la place de la \enquote{pensée informatique} dans la géographie ?}
\label{p:Tournantenseignements}

Comme le disait déjà Michel Serres en 2011 dans sa séance d'introduction aux nouveaux défis de l'éducation \enquote{Avant d’enseigner quoi que ce soit à qui que ce soit, au moins faut-il le connaître. Qui se présente, aujourd’hui, à l’école, au collège, au lycée, à l’université ?} \autocite{Serres2011} A tous ceux qui pensent la programmation comme un apprentissage hors de portée de nos étudiants en licence, il faut leur ouvrir les yeux sur ce nouveau monde, celui bien réel décrit par Michel Serres dans \enquote{Petite Poucette} \autocite{Serres2012}, celui des étudiants qui se présentent aujourd'hui dans nos enseignements, et sur les initiatives de ceux qui ont déjà compris qu'apprendre à programmer était une activité aujourd'hui accessible dès le plus jeune âge.

Si l'on prend l'exemple d'autres pays, comme aux États-Unis en 2013, lorsque le président Obama prend la parole pour la \textit{Computer Science Education Week} organisée par la fondation \href{http://code.org}{@code.org} créee la même année : \foreignquote{english}{If we want America to stay on the cutting edge, we need young Americans to master the tools and technology that will change the way we do just about everything.} Si l'on peut regretter que cette initiative soit portée essentiellement par les fonds de grandes sociétés informatiques américaines, force est de reconnaître que l'intention politique est bel et bien là. D'autant plus que ce type d'initiative est loin d'être isolé, aux États-Unis, mais également sur la scène internationale. En France un réseau gagnant peu à peu le territoire est en train d'émerger, dans différents lieux d'accueil (bibliothèques, musées, FabLab, réseau des cantines numériques, écoles d'informatiques, clubs, etc.), des regroupements d'initiative locale, ou nationale.

Par exemple, l'initiative \href{http://Simplon.co}{@Simplon.co} \Anote{simplon} créée par \textcite{Bardeau2014} (le livre est conçu comme un manifeste pour \enquote{Lire, Ecrire, Compter, Coder}), est une entreprise au format juridique original (modèle ESS hybride : une entreprise solidaire d’utilité sociale agréée et une association d’intérêt général) qui propose des formations intensives de 6 mois pour les moins de 25 ans, sur dossier, quelque soit leur parcours scolaire. Récemment reconnue et soutenue par le gouvernement comme d'intérêt général, cette initiative, en train d'essaimer un peu partout en France, n'est qu'une parmi des centaines d'autres initiatives aux origines, aux formes différentes, mais aux objectifs communs : \enquote{La programmation pour tous}. Ce même groupe tient par exemple un annuaire \autocite{Simplon2015} qui compte aujourd'hui plus de 42 ressources différentes (club, réseau, écoles, sites, événements, etc.) à destination de l'apprentissage de la programmation chez les plus jeunes, et ce n'est évidemment qu'un échantillon.

Autre exemple, l'initiative des enseignants néozélandais de \textit{Computational Science Unplugged}, qui donne corps au principe universel de \enquote{pensée informatique} (\textit{Computational Thinking}) au travers d'un livre d'activités illustré, pensé pour être utilisé par tout le monde, déjà traduit\Anote{traduction_unplugged} en plusieurs langues\Anote{texte_csunplugged}.

En France, il aura fallu l'émergence spontanée et de plus en plus évidente d'un réseau d'initiatives citoyennes, la pression d'une communauté internationale bien plus au fait de ce mouvement, et l'échec répété de plusieurs projets nationaux, pour qu'en 2012-2013 soient \textbf{ enfin consolidées des instances (Conseil National Numérique) et une politique nationale à ce sujet}.

Les lignes énoncées par l'Académie des Sciences en 2013 \enquote{L’enseignement de l’informatique en France. Il est urgent de ne plus attendre} \autocite{AScience2013}, ou encore les différents rapports du récent Conseil National du Numérique (dont le dernier Jules Ferry 3.0) \autocite{CNNum2014}, les soutiens solides entre ces acteurs\Anote{appui_academie_science}, les lettres ouvertes répétées des associations comme la Société Informatique de France \href{http://www.societe-informatique-de-france.fr/lettre-ouverte-a-monsieur-francois-hollande-president-de-la-republique-concernant-lenseignement-de-linformatique/lettre-ouverte-a-monsieur-francois-hollande-president-de-la-republique-concernant-lenseignement-de-linformatique-2/}{@SIF}, soutenues et relayées par l'association quadragénaire spécialiste de ces questions \textcite{EPI2014}, \textbf{tous ces écrits sont unanimes, il faut agir au plus vite pour dispenser un véritable enseignement informatique, et former ainsi des citoyens qui seront acteurs du monde numérique}.

La réforme en cours pour le collège a annoncé très récemment (\autocite{SIF2015} 18 mai 2015) des pistes concrètes qui rejoignent les lignes énoncées\Anote{extrait_CNN} par le Conseil National du Numérique dans son dernier rapport \autocite{CNNum2014}. Reste à voir quels moyens effectifs seront alloués pour mettre en place et pérenniser véritablement ces annonces :
\begin{itemize}[label=\textbullet]
\item une initiative au codage dès le primaire,
\item la programmation informatique au collège,
\item une nouvelle option informatique en seconde et en première.
\end{itemize}

Un projet dont il faut espérer qu'il ne sera pas abandonné au prochain mandat, car ce n'est pas la première fois que de telles annonces jalonnent l'histoire assez chaotique des relations entre l'éducation, l'informatique et les politiques\Anote{historique_EPI}.

\medskip

\textbf{Quid de l'université et des sciences sociales dans ce mouvement ?}

\medskip

Pour le moment, il est triste de voir que les initiatives prises dans l'enseignement supérieur de la recherche, comme celle de \enquote{France Université Numérique}, ne traitent cette révolution que sous l'angle unique des \textit{Massive Open Online Course} (MOOC) et d'un usage finalement très partiel et passif des technologies au service de l'éducation. L'avenir nous dira donc si cette prise de conscience finira par toucher un système universitaire, pour le moment absent dans ce débat.

Il existe déjà des filières spécialisées dans l'enseignement cumulé des mathématiques et de l'informatique pour les Sciences Humaines et Sociales (MIASHS), offrant des parcours interdisciplinaires intéressants. Mais ce que nous disent aujourd'hui ces différents rapports et cette multitude d'initiatives citoyennes, c'est qu'il est tout à fait possible d'enseigner l'informatique et sa logique sans faire appel à un support mathématique, ou même à un langage informatique en particulier\Anote{simple_bloc}, des outils qui peuvent intervenir bien plus tard dans la formation.

On observe également depuis quelques années déjà l'émergence de la thématique \textit{Digital Humanities} dans les débats sur l'informatique et les SHS. Ce mouvement\Anote{definition_complexe} qui se fait avant tout l'écho d'une relation renouvelée des SHS au regard des évolutions touchant l'informatique\Anote{humanite_digitale_histoire}, se retrouve aussi dans des structures fédératives où sont plus ou moins mises en valeur certaines composantes de ce mouvement. Le TGIR Huma-Num par exemple fait avant tout ressortir un projet dont les termes\Anote{huma_num} paraissent assez éloignés des problématiques de programmation, de simulation, ou même tout simplement de calcul, alors même qu'il existe dans le giron de cette infrastructure des utilisateurs potentiels ou déjà actés de ce type de ressource : ALPAGE \autocite{Costa2012}, EQUIPEX MATRICE, GeoDiverCity, etc.

En effet même si cette ressource HPC est bien retenue dans la grille de services proposés par le TGIR, elle semble d'une part très peu valorisée dans les formations et les guides pratiques proposés par la structure, et d'autre part largement insuffisants pour des applications réelles, par exemple en simulation en géographie\Anote{human_num_notecout}.

Mais tout cela concerne la partie recherche d'un mouvement où l'heure en est encore au manifeste et à la réflexion, dont on trouve extraits dans : les discussions de la version française des \textit{Humanities and Technology Camp} \href{http://tcp.hypotheses.org/}{@ThatCAMP} et leur manifeste \autocite{THATCamp2010}, les livres comme celui de \textcite{Deuff2014} ou du collectif rassemblé autour de \textcite{Mounier2012}, le numéro 2 de la nouvelle revue électronique transdisciplinaire \href{http://rsl.revues.org/355}{@RSL}, le rapport commandé par l'Institut français \autocite{Dacos2015}, etc.

Il est donc encore assez difficile de voir quels impacts ce mouvement a sur l'enseignement. On trouve toutefois une tentative pour le THATCamp2012 de \href{http://pireh.univ-paris1.fr/DHfrancophone/index.php?rem&val=g%C3%A9ographie}{@cartographie} des enseignements en humanités numériques sur la France \autocites{Berra2012b,Clavaud2012}, régulièrement mise à jour, et où semble-t-il aucune formation en géographie n'apparait. S'agit-il d'un retard de notre discipline dans l'intérêt qu'elle porte à ce type de mouvement, ou plutôt d'un effet de catégorisation lié au cloisonnement disciplinaire francais, la question amène probablement un débat qui dépasse de loin cette introduction. Parmi les rares acteurs de la géographie ayant pris conscience des enjeux scientifiques se cachant derrière cette première ouverture disciplinaire, on trouve une intervention de Thierry Joliveau à la conférence plennière du THATCamp de Lyon en 2014 \href{http://dhlyon.hypotheses.org/587}{@Video}).

Ces quelques pistes n'offrent qu'une entrée très limitée dans un débat méritant un travail de recherche plus conséquent, qui reste encore à faire. Il est difficile en effet de trouver une réflexion globale et actuelle interrogeant les rapports existants entre l'informatique et la géomatique, l'informatique et la géographie, la géomatique et la géographie, au niveau de la recherche et des enseignements, mais aussi dans la place que ces débats sont amenés à jouer dans celui plus large des humanités numériques.

%Quelle projection peut on faire au delà d'une utilisation passive du SIG, ou de la simple utilisation de logiciels de statistiques, une vision \enquote{utilitariste} de l'informatique dont on a vu qu'elle serait amené à changer.

Déjà cité comme étant un des rares géographes/géomaticiens étant intervenu dans les \textit{THATCamp}, Thierry Joliveau fait aussi partie de ces quelques scientifiques à même aujourd'hui de questionner l'actualité des relations entre géographie et géomatique. Dans le volume 5 de son HDR on trouve ainsi une section \enquote{Géomatique et géographie, débats et enjeux} qui ouvre une première réflexion sur ce sujet en 2004.

Dans son argumentaire, Joliveau n'hésite pas à s'appuyer sur le malaise et les querelles importantes soulevées par le SIG chez les Anglo-saxons dans les années 1990 \autocite[472-473]{Joliveau2004}. La lecture presque en parallèle des écrits d'Openshaw montre bien l'extension prise par ce débat entre informatique et géographie, loin d'être finie au tournant des années 2000. En effet, dans leur manifeste pour le HPC \autocite[2]{Openshaw2000}, paru la même année que l'ouvrage sur la GéoComputation \autocite{Openshaw2000b},  Openshaw et Turton n’hésitent pas à engager les géographes dans une métaphore qui les met en proie à une forme de maladie des plus virulentes. S'en prenant plus spécifiquement aux géographes \enquote{SIG-istes} (et accessoirement aux sciences sociales en général), c’est une forme particulièrement menaçante de \foreignquote{english}{let others do the programming for us} qui condamnerait peu à peu ces scientifiques à délaisser la programmation plutôt qu’à s’en emparer pour devenir acteurs de leurs propres outils\Anote{openshaw_virus}. Dans la continuité de sa guerre engagée contre les technophobes, Openshaw s'attaque  maintenant à une forme plus perverse de ce mouvement, celui-ci fustigeant l'immobilisme technologique et l'idée qu'on puisse penser l'outil et son utilisation comme une fin en soi. En se démocratisant, les outils deviennent d'une façon ou d'une autre bornés par les questions qu'ils permettent d'aborder. Il faut toutefois sortir de ce qui peut apparaître comme une fatalité pour l'étudiant, en lui donnant les moyens d'inventer seuls, ou avec de l'aide, ses propres solutions à ces nouvelles questions.

Bien que le contexte dans lequel le débat se pose initialement, et la forme prise en réponse aux critiques soit au final assez différente de l'autre côté de l'Atlantique, il semble bien qu'il y ait un socle commun à la base de cette prose critique questionnant les usages du SIG en géographie.

\blockquote[{\cite[474]{Joliveau2004}}]{Le débat dans la géographie anglo-saxonne traduit la réaction méfiante d'une partie de la communauté géographique par rapport à cette technoscience triomphante. C'est une réaction du même ordre qui a conduit les géomaticiens français issus de la géographie comme de l'informatique à veiller à se placer en dehors de cette vague de la géomatique \enquote{industrielle et commerciale} et à travailler en amont, sur des questions et des concepts, des méthodes ou des outils non pris en compte dans les produits logiciels du marché.}

Une méfiance, voire une prise de recul nécessaire, qui ne constitue pas en soi un problème, à condition toutefois de ne pas déroger à l'exercice nécessaire et légitime d'un questionnement scientifique sur le statut des connaissances ainsi produites par ce qui serait une nouvelle \enquote{ingénierie géographique}.

Sur ce point le débat reste ouvert, et \textcite[474-477]{Joliveau2004} nous renvoie vers une lecture plus approfondie des positionnements épistémologiques empruntés par une GIScience anglo-saxonne, qui s'est depuis armée d'un discours en réponse à une \enquote{[...] tradition explicative de la géographie savante [...]} qui en général \enquote{[...] ne se satisfait pas d'un programme perçu comme essentiellement descriptif et principalement technique. La tradition critique de la géographie y voit même un projet dangereux}.

Concernant ce socle commun, \textcite[477]{Joliveau2004} nous dit que \enquote{Les critiques des SIG du débat anglo-saxon auront sans doute rappelé d'autres débats plus anciens. P. George était parti en 1972 à l'assaut de \enquote{l'illusion quantitative en géographie} (George 1972). Il est frappant de constater que nombre de ses arguments correspondent point par point à ceux des critiques actuels des SIG. [...] 30 ans plus tard, certains continuent à s'inquiéter d'une dérive technique de la géographie à cause des SIG (voir plus loin).} L'auteur fait référence notamment aux velléités édulcorées (par rapport aux critiques anglo-saxonnes) et peu renseignées de \autocite{Staszak2001}. Mais pour Joliveau ce n'est pas tant le débat que l'absence de débat qui pose problème en France. La menace d'une dérive technologique sous-entendue par ces auteurs prend alors, au grand plaisir de ces derniers, des atours de prophétie auto réalisatrice\Anote{joliveau_texte}. Des auteurs dont Joliveau\Anote{joliveau_pgeorge} et Openshaw sont, il me semble, tous deux bien d'accord pour dire à propos de leurs discours que ceux qui s'y accrochent encore n'y connaissent en définitive absolument rien, et ne méritent donc aucun crédit.

%Alors même que la place du SIG fait parfois encore débats pour ce qui est de sa place dans l'enseignement - même si la situation s'est déjà grandement amélioré si on en croit Joliveau sur son blog en 2009 -, devrait on en plus se préparer à être totalement exclus de la majorité des futurs projets necessitant l’usage du HPC, c'est-à-dire de la programmation pour le HPC, en géographie ? Sans aucun doute, comme on l'a déjà remarqué au tout début de cette introduction.

Les enjeux semblent importants, et le débat passionnant. Presque 10 années plus tard, qu'en est-il de cet état de la question et de la place du SIG en géographie dans la recherche et l'enseignement ?

Au vu de la rapidité d'évolution des techniques, et de la démocratisation massive qu'a connu le SIG dans de multiples disciplines ces dernières années, il serait aventureux de se lancer dans une quelconque projection de cet état de fait datant de 2004, en 2015. On pourra par contre toujours se référer aux écrits pertinents de Joliveau et continuer ainsi à prendre le pouls d'un débat intervenant à la frontière du monde professionnel et universitaire, entre la géographie et géomatique, tel que développé sur son blog \href{https://mondegeonumerique.wordpress.com/}{@Monde-GeoNumérique}.

Il est intéressant par contre de prolonger les critiques énoncées en les transposant dans ce parti pris souhaité par Openshaw, celui d'un saut technologique supplémentaire, au-delà du SIG et même de la programmation, dans la maîtrise de la programmation spécifique au HPC. Car si ces critiques interviennent encore de façon dommageable dans l'intégration du SIG comme outil standard à disposition des géographes, alors il n'est pas difficile d'extrapoler, et de se dire qu'elles le sont probablement tout autant pour la programmation, et pour le HPC.

A ce stade, est-il possible d'envisager, comme Openshaw en fait déjà état en 2000 pour les Anglo-saxons, un retard dans l'adoption du HPC en géographie en France ? Doit-on reprendre à notre compte ces craintes d'une géographie dépassée, en retard, oubliée, déjà avancées par plusieurs géographes dans le passé\Anote{joliveau_peur} ?

Ne prend-on pas le risque de passer à côté de thématiques comme le \textit{Big-Data}, les \textit{Smart-Cities}, qui engagent derrière cette couverture marketing des financements importants amenant la possibilité de traiter et d’analyser des données géographiques récoltées à des échelles et avec des moyens inégalés jusque là ? Alors même que l’Europe est passée très près de débourser plus d'un milliard d’euros sur le projet FuturICT intégrant une très forte composante simulation, la programmation pour la modélisation est loin d’être couramment enseignée chez les géographes et les chercheurs géographes. Quant à l'introduction aux moyens et aux techniques du HPC, elle est quasi-inexistante, voire utopique. Doit-on s’attendre à voir ces précieuses données géographiques analysées comme de simples données \enquote{xy}, au détriment des géographes français et aux profits de seules sociétés privées, surfant sur la vague des financements, et avec qui il faudra ensuite peut-être négocier pour accéder à ces données ?

%; c'est donc sans étonnement qu'on apprend les désagrement qu'a connu Openshaw avec les critiques dès la présentation de son prototype AMS.

En ce sens, l'urgence pointée par Joliveau de repenser le rapport de la géographie à l'informatique dans l'enseignement\Anote{formation_informatique} est essentielle pour le futur de la discipline, mais celle-ci dépasse de loin il me semble la seule question du SIG. Il ne s'agit pas simplement pour l'université de donner un débouché professionnel aux géographes en leur apprenant à programmer dans des master spécialisés - ce qui est un début, c'est vrai -, mais de leur donner de façon plus générale, comme on pu le faire pour les statistiques en leur temps, les bases suffisantes pour qu'ils puissent comprendre et améliorer les outils développés par les géographes dans le passé\Anote{aquoicelasert}, établir des collaborations actives et non plus passives avec les autres disciplines déjà rompues à l'informatique, mais aussi produire des outils de recherche en accord avec les développements de la société actuelle.

La plupart des auteurs abordant ce débat semblent d'accord, l'apprentissage de la programmation que cela soit pour la modélisation chez les géographes \autocite[64]{Banos2013}, dans les réflexions de géomaticiens \autocite{Joliveau2004}, dans les récits des géographes modélisateurs \autocites{LeBerre1987, Cuyala2014}, en histoire dans l'enseignement \autocite{Genet1993}, ou encore dans le cadre plus large des humanités numériques\Anote{code_humanite}, dans le cadre des initiatives citoyennes, ou depuis peu dans les institutions, \textbf{l'apprentissage de la programmation n'est pas une fin en soi, elle est un moyen pour une fin} \autocite[67]{Deuff2014}. Elle est ce socle de connaissance initial, dont le contenu reste encore à définir, qui permet d'engager cette co-construction autour de l'informatique entre les disciplines.

L'accès à la programmation, ou du moins à la \enquote{pensée informatique} pour tous, dans les filières universitaires, et plus spécifiquement en géographie (en dehors des seuls master spécialisés), est un projet qui est, semble-t-il, encore à construire.

%Tout comme Netlogo a su ces dernières années fédérer les modélisateurs, ce mouvement doit être de nouveau capturé dans la création d’un outil accueillant ces échanges pour ce qu’ils sont, c’est à dire bilatéraux (et non pas l’expression d’une commande), imparfait (car manipulant des concepts au sens différents, voire conflictuel), et imprégnés (il n’est pas facile d’adopter un nouvel angle d’analyse). L’objet de cette co-construction ne se concentre plus uniquement sur le modèle, mais sur l’exploration de celui-ci, et cela en utilisant tout la réunion des moyens humains et informatique faisant déjà usage du HPC.


%http://web.media.mit.edu/~mres/papers/designing-for-tinkerability.pdf

%Nous verrons par la suite que la solution proposé basé sur l'utilisation du logiciel OpenMOLE réduit encore un peu plus ces efforts, et vient s'accoler au plus pret des usages actuel chez les géographes modélisateurs.

% Seconde question

%\hl{transition transition transition}

Si le HPC est un acronyme à la définition relative, il doit être étudié \textit{in situ} pour mettre face à face challenges scientifiques et moyens computationnels au moment où ils sont disponibles.

%On a bien vu dans le chapitre 1 que les géographes, et certains modélisateurs pionniers dans les sciences humaines, s'étaient déjà tournés vers ce qui était alors les rares et uniques ressources informatiques permettant d'être en adéquation avec les enjeux scientifiques en ces temps. Cette motivation, il me semble que les géographes l'ont trouvé déjà une première fois en françe dans les années 1970, dans l'application notamment de toute nouvelles techniques pour la statistiques ou la simulation, les deux marchant de paire avec la découverte de l'informatique.

%Nous verrons dans la section \ref{sssec:centre_habite} que ces traces existent, même si elles sont encore assez peu documentées.

Repris de façon hétérogène par les sciences humaines, l’usage de \textit{supercomputers} est resté pendant quelques décennies le premier contact \enquote{contraint et forcé} que ces pionniers ont eu avec l'informatique. A ce titre, le chapitre 1 a déjà donné un aperçu des nombreux travaux réalisés par ces chercheurs en sciences humaines, et il me semble important de re-souligner le courage qu'il a fallu pour braver à plusieurs, ou parfois seul, toutes les contraintes à la fois intellectuelles, financières et physiques permettant d'approcher de telles machines, avant tout construites pour les besoins des sciences physico-mathématiques.

En résumé, on pourrait donc arguer qu'il s'agissait \enquote{d'un autre temps}, et que le \textit{supercomputer} était alors le seul moyen de faire de l'informatique. Oui, c'est vrai, mais en partie seulement.

Pour mieux comprendre dans quel contexte nous nous situons à cette époque, il faut tenter d'imaginer un monde quasi-dénué de toutes perspectives informatiques impliquant une utilisation autonome de l'ordinateur\Anote{gibson}. Difficile alors d'évoquer cette rupture conceptuelle et physique qu'a pu être l'arrivée de la micro-informatique dans certaines pratiques bien installées des sciences humaines. Tout d'un coup un objet un peu plus gros qu'une calculette apparait sur votre bureau, et permet de réaliser des calculs quasi-similaires à ce que vous faisiez en à peine quelques heures de plus sur des \textit{supercomputers}.

Considérée comme un événement bénéfique par quelques scientifiques déjà convertis à cette pratique de l'informatique depuis 10 ans, ou objet considéré comme inutile, décoratif, voire dangereux pour beaucoup; force est de constater que l'implantation progressive des \textit{micro-computer} dans les facultés au cours des années 1980 a quand même fini par rendre en partie obsolète une pratique de l’informatique jusqu'alors contrainte à l'utilisation des centres de calcul.

Toutefois, il ne faudrait pas se contenter ici de dérouler un argument trop commode, selon lequel dès lors que la micro-informatique apparaît, tous les géographes disparaissent des centres de calcul fréquentés auparavant.

Si la majorité des géographes vont effectivement se tourner vers la micro-informatique dès lors qu'elle se démocratise, on retrouve aussi dans plusieurs équipes des accointances plus ou moins durables avec certains centres de calcul, accointances dont on trouve encore trace physique aujourd'hui dans des politiques d'équipement, mais aussi dans la fréquentation ou la construction de structures d'accueil scientifique.

Pourtant, si l'on applique de façon rétroactive l'argument d'Openshaw faisant du challenge scientifique une nécessité motivant l'usage du HPC, alors il nous faut aller plus loin que ce simple constat et montrer en quoi l'apparition et la démocratisation des micro-ordinateurs n'a pas été un événement suffisamment convaincant pour que certaines équipes décident de continuer à pratiquer ces centres dans le courant des années 1980-1990.

Une entrée même partielle dans cet historique des pratiques - tel qu'on s'y essaye par la suite dans la section \ref{sssec:histo_centrecalcul} - permet, à défaut de produire une étude exhaustive, d'apporter quelques éclairages sur cette question.

Devant les difficultés d'usages des moyens informatiques de l'époque, la \enquote{mise en oeuvre} même de l'informatique apparaît en elle-même comme un défi nécessitant co-construction. Celle-ci se fait au croisement des formations de certains universitaires à la programmation, et de la fréquentation de creusets interdisciplinaires dans, et/ou autour des centres de calcul, que cela soit en SHS, ou chez les géographes (Strasbourg, Besançon, Montpellier, Paris). Il nous faut rappeler qu'à cette époque, il est impossible d'utiliser un programme de statistiques ou de cartographie sans avoir recours à des notions de FORTRAN.

Cette diversité des pratiques, des collaborations, est donc la preuve qu'il y avait dans la pratique de ces centres interdisciplinaires d'autres projets, d'autres challenges plus stimulants que celui du seul accès à une ressource informatique pour faire des statistiques. Car à y regarder de plus près, l'autonomie et l'indépendance permises par l'utilisation du micro-ordinateur, sont des qualités qui peuvent à l'inverse être aussi approchées comme une invitation au repli sur soi, à la dispersion des efforts impliquant l'apparition de \enquote{re-création informatique} stérile\Anote{remarque_informaticien_roue}, et l'effacement possible de ce riche horizon interdisciplinaire et de l'activité de co-construction qu'il supporte \autocite{Banos2013}.

En un sens il n'y a donc aucune raison pour que certaines de ces interactions interdisciplinaires, une fois nouées, ne disparaissent toutes subitement avec l'apparition de nouveaux logiciels ou de nouveaux équipements. Il est impossible de donner un point de vue global sur ce point, et nous verrons, comme l'a déjà noté \autocite[372]{Cuyala2014}, que les premiers géographes formés à la programmation informatique adoptent vis-à-vis de celle-ci, et par rapport à l'enseignement ou à leur recherche, des postures personnelles ou collectives tout à fait diverses. La seule chose que l'on peut avancer de façon plus certaine, c'est qu'il y a bien eu un jour la disparition des enseignements de programmation d'une partie des cursus de géographie quantitative, au profit de l'utilisation d'outils.

En résumé, le \textit{supercomputer} a continué à exister et à être accessible tant qu'il y a eu de la place pour les SHS dans les centres de calcul. Certains géographes ont donc continué à l'utiliser, d'autres non, pour diverses raisons dont on donnera un aperçu par la suite.

% Voir si le paragraphe ne meriterait pas d'etre plus haut

Concernant le cas plus spécifique de la simulation, nous pourrons d'ailleurs revenir plus précisément dans la section \ref{sssec:contexte_modelisateur} sur une série de collaborations entre géographes et physiciens/chimistes qui illustre ce que Denise Pumain décrit aussi comme l'assurance \enquote{de se donner des instruments à la hauteur des questions que l'on se pose}, preuve supplémentaire que les défis scientifiques s'inventent aussi dans l'importation et l'adaptation de techniques et de méthodologies issues d'autres disciplines.

%Une matrice de 9 par 9 ou de 73 par 73, telle qu'elle est utilisée par exemple par Openshaw dans son générateur de modèle AMS \autocite[40]{Openshaw1988}, est intéressante d'un point de vue théorique, mais ne permet pas une montée en complexité suffisante permettant la comparaison avec l'empirie. Comme l'avait déjà souligné Openshaw, ce n'est qu'une fois enclenchée la transformation du HPC au milieu des années 1980 que certaines applications peuvent enfin dépasser le stade de prototype. On évoque dans la section \ref{sssec:Tournant1980} abordant plus en détail les changements opérant à cette période les difficultés d'usages par exemple des automates cellulaires sur de large grille.

%N'est on pas dans une situation similaire ou la construction et l'exploration de nos modèles de simulation est encore largement contrainte par les ordinateurs équipants nos postes de travails, et cela probablement pour encore quelques temps ?

% Rejoignant le constat du chapitre 1, cela prouve aussi que les sciences humaines et les géographes ont été capables d'apprendre l'informatique et de surmonter des obstacles bien plus importants que ceux pouvant se dresser devant nous aujourd'hui pour accéder au HPC (ce qui explique aussi le manuel d'Openshaw sur le HPC \autocite{Openshaw2000}, qui croyait vraiment à cette possibilité de dépassement dans la discipline), preuve que lorsque les enjeux scientifiques sont à la hauteur, rien n'est impossible.

\subsubsection{Un bref historique des usages du HPC en géographie et en Sciences Humaines et Sociales}
\label{sssec:histo_centrecalcul}

%Avant de revenir sur cette question, il est important de noter que ces centres de calcul, à la différence peut être de ce qu’ils sont devenus pour certains ces dernières années, sont avant tout des lieux propices à l’inter-disciplinarité dans les années 1970-1980.

% SMA encore existant ?
% PACTE : Sonia Chardonnel, Elise Beck

En 1984, à l'occasion du Congrès International de Géographie se tenant pour la première fois à Paris, \textcite{Faugieres1984} cite dans l'ouvrage de synthèse \textit{La recherche géographique Française} une enquête du CNRS datée de 1982\Anote{bulettin_intergeo}. Il en tire les éléments suivants : sur $162$ formations en géographie ou dirigées par des géographes, $31$ seulement ont pu fournir des indications sur l'utilisation de moyens informatiques, $8$ départements de géographie n'ayant aucune activité informatique. Sur ces $31$, l'utilisation se répartit de la façon suivante, $9$ utilisent des centres de calcul, $12$ ont à leur disposition des terminaux, $2$ ont accès à des \enquote{mini}, et $13$ ont acquis un ou plusieurs micro-ordinateurs ($23$ dans l'ensemble des départements).

En 1983, \textcite[367]{Lecarpentier1983} expose dans les \textit{Annales de Géographie} une première description technique du \textit{micro-ordinateur} et de ses avantages, en partie pour contrebalancer un usage de l'ordinateur en géographie qui selon lui \enquote{ [...] reste donc marginale en regard des énormes possibilités offertes. En ignorant cet outil-là, après bien autres, la géographie laisse le champ libre à d'autres disciplines plus réceptives à l'évolution des mentalités et des techniques. Souhaitons que ce numéro spécial des \textit{Annales de Géographie} soit l'occasion - et le signe - d'un renversement de tendance et passons en revue la tâche immense qui attend le géographe, s'il veut bien apprendre à utiliser l'ordinateur et notamment le micro-ordinateur autrement que de façon marginale et accessoire.}

Il commente également (p 348) les résultats d'une enquête à laquelle il a eu accès pour fonder son analyse, sans donner les chiffres, mais qui tend à vérifier les conclusions données précédemment par Faugières\Anote{bulettin_intergeo_a}) : \blockquote{ Une enquête récente menée auprès de géographes universitaires français a montré que le recours aux centres de calcul reste le moyen le plus répandu mais que l'utilisation du mini- ou du micro-ordinateur en est souvent complémentaire. Les grandes universités (Paris, Montpellier,  Grenoble, Strasbourg) utilisent surtout ou exclusivement leur centre de calcul et apparaissent sous-équipées en matériels spécifiques. Les micro-ordinateurs se diffusent rapidement un peu partout mais leur généralisation est gênée par l'absence de logiciels adaptés aux besoins des géographes. Quelques équipes (Besançon, Lille, Rouen) ont entrepris la réalisation de logiciels de ce genre [...] Finalement, malgré sa diffusion rapide dans de nombreux secteurs d'activités, le micro-ordinateur reste un outil mystérieux et inconnu pour la plupart des géographes.}

Cette période des années 1980 reste d'approche assez délicate, car si les centres de calcul semblent rester un pôle d'accroche important pour de nombreux géographes, elle est aussi marquée par l’émergence à la fois externe de nombreux \textit{packages} logiciels développés par une nouvelle industrie florissante du logiciel informatique, ou interne, par les développements de géographes devenus pour certains experts en programmation informatique. Pour \autocites[444]{Joliveau2004}{Waniez2010}, la diffusion de la micro-informatique et de logiciels plus évolués sur le plan graphique et de l’interaction homme-machine courant des années 1980 a ainsi rendu complétement obsolète la fréquentation des centres de calcul pour l’usage statistique et cartographique. \textit{Quid alors des autres usages ?}

Pour appuyer les discussions sur cette époque, quelques témoignages ont pu être collectés durant le premier semestre 2015. Je remercie d'ailleurs par avance la géographe et professeure Colette Cauvin, le sociologue et professeur Philippe Cibois, l'historien et professeur Jean-Philippe Genet, l'ingénieur de recherche au CNRS Alexandre Kych, la géographe et professeur Denise Pumain, et la directrice de recherche Lena Sanders pour les informations qu'ils ont bien voulues me transmettre sur ces pratiques pionnières de l'informatique dans les SHS, et plus spécifiquement sur les usages des centres de calcul alors à leur disposition. Si l'apparition de non-géographes dans cette liste peut apparaître comme étrange en premier abord, ces témoignages seront aussi l'occasion de mettre en avant les autres \enquote{fonctions} supportées par ces lieux, qui reviennent souvent dans les entretiens comme les creusets d'une pratique solidaire et/ou interdisciplinaire de l'informatique. Les extraits qui viennent sont pour certains tirés de témoignages et d'échanges dont on trouve une reproduction intégrale en annexe \ref{chap:entretiens} de la thèse.

L'apprentissage de la programmation FORTRAN et la maîtrise d'un contexte matériel complexe daté des années 1970 est un passage quasi-obligé pour la plupart des géographes français voulant appliquer les techniques quantitatives. Cette aprentissage apparaît nécessaire pour exécuter les programmes informatiques \enquote{révolutionnaires} venus des Etat-Unis \autocite[150,127]{Cuyala2014}, mais également pour exécuter les nouvelles statistiques unies et multivariées, quand ce n’est pas tout simplement pour maîtriser à minima les entrées / sorties d’un superordinateur. Information difficile à trouver sans réaliser un travail historiographique important, la thèse de \textcite{Cuyala2014} nous donne quand même quelques informations sur le bagage techniques de certains des premiers géographes quantitativistes francais : Sylvie Rimbert (formation au Fortran à Ottawa p 137) ;  Cosinschi, Racine, Lemay, Cavalier (formation Fortran à Ottawa p 150), Pumain ( Montréal p 153), Marchand (p 154), Durand-Dastès (p 313) Dumolard (p 323), et sûrement d’autres du fait de leur origine québecoise, de leur profil hybride, ou de leur attirance pour une certaine rigueur mathématique (p 157-158).

Pour introduire les usages de l'informatique et des centres de calcul chez les géographes dans les années 1970-1990, on fait appel à une typologie en trois classes introduite par \textcites{Wieber1980}[448]{Joliveau2004} : l’accès aux packages logiciels ou à la programmation directe en cartographie et en statistiques se fait soit en interne lorsque les équipes ou les universités possèdent du matériel, soit par un accès direct aux centres de calcul, soit par un accès indirect par des terminaux en contact avec ces mêmes centres. Si la présence de centres à proximité des équipes coïncident souvent, la qualité de l'activité ne saurait se rapporter à cette seule présence ou à la taille d'une université.

L'innovation à Besançon tient par exemple de cette première catégorie. \textcite[131]{Cuyala2014} souligne l’importance des collaborations entre Jean-Claude Wieber, géographe alors issu de l’institut de géographie alpine sous le patronage éclairé de Paul Claval, et le mathématicien Jean-Philippe Massonie.

%IBM 1630 ?
\textcite{Massonie1986} a créé le laboratoire Mathématique Informatique et Statistiques (MIS) en 1964. Implanté dans une UER de Lettres et Sciences Humaines à l’université de Franche-Comté de Besançon, ce laboratoire atypique accueille une inter-disciplinarité qui n'est pas seulement dirigée vers la géographie\Anote{massonie_texte}. Disposant d’un petit centre de calcul rattaché à l'UER, la présence initiale d'un matériel modeste (IBM 1130 jusqu'en 1975 \autocite[22]{Wieber1980}, puis IRIS 50, IRIS 80 plus puissant \autocite{Massonie1986}) n'empêche en rien l'innovation. Cette équipe partie des premières à user des toutes nouvelles méthodes statistiques \autocite{Massonie1971}, mais c'est également elle qui va impulser l'important colloque interdisciplinaire de Besançon à partir de 1972. Intitulé \enquote{L'application des méthodes mathématiques à la géographie}, ce lieu d'échange joue selon \autocite[331]{Cuyala2014} un poids indéniable dans l'histoire de la géographie quantitative française; on trouvera pour plus d'informations un compte-rendu des dix ans d'activités dans les Annales de Géographie en 1983 \autocite{Massonie1983}, deux ans avant la disparition du colloque.

Avant même le début des années 1980, ce groupe investira très vite dans du matériel micro-informatique, en partie du fait de nouvelles contraintes liées à l'utilisation du matériel, comme le laisse deviner le commentaire de Massonie sur sa page Internet personnelle à propos de l'IRIS \enquote{Cela devient un vrai ordinateur parce qu'on y a plus accès. La bête est enfermée dans une cage de verre, des techniciens le font marcher. Le matin on dépose ses cartes dans un bac, à midi on récupère les cartes et le listing des résultats. On a perdu le côté ludique. Les informaticiens ont pris le pouvoir. Certes il est plus rapide, mais que m'importe que mon calcul qui durait 1 heure, ne dure plus que 10 minutes, si je ne récupère les résultats que 3 heures après.} Un micro-ordinateur est acquis par Massonie dès 1978 pour développer \enquote{un logiciel d'analyse des données dont l'utilisation soit bon marché et accessible à des non-informaticiens.} Devant les performances satisfaisantes de ces micro-ordinateurs, et l'autonomie qu'ils permettent, de nombreux logiciels micro sont adaptés ou créés à partir de là (ANACONDA, SADE, PATATE, etc.)\Anote{massonie_1978} \autocite{Massonie1986}. Cette volonté d'une \enquote{informatique conviviale} \autocite{TSH1984}, on la retrouve par la suite tout au long des transformations successives englobant ou accompagnant cette structure initiale originale.

Ce laboratoire a été le support dans la décennie 1980 d'un secteur de recherche (AMMSIOSHE) groupant plusieurs laboratoires (Géographie, Histoire, Littérature, Philosophie, etc.), mais également d'un GIS \enquote{Techniques nouvelles en sciences de l'homme} et d'un DEA pluridisciplinaire \enquote{Méthodes et Techniques nouvelles en sciences de l'Homme et de la Société} \autocite{TSH1984}. Le laboratoire MIS deviendra en 1997 le MTI SHS (Méthodologie et Technologies de l'Information appliquées aux Sciences de l'Homme et de la Société) \autocite{Girardot2004} et regroupera dans son projet l'ambition d'être à la fois une plateforme technologique, un lieu d'interdisciplinarité pour les sciences de l'homme et de la société, et un partenaire pilote dans la formation à l'innovation mêlant informatique et sciences humaines (ISTI). Après le départ à la retraite de Jean-Philippe Massonie, ce projet interdisciplinaire construit sur 30 ans se poursuit également avec l'émergence d'un nouveau projet scientifique autour de la MSHE C. N. Ledoux (2002), structure commune aux deux universités de cette région \autocites{Favory2003, Favory2009}. Le MTI SHS est aujourd'hui intégré comme équipe de recherche technologique (ERT) pour l'intelligence territoriale au sein de l'UMR multi-site ThéMA créée en 1999, qui participe également dans certains pôles de la MSH de Dijon et de la MSHE Ledoux de Besançon. Cette MSHE sert une optique mutualiste, avec la mise à disposition des autres disciplines de services humains et matériels appartenant initialement à ThéMA. La structure de la MSHE Ledoux dispose en effet en 2015 de ses propres calculateurs de taille moyenne (PowerEdge R815, Dell Précision 7500) destinés aux traitements locaux de données lourdes, preuve aussi que cette \enquote{culture du centre de calcul} reste une condition importante dans la marche d'un environnement interdisciplinaire. Une remarque aussi à mettre en perspective du tout centralisé, et de l'absence d'équipements similaires dans d'autres MSH en sciences humaines. Il existe également un partenariat entre ce laboratoire et le mésocentre (centre de calcul régional) de Franche-Comté \autocite{Asch2012}, sur lequel par exemple tournent la plupart des simulations du modèle LUTI Mobisim.\autocites{Thema2010, Hirtzel2015}

Le pôle strasbourgeois impulsé par Sylvie Rimbert regroupe à partir de 1968-1969 Etienne Dalmasso, Monique Schaub, Colette Cauvin\Anote{informations_colette_cauvin}. En 1970, le géographe ayant acquis de solide compétence en statistique et en informatique Michel Pruvot intègre l'équipe à son retour du Canada, également complété en 1971-72 par Gérard Schaub, puis Henry Reymond en 1973 \autocite[135-153]{Cuyala2014}. De retour du Canada, ce dernier remplace Etienne Dalmasso alors nommé à Paris. L'équipe active en statistique informatique est principalement composée de Michel Pruvot, Colette Cauvin, Sylvie Rimbert et Henri Reymond. %@bla

Fort d'une formation à l'étranger pour certains, apprentissage initié ou complété par les formations du CNRS (1971 à Strasbourg, 1972 à Paris) pour d'autres, ce petit groupe est en tout cas très rapidement opérationnel en informatique, en statistique et en cartographie, dès le début des années 1970. C'est cette triple compétence qui permet à l'équipe d'utiliser au mieux et très rapidement les ressources informatiques à leur disposition sur le campus universitaire de Strasbourg. En 1971, deux projets sont finalisés, une des toutes premières applications d’Analyse en Correspondances Principales (ACP) sur un milieu géographique intitulé \textit{Essai d’application de quelques méthodes statistiques à la région milanaise} \autocite{Dalmasso1971}, mais également une commande d'atlas informatisé pour l'agence d'urbanisme de la ville de Strasbourg. Un véritable tour de force chez les géographes de cette époque, car ce travail s'appuie sur l'utilisation, dès le dernier trimestre 1970, des ressources informatiques mises à disposition par le Centre de Calcul de Strasbourg-Cronenbourg (CCSC) créé sur le campus en 1968-69 pour les besoins de la physique.

C'est grâce aussi à la collaboration régulière avec Anne Engelmann, une physicienne et informaticienne du CEREG disposant d'un bureau au CCSC, que les géographes de cette équipe vont accéder à la fois à des facilités matérielles\Anote{collette_ccsc_centre} et logicielles; une collaboration qui s'inscrit physiquement dans un lieu où on ne s'attendait sûrement pas à voir résider de façon durable des géographes. Ainsi, outre les bibliothèques logicielles déjà accessibles aux utilisateurs du centre, comme BDMP par exemple\Anote{bdmp_package_strasbourg}, Anne Engelmann a assuré plusieurs installations de logiciels à destination des géographes : le logiciel SYMAP (1965) d'Harvard dont une version en Fortran a été apportée par des étudiants canadiens entre 1970 et 1971, divers logiciels de cartographies achetés ou récupérés par Sylvie Rimbert entre 1973 et 1979, etc.

Alors que l'arrivée de la micro-informatique dans le laboratoire se fait de façon progressive à partir de 1982-1983, les activités de programmation peuvent se diversifier, sans mettre en danger semble-t-il cette liaison avec le centre de calcul. Une fréquentation du centre qui restera continue jusqu'à la fermeture du centre en décembre 1993, avec l'organisation d'enseignements, et/ou la mise en oeuvre d'activités de recherche impliquant l'installation, l'utilisation ou le développement de logiciels. En 1985-90, l'activité des géographes de Strasbourg se développe même sur un double lien, avec la mise en oeuvre d'une liaison ponctuelle avec le CNUSC (Centre Universitaire de Calcul du Sud), essentiellement motivée par des collaborations du GIP RECLUS initiées par les géographes de Montpellier.

Michel Pruvot développera des logiciels sur les différents équipements micro successifs du laboratoire, alors que Colette Cauvin et Sylvie Rimbert, bien qu'écrivant également de petits programmes, préfèrent mettre l'accent sur le dialogue et la co-construction avec les développeurs spécialistes de l'équipe. C'est le cas avec Anne Engelmann (jusqu'en 1978-1980), mais également avec deux scientifiques rodés à l'informatique ayant rejoint l'équipe dans les années 1970. Ingénieur d'étude puis de recherche, Jacky Hirsch intègre l'équipe dès 1977 (avec des contacts dès 1976), et dispose aussi comme Engelmann d'un bureau au CCSC.

Ces derniers ne se contentent pas de développer et fournissent également un support humain (enseignements, formations) et informatique important aux deux équipes du laboratoire, celle de Sylvie Rimbert mais également celle de géographie quantitative. C'est le cas par exemple de Charles Schneider, un ingénieur d'étude devenu chercheur par la suite qui opère dans le groupe de Silvie Rimbert. Les recherches de ce dernier sont centrées sur la méthode cartographique et la télédétection, et il travaille en forte collaboration avec Jacky Hirsch, alors en charge de la réalisation informatique des logiciels. De leurs efforts mutuels émerge par exemple en 1983 le logiciel de télédétection Cartel utilisé sur l'Univac 1100 du centre.

C'est Colette Cauvin qui gère les relations et les finances avec le centre pour les stages/cours des étudiants jusqu'à sa fermeture et son remplacement en 1994 par le Centre Universitaire Régional de Ressources Informatiques (CURRI) dans le nouvel ensemble universitaire de l'ULP à Illkirch. Elle y occupe les mêmes fonctions\Anote{calcul_curri}.

Il faut en effet rappeler que l'institut n'a disposé de véritable salle pour la micro-informatique que vers les années 1987-88, des appareils utilisés en premier lieu pour les cours de statistiques assurés par Michel Pruvot.

Les autres cours, mais également une partie des logiciels de l'équipe implémentés initialement sur les \enquote{grosses machines} du centre de calcul, doivent donc s'adapter très régulièrement aux changements de matériels informatiques qui touchent le centre : en 1968-69 un IBM aurait apparemment existé, remplacé dès 1970-71 par un Univac 1108 \autocite{Dalmasso1971}, un Univac 1100 en 1983, puis au plus tard en 1985 un IBM3060/IBM3090. Ce dernier système marque la fin des perforatrices et l'arrivée de terminaux interactifs IBM 3179 \autocites{Rimbert1984,Cauvin1986}. Un peu plus tard encore, l'arrivée progressive de terminaux à l'université va permettre à l'équipe de se connecter au centre distant de quelques kilomètres. Une évolution qui rend enfin possible l'exécution d'un certain nombre d'opérations à distance, les listings résultants étant renvoyés par le centre deux à trois fois par jour.

Colette Cauvin, qui dispose d'un \enquote{Vic} personnel, l'apporte en cours pour certains TP entre 1982 et 1985. Bruno Guérin développe également des logiciels servant pour l'enseignement de l'analyse spatiale sur un \enquote{Amstrad}. Michel Pruvot, pour sa part, responsable des enseignements de statistiques au CCSC, se met rapidement au développement de logiciels de statistiques sur micro-ordinateur et sur Macintosh dès que ces équipements sont disponibles. C'est lui également qui conçoit et gère la première salle d'informatique en libre d'accès pour les étudiants en 1987-1988, il y donne les TD de statistiques sur des logiciels qu'il a pour la plupart lui-même développés, et cela dans un but avant tout pédagogique. En parallèle, les chercheurs s'équipent peu à peu de logiciels achetés pour la cartographie (Carto2D, MacGrizdo, etc.), ou développés pour les statistiques par Michel Pruvot, avec l'arrivée de matériel Macintosh dans le laboratoire. Il y aura également l'arrivée de un ou deux PC, clôturant ainsi l'équipement disponible jusqu'en 1993-1994.

En dehors des formations en statistiques qui vont basculer sur les micro-ordinateurs lorsque ceux-ci seront disponibles, la plupart des formations, comme les stages, puis les cours de cartographie démarrés au centre en 1973-1974, et ceux de télédétection démarrés en 1982-1983 se feront toujours au centre jusqu'à sa fermeture en 1993. Avec, comme on l'a vu, des adaptations majeures : les perforeuses ont été remplacées par des terminaux plus récents, les programmes ont souvent été réécrits plusieurs fois. Il faut aussi prendre en compte l'arrivée de développeurs ou l'amélioration des compétences de certains chercheurs en informatique ayant permis le développement de programmes propress aux besoins d'enseignements et de recherches. Ainsi, dès l'arrivée de Jacky Hirsch, celui-ci développe (parfois avec l'aide d'étudiants, comme Aziz Serradj, enseignant en cartographie aujourd'hui) des logiciels spécifiques à partir de librairies acquises par le CCSC (par exemple la librairie d'application graphique danoise Uniras acquise en 1979-80) ou de tout nouveaux logiciels comme Cartel, justement créé pour les stages de télédetection. Ces logiciels sont souvent utilisés dans le cadre des recherches menées par les différentes équipes du laboratoire.

Avec la fermeture du CCSC et le passage au CURRI, la plupart des enseignements et des travaux de recherche ont pu continuer grâce à l'adaptation des logiciels faite par Jacky Hirsch. Ensuite, le laboratoire s'est peu à peu équipé de logiciels PC (cartographie, statistique, SIG) de tout type, pour l'enseignement ou la recherche, par exemple en 1994 Arc/Info (enseignement et recherche) et Idrisi (surtout pour l'enseignement). Le centre de calcul CURRI est quant à lui resté nécessaire pour avoir accès à des logiciels comme SPAD, Hercule, ainsi que pour la télédetection jusqu'au début des années 2000, avant l'achat de logiciels spécialisés.

% le seul on imagine à disposer de l'équipement adéquat pour les sorties de logiciels de cartographie.

A Grenoble (H. Chamussy , P. Dumolard, M. Durand, et Joël Charre avant de partir à Avignon, Maryvonne Le Berre partie à Besançon par la suite) c’est l’Institut de Géographie Alpine (IGA) qui se coordonne avec le Centre Inter-universitaire de Calcul de Grenoble (CICG, formalisé en 1972). Le modèle AMORAL développé par une partie des membres de cette équipe a été écrit dans le langage DYNAMO, après l'impulsion donnée par François Rechenman de l'IRIA, et Patrice Uvietta du laboratoire Atelier de Recherches sur les Techniques Mathématiques et Informatiques de Systemes (ARTEMIS) de l’Institut d'Informatique et de Mathématiques Appliquées de Grenoble (IMAG). A la fin des années 1989, Joël Charre et Pierre Dumolard, qui bénéficient chacun d'un bagage informatique solide (Dumolard a fréquenté le CICG assidûment peu après son ouverture \autocite[323]{Cuyala2014}), se sont rapidement spécialisés dans la micro-informatique. En 1989 ils mettent à disposition des géographes le logiciel INFOGEO \autocites{Charre1989,Theriault1989,Ferrier1991}.  %Ce dernier intervenant également de façon plus générale chez les géographes par le biais de son inscription au groupe DUPONT, et dans la participation aux différentes formations associées à cette période à l'analyse de systèmes en géographie, comme celle par exemple organisée par Guermond en 1982 \autocite{Guermond1984}.

Le pôle d'informatique grenoblois, et notamment l'IMAG, est historiquement lié à l'émergence de la discipline informatique comme science du même nom en France. Une des impulsions importantes est donnée par l'universitaire Grenoblois Jean Kuntzmann dans les années 1950. Ce mathématicien et supporteur précoce de l'analyse numérique va militer pour la création, la formation, et la diffusion de ce qui va devenir par la suite l'informatique. Il est le fondateur et dirigeant de l'IMAG jusqu'en 1977, un des premiers laboratoires associés au CNRS, disposant d'un rayonnement international y compris sur le plan non-académique. Au milieu des années 1960, le laboratoire emploie 140 personnes, dont 60 doctorants. Si le Centre Inter-Universitaire de Calcul de Grenoble (1972) n'est vraiment officialisé qu'en 1972, de nombreux ordinateurs analogiques (1951), puis digitaux (Gamma ET, Marchant 10FA en 1957, IBM 7044 et 1401 en 1963, IBM 360-67 en 1968) seront amenés à passer bien avant dans ce premier Centre de Calcul. En 1977, cet effectif se porte à 300 personnes. Celui-ci est également le créateur de l'Association Française de Calcul (AFCAL) en 1957 qui deviendra par la suite AFCALTI, AFIRO, puis l'AFCET (Association Française pour la Cybernétique), dont les colloques et conférences ont constitué un des canaux importants de diffusion de la systémique dans le milieu scientifique et technique, y compris en géographie quantitative \autocite{Pumain2003}. Il n'est donc pas étonnant de voir l'utilisation appliquée de systèmes dynamiques émerger de ce centre en particulier, car Kuntzmann lorsqu'il crée l'Ecole Nationale Supérieure d'Informatique et de Mathématiques Appliquées de Grenoble (ENSIMAG) lui insuffle cette volonté d'une mathématique, et d'une informatique résolument tournée vers un usage applicatif, car \enquote{Il ne faut pas séparer une science de ses applications; à longue échéance, les mathématiques coupées des applications deviendraient stériles.} \autocites[422]{Mounier2012}{Chiffres1977}

\begin{table}[!htb]
	\centering
	\begin{sidecaption}[Les ressources informatiques au CNRS]{Une idée des ressources informatiques disponibles au CNRS, de l'IBP à l'IDRIS. Les sources pour construire ces deux tableaux proviennent de \autocites{CNRS1972, Boucher2000, Mounier2010}}[tab:machines_circe]
		\begin{minipage}{1\textwidth}
			\centering
			\subbottom[\label{machines_a}]{
				\resizebox{0.7\textwidth}{!}{%
				\begin{tabular}{@{}cll@{}}
				\toprule
				\multicolumn{3}{l}{A l’Institut Blaise Pascal IBP ( 1946 - 1969)} \\ \midrule
				Entrée      & Identifiant machine      & Remplacement         \\ \midrule
				1955        & Elliot 402               & (1962)               \\
				1957        & Gamma 3, AET             & ( ? ,  ?)            \\
				1958        & IBM 650                  & 1966                 \\
				1961        & IBM 1401                 & ?                    \\
				1962        & Elliott 803, IBM 704     & ( ? , 1964/1966)     \\
				1963        & CAB 500, IBM 1401        & (? , ?)              \\
				1966        & CDC 3600                 & ( 1972 )             \\ \bottomrule
				\end{tabular}%
				}
		 	}
		 \end{minipage}\qquad
		 \begin{minipage}{1\textwidth}
		 	\centering
			\subbottom[\label{machines_b}]{
				\resizebox{0.7\textwidth}{!}{%
				\begin{tabular}{@{}cll@{}}
				\toprule
				\multicolumn{3}{c}{Au CIRCE ( 1969 - 1993 ) à Orsay} \\ \midrule
				Entrée   & Identifiant machine      & Remplacement   \\ \midrule
				1965     & Univac 1107 (Faculté)    & 1968           \\
				1966     & CDC 3600, IBM 360 / 40   & (1972, 1967)   \\
				1967     & IBM 360 / 50 et / 75     & (1972, 1972)   \\
				1968     & Univac 1108 (Faculté)    & 1970           \\
				1969     & IBM 360/75               & (1972, 1972)   \\
				1970     & Univac 1106 (Faculté)    & ?              \\
				1972     & IBM 360/20, 370/165      & ( ?, ?)        \\
				1985     & AMDAHL 470/V7, NAS9080   & (?, ?)         \\
				?        & IBM 3090/200             & ?              \\ \bottomrule
				\end{tabular}%
				}
			}
		\end{minipage}
  \end{sidecaption}
\end{table}

%IBM 704 : 20 KIPS/12 kFLOPS
%CDC 3600 : 1.3 MFLOPs
%CRAY1 : 160 MFLOPS

%http://www.tablesgenerator.com/latex_tables

% A l’institut blaise pascal ( 1946 - 1969) :
% 1957 :  un  Gamma  3  puis  un  AET.  De  Possel,  chef  du  labo  à  l' IHP,  passe  à  l' IBP.
%  1958 :  un  IBM  650 ;  de  Possel  devient  directeur  de  l' IBP.
% 1961 :  un  IBM  1401
%  1962 :  un  Elliott  803  (acheté  1  MF)  puis  une  IBM  704
%  1963 :  une  CAB  500  et  une  seconde  1401
%  1964 :  une  CDC  3600,  remplaçant  la  704
%  1966 :  une  IBM  360 / 40  à  disques  et  bandes,  dans  le  nouveau  local  du  CIRCE  à Orsay
%  1967 :  une  IBM  360 / 50  remplaçant  la  prècédente

% Au Circe (1969 - 1993) :
% Univac 1107 -> 1108 (1968)
% Univac 1106 (1970)
% CDC 3600 (1968-1972) et IBM 360/75 ( ?- 1972), IBM 360/20, 370/165 (1972 - ?)
% 1985 : AMDHALV7 et NAS9080
% IBM 3090/200

% A l’IDRIS (1993 - maintenant ):
% Vectoriel Cray C98 (en 1993) et Cray C94 (en 1994) jusqu’à 2000
% Cray T3D fut installé en 1995
% Cray T3E en 1996
% 2001 IBM Power4
% etc.

%http://pireh.univ-paris1.fr/mv/num9.html

Le Centre de Calcul de l'université Paris 1 dépend depuis sa création de l'UFR d'économie, sous la tutelle originale du professeur Claude Fourgeaud.

Ce centre était situé au sous-sol du Panthéon rue Cujas, à la fac de droit, au pied du grand escalier. Celui-ci était composé d'une salle avec l'ordinateur local, et d'une autre salle, plus petite, où deux femmes étaient responsables de la perforation des cartes pour les utilisateurs et les chercheurs. Un bac était à disposition des envois de cartes perforées pour exécution au Centre Inter-Régional de Calcul Electronique (CIRCE, voir tableau \ref{tab:machines_circe}) à Orsay\Anote{remarque_denise_centrecalcul}. C'est ainsi que les premiers calculs statistiques sont réalisés par les géographes, par exemple lors des cours de 1969-1970 de Bernard Marchand où les étudiants sont amenés à apprendre le FORTRAN avec l'aide d'informaticiens, les cartes perforées des étudiants finissant dans ce bac, avant d'être envoyées au CIRCE pour exécution \autocite[127]{Cuyala2014}.

Parmi les utilisateurs les plus assidus du centre en SHS, alors en libre service pour les enseignants chercheurs, il y avait semble-t-il l'historien Jean-Philippe Genet, les économistes Pierre-Yves Hénin et Michel Pouchain, et enfin les trois géographes Thérèse Saint Julien, Denise Pumain, et Yvan Chauviré.

A son arrivée à Paris 1 en 1969-70, l'historien médiéviste Jean-Philippe Genet précédemment formé à l'informatique par l'équipe de Marc Barbut à Paris 5, indique la présence au centre de calcul de Paris 1 d'un IBM 1130 assez poussif, assez vite remplacé par un Philips P880.

D'autres utilisateurs sont également amenés à travailler au centre, par exemple pour encadrer des développemements ou des utilisateurs, pour des logiciels génériques, ou pour des applications plus spécifiques.

Denise Pumain parle ainsi d'\enquote{un groupe d'informaticiens et de mathématiciens-statisticiens (dont Yvonne Girard) préposés à aider les usagers du centre de calcul situé au Panthéon.} Pour Genet, en dehors de cette tutelle des économistes, assez peu visible, le fonctionnement du centre au quotidien était principalement assuré par la mathématicienne Yvonne Girard, et ce groupe des plus assidus, toujours prêt à s'entre-aider.

Genet cite également dans le personnel actif, l'importance de Jean-Paul Trystram, un directeur de recherche à l'IRIA (ancien INRIA) ayant travaillé au centre avec Xavier Debanne, créateur par la suite du logiciel BDP (Banque de Données Paris 1), une forme de logiciel type SPSS qui sera longuement utilisée dans ses différentes versions sur le Philips P880. Trystram a également enseigné comme professeur pendant quelques années à l'UFR de géographie de Paris 1.

Genet dont le niveau en informatique n'est pas forcément suffisant pour développer de nouveaux logiciels, fait appel au professeur de Paris 1 et militaire Édouard Valensy pour lui fournir des ingénieurs collaborateurs dotés d'un haut niveau informatique, comme Jacques Mondelli ou François Hucher.\Anote{pionnier_genet}

Ce dernier estime que le centre de calcul lui a été utile de son arrivée en 1970-1971 jusqu'aux années 1985-1990 environ, période à partir de laquelle il est devenu beaucoup moins intéressant d'utiliser cet équipement. Il aura fallu ce temps-là pour que les logiciels soient convertis ou achetés sur micro-ordinateur, et que les chercheurs s'équipent, souvent à leurs propres frais, entre autre du fait d'une absence (volontaire?) de politique générale du CNRS. A partir de 1987 certains départements d'histoire reçoivent une circulaire annonçant l'implantation de micro-ordinateurs. Une décision qui s'est faite sous l'impulsion de Jean-Pierre Bardet, un historien démographe de Paris 4 ayant travaillé avec Genet. Une fois Bardet entré au Ministère de la Recherche comme chef de mission scientifique, celui-ci décide de changer les conditions de travail des historiens par cette expérience pilote. Une mise à disposition de moyens qui ne se prolongera pas, comme en témoigne cet appel d'urgence pointant l'insuffisance des équipements chez les historiens en 1993 \autocite{Genet1993}. % Ajout ref http://pireh.univ-paris1.fr/mv/num9.html

Dans le cas de l'UFR d'Histoire de Paris 1 c'est quand même douze Bull 45 (ou Bull 35, selon le témoignage) reçus en 1987 qui ont permis aux historiens de relâcher l'usage du centre de calcul, et de relancer en 1989, dès qu'une salle a été trouvée pour les installer, une UV d'histoire et d'informatique permettant une formation plus systématique des historiens à cette matière. En effet, le centre de calcul étant réservé avant tout aux chercheurs, la formation lancée par Genet en 1975-1976 (ou 1978-1979 selon le témoignage), utilisant le centre de calcul normalement limité aux chercheurs, est interrompue, et cela apparemment jusqu'en 1989. En juillet 1993, cette salle sera rééquipée avec dix PC 386 SX, un changement qui fait figure d'exception dans cette situation de crise, comme pointé dans le paragraphe précédent \autocite{Genet1993}.

Parmi les autres utilisateurs du centre figurent les membres de la future équipe P.A.R.I.S (Denise Pumain, Thérèse Saint-Julien, et plus tard Lena Sanders) qui n'hésitent pas à mobiliser les ressources offertes par le centre soit pour la réalisation d'analyses statistiques, soit un peu plus tard pour des modèles de simulations. Cet usage sera détaillé en particulier dans une prochaine section \ref{sssec:contexte_modelisateur}.

A son retour du Canada en 1969-1970, Denise Pumain fait en effet partie du groupe de géographes autonomes capables de programmer en Fortran. Genet semble alterner pour ses recherches entre le centre de calcul de Paris 1 (prosopographie sur le P880) et d'autres centres, utilisés dès que les besoins en calculs étaient plus importants (pour de l'analyse lexicale notamment). Le  CIRCE est le premier centre fréquenté, puis à partir du moment où l'équipe devient CNRS, c'est au LISH boulevard Raspail que se déroule l'essentiel des recherches de Genet. En comparaison, les géographes semblent faire un autre usage du matériel en place au centre de calcul de Paris 1. A cette époque ce matériel est principalement utilisé pour calculer les analyses multivariées, un usage qui permet aux géographes de rencontrer les archéologues et les économistes de l'Université. Certes le P880 est utilisé, mais Denise Pumain pointe surtout l'utilisation au tournant des années 1981 d'un tout nouveau matériel (terminaux) permettant cette fois-ci une connexion directe au CIRCE, sans utiliser de cartes perforées\Anote{denise_extrait_ccparis}.

Ce centre sera aussi utilisé par Denise Pumain dans le cadre des enseignements d'analyse spatiale dispensés aux étudiants de géographie de deuxième année dès 1972 et à ceux du DESS de cartographie de Paris 1. Les logiciels utilisés dès 1970 étaient ceux de SPSS (remplacés pour les cours une dizaine d'années plus tard par SAS) et ceux élaborés par l'équipe de Benzecri, (logiciels appelés par la suite sous le nom de l'association ADDAD, association fondée par J. P. Fenelon, avec Lebart et Morineau), Michel Jambu écrit un programme de CAH dont il autorise l'usage rapidement aux géographes.

Les chercheurs géographes abandonnent le centre de calcul de Paris 1 à la réception d'un micro-ordinateur dans leur laboratoire de la rue du Four, un événement que Denise Pumain place peu de temps après la réception d'un ordinateur à l'INED en 1986.\Anote{denise_extrait_ruefour}

L’ORSTOM (maintenant IRD) dispose également d’un certain matériel informatique et de connexion avec la plupart des centres de calcul parisiens : la société STAD, le CIRCE, etc. \autocite{Dejardin1992}

A Montpellier, la maison de la géographie ouverte en octobre 1964 accueille le GIP RECLUS (1984-1997) piloté par Brunet \autocite{Brunet1988}. Cette structure qui intègre dans son projet de multiples laboratoires de géographie collabore sur plusieurs aspects (stockage des données, calcul scientifique, cartographie) avec le Centre Universitaire Sud de Calculs (CNUSC) \autocite{Waniez2010}. Ouvert en 1981, celui-ci est conçu comme un équivalent du CIRCE en tout point : formations, services d’aide, logiciels et matériels étant à disposition des sciences humaines et sociales pour leurs travaux\Anote{presentation_cnusc}\Anote{centre_formation}.

Disposant la plupart du temps d'équipements assez légers, mais équipés de connexion avec les grands centres de calcul, le CNRS va également mettre à disposition des SHS des pôles dédiés qui semblent plus accessibles que les grands centres. C’est le cas par exemple du Laboratoire Informatique pour les Sciences Humaines (LISH) \autocite{MSH1975} créé en 1975 par Mario Borillo. Le LISH dispose de deux unités à Marseille (une \enquote{Unité de Recherches Méthodologiques} reprenant l'équipe de mathématiciens, informaticiens et linguistes de l'URADCA, une \enquote{Unité d’Applications Informatiques} au centre de calcul du Pharo), un \enquote{Service de Calcul} à Paris, et à partir de septembre 1980 une unité au Mirail à Toulouse \autocites[154]{Mathieu2014}{MSH1975,LISH1981a,LISH1980a,LISH1980b,LISH1981b,LISH1982a,LISH1982b,LISH1984}. Déjà installé depuis 1970 à la Maison des Sciences de l'Homme du 54 boulevard Raspail, le CNRS décide en 1975 de réorganiser le Centre de Mathématiques Appliquées et de Calcul de la Maison des Sciences de l’Homme (CMAC) pour en faire le laboratoire de service CNRS-LISH, logé au sous-sol du bâtiment. Il est à noter que ce service dispose avant même sa restructuration d’ordinateurs et de terminaux d’accès à CNUSC et CIRCE (rapport d’activités \autocite{CNRS1972} et numéro $0$ du bulletin d'informations de la MSH \autocite{MSH1973})

Dans une conjoncture de mutation très forte, autant dans les sciences humaines qu'en informatique, le LISH tente d'assurer cette mission de plus en plus complexe de réunion des deux mondes autour de 5 activités principales : accueil et assistance, formation des nouveaux utilisateurs\Anote{note_documentation_ccalcul}, animation et organisation de réunions et rencontres, recherches et collaborations scientifiques.

%Genet1981

% Voir information ici http://cams.ehess.fr/document.php?id=947
Dirigé par Guilbaud en 1955 dans le cadre de la VIème section, le Groupe de Mathématiques Sociales (GMS) devient ensuite associé en 1967 au CNRS et prend successivement le nom de Centre Mathématiques Sociales (CMS) puis de Centre d’Analyses et de Mathématiques Sociales (CAMS) en 1981 sous la direction de M. Barbut. Parmi ces autres laboratoires de la VIème section - sciences économiques et sociales - de l'EPHE venant s'installer dans le bâtiment de la MSH (qui deviendra l'EHESS après sa déclaration d'autonomie en 1975), il est un des plus importants dans l'histoire de la formation des SHS aux mathématiques et à la statistique.

La partie recherche de l'unité LISH de Paris est décrite ainsi par son responsable Gian Perro Zarri : \foreigntextquote{english}[\cite{Zarri1981}]{On 27th November 1980, the Control board of the \enquote{Laboratoire d'Informatique pour les Sciences de L'Homme} ratified the reorganization of L.I.S.H. in the form of a \enquote{network}, composed of several Units, each with a certain amount of autonomy [...] the principal objective of this Unit is to lead a series of pilot actions, designed to demonstrate the utility of conceptually advanced Computer Science instruments in the domain of the Humanities and the Social Sciences. Particular importance is given to Artificial Intelligence and Knowledge Representation techniques. [...]  The permanent C.N.R.S. staff which makes up this new Unit, comprises, apart from myself, Pierrette Andres(secretary), Wenceslas Fernandez de la Vega, Ruddy Lelouche, Jacqueline Leon, and Vincent Meissonnier}

Pour donner un aperçu des activités et de l'implication des membres du LISH dans l'outillage et la formation des SHS; Ruddy Lelouche est par exemple à l'origine de la première installation du package \textit{Statistical Package for Social Sciences} (SPSS) en février 1976 (à la demande d'un étudiant américain). Il est également le premier représentant de la France au IVème congrès international annuel des utilisateurs de ce package, pour lequel il organise ensuite de nombreux stages de formation et rédige des manuels.

De façon plus générale, outre leurs différentes spécialisations sur le plan de la recherche, ces différentes unités proposent en général : une forte activité éditoriale dans différents bulletins, des publications scientifiques, l'organisation de conférences, des stages de formation dans les centres de calcul nationaux, des formations à différents packages, différents langages (FORTRAN, LOGO en 1984), des ateliers thématiques et techniques, la participation à des écoles d'été (Grenoble), des échanges internationaux, etc. Le lecteur curieux d'en savoir plus pourra se référer aux différentes descriptions des activités de ce laboratoire dans les bulletins de la MSH, et dans différents journaux, en particulier dans \enquote{le médiéviste et l'ordinateur}, un des rares journaux (pour le moment, des projets de numérisations sont en cours pour la \enquote{Gazette des Messaches} par exemple) issus de cette \enquote{littérature grise} qui a été entièrement numérisée\Anote{litterature_legere}.

On peut également citer l'exemple de Philippe Cibois, qui a eu des responsabilités à l'unité de calcul parisienne du LISH de 1975 à 1989. En formation sur l’Analyse de données, il réalise son stage d’informatique (1971) appliquée aux sciences humaines sur les premiers ordinateurs de la MSH et des ATP, et il collabore fréquemment avec Marc Barbut (1971-1975). C’est ainsi rompu à la programmation et à l’analyse de données qu’il passe son mémoire (puis plus tard sa thèse en 1980) avec Boudon au GEMAS où il deviendra coordinateur technique amené à travailler sur la partie informatique/statistique de divers contrats et projets (\textit{l’inégalité des chances}). C’est suite à un article dans \textit{Informatique et Sciences Humaines} en 1975, inspiré par son travail d’enquête l’ayant mené au colloque  organisé par Gardin en 1972 que celui-ci est remarqué par Mario Borillo, un des proches collaborateurs de Gardin, qui l’invite aussitôt à intégrer comme \enquote{secrétaire scientifique} la toute nouvelle unité du LISH de Paris en 1975 où il restera jusqu’à 1989. A partir de là, il arrête de travailler pour Boudon (il soutiendra toutefois sa thèse en 1980 sous la direction de Boudon), et il organise pendant 10 ans (1979-1989) de très nombreuses formations sur la programmation, les statistiques au LISH mais également lors de stages dans divers organismes. En plus de son activité dans \enquote{la feuille d’avis du LISH}, il co-édite également un journal dédié aux statistiques en sociologie à partir de 1983 (Bulletin de méthodologie sociologique). Il est également l’auteur d’un logiciel TRI-DEUX démarré en 1970, toujours utilisé et actualisé. Ses travaux et Ses publications ont probablement joué un grand rôle dans l’évolution de l’analyse de données en sociologie, et l’introduction des analyses factorielles au plus grand nombre. Il raconte dans son HDR comment l’usage de l’informatique était organisé au sein du LISH lorsqu’il y était animateur :

\blockquote[{\cite[21]{Cibois1993}}]{Pour ce qui est de l'utilisation de l'informatique, je soutins, en collaboration étroite avec la responsable du service, Monique Renaud, que le centre de calcul ne pouvait correctement fonctionner qu'en \enquote{libre-service}, c'est-à-dire avec un mode de fonctionnement où les utilisateurs créaient eux-mêmes leurs cartes perforées et sortaient eux-mêmes les listings qui en résultaient. Cela pouvait sembler peu de chose mais cette organisation supprimait l'ancien système du travail \enquote{à  façon} où un ingénieur  prend (mal) en charge des travaux  qui sont ensuite examinés (sans investissement suffisant) par un chercheur. La nouvelle organisation permettait au chercheur de faire lui-même les travaux informatiques à la condition qu'il y soit formé : des stages nombreux permirent à des utilisateurs de s'initier aux joies du système d'exploitation IBM 360 du Circe et le LISH grâce à cette nouvelle organisation devint rapidement un centre de calcul performant et très fréquenté.}

La géographe Nicole Mathieu ferait partie des quelques géographes à avoir fréquenté ce laboratoire au début des années 1980, comme en témoigne sa réponse à Denise Pumain lors d'une rencontre sur l'interdisciplinarité \enquote{L'époque de la carte perforée a commencé bien avant celle que tu mentionnes et a duré jusqu'à la fin des années 1970; elle a été suivie par une phase où nous élaborions des fichiers électroniques qui étaient traités au Lish, boulevard Raspail au début des années 1980 [...]} \autocite[154]{Mathieu2014}

En recoupant les informations données par Denise Pumain et Lena Sanders, et suite à une correspondance privée avec Alexandre Kych et Philippe Cibois, d'autres géographes ont également travaillé au LISH :

\begin{itemize}[label=\textbullet]

\item Ceux qui ont travaillé pour Françoise Cribier (Géographie Sociale et Gérontologie) et d'autres chercheurs apparentés (Catherine Omnès, Elise Feller) : Alexandre Kych, Catherine Rhein et des doctorants de Benzécri comme Jean Marc Blosseville.

%\item Fla, Marie-Odile et Irène: Jeanine Cohen (Laboratoire de Géographie Humaine) \hl{Auriez vous par hasard les noms de ces trois personnes Denise ?}

\item Jeanine Cohen (Laboratoire de Géographie Humaine)

\item Gérard Joly alimentant la base de l'INIST

\item Françoise Pirot qui s'est intéressée très vite aux systèmes d'informations géographiques, dans le cadre d'un laboratoire à l’institut de géographie, puis à la MSH.

\item Ceux qui ont suivi des formations à FORTRAN-4 ou aux statistiques de base, formations organisées par Claude Deniau et quelques autres au début des années 70: les futurs géographes quantitatifs de région parisienne comme Denise Pumain, Thérèse Saint-Jullien, François Durand-Dastès

\item Lena Sanders est amenée à fréquenter le LISH sur les conseils de François Durand-Dastès à partir de l'année 1978-79, et cela jusqu'en 1982. Elle réalise durant cette période des analyses de données sous SPSS. C'est là également qu'elle rencontre Bernard Marchand, qui l'embauche pour donner des cours de FORTRAN pour l'institut d'urbanisme à Créteil. C'est d'ailleurs la seule formation FORTRAN dans cette université au début des années 1980 d'après Lena Sanders, qui raconte avoir dû adapter son cours pour des élèves informaticiens non prévus à l'origine.
\end{itemize}

Ces géographes travaillent pour certains, comme Françoise Pirot ou Alexandre Kych, également directement au CIRCE, ou à Jussieu.

\blockquote[\textit{Alexandre Kych, échange daté de mai 2015}]{Au CIRCE, on utilisait une machine IBM, avec MVS comme système d'exploitation et TSO et SPF comme interfaces. Au même moment Françoise Pirot travaillait au centre de calcul de Paris-7 à Jussieu. On y trouvait une machine d'un autre constructeur dont j'ai oublié le nom. Et nous avions le plus grand mal à nous comprendre et à échanger des données, car l'organisation matérielle et le système d'exploitation étaient tous différents. Pendant ce temps, l'INED travaillait avec un mini-ordinateur et UNIX.}

Avant de travailler au LISH, Lena Sanders travaille également au laboratoire de Paris-7 à Jussieu. Elle réalise principalement des analyses de données en utilisant les programmes de Benzécri pour des projets pilotés par François Durand-Dastès. Ce dernier l'introduit ensuite au LISH durant l'année 1978-79, date à partir de laquelle elle ne fréquentera quasiment plus ce centre de calcul de Jussieu.

On sait également que certains liens se nouent entre des acteurs du LISH de Marseille et les géographes du sud-est, comme en témoigne la personne de Jean-Paul Cheylan, architecte et géographe affilié au LISH-Marseille entre 1978 et 1984. Celui-ci va ensuite participer aux travaux de conception, de mise en place et de fonctionnement du GIP RECLUS, cela à partir de 1982.

En restant très prudent, mais parce qu'il faut bien faire un premier bilan à la suite de ces témoignages, on constate à quel point les chercheurs résidants et le personnel pilotant le fonctionnement de ces centres de calcul ont joué un premier rôle important dans l’accompagnement de la plupart des géographes, mais aussi plus généralement des chercheurs en sciences humaines. Que ces centres soient nationaux comme le CIRCE du CNRS, ou nationaux universitaires comme le CNUSC, plus thématiques du CNRS comme les unités du LISH, ou encore petits et intégrés aux universités, ce schéma semble se répéter. Une plus grande analyse reste à faire sur ce point.

Ces pôles sont également le creuset d'une interdisciplinarité qui ne se limite pas dans sa durée de vie à cette seule période temporelle, et dont les témoignages tendent à montrer qu'elle est regrettée par la suite :

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{Les centres de calcul comme le LISH, comme le CIRCE, comme celui de Paris 1, c'étaient des endroits formidables, car on travaillait tous ensemble. Et on avait du temps, car on attendait les résultats. Donc quand on se trompait, c'était la consultation générale, qu'est-ce-que ça peut être, etc..  Il y avait un échange réel ! J'ai revu plusieurs fois Denise Pumain, car j'ai été président du comité de la section histoire, et par la suite je l'ai aussi croisée dans toutes les instances, mais je n'ai plus jamais parlé avec elle comme je pouvais parler quand on se disait \enquote{mince, qu'est-ce qui n'a pas marché cette fois-ci ?}. C'est là que se faisait le vrai travail, c'est là où on pouvait vraiment discuter. Philippe Cibois a été une aide précieuse pour tous les gens qui ont importé, car c'était un des esprits les plus clairs que je connaisse dans le domaine de l'informatique. Michaël Hainsworth aussi, c'était des gens avec qui on pouvait vraiment travailler. Et puis quand il y avait vraiment de gros problèmes, il y avait des mathématiciens ou des physiciens, qui avaient un niveau informatique d'un niveau bien plus élevé encore.}

\blockquote[\textit{Colette Cauvin, échange daté de juin 2015}]{L’interdisciplinarité est surtout due au fait qu’on croisait sans cesse des personnes de différentes disciplines, sciences humaines comme sciences dures (la géographie à Strasbourg était rattachée aux sciences dures heureusement) que ce soit quand on attendait nos résultats, ou lors des stages d’apprentissage de nouveaux logiciels, qui étaient communs. Les contacts ont perduré par la suite.}

\blockquote[\textit{Alexandre Kych, échange daté de mai 2015}]{Et il rejoint un aspect du LISH (et du CIRCE) qui m'a toujours semblé important et que je continue à regretter. Nous, utilisateurs du LISH, n'étions pas très nombreux au regard des chercheurs et ingénieurs de chaque discipline. Par contre, nous travaillions côte à côte tous les jours. On vivait au quotidien la pluridisciplinarité. Nous étions géographes, historiens, économistes, psychologues, politologues, démographes, sociologues, anthropologues (et même physiciens) et nous parlions entre nous. Pour ma part, j'ai eu plus de contact avec le LISH qu'avec l'équipe PARIS, que je connaissais et qui était à moins d'un quart d'heure à pieds du LISH. Avec le CIRCE et le LISH nous n'avions à nous occuper que de ce qui nous intéressait, le travail scientifique. Par contre, nous n'avions pas à nous occuper des soucis matériels et nous avions toujours une réponse aux questions que nous leur adressions. Avec l'avènement généralisé des micro-ordinateurs, nous sommes devenus de fait ingénieurs système, ... sans en avoir la compétence.}

S'il est impossible de donner une liste exhaustive de facteurs pouvant expliquer l'abandon de ces structures interdisciplinaires, on peut essayer toutefois essayer de donner quelques pistes à partir des témoignages récoltés.

Comme témoigne Philippe Cibois, \textquote[\textit{Philippe Cibois, échange daté d'avril 2015}]{La micro-informatique a fait perdre au Lish sa raison d'être de centre de calcul : nous avons formé des chercheurs à la micro-informatique et ils sont devenus autonomes}.

Un discours que l’on retrouve aussi dans les mots de Michaël Hainsworth, directeur du LISH en 1983; avec l’expression d’une nouvelle orientation de structure pour le service, qui bien qu’elle continue à offrir un double service sur les petits et gros équipements, semble assez différente de celle dictée par Cibois en 1975.

\blockquote[\cite{Hainsworth1983}]{Depuis janvier 1983, le Centre de Calcul du LISH a changé de direction. Ce changement de direction ne signifie pas tant le remplacement d'un homme par un autre, mais la mise en place d'une structure différente. Il s'agit désormais pour nous d'être à la fois une vitrine de l'informatique et une station service pour les chercheurs en Sciences Humaines et Sociales. [...] Point d'intersection de cette structure modulaire, lieu de rencontre, de débats, d'enseignement, le LISH devrait aussi offrir, ce qu'il offre déjà partiellement, un service de programmation. Il est en effet hors de question de former tous les chercheurs qui ont besoin de l'informatique, surtout si ceux-ci ont des besoins très réduits. [...] On pourrait croire à la lecture de ces lignes que nous souhaitons réaliser un service très centralisé et la nature exceptionnelle du Centre de Calcul du LISH, unique dans ses prestations destinées à un public aussi particulier que celui des sciences humaines, pourrait sembler aller dans ce sens. Nous sommes, au contraire pour une informatique répartie dans les laboratoires, sur les lieux où se trouve l'information, dans un grand réseau où chacun se retrouve lié avec tous les autres. Nous ne souhaitons constituer qu'un lieu de passage dans lequel les chercheurs du CNRS et leurs homologues universitaires ou étrangers puissent trouver l'information dont ils ont besoin ou celle qui leur fait défaut.}

Parmi les nombreux facteurs intervenant en faveur de cette démocratisation de la micro-informatique, on remarquera que si le calcul devient certes moins rapide sur micro-ordinateur (les centres de calcul possèdent toujours des machines beaucoup plus puissantes !), ces contraintes sont largement compensées par la disparition des quotas d'utilisation, et surtout de \textbf{la facturation des temps de calcul jusqu’à alors imputée par le CNRS à ses propres chercheurs ...} Il faut en effet rappeler que pour la plupart des centres du CNRS, toutes ces prestations de calculs étaient facturées, cela dès le début des années 1970 : temps de calcul, sorties sur les périphériques.

\blockquote[\textit{Alexandre Kych, échange daté de mai 2015}]{Pourquoi, dans ces années-là, tous les géographes n'ont-ils pas travaillé sur gros systèmes ? D'abord, il y avait un coût. Tous les mois, le laboratoire recevait une facture avec un ticket modérateur qui tenait compte du CPU, de la mémoire centrale et des espaces disques utilisés, du nombre de montages de bandes et lignes imprimées, et des priorités demandées. Il était hors de question de disposer d'un terminal et de ses accessoires dans le labo même, il fallait donc aller au LISH qui disposait de l'équipement utile : terminal, lecteur de cartes, imprimantes, traceur Benson, sans compter les imprimantes et les perfo-vérif. Cela entrainait que l'on était absent de son labo pendant des périodes assez longues, sans pouvoir jouir d'un espace personnel de rangement, en étant soit au LISH (au 2ème sous-sol quand même, c'est-à-dire sans lumière du jour et avec quelques fois des odeurs particulières) soit au CIRCE même, à Saclay, à une demi-heure à pieds de la station \enquote{Le Guichet}.}

Il fallait donc faire régulièrement des demandes d'aides, dont Jean-Philippe Genet décrit par ailleurs la difficulté d'obtention.

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{La grande différence par rapport à ce qu'on faisait en local, c'est que pour travailler à Orsay, il fallait des crédits CNRS.[...] Ce n’est pas que c'était très cher, mais il fallait surtout demander au comité national du CNRS l'autorisation, et obtenir un crédit, faire un dossier, etc. bref c'était compliqué. Donc je l'ai fait, j'ai obtenu un crédit, je m'en souviens très bien, mon premier crédit était de mille francs, et je recevais un carnet de chèques. C'était très bien d'ailleurs, la gestion n'était pas très compliquée, mais ça marchait très bien. Ceci dit cela ne servait à rien dans le cas d'Orsay, puisque là c'était des virements internes, donc je n’ai jamais touché au carnet de chèques.}

Dans le cas des formations utilisant ces centres, certaines universités pouvaient heureusement prendre en charge ces crédits, comme au CCSC :

\blockquote[\textit{Colette Cauvin, échange daté de juin 2015}]{Au CCS, les crédits ne concernaient que le temps de calcul et les sorties sur les différents périphériques. Certains logiciels étaient fort gourmands en temps (Catia par exemple). Les formations étaient rarement payantes ou alors un coût minime. Pour les étudiants, nous devions obtenir des crédits spécifiques à l’université.}

La diffusion de la micro-informatique est donc un des facteurs qui transparait comme évident dans la disparition des usages des centres de calcul, mais il n'explique pas tout. Car ces centres auraient pu être pour certains des lieux d'échange autour de la micro-informatique, comme le LISH avait commencé à l'envisager dans l'organisation de clubs, où règne souvent un esprit de découverte et de partage autour de ces nouveaux outils. L'\enquote{esprit micro}, c'est l'occasion pour les utilisateurs de réinventer les règles du jeu\Anote{esprit_micro_jeu}, de se regrouper en club d'utilisateur \autocite[57]{LISH1984}, d'entamer la création ou la migration de logiciels des centres de calcul vers les postes micro, etc ...

En définitive, il semble aussi que la gestion du CNRS vis-à-vis des SHS et de leur rapport à l'informatique ait été critiquable en bien des points.

Pour mieux comprendre quels sont les enjeux réels sur la question des usages des ressources de calculs dans les grands centres, il est par exemple intéressant de constater que les sciences humaines sont en 1972, avant même la création du LISH, déjà consommatrices de ressources de calcul. Le rapport d’activités du CNRS de 1972 faisant état pour l’année 1972 sur le 3600 CDC de CIRCE de 2 517 heures de calcul dont 3.25\% proviennent des sciences humaines, ce qui n’est pas du tout négligeable quand on connait la difficulté de ce contexte technique pour ces chercheurs ! Autre statistique étonnante, les utilisateurs se répartissent sur cette année-là entre toutes les disciplines  de la façon suivante : mathématique et informatique 10\% environ, physique 40\% environ, chimie 25\% environ, sciences humaines 15\% environ, sciences de la terre 3\% environ, sciences de la vie 3\% environ, gestion-documentation 4\% environ.

Alexandre Kych reprend un argument similaire\Anote{kych_notecalcul} pour expliquer comment les SHS ont été écartées durant la restructuration du centre de calcul CIRCE vers l'actuel IDRIS.

\blockquote[\textit{Alexandre Kych, échange daté de mai 2015}]{Pendant longtemps c'est Arlette Faugères, ingénieure à l'ENS travaillant surtout avec des historiens, qui a représenté les sciences sociales au comité des utilisateurs du CIRCE. Puis elle en a eu assez et m'a demandé de la remplacer. On ne se bousculait pas au portillon et je fus choisi. Manque de pot, cela s'est passé au moment où le CNRS a choisi d'abandonner le CIRCE et de le remplacer par l'IDRIS. J'ai assisté à pas mal de réunions avec Fayole, alors directeur du CIRCE. En gros, le CNRS ne voulait plus de sciences sociales au futur IDRIS, arguant que nous n'avions pas besoin de ce type d'équipement et que les centres universitaires suffisaient, en particulier le CNUSC à Montpellier. Les utilisateurs du LISH n'étaient pas d'accord. Fayole nous soutenait tout en faisant remarquer que les sciences sociales utilisaient moins de 5\% des ressources informatiques du CIRCE, mais plus de 90\% des demandes d'aide. Mais, en gros, tous les représentants âgés des sciences dures nous soutenaient comme un grand frère soutient sa petite soeur. Par contre, les représentants jeunes des sciences dures étaient unanimes pour nous renvoyer vers d'autres centres de calcul. Bel effet de génération.}

Dans les années 1990, de grandes restructurations commencent dans le paysage du HPC français, et si certains centres sont amenés à se transformer comme le CIRCE, d'autres comme le centre de calcul de Strasbourg en 1993 sont quant à eux fermés. Ce dernier devient un CRI intégré à l'ULP sous le nom de CURRI, avant d'être de nouveau regroupé, délocalisé et regroupé avec d'autres laboratoires en 2007 sous la forme d'un méso-centre. En même temps que le Laboratoire Image Ville Environnement (LIVE) s'équipait de son propre matériel et logiciels pour effectuer des calculs plus complexes (station IBM RS 6000 par exemple), en partie grâce aux crédits suivant la fermeture du CCSC, celui-ci a réussi à maintenir une activité liée avec un centre de calcul à la forme changeante. Une adaptation possible grâce à la présence de collaborateurs informaticiens en interne, comme Jacky Hirsch, capables de réécrire ou d'adapter les programmes aux nouveaux environnements. Les usages ont également évolué suivant la progression de l'informatique, avec la possibilité de faire de la télédetection et de la cartographie plus poussée sur micro-ordinateur. Le mésocentre est quant à lui toujours utilisé par des équipes de ce laboratoire \autocite{Asch2012}, principalement pour faire de la modélisation, comme les travaux de Nadège Blond par exemple. Par chance, les travaux d'Alexandre Kych sont par chance rapatriés au CNUSC.

\blockquote[\textit{Alexandre Kych, échange daté de mai 2015}]{Finalement, le CNRS nous a envoyés au CNUSC. C'était un moindre mal dans la mesure où c'était une machine IBM sous MVS. Le transfert de nos données s'est faite sans difficulté particulière. Mais moins de deux ans après, on nous a annoncé que nous devions quitter le CNUSC, là encore pour une refonte des centres de calcul universitaires. Et cette fois, comme j'avais rejoint entretemps le LASMAS, nous avons dû nous installer au centre de calcul de l'université de Caen. Nous l'avons quitté il y a 3-4 ans. Et afin d'assurer une sauvegarde sérieuse à nos données, nous l'avons confiée à l'IN2P3, à Lyon. Voilà un gros système dont on parlait déjà dans les années 70, mais à Paris..}

Malheureusement, on suppose que tous les travaux et tous les centres de calcul accueillant les SHS n'ont pas eu cette chance de continuer à exister, comme c'est le cas du LISH évoqué juste avant, poussé à la fermeture. Sur ce point, le témoignage précédent de Kych racontant l'éjection des SHS lors de la restructuration du CIRCE vers l'IDRIS trouve un renfort supplémentaire dans le témoignage de Genet :

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{[...] Celui qui a vraiment engagé le bras de fer, c'est Michaël Hainsworth. C'était un égyptologue, et donc en tant qu'égyptologue, il avait une grosse protection, car il faisait la partie informatique du travail de Jean Leclant, qui est devenu très vite le secrétaire perpétuel de l'institut. [...] Fort de cette protection, Michaël Hainsworth a essayé d'imposer un développement de la micro-informatique en accès libre très rapide, et ça a posé problème rapidement, car la direction du CNRS ne voulait pas lâcher la bride. Je le sais d'autant plus, car à l'époque j'étais à la direction scientifique du CNRS, et j'ai donc assisté de près au clash entre Hainsworth avec qui je travaillais, et les gens de la direction scientifique. Une direction censée être de gauche, ouverte, mais les réflexes administratifs français ... La priorité du CNRS, les physiciens y veillaient, c'était les gros équipements, c'était le CIRCE, et il ne fallait surtout pas aider le LISH à pousser et à éparpiller la micro-informatique chez les utilisateurs.}

A lire ces deux témoignages donnant un aperçu de la position prise par le CNRS, on comprend que le LISH n'aura bientôt ni les moyens d'être une vitrine pour diffuser la pratique de la micro-informatique dans les SHS, ni les moyens d'accueillir les chercheurs souhaitant continuer à utiliser de façon encadrée ou autonome des ressources informatiques HPC plus importantes. En résumé, si d'un côté le CNRS semble vouloir repousser l'implantation de la micro-informatique dans les laboratoires au profit de l'auto-financement par les chercheurs de sa propre infrastructure de calcul, elle pousse un peu plus tard et de façon paradoxale lors des diverses restructurations les SHS hors de ces mêmes centres.

Pour les plus malchanceux, reste donc le retour obligé à la micro-informatique, contraignant les chercheurs à faire au mieux avec le maigre héritage laissé par ce qui semble être une non-politique d'équipement micro dans les SHS. Cette double politique du CNRS a, semble-t-il, été très dommageable pour certaines disciplines, comme chez les historiens, où Genet témoigne :

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{Tout ça a duré assez longtemps, car le CNRS a beaucoup renâclé à pousser la micro informatique. [...] Ici à Paris 1 on n’avait pas de micro-ordinateur, on travaillait toujours sur le P880, et surtout on n’avait pas les logiciels, donc on continuait à travailler avec le logiciel BDP4 qui fonctionnait bien. J'avais mon système qui s'appelait ALINE pour l'analyse linguistique qui fonctionnait également bien, donc je n'allais pas changer pour des micro-ordinateurs pour lesquels il n'y avait aucun logiciel. D'ailleurs le CNRS ne nous y encourageait pas, il ne voulait pas nous donner d'argent pour qu'on achète des micro-ordinateurs, c'était explicite. Et parce qu'il voulait surtout continuer à nous faire financer... En fait il donnait aux équipes des sciences humaines des crédits, pour faire de l'informatique, mais ces crédits étaient ensuite convertis automatiquement en crédits reversés au centre de calcul du CIRCE à Orsay. En fait l'argent ne faisait qu'un détour par nous, il tournait en rond et revenait dans leur système. Donc il ne nous poussait pas du tout à faire de la micro-informatique. [...] Donc il fallait que l'on continue à travailler avec les grosses machines.}

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{Quand je revois toute cette période là, dans les années 1985, où on travaillait sur gros systèmes, franchement par rapport aux Anglais on a été bien meilleurss. On travaillait au niveau des Américains. Avec le virage de la micro-informatique on s'est retrouvé dix coups derrière, c'était fini. Et incapable de réagir.}

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{Tout ce qui était interdisciplinarité, centrée sur l'usage informatique a disparu avec la micro-informatique. Vous avez cité \enquote{ le médiéviste et l'ordinateur }, ça marchait très très bien. Du jour où il y a eu la micro-informatique, le déclin a été continu. On avait une association qui s'appelait \textit{International Association for History and Computing} basée en Angleterre. J'en ai été le premier président, cela marchait très bien, on a fait d'importants colloques. Mais passé 1995, tout ça a disparu. Il y avait la branche française dont s'occupait André Sitzberg qui avait un bulletin publié par le LISH, ça a également disparu. Tout ça est fini. La seule chose qui surnage de cette époque, c'est parce qu'on s'était refusé à faire de l'informatique, c'était la revue \textit{Histoire et Mesure}. Une revue qui était au CNRS, mais qui est retenue aujourd'hui par le CRH de l'école des hautes études.}

Ces extraits sont tirés de l'entretien réalisé en mai 2015, mais Jean-Philippe Genet insiste régulièrement sur le rôle de la politique du CNRS dans cet essoufflement du numérique dans la discipline\Anote{essouflement_genet}.

On trouve également dans le numéro 9 de Mémoire Vive l'appel d'urgence lancé par \textcite{Genet1993} à destination des institutions, dans laquelle la vision tout à fait lucide et visionnaire de Genet - trouvant largement sa place de précurseur dans le mouvement de pensée interdisciplinaire des \enquote{nouvelles} humanités digitales\Anote{histoire_informatique} - est malheureusement confrontée à cette dure réalité des problèmes de sous-équipement et de sous-représentation que subissent la plupart des historiens voulant se former à l'informatique en France en 1993.

Tout cela sans compter qu'il y avait apparemment dans certaines disciplines des SHS un véritable problème vis-à-vis de la valorisation par les institutions du travail informatique ainsi réalisé, dans l'université et probablement aussi du côté du CNRS, comme l'ont montré les derniers paragraphes. Le témoignage de Jean-Philippe Genet est aussi il me semble une invitation à se poser cette question dans notre discipline, par rapport à la construction de base de données, et des modèles de simulation. De quelle façon doit-on, et peut-on les valoriser aujourd'hui vis-à-vis d'institutions d'évaluation toujours plus pressantes ?

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{Parmi les historiens qui ont beaucoup fréquenté le LISH, il y avait André Sitzberg, Barbet, Laurent Ladury. Mais voilà, par exemple, Laurent Ladury ce n'est pas lui qui réalisait la partie informatique de son travail, c'est Sitzberg qui a principalement travaillé pour lui. Il y avait donc des gens qui travaillaient pour les autres. Par exemple c'est Jule Roméro, le premier enseignant en histoire et informatique ici, qui a fait toutes les thèses de Paris 4. Par conséquent, il n'a pas pu faire la sienne. C'est assez injuste. Hainsworth lui a été viré, voilà ce qui arrive finalement aux gens qui ont compté.}

\blockquote[\textit{Jean-Philippe Genet, entretien daté de mai 2015}]{De toute façon ce travail n’est pas valorisé dans la profession. Il ne l'est absolument pas. Peut-être cela va changer, je ne sais pas. Moi j'ai soutenu ma thèse très tard, parce que j'avais toutes ces bases de données à faire. Il y a une base de données, pas le lexical je l'ai laissé tomber, tant pis je ne l'ai pas mis, mais tout ce que j'avais fait sur la prosopographie j'ai dit \enquote{ben voilà j'ai cette base de données}, et on m'a répondu \enquote{mais vous n'y pensez pas !} Donc j'ai imprimé le contenu de ma base de données, et j'ai soutenu sur une thèse de plus de 8000 pages. Personne n'a été ouvrir un volume depuis, évidemment. Donc vous pouvez mettre dans votre bibliographie 4 pages que vous avez faites dans les annales sur un compte rendu de mauvais bouquin, ça compte... mais vous faites une base de données qui vous a pris 20 ans ça, ça ne compte pas. En plus, les gens qui sont importants dans l'Histoire en France, ils n'ont jamais fait d'informatique. Puisque ce sont les meilleurs historiens de toute façon, et qu’eux n'en ont pas fait, comment voulez-vous que cela compte ?}

Il faudra toutefois essayer de nuancer et de ne pas généraliser cette position du CNRS. Dès 1986, et dès lors que l'équipe de géographe PARIS est agréée comme Jeune Equipe, le CNRS facilite l'embauche d'un ingénieur informaticien (Ky Nguyen), et permet ainsi au laboratoire d'être un des premiers en SHS à s'équiper d'un réseau informatique.

On a vu également que dans certains cas, comme au LIVE de Strasbourg, ou encore à Besançon par l'intermédiaire de ThéMA et de la MSHE, des liens avec les centres de calcul avaient continué à exister, et l'interdisciplinarité à perdurer. Denise Pumain disait qu'il faut \enquote{se donner des instruments à la hauteur des questions que l'on se pose}, mais une autre formulation pourrait également s'avérer intéressante, en essayant par exemple de mobiliser au travers des instruments que l'on utilise de nouvelles questions à la hauteur des théories qu'il nous faut confronter\Anote{amoral_note}. Les géographes ont, il me semble, toujours su faire preuve de suffisamment d'ouverture et d'imagination pour saisir dans les outils à leur disposition, l'émergence de nouvelles questions, surtout lorsque celles-ci sont, comme le HPC, théoriquement liées à un horizon des usages computationnels pratiquement infini. Même si cette étude est partielle, et mérite d'être prolongée dans de futures recherches, elle montre qu'il y a eu pour certains laboratoires ou géographes une forme de continuité dans la nécessité d'avoir des ressources HPC à disposition d'une activité de recherche en géographie.

Dans le cadre des géographes modélisateurs français, nous allons étudier le cas plus particulier de l'expérience menée par les géographes du laboratoire Géographie-cités, afin de montrer que la question de l'exploration des modèles en usant du HPC a depuis le début été inscrite comme un besoin quasi-immédiat pour l'évaluation et la compréhension des modèles de simulation géographiques confrontés à l'empirie, et cela même si ce besoin a subi une latence forcée dans son utilisation, principalement du fait de l'insuffisance des ressources alors en place. Nous verrons par la suite comment ce besoin en dormance sera rappelé et réexprimé à la fin des années 2000.

%CRISS Centre  de  Recherche  en   Informatique  appliquée  aux  Sciences  Sociales
%Il existe également à Grenoble une UER pour l'Informatique et mathématiques en sciences sociales (IMSS) de l'UFR SHS de l'université Pierre Mendès France.

%IHRT Grenoble
%Jacques Rouault

%1984 LOGO

\subsubsection{Focus sur les usages de ces centres par les premiers modélisateurs, et l'exemple plus spécifique de l'équipe PARIS}
\label{sssec:contexte_modelisateur}

Cette sous-section commence par quelques mots sur l'absence inquiétante d'un programme d'archivage en géographie. Les \enquote{pratiques scientifiques} qui consistent dans la découverte, l’exécution et la modification de codes sources, et la création, l'utilisation de logiciels ou de matériels informatiques ont fait l'objet de peu d'attention jusqu'à présent chez les historiens des sciences, y compris en géographie. Certaines données, logiciels, ou codes sources, non imprimés, et seulement disponibles sur supports informatiques de l'époque auront probablement disparus d'ici peu, si ce n'est pas déjà le cas. La prise en compte du patrimoine informatique de notre discipline est urgente, pas seulement pour assurer un point de vue historique, mais également d'un point de vue scientifique, car bien des programmes développés restent aujourd'hui pertinents et méritent d'être redécouverts. Le lecteur intéressé par ce sujet pourra se tourner vers un certain nombre de mot-clef pour trouver plus de documentation : \textit{Digital Dark age}, \textit{Software Archaeology}, \textit{Computational Archaeology}, \textit{Digital preservation}.

Si la pratique des statistiques a eu son lot de manuels et d'ouvrages écrits par et pour des géographes, ce n'est pas le cas semble-t-il des aspects algorithmiques, ou techniques pour la modélisation, surtout entre 1980 et 1990. On notera celui de \textcite{Guermond1984} publié à la suite du stage de Rouen, et celui de \textcite{Dauphine1987}. Sur ce dernier, l'auteur lui-même admet le goût d'inachevé que le livre peut laisser aux lecteurs, de plus le choix de DYNAMO et de son a-spatialité pour mettre en avant les exemples est étonnant. Enfin, à la même période chez les Anglo-saxons, une révolution conceptuelle est en train de naître avec les écrits de \textcite{Couclelis1985}, ou encore avec les écrits et les travaux pionniers d'Openshaw \autocites{Openshaw1983, Openshaw1988, Openshaw2000}.

Une observation qui semble valable également pour les modèles de simulations anglo-saxons. On a donné quelques pistes dans le chapitre 1 pour récupérer les programmes des pionniers américains des années 1960-1970, disponibles pour certains sous forme de \textit{listing} dans certains rapports et ouvrages archivés aux Etats-Unis. Mais si on prend par exemple certaines publications anglaises des années 1970, comme celle de Batty \autocite{Batty1976} ou de Wilson, on s'aperçoit qu'elles ne présentent aucun code source des modèles réalisés. Pourtant, dans ce cas-là, on sait qu'ils savent programmer tous les deux depuis leurs débuts \autocites{Batty1971b, Batty2014}\Anote{batty_code}. Le code source est probablement un objet considéré comme sans intérêt\Anote{code_americabatty} pour la publication en comparaison des équations mathématiques.

Du côté français il existe donc probablement d'autres projets de modélisation menés entre les années 1970 et 1990, et seule une étude plus approfondie des ouvrages, ou une interrogation orale des principaux acteurs, similaire à celle réalisée par \textcite{Cuyala2014}, pourrait apporter un nouvel éclairage sur ces aspects plus techniques des pratiques informatiques en géographie. Les exemples pris ici reflètent donc plus l'état d'une littérature qui ne donne à voir que ses succès, et/ou ses projets les plus aboutis, mais ne doit pas masquer l'existence très probable d'initiative similaire chez d'autres géographes.

\paragraph{Le contexte au début des années 1980}

Du point de vue de la pratique de la simulation en géographie, le stage du C.N.R.S organisé en décembre 1982 par l'équipe MTG de Rouen est un événement important. Comme introduit par Guermond dans la publication des cahiers de Rouen publiés sur ce stage, \enquote{Les travaux présentés ici ont pour ambition de faciliter un nouveau pas dans l'évolution de la géographie théorique et quantitative française [...]} \autocite{Guermond1983}.

Cette volonté d'une mise en pratique ressort aussi dans la présence de corrigés d'exercices informatiques dans la retranscription des Cahiers Géographiques de Rouen 82-83 \autocite{CGR1983}, et dans la publication plus diffusée qui suit en 1984 \autocite{Guermond1984}. A partir de là, on imagine quelles ont pu être les activités menées pendant les ateliers. Laurini, déjà responsable d'une présentation algorithmique des modèles, présente ainsi la correction en FORTRAN de certains d'entre eux : modèle de Bussieres, de Lakshmaman et Hansen, d'Echenique, de Wilson, ainsi qu'un modèle démographique et de localisation résidentielle. Le Carpentier présente également un corrigé des exercices en BASIC des modèles de Lakshmaman, de Wilson, et d'un modèle démographique.

Il est vrai aussi que peu de véritables tentatives de modélisation - au sens implémentation, et non seulement conceptuel - ont eu lieu avant les années 1980. Une jeune équipe CNRS comme le MTG de Rouen, alors dirigée par Guermond en 1983, intègre pourtant lors de sa formation des unités dont certaines contiennent dès le départ presque plus de mathématiciens/informaticiens que de géographes\Anote{note_micro_infographie}. Pourtant à cette période, comme dans la majorité des autres laboratoires de la discipline, seuls des modèles de simulations relativement simples ont été expérimentés, comme en témoigne Patrice Langlois pour Rouen \enquote{Au début des années 1980, on avait déjà organisé à Rouen une école thématique sur la dynamique des systèmes– Peter Haggett et Allen étaient déjà dans l’air du temps – mais, jusque-là, on ne s’était pas lancés dans la programmation de modèles dynamiques.} \autocite{Mathieu2014}\Anote{description_laurini_algo}

Une telle timidité dans les réalisations de modèles de simulations à la charnière des années 1970-1980 est tout à fait compréhensible, les géographes ayant déjà fort à faire pour rattraper leur retard dans une décennie qui consacre le décollage du mouvement quantitatif \autocites{Cuyala2014, Orain2009}. Les occupations ne manquent pas, avec par exemple la lecture cumulée de vingt années de littérature anglo-saxonne, les conflits avec l'institution géographique de l'époque, la création de nouveaux journaux pour se faire entendre, l'acquisition et le perfectionnement des techniques mathématiques et statistiques, la mise en route de formations et de réseaux pour la diffusion, et l'émergence, ou plutôt la réémergence de l'idée de systèmess en géographie (avec un pic entre 1979 et 1984 selon \textcite{Orain2001}).

Cette dernière activité par exemple engage les géographes dans un processus réflexif qui s'accompagne d'une formalisation (voir le contenu du Géopoint1984 par exemple), entre autres pour parer à d'éventuelles dérives dont on sait qu'elles ne manqueront pas d'apparaître, la systémique étant utilisée jusqu'alors plus majoritairement sous sa forme heuristique (diagrammes sagittaux), peut-être plus facile à pervertir, qu'une systémique à destination algorithmique. Une remarque qu'il faut tout de suite relativiser car la discipline connaît malgré tout la parution de très bonnes études systémiques heuristiques (Durand-Dastès, François Auriac, etc.), alors que la rigueur mathématique n'empêche en rien l'apparition de modèles au faisceau d'hypothèses a posteriori très contestables. \autocite{Orain2001} %Enfin, c'est encore sans compter la diffusion d'un nouveau type de modèles de simulations, dont certains  (Wilson, Allen) ne demandent qu'à être appliqués, répliqués sur des jeux de données empiriques dont il faut pour certain encore les construire\Anote{consommateur_data}.

Pour \textcite[320-321]{Cuyala2014} cet événement de Rouen marque également une deuxième phase dans les orientations du petit groupe de géographes quantitativistes français\Anote{sylvain_deuxiemephase}. Des mathématiques à l'analyse spatiale, ce changement de projet va de pair avec l'arrivée d'une nouvelle génération de géographes. C'est par exemple lors de cet événement à Rouen que Lena Sanders prend pour la première fois contact avec la modélisation. Cette génération cumule ainsi les apprentissages de la génération précédente, et engage le mouvement vers l'auto-suffisance et l'auto-reproduction; ce qui se traduit aussi par une plus grande prise en charge des formations et de leurs thématiques, par exemple dans le cadre des modèles de simulation : Banos, Daudé, Badariotti, Tannier, etc. pour les SMA, Josselin pour les réseaux de neurones \autocite{Dumolard1994}, etc.

\paragraph{Liste des projets informatiques}

La section précédente a montré que toute une partie de cette première génération de géographes était capable de mobiliser des compétences diverses en programmation. L'apparition et la diffusion de la micro-informatique a donné la possibilité pour quelques uns de développer cette compétence et de créer seul ou à plusieurs de véritables logiciels (Pruvot, Dumolard, Charre, Langlois, Lannuzel, LeCarpentier, Waniez, Laurini, Massonie, etc.) \autocites[191]{Mathieu2014, Massonie1986, Charre1989}, alors que d'autres géographes vont plutôt mettre leur compétence en programmation au service de collaborations internes, lorsque la compétence est disponible (Strasbourg, Rouen), et/ou tournée vers l'extérieur, comme c'est le cas des modélisateurs de Géographie-cités entre 1980 et 1990.

Cette communauté de géographes ayant \enquote{mis les mains dans le cambouis} est en définitive relativement restreinte\Anote{pumain_main_cambouis}. A cela, il faut ajouter l'absence de diffusion de code source dans les publications et le coût informatique et humain qu'un tel développement depuis zéro présuppose pour des langages tels que FORTRAN, à la fois sur le plan de l'expertise conceptuelle (simulation événement discret), mais aussi sur le plan technique. Les capacités informatiques limités sont également à prendre en compte, et il faut développer une certaine subtilité dans l'écriture de programmes capables de jouer avec ces contraintes. En résumé, il est tout à fait logique de voir ces équipes se diriger vers des collaborations\Anote{denise_collaboration}, ou des intégrations de mathématiciens et d'informaticiens dans leur équipes \autocite{Pumain2014}. %On comprend mieux aussi dans ce contexte l'appel par Denise Pumain lancé aux géographes quantitativiste lecteur de l'espace géographique lorsqu'elle titre \enquote{Après l'analyse factorielle, quoi de neuf en géographie ?} \autocite{Pumain1984}

Parmi les équipes françaises présentes au stage d'apprentissage de Rouen, certaines s'appuient justement sur des collaborations pour présenter des premiers résultats d'applications, comme l’équipe de Grenoble, auteure du modèle Analyse et MOdélisation Régionale des ALpes (AMORAL), et les diverses tentatives de l’équipe PARIS.  %Ces deux équipes sont formés de géographes qui ont été formés à l’informatique, soit en autodidacte, soit à l'étranger, soit au cours des différentes formations entre 1970 et 1980.

Ces modèles dont la compréhension appelle la maîtrise d'un tout nouvel outillage conceptuel et mathématique, sont découverts par d'autres réseaux (groupes de travails AFCET, Club de Rome, publications anglo-saxonnes, stages CNRS, etc.) que ceux proposés directement par le CNRS pour l'informatique et les sciences humaines, du moins tels qu'on les trouve dans les annonces du LISH. En parcourant ces annonces trouvées dans les bulletins du \textit{Médiéviste et L'ordinateur} ou de la Fondation Maisons des Sciences de l'Homme (FMSH), on voit bien d'ailleurs que les formations ne sont pas axées vers la modélisation ou la simulation, mais plutôt vers la programmation, les statistiques, ou parfois l'intelligence artificielle à destination de la classification ou du langage.

Concernant ce premier groupe de Grenoble, si on en croit les travaux de fouille bibliographique réalisés par \textcite{Rey1983} pour un numéro spécial des annales de géographie (numéro 511 spécial \textit{Informatique et Géographie}), le modèle AMORAL se positionne comme la seule tentative de modélisation sur une décennie de travaux dans la géographie rurale \enquote{Quantitative et Théorique}. Celui-ci est initié en 1977, mais la première publication date de 1981. Si on recoupe les informations données dans les différentes publications sur ce modèle de système dynamique, le témoignage de \textcite{LeBerre1987}, et l'avant-propos du rapport final pour la DATAR en 1983, on apprend\Anote{note_amoral_difficulté} que ce sont les mathématiciens/informaticiens du laboratoire ARTEMIS de l’Institut de Mathématiques Appliquées de Grenoble (IMAG) qui contactent en premier lieu les géographes à la recherche d’applications concrètes de leur travail, cela dès 1976. Ces deux derniers ayant déjà publié un manuel sur les systèmes dynamiques, ils sont également l'intermédiaire des géographes avec le groupe \enquote{Dynamiques de Systèmes} de l'AFCET.

Patrice Uvietta et François Rechenmann, chercheur à l'IMAG et l'IRIA, membres tous les deux du groupe Dupont, sont des scientifiques qui jouent probablement un grand rôle dans cette médiation entre informaticiens et géographes sur cette question des systèmes dynamiques. Avant même le début des années 1980, ils réalisent des publications généralistes \autocite{Rechenmann1977} sur les systèmes dynamiques, puis plus tard des encarts méthodologiques et théoriques sur les Systèmes Dynamiques pour différentes publications à destination des géographes \autocites{Rechenmann1976, Geopoint1984, CGR1983, Guermond1984}.

Ce modèle est une construction originale, résultat d'une commande de la DATAR, orienté pour la décision et la formulation de scénario, mais qui ne renie pas la dimension explicative pour autant. Le choix des systèmes dynamiques et du langage DYNAMO était donc un pari risqué provenant d'une équipe de géographes - même très fortement épaulés par des informaticiens - n'ayant jamais construit de modèle aussi complexe. Le retour d'expériences qu'ont tenu à faire ces pionniers est également un témoignage finalement assez rare (toujours aujourd'hui) d'une méthodologie de construction ayant abouti à un modèle opérationnel, cela même s'il est incomplet. Présenté comme un \enquote{laboratoire}, ce modèle de simulation, bien qu'il soit également critiquable par son choix de support en DYNAMO - \textit{top-down}, en partie a-spatial - emploie dans sa méthodologie de construction un cycle de développement incrémental et descendant qui semble particulièrement honnête vis-à-vis du graphe de dépendance finalement proposé\Anote{amoral_reproductible}. Il faut noter également que les auteurs sont tout à fait conscients de la non-neutralité (motrice ici, car la construction du modèle est guidée par des scenarii et des objectifs decisionnels) et de la non-unicité d'une telle construction. Il serait trop long de rentrer dans les détails ici, et il faudrait tester et exécuter le modèle pour en avoir le coeur net\Anote{source_amoral}. On se contentera seulement de citer un passage qui garantit au moins dans les mots, cette volonté d'éviter une \foreignquote{english}{Forrester strategy} \autocite[7-8]{Batty2001} où la structure causale affichée n'a jamais été véritablement justifiée ou testée.

Ainsi en parlant de la démarche systémique, \enquote{Tout au plus peut-on dire qu'elle consiste, dans le cas précis de l'objet de ce travail, à rechercher dans un certain espace (synchroniquement), mais aussi dans un certain temps (diachroniquement : il ne s'agit pas d'un instantané ...) quelles sont les relations considérées comme essentielles à la compréhension du fonctionnement dudit espace. \enquote{\textbf{Essentielles}} est pris au sens étymologique, et non pas au sens dérivé et affaibli de \enquote{\textbf{très important}}. Cela veut dire que le chercheur tente de trouver les relations d'un système dont la nature serait changée par la disparition \textbf{d'une seule} d'entre elles.} \autocite{AMORAL1983}

Pour honorer ce contrat et garantir l'essentialité des relations incluses dans le modèle (142 équations tout de même et un graphe de dépendance qui tient sur une page A3), des analyses de sensibilités ont été réalisées \autocite[34]{AMORAL1983}, et cela probablement en s'appuyant sur les ressources du centre de calcul du CICG. On ne trouve malheureusement aucun détail supplémentaire sur la nature de cette analyse, ni sur les modalités de sa mise en pratique (utilisation du centre de calcul, participation des géographes, etc.), ni sur les résultats. Nous ne pourrons donc pas aller plus loin pour voir jusqu'à quel point l'usage de HPC est intervenu dans la méthode de construction, et l'exploration du modèle. Un entretien prévu avec François Rechenmann devrait permettre d'apporter des précisions dans le futur.

Un autre modèle sur le vignoble, nommé BIBINE, à l'état d'ébauche encore en 1989, est développé sur Macintosh avec STELLA, un logiciel dédié à la formalisation de système dynamique. \autocite{Chamussy1989}

Du côté de ce qui va devenir par la suite la toute jeune équipe de géographes de PARIS (1984), si des expériences sont menées dès les années 1970 au centre de calcul de Paris 1, le début des années 1980 constitue là aussi un tournant. La démarche est très différente de celle proposée par l'équipe de Grenoble, et l'équipe prend rapidement ses distances avec les contraintes du vieux langage DYNAMO (1958) : non événement discret \autocite{Nance1993}, limité par la représentation d'une seule échelle à la fois, mal adapté à l'expression spatiale des hypothèses et à l'introduction de conditions dans le déroulement des programmes. L'équipe se tourne vers de nouveaux outils, qui semblent plus flexibles, en s'appuyant sur les travaux en cours d'équipes internationales (Allen, Wilson). Mais encore faut-il comparer, et tester ces modèles \autocite{Pumain1983}, voir si le résultat paraît engageant, se les approprier en les confrontant aux données alors à disposition. La récupération et l'intervention sur les programmes informatiques de ces modèles de simulation sont au départ principalement motivées par la tentative d’un ancrage empirique de ceux-ci, ce qui constitue déjà une innovation à part entière ! \autocite{Pumain1982}. Les réalisations des années 1980-1990 se font donc sur le mode de la collaboration.

Ces modèles intègrent un certain nombre de particularités. Déjà explicités maintes fois au niveau des bénéfices conceptuels et géographiques \autocites{Pumain1989, Sanders1992}, d'autres particularités de ces modèles sont moins souvent citées, car elles tiennent principalement du support informatique utilisé.

Malgré le peu de matière algorithmique ou informatique donné par Peter Allen dans ses publications, il est évident que ce modèle de simulation, parce qu'il n'intègre pas de façon simultanée les équations, et parce qu'il intègre du spatial, est préposé à l'expression d'une forme de stochasticité à même de modifier sa dynamique. Les choix d'implémentation sont également susceptibles d'intervenir dans la dynamique du modèle, or à la lecture des publications d'Allen on ne connait pas quel est le mécanisme d'ordonnancement dirigeant l'ordre d'intégration des fonctions attribuées à chacune des zones, ce qui pourrait poser problème \autocite[231-233]{Varenne2014b}. Si sur un modèle théorique uniforme cela ne peut avoir aucune conséquence, c'est beaucoup moins vrai dans le cadre d'une application du modèle sur un territoire fait de zones hétérogènes.

Enfin, comme l'indique aussi \textcites[850]{Sanders2013}{Pumain2013} les modèles de synergétique et de Peter Allen sont le résultat d'une \enquote{approche interactionniste}, capable de mêler les différentes approches micro, méso, et macro, et cela bien avant les AC, et les ABM, à qui on attribue trop souvent cette rupture d'une vision \textit{bottom-up}, et dont on a vu dans le cadre de la partie 1 qu'elle opère bien avant sur le plan temporel, et pas uniquement du point de vue conceptuel \autocites{Orcutt1957, Hagerstrand1967a, Ward1973, Doran1970};

Ces modèles ne sont pas nouveaux d'un point de vue du formalisme informatique, et renoue finalement avec une forme de simulation à événement discret basé sur un langage généraliste (Fortran) telle que la géographie l'a déjà connue avant que des langages spécifiques plus orientés pour la simulation n'apparaissent (Simula, GPSS, Dynamo, etc.). Hägerstrand, Marble, Pitts et d'autres géographes programmeurs ou aidés pour la programmation par des informaticiens ont implémentés des modèles directement en assembleur ou en Fortran, sans avoir recours spécifiquement à un langage de simulation dédié.

En un sens, c'est donc plus cette triple combinaison des idées de la systémique, d'une méthodologie de construction descendante, et d'une formalisation dans un langage graphique, puis informatique dédié comme DYNAMO/STELLA, qui a séduit les géographes au départ. Ce formalisme a également connu un engouement international avec les modèles présentés au club de Rome, et la diffusion par le biais de l'AFCET, et la présence chez les géographes de collaborateurs informaticiens maitrisant déjà cet outil. Sans compter l'impact sur les autres langages de simulation, même à événement discret \autocite{Nance1993}, que ce langage a eu.

Mais comme le montre à la fin des années 1970 les modèles de simulations inspirés par les structures dissipatives de Prigogine, ou celle de la Synergétique de Haken, il n'y a nul besoin d'utiliser un langage aussi contraignant et peu adapté pour le spatial que DYNAMO si l'on veut implémenter des modèles de simulations manipulant à la fois les concepts issus de la théorie des systèmes et les mathématiques issues des recherches dans les systèmes dynamiques (une discipline que l'on fait souvent remonter à Poincarré, connu pour être également l'inventeur du terme \enquote{bifurcation} en 1885), et cela à moins de rechercher une expression des processus continus, construits comme des \textbf{flux simultanés} à l'oeuvre dans \textbf{un seul et même système}. Toute création de nouveaux flux ou stocks renvoyant en effet le modélisateur à la construction d'un nouveau système. Cette rigidité explique aussi le peu de modèles développés dans ce formalisme par la suite, alors même que les langages généralistes, très répandus, comme FORTRAN (1954) ou BASIC (1964\Anote{basic_histoire}, très populaires sur micro-ordinateur dans les années 1970-80) restent beaucoup plus flexibles. % Si cette modélisation permet de se focaliser sur la formalisation des relations, ce mode de pensée reste au final très éloigné de la façon dont on se représente les interactions entre acteurs en sciences humaines.

En engageant ces modèles au départ théoriques dans une nouvelle trajectoire plus appliquée \autocite{Banos2013a}, une forme implicite de reproductibilité est mise en oeuvre. Ces modèles sont réengagés dans un processus de validation externe, impliquant alors une nouvelle expertise des modélisateurs, ces derniers étant forcément amenés à questionner le réseau d'hypothèses mis en oeuvre dans le modèle (conditions initiales, faits stylisés et implémentations sélectionnées, paramètres et valeurs de paramètres), voire dans une seconde étape, à le modifier. Ce sont d'ailleurs souvent les modèles théoriques comme ceux-ci (n'ayant pas forcément fait l'objet de validation externe) les plus critiqués et débattus par les modélisateurs ne les utilisant pas, mais qui sont aussi les plus repris, modifiés, usités, malmenés et finalement intégrés dans les trajectoires de développement d'autres modèles. Le modèle ressort inévitablement changé d'une telle comparaison avec l'empirie. Seule la communauté de modélisateurs prompte à comprendre ce placement relatif des modèles sur un gradient opposant modèles purement descriptifs et purement explicatifs est à même d'évaluer l'apport en connaissances de tels modèles \autocites{Bulle2005, Rouchier2013}.  %Autrement dit, le modèle est censé ressortir changé de cette comparaison avec des données empirique.

Autrement dit, il est impossible qu'un modèle construit de façon purement théorique puisse convenir d'emblée, d'un point de vue explicatif, aux spécificités historiques caractérisant les trajectoires de différentes villes. D'un point de vue opérationel par contre, c'est tout à fait envisageable, tout dépend du temps et des degrés de liberté offerts à l'optimiseur, humain ou ordinateur. Un point déjà discuté dans la section \ref{ssec:evaluation_construction}.

La position tenue de façon explicite par l'équipe PARIS face à ces modèles est évidemment celle du critique, car c'est justement la variabilité de ces comportements du modèle à structure égale mais à situations/données différentes qui intéresse les géographes \autocite[99-103]{Pumain1989}.

\pagebreak

\paragraph{Le HPC et les projets de l'équipe PARIS}

De façon générale, Denise Pumain intervient dans l’implémentation informatique de divers projets, entre 1970 et jusqu’en 2015, et cela parfois en collaboration étroite avec des acteurs d’autres disciplines. Dans cette liste évoquant les collaborations ayant impliqué de la part des géographes une activité de programmation sur les modèles - ici de la part de Lena Sanders et/ou Denise Pumain - on indique pour chaque entrée la date initiant la collaboration, et entre \textbf{(parenthèse)} le nom du centre de calcul utilisé s'il y a lieu. Il s'agit là d'une première liste qu'il faudra encore compléter et re-contextualiser, probablement dans des travaux futurs.

\begin{itemize}[label=\textbullet]

\item \textbf{1981 (CC P1)} - A partir de la rencontre  de Peter Allen en juin 1981 à l’université de Créteil, et du lancement d’un contrat CNRS PIREN marquant la collaboration entre les équipes belge (ULB) et francaise (PARIS) (Pumain1984), le modèle FORTRAN est récupéré dès 1981 (lors d’un voyage à Bruxelles, sans contrat de recherche) par Denise Pumain et Thérèse Saint-Julien pour application sur des villes françaises.

Il est alors nécessaire d’adapter au format d’entrée du modèle les données empiriques existantes. Sur ce même modèle, des sorties visuelles plus adaptées mais encore très rudimentaires (sortie des nombres simulés pour chacune des six variables d’état sous forme de \enquote{ cartes } des communes de l’agglomération géolocalisées par des lignes de format FORTRAN) sont également programmées. Les modifications et les exécutions du programme sont faites depuis le Centre de Calcul de Paris 1, durant toute la période de 1981 à 1985-86, en collaboration avec Thérèse Saint-Julien depuis le début, avec un jeune étudiant stagiaire en informatique et urbanisme envoyé par Bernard Marchand (Alain Ozan), et avec l’aide de Lena Sanders à partir de l’automne 1982. Elle a rencontrée cette dernière dans une école d'été de l'OTAN en 1982, à San Miniato. Lena Sanders rencontre l'équipe de Peter Allen (Michèle Sanglier, Guy Engelen, François Boon) la même année pour discuter de la modification du programme. Elle fréquentera le centre de calcul de Paris 1 uniquement pour ce projet entre 1982 et 1985-86, période où elle fréquente également le LISH, sur d'autres projets.

\item \textbf{1983 (Collège de France)} - Denise Pumain travaille avec Bertrand Roehner pour calibrer le modèle de Peter Allen avec le programme MINUIT du Cern. Ce point est développé par la suite dans la section \ref{p:experience_minuit}.

\item \textbf{1984 (CC P1)} - L'équipe PARIS est également amenée à travailler sur une comparaison entre le modèle de Wilson et Allen sur une ville fictive, mais pas avant 1984. La maîtrise de Sandrine Berroir portera d'ailleurs sur une partie de ce projet. Cette comparaison a été effectuée avec Sylvana Lombardo et Giovanni Rabino et a donné lieu à publication plusieurs années après de \autocite{Sylvana1988} : \foreignquote{english}{Lombardo S., Pumain D., Rabino G., Saint-Julien T., Sanders L., 1988, Comparing urban dynamic models: the unexpected differences in two similar models. Sistemi Urbani, 2, 213-228}.

\item \textbf{1984 (CC P1)} - En 1984 également, il y a cette écriture d’un programme de calcul de distance au plus proche voisin entre les villes françaises, réalisé en collaboration avec Bertrand Roehner, du laboratoire de Physique Théorique de l’Université Paris 6, et utilisé pour une publication dans \textit{Géoscopie de la France} \autocite{Quant1984}.

\item \textbf{1985} De la même façon, plusieurs analyses utilisant le modèle synergétique de Weidlich et Haag seront également lancées. On peut identifier deux phases : la \textbf{première phase} démarre à partir de l’automne 1985, d’abord sur des matrices de migrations interrégionales françaises par \textcite{Pumain1987}, puis par Lena Sanders sur des matrices de migration interurbaines avec un accès limité au programme jusqu’en 1991, et la publication de son ouvrage \enquote{ Système de villes et synergétique } en 1992 \autocite{Sanders1992}.

Le programme n'est pas accessible en local, mais exécuté via le réseau à Stuttgart : \enquote{ [...] nous avions à l’époque un premier système de transmission de données par Transpac et un logiciel pré-internet jusqu’à la publication de \autocite{Pumain1988} : \textit{Pumain D. 1988, France, in Weidlich W., Haag G. (eds.): Interregional migrations. Dynamic theory and comparative analysis, Berlin, Springer Verlag, 131 - 148.}}

Lena Sanders, qui travaille seulement à partir de 1986-87 sur ce modèle indique que les exécutions se font depuis le laboratoire de la rue du four à Paris, grâce à l'acquisition d'un premier ordinateur connecté à Transpac dès 1986. Ce réseau rend accessible le modèle de simulation depuis Stuttgart, et permet également de se connecter directement au centre de calcul CIRCE.

Le programme était écrit en Fortran, et c’est grâce à cela que Denise Pumain a pu rectifier une erreur d’interprétation par les collègues physiciens allemands, du fait d’une inversion du sens de lecture des indices ($M_{ij}$ signifiant pour eux migrations de $j$ à $i$ alors que pour les géographes ou des statisticiens cela signifiait migrations de $i$ vers $j$).

Une \textbf{deuxième phase} a lieu ultérieurement en 1997, Lena Sanders et Hélène Mathian modifient et exécutent le programme avec des nouvelles données de recensement, pour réaliser des comparaisons avec les expériences précédentes. Les résultats sont publiés en 1998. A la différence de la première phase, le programme en FORTRAN est cette fois-ci récupéré auprès de Weidlich et Haag, et s'exécute sur un ordinateur du laboratoire.

 %Les simulations étaient effectuées alors sur des terminaux situés rue du Four, dès lors que le laboratoire est équipé après 1986 (\hl{voir Lena pour confirmation})

\item \textbf{1985 (CC P1)} \textcite{Poulain1985} réalisent des travaux d’estimation de modèles d’interaction spatiale sur les migrations avec le démographe Michel Poulain : \foreignquote{english}{Poulain M., Pumain D., 1985, Une famille de modèles spatiaux et leur application à la matrice des migrants interdépartementaux français pour la période 1968-1975. Espace, populations, sociétés, 1, 33–42.}

\item \textbf{1991 à 1997} Lena Sanders et Hélène Mathian vont en Suède à plusieurs reprises pour modifier et exécuter des programmes de micro-simulation écrits en SIMULA et C++

\end{itemize}

\paragraph{Problème de la calibration des modèles - l'expérience MINUIT}
\label{p:experience_minuit}

Parmi les différentes collaborations exposées auparavant, celle de 1983 avec Bertrand Roehner attire en particulier notre attention, car elle va déboucher sur une première tentative d’utilisation de calcul intensif en vue de calibrer le modèle de Peter Allen avec des données empiriques.

L'équipe de géographes modélisateurs PARIS ne publie pas plus les codes sources que ses collaborateurs à qui elle a emprunté les programmes. Il ressort toutefois des publications les difficultés méthodologiques rencontrées pour calibrer les modèles ainsi récupérés. Cela n'empêche pas cette équipe d'appliquer ces modèles sur plusieurs villes françaises, afin d'en dégager une connaissance relative \autocite[134]{Pumain1984} : \enquote{Des applications de modèles de P. Allen sont en cours dans le cadre d'un contrat PIREN du CNRS sur les agglomérations de Rouen, Nantes, Bordeaux et Strasbourg (par l'équipe PARIS) et à l'université Libre de Bruxelles sur les régions hollandaises et l'agglomération de Bruxelles (par P. Allen, G. Engelen et M. Sanglier)}

A la suite de sa soutenance de thèse en 1980 et de la publication de la dynamique des villes en 1982 \autocite{Pumain1982}, Denise Pumain est contactée par Bertrand Roehner. Maître de conférence à Paris 6 et chercheur au laboratoire de physique de Jussieu, ce dernier est un physicien ayant rapidement bifurqué sur les sciences sociales, comme en témoignent ses différentes publications en sociologie et en économie.

Accompagné dans ses travaux par un Autrichien du \textit{Bordalier Institute} nommé Peter Winiwarter, il se trouve que les deux physiciens travaillent également à cette période sur les diverses applications possibles de loi de Zipf/Pareto.

En 1983, et pendant environ un mois, c’est depuis le sous-sol du collège de France, connecté par un terminal au CIRCE, que Bertrand Roehner et Denise Pumain tentent d’appliquer à plusieurs reprises le programme MINUIT du CERN pour calibrer certains paramètres du modèle de Peter Allen, impossible à estimer autrement.

Ce programme du CERN développé au début des années 1970 par Fred James \autocite{James1972}, toujours usité aujourd’hui (réécrit en C++ 25 ans plus tard, et intégré depuis dans une autre boîte à outils nommée ROOT, toujours dans le cadre du CERN), permet de \textit{fitter} une fonction objectif à $n$ paramètres en utilisant notamment une descente de gradient capable d’évoluer dans un environnement multi-dimensionnel (module MIGRAD).

MINUIT met à disposition de l’utilisateur différentes stratégies (utilisable seule ou combinées) pour minimiser une fonction objectif (SIMPLEX, MIGRAD, MINOS), et différents outils pour cartographier l’erreur de façon interactive.  Il serait intéressant de comprendre pourquoi le programme n’a pas donné de résultats satisfaisants à l’époque. Ce ne sont que des hypothèses, mais : le fait que MINUIT ne procède dans ses variations que paramètre par paramètre, la très grande sensibilité du modèle, la présence de minima locaux, associés au fait que MINUIT ne permet de tester qu’une seule descente de gradient à la fois, tout cela est probablement une piste pour expliquer la difficulté à calibrer ce modèle en utilisant cette unique boîte à outils.

Dans cette famille d’algorithmes de minimisation usant de la technique de descente de gradient, les performances de MINUIT (avec MIGRAD) n’ont été dépassées que récemment par des méta-heuristiques. C’est le cas de l’algorithme de type \textit{Evolution Strategy} (ES) populationel CMA-ES, plus robuste aux minima locaux que MINUIT, car pouvant suivre différentes pistes de descente de gradient de façon concurrentes en une seule génération/itération \autocite{Berlich2003}.

Les résultats des premières analyses sont disponibles dans une publication \autocite{Pumain1983b} réalisée pour la conférence de 1983 aux Etats-Unis marquant la création \autocite{Andersen2007} de la \textit{System Dynamics Society}

Dans ce premier compte-rendu d'expériences auquel participe Roehner, il est explicitement pointé l’avantage d’une telle méthode automatique ( \textbf{la fonction objectif alors utilisée est une méthode de minimisation de moindres carrés non linéaires }) par rapport aux calibrages manuels, avec même la découverte de quelques avancées encourageantes : \foreignquote{english}{That is why, after more than a hundred of these tentative simulations, we decided to use an automatic technique for calibration [...] The adjustements that we obtained until now are not good enough to be considered \enquote{best fit} and to allow calculation of residuals. But we have a serie of interesting results with various configurations according to different values of parameters [...]} Dans les deux ouvrages suivants par contre, cette méthode automatique est finalement indiquée comme impossible, et tous les calibrages sur ce modèle seront faits en définitive par essai/erreur et sur un ensemble assez restreint de simulations.

Le calibrage du modèle sur Rouen indiqué dans la thèse de \autocites[348,354]{Sanders1984}{Sanders1985} fait état de $200$ simulations, pour $32$ paramètres à estimer, sur $17$ communes. D'autres articles seront publiés en 1986-1987 \autocites{Pumain1986, Pumain1987}, puis enfin dans un ouvrage collectif en 1989. Toutefois, la conclusion de \textcite[112]{Pumain1989} est similaire à celle de 1984, pour $300$ simulations et $32$ paramètres à estimer pour $102$ équations : \enquote{L’utilisation de méthodes de calibrages automatiques est rendue impossible par le nombre de paramètres et le peu d’informations sur l’ensemble des valeurs théoriquement possibles. Nous avons donc dû procéder tout simplement par tâtonnement, en opérant par plusieurs phases.}

Une autre tentative d'exploration du modèle est réalisée un peu plus tard dans les années 1992 \autocite{Milan1992}, probablement sans utiliser les moyens d'un centre de calcul. Pour l'application à la ville de Bordeaux, le polytechnicien Olivier Milan reproduit \autocite{Wilensky2007a} pour son stage le modèle de Peter Allen en utilisant le langage PASCAL (le listing du code source est cette fois-ci inclus dans le rapport), et propose de le calibrer avec un algorithme de descente de gradient minimisant une fonction \textbf{mono-objectif calculée en utilisant l'erreur quadratique entre valeurs réelles et simulées}. Une fois les paramètres estimables a priori enlevés, il reste selon lui 68 valeurs de paramètres à estimer, intervenant dans 138 équations consécutives.

Du fait du trop grand nombre de paramètres à ajuster, et de la sensibilité trop grande de certains paramètres (une variation de l'ordre de $\SI{1E-6}{}$ pouvant entrainant une bifurcation sur certains paramètres hypersensibles), cette tentative est également mise en échec. Comme il le dit lui même, \enquote{Il a donc fallu se résoudre à calibrer les paramètres \enquote{manuellement}, par tâtonnements successifs, au prix fastidieux de centaines d'essais, en ajustant à chaque fois certains d'entre eux.}  %On est très loin donc du modèle simple comportant 5 paramètres ayant déjà nécessité d'accumuler entre 2012 et 2014 des millions de simulations pour être calibré, ce qui donne un aperçu de la difficulté de cette tâche \autocite{Schmitt2015}.

On s'expose donc à une forme de paradoxe dès la prise en main de ces modèles chez les géographes, l'attrait pour cette double capacité des systèmes dynamiques non-linéaires à cumuler tout à la fois des comportements éloignés de l'équilibre, tout en restant très sensibles aux conditions initiales, aux valeurs de paramètres, et à certaines fluctuations aléatoires devient aussi ce qui empêche toute possibilité d'un calibrage manuel de ces modèles.

Il devient également possible d'obtenir des jeux de valeurs de paramètres exposant des comportements finaux similaires (attracteurs), qu'il faut découvrir pour pouvoir donner une mesure de leurs spécificités dans l'établissement d'un comportement attendu du modèle. Sinon comment être sûr que le modèle ne permet pas d'exposer n'importe quel comportement attendu en compensant certaines valeurs de paramètres par d'autres ? Est-on capable de donner les lois qui régissent ces compensations entre paramètres ?

Même lorsque ces valeurs de paramètres sont fixées \textit{a priori} par expertise, d'une part ils n'échappent pas à la remarque formulée dans le paragraphe précédent, et d'autre part ils ne représentent jamais qu'une des expertises possibles sur cette valeur. Il est donc tout aussi intéressant de les faire varier \textit{a minima} dans une fourchette définie comme acceptable, ne serait-ce que pour mieux connaitre et mesurer la robustesse du système à de telles variations.

Enfin, il faut encore intégrer l'impact que peut avoir cette incertitude contenue dans les données une fois reportées sur l'évolution d'un système, par exemple en injectant du bruit dans les données initiales. Ce dernier pouvant tout à la fois être robuste (jusqu'à un certain seuil de bifurcation), ou à l'inverse chaotique, ou les deux.

Enfin le calibrage du modèle, même fait de façon automatique en se basant sur un seul point de comparaison entre données réelles et données simulées, ne peut pas être une fin en soi, car il ne garantit pas la qualité même de la dynamique au cours de la simulation. D'autres critères empiriques peuvent donc être intégrés au processus de calibrage pour renforcer la crédibilité structurelle du modèle, justifiant ainsi de mécanismes intégrés pour leur impact (ou leur non-impact, si l'on cherche à mettre en difficulté une théorie) réel dans l'ensemble des dynamiques, un point déjà débattu au cours de la section \ref{ssec:evaluation_construction} \autocites{Hermann1967, Grimm2005,Cottineau2014b}.

Ne pas étudier cette structure, c'est se couper d'une partie de la connaissance que peut nous apporter le modèle, et prendre aussi le risque de donner de fausses interprétations. Déjà au fait des écrits anglo-saxons sur la calibration \autocite{Batty1976}, et entourés de mathématiciens, de physiciens et d'informaticiens compétents, toutes ces nouvelles problématiques sont rapidement intégrées par l'équipe de géographes de PARIS \autocites{Sanders1984, Sanders1985, Pumain1989}. C'est d'ailleurs fort probablement ce qui les pousse très rapidement vers une tentative d'application de solution de calibrage automatique.

Dans le cas du modèle de Peter Allen, cette mise à l'épreuve se fait sur le seul critère d'une recherche de correspondances entre données simulées et données réelles. Indiquée par les géographes comme une tentative de calibrage, elle masque déjà en fait l'utilisation d'une fonction de minimisation. Dès lors, ce qui est mis en jeu est une optimisation qui ne vise rien d'autre que la mise en défaut de cette capacité structurelle initiale du modèle. Arrivera-t-on à calibrer le modèle en l'état face à telles ou telles données ? Et si oui, que nous dirons au juste les valeurs de paramètres, ou plutôt l'écart entre les valeurs prises par ces paramètres ?

Car sans analyse de sensibilité préalable, l'existence même d'un jeu de valeur de paramètres permettant de minimiser cette fonction, même à la perfection, n'indique absolument rien sur la valeur explicative des hypothèses ainsi mises en jeu. D'une part, le remplacement d'une équation par une autre dans le modèle peut n'avoir aucun impact; voire pire si ce remplacement a un impact, celui-ci peut tout à fait être compensé par un autre mécanisme du modèle sans que l'on n'en sache quasiment rien.

C'est ce qui a pu se passer par exemple lorsque les membres de l'équipe PARIS ont voulu tester des équations de formes différentes dans le cadre du modèle de Peter Allen, avant finalement de revenir aux équations originales \autocites{Sanders1984}[147]{Pumain1989}.

Face aux données présentées, le modèle doit donc être au préalable épuré de ces dynamiques inutiles (cf. ce qui était arrivé au modèle de Forrester, dont on a parlé dans la section \ref{sssec:forrester_impact}), puis probablement reconstruit, en ne gardant que l'essentiel. Une piste là aussi introduite par \textcite{Sanders1984} dans sa conclusion de thèse.

Des techniques d'analyse de sensibilités manuelles ou automatiques comme \textit{One At a Time} (OAT) sont très vite limitées, ne serait ce que d'un point de vue conceptuel. En effet, à l'image des mécanismes, ce n'est pas parce qu'un paramètre a été choisi dans une équation pour dénoter ou exemplifier un phénomène du réel, qu'il se comporte forcément comme attendu une fois plongé dans un système dominé par de multiples interactions non linéaires. Il ne faudrait donc pas confondre ce que l'on attend d'un paramètre, son expression isolée dans une dynamique, et les possibilités d'expression plus étendues qu'il recouvre une fois replacé dans ce réseau complexe d'interactions. D'autre part, avec des telles sensibilités observées sur certains paramètres du modèle, et en tenant compte de cette capacité de compensation du modèle, il semble impossible de mettre en place un plan d'expérience qui puisse être à la fois \enquote{humainement faisable} et satisfaisant dans sa capacité de couverture des dynamiques possibles exprimables par le modèle.

N'ayant pas réussi à valider une technique de calibrage automatique du modèle, et devant l'exercice très coûteux que représente le calibrage manuel du même modèle sur des données différentes, l'exercice critique des géographes de PARIS n'a pas pu aller au-delà d'une conclusion générale basée sur très peu de simulations. La seconde étape qui aurait dû voir une épuration ou une modification structurelle du modèle n'a pas été entamée, même si des éléments ont été avancés dans la conclusion de \textcite{Sanders1984}. En ce sens, l'expérience décrite avec ces modèles correspond parfaitement à l'argument avancé par Openshaw justifiant d'un retard, ou d'un désintérêt face au HPC les années qui suivent : la puissance de calcul disponible, ou les techniques algorithmiques pour l'exploration utilisées (les techniques multi-objectif progressent par exemple surtout à partir des années 1990 ! voir section \ref{sssec:historique_EA}), n'étaient à l'époque probablement pas adaptées et/ou suffisantes pour mener à bien une telle expérimentation dans des délais raisonnables. Une expérience qui rend finalement peu attirante ces techniques pour l'époque, surtout vis-à-vis de l'investissement nécessaire pour être autonome sur ce type d'outils dans les années 1980-1990.

\subsubsection{De nouvelles perspectives}
\label{sssec:synthese}

% Recherche d'une solution adapté
Aujourd'hui, les ressources de calculs HPC à notre disposition sont des dizaines de milliers de fois plus importantes qu'il y a 30 ans. Pour donner un ordre d'idée sur l'évolution des moyens de calcul disponibles au CNRS de 1990 à 2015 on peut regarder l'évolution des supercalculateurs de l'IDRIS (ancien CIRCE refondé en 1993) dans le tableau \ref{tab:prankingIDRIS} : entre les deux calculateurs vectoriels Cray disponibles en 1993/94, le premier bond réalisé en 1996 par le passage à un autre type d'architecture parallèle, et la puissance aujourd'hui disponible sur les calculateurs ADA et TURING, on est passé du GFlops au PFlops (1 PFlops = $\num{1000000}$ GFlops).

La présence de cette puissance informatique à disposition de tous les scientifiques (ou presque) au CNRS devrait à elle seule être un motif suffisant pour amener les géographes à imaginer de nouvelles applications. Car il y a fort à parier que les défis computationels d'hier puissent aujourd'hui être abordés de façon beaucoup plus sereine, et pas seulement sur les seuls aspects matériels, car la recherche informatique a aussi progressé sur le plan théorique, algorithmique.

\begin{table}[!htbp]
\begin{sidecaption}[Classement des ordinateurs de l'IDRIS depuis 1993]{Classement basé sur la brochure pour les 20 ans du centre IDRIS}
	[tab:prankingIDRIS]
	\centering
	\resizebox{1\textwidth}{!}{%
	\begin{tabular}{@{}llllll@{}}
\toprule
\multicolumn{6}{c}{A l'IDRIS (1993 - maintenant)} \\ \midrule
Durée & Identifiant machine & Nom & Type & Processeurs & Puissance \\ \midrule
1993-1999 & Vectoriel Cray C98 & Atlas & vectoriel & 8 & 7,6 GFlops \\
1994-1999 & Vectoriel Cray C94 & Axis & vectoriel & 4 & 3,8 GFlops \\
1995-1997 & Cray T3D & Kaos & processeurs & 128 & 19,2 GFlops \\
1996-2002 & Cray T3E & Aleph & processeurs & 256 & 153,6 GFlops \\
1999-2006 & NEC & Uqbar & vectoriel & 38 & 304 GFlops \\
2002-2008 & IBM power 4 & Zahir & processeurs & 1024 & 6,5 TFlops \\
2008-2012 & IBM Blue Gene & Babel & processeurs & 40960 & 139 TFlops \\
2013 & IBM x3750 & Ada & coeurs & 10624 & 230 TFlops \\
2013 & IBM Blue Gene & Turing & coeurs & 98304 & 1258 TFlops \\ \bottomrule
\end{tabular}%
		 }
\end{sidecaption}
\end{table}

% DEFI pas lié aux agents
Ce discours sur le HPC, on a vu qu'Openshaw et ses collègues de l'école de Leeds, ont fait plus que l'appliquer, ils en ont fait un manifeste pour la \textit{GeoComputation} \autocite{Openshaw2000b} dès la fin des années 1990. Pourtant là encore on a vu que ce message et cette tentative pour démocratiser le HPC n'a pas dépassé, au moins pour la simulation, le cercle assez restreint de géographes ou d'équipes de géographes disposant de doubles compétences informatiques adaptées pour saisir et acter ce message dans des projets. On pourra citer à titre d'exemple la lente progression du nombre de références\Anote{ca_calibrage} qui semblent en géographie se référer à des méthodes susceptibles d'utiliser le HPC pour calibrer automatiquement des modèles de simulation \autocites{Diplock1996, Ngo2012, Straatman2004, Heppenstall2007, Schmitt2015, Wu1998, Wu2002, Li2001, Ahmadi2009}. Le nombre de modèles de simulation publié a par contre sensiblement augmenté et suit la courbe de démocratisation de l'outil. Il n'y a rien d'étonnant donc à voir buter les évaluations de modèles de simulation sur les mêmes problématiques techniques qu'il y a 15 ans, car l'évolution de la micro-informatique, même fulgurante, ne permet toujours pas d'aborder l'exploration des modèles efficacement.

%Toutefois, même s'il y a bien eu des efforts conséquents dans l'autonomisation de certains géographes par rapport aux outils de modélisation disponibles dans les années 1990, force est de constater que la thématique HPC (algorithmes, logiciels, méthodologie, cas d'utilisation) se fait assez rare dans les publications de la communauté agent et en géographie. La mise à disposition du HPC pour l'exploration des modèles de simulation, même si elle a été tentée à plusieurs reprises par certains modélisateurs géographes français ou anglo-saxons \autocite{Heppenstall2007} \hl{ajout ref}, n'a jusqu'à présent pas débouché sur une systématisation des pratiques, et semble encore être une pratique qui tient du domaine du cas particulier.

Cette immersion plus descriptive dans l'historique des pratiques a permis de montrer qu'il y avait eu plusieurs usages de ces ressources, et a souligné l'importance de ces centres comme lieu d'innovation et d'interdisciplinarité. Ces usages, ces pratiques, mais également cet esprit d'ouverture, dont on trouve traçe encore dans certains laboratoires (comme au LIVE de Strasbourg, ThéMA et la MSHE à Besançon), méritent probablement d'être mis plus en avant dans notre communauté s'ils veulent être diffusés et/ou adoptés dans le futur. Une remarque qui pourrait passer pour fantaisiste si l'on ne savait pas par ailleurs la capacité, depuis les années 1970, que possède cette branche quantitative de la discipline géographique à tirer le meilleur partie des outils qu'on lui soumet. Le HPC serait enclin à devenir dans le futur un outil parmi les autres, dont on a bien souvent oublié la difficulté de mise en oeuvre initiale (AFC, SIG, simulation, etc.).

Au vu du relatif échec d'Openshaw dans le domaine de la programmation HPC, et compte tenu du faible niveau d'engagement dans l'informatique des géographes, la solution de cette démocratisation des usages se trouve probablement dans l'invention d'un logiciel plus adapté au niveau et aux usages réels des modélisateurs. Car, contrairement à ce qui se passe dans l'enseignement pour l'acquisition des principes fondamentaux en informatique, il semble que du côté des chercheurs en SHS, y compris en géographie, une transformation s'opère lentement depuis les années 1990, notamment grâce aux efforts de toute une génération de chercheurs ayant accumulé les outils et réflexions des premiers modélisateurs géographes \autocite{Cuyala2014}, qui guide à présent une nouvelle génération de chercheurs en géographie vers la mise en oeuvre d'une pratique de la simulation plus \enquote{libérée} \autocite{Banos2013}

L’argumentation sur la \textit{GeoComputation} d’Openshaw fait état d’une double dynamique, prise d'autonomie et ouverture disciplinaire, que l’on pourrait en premier lieu interpréter comme contradictoire. Mais cette autonomisation des géographes vise non pas à se défaire de toute dépendance, une illusion d’autant plus dangereuse qu’elle pousse dans sa forme extrême à l’isolement, et engage au contraire les individus issus de multiples disciplines dans une activité de co-construction bien plus fertile. Cet argument est également défendu par \autocite[64]{Banos2013}, qui différencie sur ce point l’autonomie de l’indépendance, car dans ce processus les géographes \enquote{élargissent leur domaine de compétence avec un enthousiasme et une énergie renouvellés, et peuvent interagir de manière bien plus efficace et pertinente avec les spécialistes de ces domaines. Ces derniers, en retour, y gagnent des collègues plus exigeants, capables de poser les problèmes à un niveau plus avancé et de contribuer réellement à la construction des méthodes, des modèles, et même des outils nécessaires à leur résolution.}

Les modélisateurs déjà initiés à la programmation sont donc tout logiquement à la fois plus réceptifs à une évolution concernant leurs outils de recherche, et moins frileux vis-à-vis des efforts à fournir pour une mise en oeuvre du HPC.  Il reste donc à leur offrir le support adéquat pour que ce pas puisse être plus facilement franchit.

%Il s'agit donc d'aller au-delà des avantages déjà assimilé par les modélisateurs, et profiter de cet intérét pour la computation pour proposer un chemin viable dans l'utilisation du HPC .

%Un signe positif laisse en effet penser que cette autonomisation est un premier pas vers l'utilisation de HPC post 1990, condition préalable pour tenir tous les engagements de la GéoComputation?

Si l'apparition de plateformes et de langages de modélisation pensés comme un cadre d'expérimentation adapté pour les non programmeurs (le fameux \foreignquote{english}{learn through tinkering} de \textcite{Resnick2013}) a permis une autonomisation des géographes modélisateurs, pourquoi ne pas utiliser une stratégie similaire, masquer cette complexité d'accès aux usages des HPC dans un langage ou des outils plus simples. La réduction des possibilités étant  compensée par un accès à un niveau de parallélisme adapté aux principaux usages des modélisateurs. Est-il possible de transférer ces principes au niveau de la construction de l'expérimentation et non plus seulement de la construction du modèle : \textit{feedback} immédiat, expérimentation fluide, exploration ouverte ? Peut-on imaginer une activité de construction des modèles de simulation sous Netlogo ou est-il possible de lancer une analyse de sensibilité (ou un autre algorithme d'exploration) sur des centaines de paramètres entre chaque modification du modèle ? La construction et l'usage d'une expérimentation du modèle ne mérite-t-elle pas la même fluidité d'usage que la construction ou l'exécution des modèles, activités auxquelles ils sont pourtant intimement liés?

Pour le moment, la plupart des guides méthodologiques et des outils, même récents, pour la construction et l'évaluation de modèle de simulation Netlogo, n'hésitent souvent pas à passer sous silence un certain nombre d'aspects problématiques liés à cette auto-censure dans l'utilisation de ressources informatiques plus appropriées \autocites{Gilbert2008, Grimm2011a}. Et cela, même si certains ont avancé des pistes intéressantes, finalement assez similaires à ce que nous essaierons de proposer comme solution (\textit{Behavior Search} de \textcite{Stonedahl2011a}). %Nous aurons l'occasion de revenir plus longuemment sur ces outils et méthodes dans une lecture plus critique en conclusion de cette thèse. L'évaluation des modèles de simulation a parfois tendance à être pointé comme plus spécifique aux modèles agents. Or non seulement on a vu que ce n'était pas le cas historiquement parlant dans le chapitre 1 (les problèmes de validation apparaissent quasiment en même temps que l'utilisation des modèles de simulation), mais en plus ce n'est pas non plus à rattacher à une quelconque technologie en particulier. Les expériences plus spécifiques des géographes avec ces premiers modèles de simulation non-linéaires semblent en effet être un flagrant contre-exemple de ce dernier point de vue.

Si l'on ne tente pas aujourd'hui l'aventure du HPC alors nos pratiques seront-elles éternellement en deçà des défis scientifiques, contraintes d'attendre les évolutions de la micro-informatique ? La loi de Moore continuera-t-elle à se vérifier encore longtemps ? (\href{https://www-03.ibm.com/press/us/en/pressrelease/47301.wss}{@IBM} vient de passer le cap en juillet 2015 des 7nm, Intel n'est pas très loin à 10nm), rien n'est moins sûr vu les problématiques physiques qui attendent les constructeurs au niveau atomique. En refusant d'aller ainsi vers le HPC, les géographes devront-ils attendre les ordinateurs quantiques, une technologie réputée plus efficace pour résoudre des problèmes à fortes combinatoires ?

%

Dans le cadre du laboratoire Géographie-cités, la découverte en 1990 de tous nouveaux paradigmes technologiques comme la programmation orientée objet, ou les langages acteurs - supports principaux du méta formalisme Agents - s'est faite en grande partie par la mise en oeuvre de collaborations. Les enseignements qui en découlent ont permis, de façon conjointe avec la démocratisation des outils de modélisation, de faire évoluer progressivement ce schéma jusqu'alors assez rigide de collaboration avec les informaticiens. La rencontre d'ingénieurs spécialistes du HPC occupant ce que l'on aurait pu appeler il y a 20 ans \enquote{un centre de calcul} a permis de constituer une équipe interdisciplinaire qualifiée non seulement pour raccrocher les tous derniers développements informatiques, mais également capable de proposer ces outils dans un référentiel technique susceptible d'intéresser une partie des géographes modélisateurs. Toutes les conditions étaient donc réunies pour expérimenter ce retour au HPC, en s'attaquant ensemble à ce défi de l'exploration plus poussé et plus systématique des modèles de simulation réalisés à Geographie-cités. Un projet qui dépasse le cadre de cette thèse, avec la remise en route d'exploration de modèles de simulation, pas forcément Agents, pas forcément nouveaux. Des expériences restées en sommeil jusque-là, faute de compétences et de moyens informatiques suffisants.

\subsection{L'évolution des pratiques de modélisation à Géographie-cités, histoire vécue d'un retour vers le HPC}
\label{ssec:hist_pratiques}

% Contingence avec Centre de calcul, expérience cumulé Arnaud, Denise, Clara, moi, Thomas, et un prototype en cours de construction.

Il n'est pas question dans cette section de faire un nouvel historique des projets de simulation Agents à Géographie-cités, car c'est un sujet déjà très bien traité au travers de multiples thèses ces dernières années. Sur les aspects historiques et l'apport géographique des modèles de simulation réalisés au laboratoire Géographie-cités, le lecteur pourra donc se référer aux thèses récentes de \autocites{Glisse2007, Louail2010, Schmitt2014, Cottineau2014b}

Je préfère insister ici sur les aspects saillants d'une aventure scientifique résolument ancrée dans l'inter-disciplinarité, mais également dans ce que l'on appelle parfois la \foreignquote{english}{slow science}. En effet, il nous aura  fallu presque 4 années de travail, et de multiples campagnes d'expérimentations, pour produire une première publication \autocite{Schmitt2015} présentant un prototype de méthodologie s'articulant autour d'un modèle (SimpopLocal), et d'une plateforme HPC (OpenMOLE).

Un embryon dont la plus grande force était déjà je crois de montrer la faisabilité d`un tel projet : \enquote{ Oui, regardez, il est possible d'utiliser une approche HPC pour évaluer vos simulations !}

%J'ai voulu faire cet historique car en définitive je n'ai jamais vraiment cru aux hasards de telles rencontres. Les belles histoires comme çà, c'est louche. On s'est quelque fois demandé si nous n'avions pas été manipulé, ou du moins fortement dirigé, par des directeurs de thèse - c'est bien connu - omnipotent.

\subsubsection{Premier moment, avant 2010}
\label{sssec:premier_moment}

Les termes vont peut-être paraitre un peu forts, mais il me semble qu'à l'époque il y avait de notre part une certaine frustration vis-à-vis des pratiques existantes. C'est sur cette base je crois qu'une première synergie opère autour des efforts de Thomas Louail pour dépasser cet état de fait. C'est du moins sur cette thématique là que mon stage de master 2 s'est construit en 2009, sous la direction de Thomas Louail et Denise Pumain \autocite{Rey2009}.

A l'époque les entrées/sorties du modèle de simulation SIMPOP2 ne sont pas satisfaisantes (documentation des entrées, documentation des sorties, format d'import des paramètres et d'export des résultats), et il est très difficile pour les géographes de faire varier les paramètres du modèle selon un plan d'expérience établi au préalable. Certes, ces expériences ne sont pas impossibles, Hélène Mathian ayant déjà mené plusieurs campagnes d'analyses sur les sorties des modèles SIMPOP, en partie grâce à des scripts SAS permettant de générer des rapports synthétiques. C'est d'ailleurs grâce à l'expertise, et les outils développés par Hélène Mathian que Clara Schmitt (également encadrée pour son stage par Denise Pumain, Anne Bretagnolle, et Thomas Louail) peut réaliser en 2008 des expérimentations et des analyses de sensibilités sur ce modèle, dans le cadre de son stage de fin d'année d'école d'ingénieur \autocite{Schmitt2008}.

L'expérience reste fastidieuse pour les utilisateurs, car les chaînes de manipulations nécessaires pour l'exécution des simulations et le traitement des données comportent de nombreuses étapes manuelles, nécessitant l'intervention d'experts, limitant de fait la tenue d'expérimentations à plus grande échelle, et menaçant la pérénnité du projet sur le long terme. L'objectif du stage de 2009 est clair, il faut standardiser les entrées/sorties d'une part pour que les exécutions de simulations puissent suivre des plans d'expérience formulés en amont par les géographes, d'autre part pour que les analyses en sortie sur les résultats puissent elles aussi être automatisées, stockées, et exploitées si possible collectivement. Il s'agit également de rendre cette expertise visible, voire transparente, et modifiable, en faisant en sorte que les différentes analyses en sortie de simulations soient extraites de leur position actuellement inacessible dans une chaine de traitement complexe, maitrisée/maitrisable par seulement quelques personnes. Ce projet est extrait des problématiques de fond abordées plus en détails dans la thèse de Thomas Louail \autocite[132-145]{Louail2010}.

\blockquote[{\cite[132]{Louail2010}}]{Les deux campagnes de simulation sur l’Europe et les Etats-Unis ont mis en lumières les difficultés de la pratique expérimentale sur des modèles ambitieux mais complexes. Ces difficultés entraînent un besoin de normalisation et de systématisation dans l’expérimentation \textit{in silico}. Nos efforts ne doivent donc pas porter sur le seul cas particulier de Simpop2, mais s’inscrire dans un projet de plus grande envergure et de plus long terme, qui vise à offrir aux géographes un ensemble d’outils interopérables, dont l’action commune pousse plus loin l’automatisation du processus d’expérimentation, et qui surtout soient neutres par rapport aux modèles qu’ils permettent d’exploiter. Simpop2 et son application aux Etats-Unis nous serviront de banc d'essai, mais notre objectif est bien sûr que ces outils puissent également servir à exploiter les futurs modèles développés au laboratoire.}

Effectivement, comme l'indique Thomas Louail, il s'agit d'une réflexion dont on ne voit pas pourquoi celle-ci ne s'étendrait pas à tous les modèles de simulation développés dans le futur. Toutes les idées d'une plateforme pour l'exécution et l'exploration de modèles de simulation sont déjà là, ou presque...

Thomas Louail est à l'époque un doctorant informaticien en immersion dans le laboratoire de Géographie-cités. Rattaché à une équipe spécialisée dans le développement de modèles multi-agents du laboratoire d'informatique IBISC - LIS, Thomas Louail entrevoit également de nouvelles possibilités pour calibrer des modèles, du fait de sa formation en intelligence artificielle, mais aussi des contacts avec les travaux de deux de ses collègues informaticiens, Benoit Calvez et Guillaume Hutzler. Ces derniers travaillent à l'époque sur le calibrage de modèles agents en utilisant des algorithmes génétiques \autocites{Calvez2006,Calvez2007}. Egalement passionné par cette problématique, \textcite[139-141]{Louail2010} dessine très clairement dans sa thèse ce que l'application de telles techniques d'optimisation peut avoir de profitable pour la simulation en géographie :

\blockquote[{\cite[139-141]{Louail2010}}]{L’idée est d’aller plus loin que le simple enchaînement de simulations selon un plan d’expérience planifié \textit{a priori}, en utilisant des techniques d’optimisation pour orienter le choix du prochain jeu de valeurs de paramètres à tester de façon à trouver le jeu de valeurs des paramètres qui optimise la valeur obtenue pour une fonction objectif. Un prérequis à l’utilisation de techniques d’optimisation est de scorer les simulations, de façon à traduire quantitativement la qualité des configurations spatiales générées.}

\blockquote[{\cite[139-141]{Louail2010}}]{Nous pouvons donc regarder le calibrage comme un problème d’optimisation. [...] De plus, un autre argument en leur faveur est que la construction d’algorithmes d’optimisation de type génétiques, spécifiques à des modèles à base d’agents, a été l’objet d’une thèse récente effectuée dans notre équipe (LIS), avec des applications à des modèles simples de colonies de fourmis sous Netlogo.}

Ayant moi-même été initié et passionné par ce type de technologie en Master 1 d'informatique, cette possibilité d'une application de méthodes d'intelligence artificielle au coeur d'une plateforme pour l'évaluation de modèles de simulation agents intègre rapidement les perspectives d'une fin de stage, prolongées en définitive dans un projet de thèse déposé à la fin de l'année 2009. A cette même époque Clara Schmitt commence également sa thèse sous la direction de Denise Pumain, avec le projet de construction de trois nouveaux modèles de simulations agents.

Avec pour point commun cette expérience d'une exploration semi-manuelle et assez fastidieuse des précédents modèles SIMPOP, il est impossible pour Thomas Louail et Clara Schmitt d'envisager la construction de nouveaux modèles de simulation sans un vrai support d'accompagnement des explorations.

\enquote{Le nouveau travail de modélisation entrepris pour cette thèse s’inscrit dans la continuité des travaux de recherche présentés dans ce chapitre. Il adopte l’approche multi-agents et plus particulièrement le concept de modélisation géographique développé pour les modèles de la famille Simpop qui semble particulièrement adapté pour étudier et simuler la dynamique des systèmes urbains. Mais pour ne pas tomber dans les mêmes écueils d’évaluation, de difficulté technique devant l’exploration massive, d’impossibilité d’appropriation des modèles Simpop existants, etc., une modification de l’approche de modélisation est nécessaire. Nous verrons dans la partie suivante que ces limitations classiques aux modèles multi-agents peuvent être surmontées grâce à la définition de principes de modélisation et à l’utilisation systématique d’outils et méthodes d’exploration adaptés.} \autocite[113]{Schmitt2014}

Si la littérature proposée de façon interne au laboratoire \autocite{Mathian2014}, ou de façon externe dans la littérature agent \autocites{Amblard2006, Sargent2010, Gilbert2008}, apporte certes des méthodologies, des conseils pour la construction et l'exploration des modèles, elle est par contre quasiment vierge de tout outil, plateforme, logiciels informatiques supportant ou facilitant cette \enquote{validation}, ou plutôt cette évaluation des modèles \autocite{Amblard2006}. A l'époque il y avait les travaux de thèse et le prototype SimExplorer réalisé par \textcite{Amblard2003}. Ce projet va recroiser notre chemin début 2010, mais à cette époque là, nous n'avions pas connaissance de ce prototype.

Pour résumer, fin 2009 nous étions il me semble dans une situation d'insatisfaction :
\begin{itemize}[noitemsep,nolistsep]
\item  vis-à-vis des modèles et des explorations existants au laboratoire, difficilement reproductibles (manque de documentation technique, chaîne de traitement des entrées,sorties manuelles, etc.)

\item vis-à-vis du processus même de construction des modèles, les plateformes disponibles ne permettent que très peu de réutilisabilité des comportements déjà implémentés. La construction des modèles ressemble parfois à une culture sur brulis, où il faut déconstruire pour reconstruire en permanence.  C'est particulièrement vrai sous Netlogo. Ce qui est un atout pour le prototypage et ne pose pas problème sur des modèles très simple, devient extrémement pénible dès que les modèles se complexifient un tant soit peu. Le risque de devoir abandonner des modèles passé un certain seuil de complexification reste on suppose très fort chez des modélisateurs débutants.

\item vis-à-vis des différentes campagnes d'explorations déjà réalisées, dont on sait en lisant cette littérature sur la validation qu'elles sont insuffisantes pour assurer le bien fondé de la structure interne des modèles, fragilisant de fait les interprétations avancées par les géographes

\item vis-à-vis également de la littérature sur la validation, les préceptes étant répétés \textit{ad nauseam}, sans que des outils ou des méthodologies soient proposés pour application sur des cas concrets guidant la construction et l'évaluation des modèles

\item vis-à-vis de l'expérience même de construction, il n'était plus question de faire développer un modèle de simulation sous contrat externe avec un informaticien, comme cela avait pu être le cas avec la plupart des modèles de la famille Simpop. %Ce qui suppose aussi un format de développement des modèles adaptés.

\end{itemize}

Les projets ainsi posés sur le papier fin 2009 ne demandaient qu'à être réalisés, mais c'était sans la mise en route d'une deuxième synergie, sous l'impulsion d'Arnaud Banos, co-directeur de la thèse de Clara Schmitt. A la différence des projets de modèles de simulation déjà réalisés au laboratoire, le développement de ceux-ci ne seront pas portés dans leur réalisation par un informaticien seul, mais par une équipe incluant des informaticiens.

\subsubsection{Deuxième moment, mise en route d'un prototype}
\label{sssec:deuxieme_moment}

Comme évoqué dans la section \ref{sssec:contexte_modelisateur}, l'équipe PARIS n'en est pas au tournant des années 1990 à sa première collaboration interdisciplinaire. Le développement du modèle multi-agents Simpop \autocites{GuerinPace1993, Bura1995} marque tout de même un nouveau développement dans cette histoire, pour plusieurs raisons. Il s'agit cette fois-ci d'un projet de création et non de modification de modèle de simulation existant, qui plus est celui-ci s'appuie sur le tout nouveau paradigme Objet, très éloigné des formations initiales en informatique des géographes de l'équipe PARIS. A contrario d'autres équipes, celle-ci ne bénéficie pas des mêmes opportunités dans sa transformation, à la différence par exemple du futur MTG de Rouen, qui a cette possibilité d'absorber\Anote{note_integration_rouen} une équipe où préexistent de mathématiciens/informaticiens proches de la géographie, comme Patrice Langlois, Philippe Le Carpentier, etc.

Le mode de collaboration choisi par l'équipe PARIS est celui que l'on trouve le plus classiquement chez les informaticiens. Les géographes passent \enquote{commande} d'un modèle de simulation, fournissent les informations initiales nécessaires à une primo réalisation, et l'avancement du modèle se construit ensuite autour d'une série de réunions où les explorations sont commentées, et les mécanismes amendés, au moins sur le papier. \textcite[10]{Louail2010} parle d'une collaboration \enquote{faiblement couplée}. Les acteurs restent dans leur rôle respectif, et cela même si une forme de transfert, de nature plus ontologique que technique\Anote{pumain_slocal} s'effectue. Car les géographes initiateurs du projet restent avant tout curieux de savoir ce qu'il est possible de faire, de dire sur les objets géographiques traités sous l'angle de ce nouveau formalisme. A l'époque personne ne peut prédire quel futur attend les modèles multi-agents dans les Sciences Humaines et Sociales\Anote{pumain_simpop}.

Dans le projet suivant, il s'agit toujours de réaliser des modèles en géographie par des doctorants en informatique. Seulement cette fois-ci la collaboration met plus l'accent sur cette \enquote{immersion} de l'informaticien chez les géographes. Le transfert de compétences n'est pas complet, et il s'agit toujours d'une construction pilotée par des géographes qui ne programment pas. Celle-ci se fait toutefois de façon plus contrôlée, plus resserrée, et l'immersion permet une acculturation progressive entre les deux parties \autocite[11]{Louail2010}. Le travail de Benoit Glisse sur \textbf{Simpop2} et surtout de Thomas Louail, sur \textbf{Simpop2}, \textbf{Eurosim} et \textbf{SimpopNano} a démontré, au moins pour les géographes, tous les bénéfices d'une immersion sur le long terme.

Comme déjà dit précédemment, les modèles développés à cette période ne sont pas encore manipulables, ou modifiables en dehors des mains expertes de leurs créateurs.

\blockquote[{\cite[112]{Schmitt2014}}]{Les modèles Simpop et SIMPOP2 ont été développés grâce à la coopération entre une équipe de géographes théoriciens et une équipe (voire une seule personne) en charge du développement informatique du modèle. Lors de ces projets de longue haleine, les allers et retours entre les deux équipes se sont multipliés avec les va-et-vient entre la théorie modélisée et le développement informatique. Les codes sources des modèles se sont allongés et complexifiés et les personnes en charge du développement sont devenues des personnes ressources, référents du code implémenté. Cette dépendance a été à l’origine de l’abandon du modèle informatique SIMPOP2 lorsque la personne porteuse du code s’est désengagée du projet Simpop. Le code du modèle SIMPOP2 est peu accessible, peu commenté et donc difficilement utilisable par une autre personne que celle qui l’a développé. Les quelques tentatives ont conclu à l’impossibilité du ré-emploi du code dans sa version originale.}

De plus, comme le rappelle Denise Pumain, se pose toujours le problème du cloisonnement disciplinaire et du devenir des doctorants informaticiens, peu enclins à rester dans la géographie, ce qui n'est pas sans rappeller cette problématique institutionelle dont dépend aussi l'inter-disciplinarité \autocite{Chapron2014}. Au constat de la nécessaire stabilité des équipes, il faut donc remarquer que la durée des thèses n'est en soi pas suffisante pour développer l'exploration des modèles ainsi construits, cet état de fait n'ayant rien de nouveau \ref{p:echec_critiques}. Au final, \enquote{ [...] l’apport de connaissances disciplinaires nouvelles de part et d’autre laisse une insatisfaction, pour l’informaticien qui regrette d’avoir eu peu d’occasions d’avancées novatrices dans sa discipline au cours de l’exercice, et pour les géographes qui retirent des expériences de simulation des intuitions mais pas de certitudes bien assurées, étant donné le tout petit nombre des expériences et l’absence d’un contrôle suffisant sur la dynamique du modèle.} \autocite{Pumain2014}

Le troisième mode de fonctionnement rompt complètement avec les deux dernières approches, l'alternance entre phase de conception et de réalisation n'est plus l'objet d'un partage des tâches entre géographes et informaticiens. Les modélisateurs deviennent plus autonomes, mais ils réintègrent aussi au passage une responsabilité jusque là déléguée aux informaticiens constructeurs des modèles. Il n'est plus question de reporter la faute d'une mauvaise implémentation à quelqu'un d'autre, le géographe dans son autonomie se retrouve donc en réalité bien démuni dès lors qu'il faut aborder frontalement l'horizon fini de ses compétences. Ce sont les deux premiers principes abordés par \textcite[77]{Banos2013} : Principe 1 \enquote{modéliser c'est apprendre}, principe  2 \enquote{le modélisateur n'est pas omni-compétent}. Cette autonomie se double donc d'une mise en danger disciplinaire, qui s'elle ne trouvait pas écho dans les compétences des autres disciplines pourrait sûrement conduire à la catastrophe. Ainsi contrairement à ce que la lecture de certains guides méthodologiques pourrait laisser penser aux géographes débutant la modélisation, modéliser seul reste un pari risqué car peu de gens peuvent se targuer d'être à la fois des spécialistes en géographie, en architecture logicielle, en métaheuristique, en mathématiques, en statistiques, en plan d'expérience, en calculs distribués, en réseau, en bases de données, en visualisation, en valorisation, etc. Autant de compétences pourtant nécessaires si on veut aujourd'hui espérer mener un projet de modélisation dans son intégralité, de façon honnête, seul, et dans un temps raisonnable.

%les géographes s'approprie directement l'outil de modélisation, et développe des compétences en informatique qui leur permettent de nouer un dialogue constructif avec les informaticiens.

Ce mode de construction des modèles se satisfait alors assez logiquement d'un développement collectif, où l'interdisciplinarité a toute sa place pour combler certains vides, d'autant plus lorsque tous les participants sont potentiellement des intervenants qui connaissent à la fois les tenants et les aboutissants du modèle. C'est un peu le jeu auquel se prêtent les modélisateurs lorsqu'ils participent à des événements comme les organise le réseau MAPS, où lors de semaines assez intensives, de petits groupes interdisciplinaires se forment pour construire ou reproduire des modèle de simulation, en étant programmeurs tour à tour, ou tous en même temps. Des réalisations faites sur un pied d'égalité, du fait de cette contrainte volontaire en mots (primitives du langage) pour créer et communiquer, une limite largement revendiquée par les créateurs de la plateforme Netlogo.

Cet objet de recherche s'insère dans un cadre de réflexion commun, où les concepts sont au final tout autant malmenés que dans les phases de conception habituelles de l'inter-disciplinarité, à cette différence près qu'ici ils peuvent se discuter au travers du prisme plus proche et plus concret du modèle tel qu'il est, et non tel qu'il devrait être. Ainsi, de la même façon que les mathématiques ne sont pas le langage universel des modèles (le principe 9 de \textcite[82]{Banos2013}), UML n'est pas non plus le langage formel le plus adapté pour comprendre ou être compris dans le cadre d'une modélisation interdisciplinaire. Quelques schémas sur le coin d'une nappe entre l'entrée et le plat de résistance, implémentés par la suite en une journée ou deux dans Netlogo, peuvent suffire à établir un canal de communication plus efficace qu'une litanie de classes UML, incompréhensible pour qui ne connait pas toutes les subtilités de ce langage. Imposer UML dans les discussions relève ainsi plus d'une forme d'instrumentation (involontaire ou volontaire?) de la part des informaticiens que d'un choix de langage raisonnable pour la description de phénomènes spatio-temporels. Ce langage dont l'importance n'est pas niée, peut venir dans un second temps et/ou pour transporter un autre message, pour lequel il est plus adapté, lorsque le modèle est amené à etre publié et reproduit par exemple.

Ainsi, comme déjà défendu par \textcite{Chapron2014}, \enquote{il nous semble que, de la même façon que l’analogie figurative, sans être un moyen fiable et scientifique d’exposer une théorie, peut servir de base à une analogie plus profonde et structurelle, qui conduise à l’identité (Paty, 2008), le dessin schématique, s’il est appuyé par un discours communément écrit, est un palliatif rapide et pratique à une formalisation plus poussée, et ceci tout particulièrement dans un contexte où les implémentations, les hypothèses et la forme des mécanismes évoluent rapidement \autocite[16-17]{Machamer2000}}. Ainsi malgré tous les défauts que l'on peut trouver à un tel support informel, les biologistes en ont pourtant fait un vecteur support important de leurs discussions, ou du moins \textit{a minima} de la diffusion des idées vers le grand public, il suffira d'acheter un magazine de vulgarisation scientifique pour s'en convaincre.

Le dialogue se construit, se perfectionne, autour d'un objet commun, mais avec des problématiques qui elles ne sont pas nécessairement identiques. Arnaud Banos parle d'un processus de co-construction, \enquote{gagnant-gagnant} pour les différents partis qui acceptent les règles du jeu de cette mise en danger disciplinaire. L'exploration des modèles ne peux plus être conçue comme une étape secondaire venant au delà d'une longue conception et d'une longue réalisation, car il faut bien alimenter de matière des discussions scientifiques où chacun se nourrit d'un désir d'action qui se rapporte aussi aux objectifs de recherche de sa propre discipline.

\blockquote[\cite{Pumain2014}]{C’est seulement lorsque sont créées des conditions de collaboration effectives entre des chercheurs des deux domaines partageant un même projet que l’interdisciplinarité produit des effets dépassant l’instrumentation réciproque d’une discipline par l’autre.}

L'informaticien est donc toujours utile, mais dans un objet commun comme le modèle, il met sa compétence informatique non pas \enquote{en retrait}, mais \enquote{au service} des problématiques abordées dans le modèle, mais également autour du modèle. Un objectif qui peut très bien intégrer des problématiques de recherche informatique, comme on va le voir.

%Le laboratoire s'équipe d'un serveur en 2004 pour exécuter les modèles.

%replacé la partie informatique sur le courant des modélisateurs autonome ici ?

C'est sur ce troisième mode qu'un groupe de travail émerge au laboratoire dès janvier 2010, avec pour principale feuille de route la construction et l'exploration des modèles prévus dans le cadre de la thèse de \textcite{Schmitt2014} :\textbf{SimpopLocal}, \textbf{SimpopNet}, \textbf{SimpopClim}. SimpopLocal, le premier modèle sur la liste, est au centre d'une production commune d'un groupe de travail, et il ne s'agit ni d'une commande, ni d'un travail encadré par une seule thèse, et/ou par un seul informaticien.

Composées principalement d'Arnaud Banos, Denise Pumain, Clara Schmitt et moi-même, les réunions de travail s'organisent très vite autour d'un premier prototype de \textbf{SimpopLocal} en Netlogo. A ce moment là, et avant même le démarrage du projet, il me semble que tout le monde est convaincu de l'intérêt de Netlogo comme substrat propre à cultiver nos idées : Clara n'est pas programmeuse à l'origine et doit se former rapidement à cette plateforme de modélisation; Arnaud montre la voie par son expérience dans l'utilisation, la formation et la diffusion de ce logiciel Netlogo qu'il considère comme un outil d'autonomisation et de médiation bénéfique pour toute co-construction interdisciplinaire \autocite{Banos2013}, et moi-même je suis plutôt partant pour une telle équipée, peu importe alors l'exotisme de la destination. En effet, ayant eu à modifier avec l'aide de Thomas Louail des parties de code sources \enquote{cryptiques} et non documentés du modèle Simpop 2 ( Java / Objective-C )  par un informaticien avant tout guidé par l'optimisation, Netlogo me parait à ce moment là le meilleur vecteur de communication avec les géographes. D'autant plus que c'est sur cette plateforme que j'ai appris à connaitre la simulation Agent, et découvert ce que pouvait être l'inter-disciplinarité, lors d'un stage encadré au tout début 2009 par Florent Le Néchet, Hélène Mathian, Mathieu Delage, Thomas Louail sur le projet \textbf{AccesSim} \autocite{Delage2010}.

Les versions du modèle vont se succèder pendant plus de deux ans sous Netlogo, en alternant très rapidement entre phase de conception, phase de réalisation, et toutes premières phases d'exploration. Sur ce dernier point Arnaud Banos a joué un rôle décisif en nous mettant en relation avec les ingénieurs de l'ISC-PIF. Il est à ce moment là rattaché à l'UMR Géographie-cités, tout en étant résident et co-directeur avec David Chavalarias de l'Institut des Systèmes Complexe Paris Ile-de-France (ISC-PIF). Il y encadre le doctorant Jéremy Fiegel sur un projet CIFRE ( \textbf{SIMTRAP} ) en partenariat avec la RATP. Réalisé avec Netlogo, ce modèle de simulation Agents tente de reproduire les comportements de milliers d'usagers en interaction lors des transferts entre les rames de RER et les entrées-sorties au niveau des rames, et des quai modélisés. De par les traitements géométriques et cognitifs qu'il mobilise, et au vu du nombre très important d'agents en interactions, l'usage du HPC s'impose comme une ressource nécessaire pour exécuter le modèle. A ce moment là, Romain Reuillon et Mathieu Leclaire, deux ingénieurs spécialistes du HPC à l'ISC-PIF introduisent une tâche spécialisée en Netlogo dans le logiciel OpenMOLE (\textit{Open MOdeL Experiment}), permettant ainsi l'exécution de Simtrap, et par extension de n'importe quel modèle de simulation Netlogo sur grille de calcul \autocite[53]{Banos2013}.

\subsubsection{Troisième moment, OpenMOLE à la croisée des chemins}
\label{sssec:troisieme_moment}

Fin janvier 2010, alors même que les réunions commencent pour la conception du modèle, Arnaud Banos nous (Clara Schmitt, Sébastien Rey-Coyrehourcq) met en relation avec l'équipe d'OpenMOLE et Jéremy Fiegel, qui dispose déjà d'une première expérience avec le logiciel.

Le gain probable d'une mise en commun des connaissances et des outils parait de suite évident aux deux parties. D'un côté, il y a une réflexion qui se place en continuité d'un long héritage de modélisation en géographie, dont les problèmes d'automatisation des expérimentations ont été cristalisés à la fois dans la thèse de Thomas Louail, et dans mon sujet de thèse; et d'un autre côté, les développeurs d'OpenMOLE animés par les mêmes problématiques, sont à la recherche de cas d'utilisation\Anote{evolution_openmole} pour encadrer un outil encore en train d'évoluer, mais qui intègre déjà une bonne partie des possibilités que nous avions imaginées pour notre plateforme. Un accord de coopération est acté quelques semaines à peine après la première rencontre, une preuve aussi que ces arguments de rapprochements étaient assez forts.

\paragraph {Genèse d'OpenMOLE}

A l'origine, il y a le projet \textbf{SimExplorer} présenté dans la thèse de  Frédéric Amblard en 2003 \autocite{Amblard2003}. Cet outil expose déjà dans sa conception et son implémentation bien des idées qui seront reprises sans le savoir dans nos démarches. En effet, bien que cet outil soit cité dans cette publication de référence sur la validation pour le multi-agent en sciences humaines et sociales \autocite{Amblard2006}, de façon étrange, ce prototype ne trouvera pas écho auprès des géographes avant sa re-découverte en 2010, au travers d'un successeur en filiation directe, à savoir OpenMOLE.

% Partie Amblard a replacer sur SimExplorer.
\blockquote[\cite{Amblard2002}]{Le modélisateur doit donc se doter des méthodes et des outils particuliers de l’expérimentateur et les adapter à ses expériences virtuelles. Dans le cycle classique de modélisation construit sur un schéma analyse -> conception -> vérification -> tests (exploration) -> validation, nous nous concentrons donc sur la partie tests ou exploration, que nous cherchons à automatiser partiellement et à adapter aux modèles individus-centrés. Dans le cas d’un modèle individus-centré, nous considérons que l’exploration du modèle procède par approximations successives : les tests sont d’abord assez grossiers (maille large dans l’exploration des paramètres, niveau d’agrégation important dans les résultats) de manière à repérer les éléments les plus saillants de la dynamique, puis ces tests sont progressivement raffinés dans différentes directions identifiées aux étapes précédentes. L’ensemble de ces opérations peut s’avérer très long, et demander de répéter les expériences de nombreuses fois si les bons indicateurs agrégés n’ont pas été identifiés. Il est donc important d’optimiser ces explorations par le biais de plans d’expérience et de se doter d’outils permettant de faciliter leur définition, ainsi que la définition des variables agrégées résultats. C’est l’objectif de l’outil générique SimExplorer, actuellement en cours de développement, qui permettra la réalisation de plans d’expérience pour la simulation, de la manière la plus indépendante possible du noyau de simulation développé.}

La genèse d'OpenMOLE est en effet lié à celle de SimExplorer. En décembre 2005, \textcite{Reuillon2008a} commence le développement de DistMe pour son projet de thèse, un logiciel pour distribuer des réplications (même jeu de valeurs de paramètres, graine aléatoire différente) de simulation stochastique sur grille de calcul. David Hill (LIMOS) est le directeur de thèse de Romain Reuillon (UMR LIMOS Clermont), mais également le directeur de thèse de Frédéric Amblard (Cemagref) qui a déjà soutenu sa thèse en 2003. En 2007 environ, celui-ci conseille à Romain Reuillon de rencontrer l'équipe de Guillaume Deffuant (CEMAGREF), situé sur le même campus, et qui travaille déjà depuis quelques années sur SimExplorer (Nicolas Dumoulin, Thierry Faure, Florent Schuffard). A ce moment là, Frédéric Amblard est déjà parti pour l'IRIT, Romain Reuillon ne l'ayant pas croisé.

L'équipe de Guillaume Deffuant incluant Romain Reuillon, dépose un projet nommé \textit{Life Grid} en 2008, financé par la région pour intégrer les travaux de DistMe dans SimExplorer. Une fois le projet accepté, Romain Reuillon travaille de janvier à novembre sur cette intégration, en coopération avec Florent Schuffard. Romain Reuillon démarre son contrat le 1 novembre 2008 à l'ISC-PIF et soutient sa thèse le 28 novembre du même mois.

Un peu avant en 2007, l'ISC-PIF est alors sous la direction de son créateur Paul Bourgine. Avec David Chavalarias, ils décident d'acheter dans le cadre d'un partenariat avec les fonds de la région Ile-de-France, un \textit{cluster} qui a pour but d'être mutualisé avec la grille de système complexe (700 000 euros) Toutefois, il y a un différentiel entre cette vision mutualiste et le niveau technique des modélisateurs des systèmes complexes. Comme le dit Romain Reuillon, à l'époque \enquote{Personne ne savait s'en servir, c'est d'ailleurs comme ça que la possibilité d'un poste m'est arrivé aux oreilles, via les conseils de Guillaume Deffuant, et une rencontre avec David Chavalarias, de passage au CEMAGREF-LISC de Clermont.}

Une fois en poste à l'ISC-PIF, Romain Reuillon propose à Paul Bourgine - au tournant de l'année 2008-2009 - de continuer à développer SimExplorer, qui deviendra environ un an plus tard un nouveau projet indépendant nommé OpenMOLE. Romain Reuillon veut en effet passer du système séquentiel de déroulement des tâches de SimExplorer, à un système de tâches indépendantes présenté sous forme de \textit{workflow}. Les concepts étant déjà réfléchis depuis un certain temps, un premier prototype de \textit{workflow} fonctionnel démontre la faisabilité des concepts quasiment un mois après le début du projet.

%Octobre 2010, tout openMOLE recodé en Scala.
Lors du passage de direction à Réné Doursat, Mathieu Leclaire est embauché en septembre 2009 sur un poste d'ingénieur grille spécifique à l'ISC (avec les crédits région de l'ISC-PIF). Il devient l'autre membre fondateur d'OpenMOLE, les deux continuant depuis à travailler à plein temps sur le logiciel.

\paragraph {Les concepts du logiciel en quelques mots}

OpenMOLE permet de construire, de modifier, et d'exécuter des workflows sur des environnements de calculs distribués. Un \textbf{\textit{workflow}} peut être vu comme une chaîne de traitement classique, composée de différentes tâches/traitements liées les unes avec les autres par des \textbf{transitions}. A la différence d'autres moteurs de \textit{workflow} existants (Taverna, Kepler, etc.) OpenMOLE peut intégrer n'importe quels types de \textbf{tâches} à ses workflows du moment que celles-ci existent soit sous forme native, soit sous forme de plugin au logiciel (figure \ref{fig:openmole_tasks}). C'est le cas du plugin Netlogo, développé à l'origine pour le modèle de Jéremy Fiegel, dont la réutilisation nous a permis d'intégrer le modèle SimpopLocal à des workflows sans aucun problème. OpenMOLE est capable de gérer des workflows ayant des \textbf{topologies complexes}, avec des boucles, des conditions de déclenchement sur les transitions, des agrégations, etc. Une des particularités d'OpenMOLE est également l'atomicité des tâches qui permet d'envisager la parallélisation des \textit{workflows} sans que l'utilisateur n'ait à gérer aucun des inconvénients habituellement liés à la gestion de la concurrence :

\begin{figure}[!htbp]
	\begin{sidecaption}[Des tâches multiples dans OpenMOLE]{Les tâches peuvent être de nature multiples.}[fig:openmole_tasks]
		\centering
		\includegraphics[width=0.4\linewidth]{task_diversity.pdf}{
		}
  \end{sidecaption}
\end{figure}


\foreignblockquote{english}[{\cite[7]{Reuillon2013}}]{In workflows, a task is an atomic execution component. Several tasks are linked with each-others by transitions to design the topology. More specifically, in OpenMOLE tasks are all portable, reentrant, immutable software components which can be run concurrently. This means that tasks have been designed so they have no interfering side effects. Therefore they can be safely dispatched on several threads, processes or computers. As far as we know, this conception of portable tasks is also a unique feature of OpenMOLE among all the workflow platforms. For us, this feature is crucial, as it makes it possible to delegate the workload entirely transparently from a user perspective, enabling the cloud aspect of OpenMOLE.}

La plus grande différence avec une chaîne de traitement classique, c'est que n'importe laquelle de ces tâches peut voir son \textbf{exécution déléguée} sur des \textbf{environnements de calcul haute performance (HPC)} de façon automatique et transparente avec (ou sans) la déclaration d'\textbf{un plan d'expérience}.

Chaque tâche comporte des \textbf{entrées} et des \textbf{sorties} en fonction du traitement. Les \textit{workflows} s'executent un peu comme un fluide que l'on aurait relâché dans un environnement contraint, en s'écoulant d'un point de départ fixé par l'utilisateur, celui-ci suivant les transitions de tâche en tâche chargées (ou charriant si l'on poursuit la métaphore) des informations qu'on lui a transmises à la fin de chaque traitement, jusqu'à parvenir à un point d'arrivée. Les valeurs des entrées sont connues au moment où la tâche est exécutée, à partir des résultats qui ont été produits et transmis au flux d'exécution par l'exécution des tâches précédentes. Car après chaque traitement, les tâches peuvent transmettre à ce flux des sorties qui seront transmises jusqu'à la tâche suivante.

Enfin, les \textit{workflows} peuvent être construits en utilisant un langage informatique dédié\Anote{dsl} (\textit{Domain Specific Language} DSL) ou composé de façon interactive et graphique en utilisant une interface utilisateur. On trouvera beaucoup plus de détails sur l'ensemble de ces concepts dans les publications associées au logiciel \autocites{Reuillon2008a, Reuillon2013}, mais également sur le \href{http://www.openmole.org}{@site} internet de celui-ci. Le mieux est encore de donner un exemple très simple et visuel de \textit{workflow} permettant d'exécuter quelques réplications d'un modèle de simulation Netlogo, comme le présente la figure \ref{fig:openmole_wfire}.

\begin{figure}[!htbp]
	\begin{sidecaption}[Exemple de plan d'expérience pour la réplication de modèle avec Netlogo]{Exemple de plan d'expérience faisant seulement varier la graine aléatoire du modèle Netlogo \foreignquote{english}{Fire}. Ce modèle de simulation très simple permet de simuler le déplacement d'un feu dans une surface boisée dont on peut faire varier la densité. C'est un exemple que l'on trouve également dans le papier \textcite{Reuillon2013}.}[fig:openmole_wfire]
	 \centering
	 \subbottom[Cette première phase de création du \textit{workflow} nécessite de paramétrer les deux premières tâches \textbf{Et (Exploration task)} et \textbf{Nt (Netlogo task)}. La tâche d'exploration \textbf{Et} nécessite le choix d'un \textit{sampling} pour ensuite être capable de générer le plan d'expérience. Le modèle Netlogo doit définir ses entrées et ses sorties : le paramètre de $density$ du modèle est fixé à une valeur de $60$, mais la \textbf{{\$seed}} doit être passée en entrée de la tâche au moment de son exécution pour que le modèle fonctionne. La sortie attendue du modèle correspond au pas de temps $t$ marquant l'extinction définitive du feu et l'arrêt du modèle : $t_{end-of-fire}$ \label{left_netlogo} ]{
	 	\includegraphics[width=.8\linewidth]{leftdoenetlogo.pdf}
	 	}\qquad
	 \subbottom[Le \textit{workflow} est exécuté. La première étape est de générer le plan d'expérience, c'est à ce moment là que les \textbf{{\$seed}} sont générées puis appariées avec les futures exécutions du modèle (8 seed pour 8 réplications du modèle). OpenMOLE gère toute la partie distribution des calculs, et les modèles instanciés avec les bons paramètres sont donc exécutés dans différents endroits de la planète. Une fois l'exécution des tâches achevée, les $8$ valeurs de $t_{end-of-fire}$ sont récupérées et agrégées dans un tableau par la tâche \textbf{At (Aggregation task)} \label{right_netlogo}]{
		\includegraphics[width=.8\linewidth]{rightdoenetlogo.pdf}
		}
	\end{sidecaption}
\end{figure}

\paragraph {Des arguments clefs pour une première adoption du logiciel}

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item Le moteur de \textit{workflow} permet de décrire des expérimentations avec et autour des modèles de simulation avec une grande flexibilité et une grande panoplie de \textit{task} à disposition,
\item Il est possible de créer des plan d'expériences de façon très simple, et de nouveaux algorithmes d'analyses de sensibilités sont en cours d'ajout à ce moment là du développement du logiciel.
\item Il existe déjà un plugin Netlogo permettant d'intégrer les modèles à des \textit{workflows}.
\item L'exécution de traitements naturellement parallèles (modèles de simulation ou autres) sur environnement de calcul distribué est prise en charge par le logiciel. Une capacité qui suppose pour sa mise en oeuvre des compétences techniques très particulières, que nous n'aurions jamais pu acquérir ou intégrer seul dans un logiciel, même avec l'aide du très bon manuel d'\textcite{Openshaw2000}.
\item Accès potentiel à des méthodes et des outils d'autre disciplines, OpenMOLE mutualisant les développements déjà réalisés pour l'expérimentation d'autres systèmes complexes : biologie, chimie, agroalimentaire, etc.
\item La très grande possibilité d'extension de la plateforme, appuyé par OSGI elle garantit l'indépendance de fonctionnement des composants ajoutés à la plateforme
\item La possibilité de bénéficier d'une expertise sur le calcul intensif et les outils associés
\item Le logiciel cumule déjà plusieurs années de développements, les concepts semblent robustes et déjà éprouvés dans des applications concrètes \autocite{Mesmoudi2010}.
\end{enumerate}

Le logiciel est encore dans un stade expérimental lorsque nous commencons à nous emparer des concepts pour produire nos premiers \textit{workflows}.L'interface graphique étant encore incomplète, c'est principalement par l'usage de scripts OpenMOLE que seront faites la plupart de ces expérimentations. De très nombreux aller-retour vont être nécessaires avec l'équipe des deux ingénieurs pour arriver à construire les tous premiers \textit{workflows} utilisant Netlogo, car cette tâche spécifique doit encore être confrontée à quelques cas d'utilisation imprévus (notamment sur la conversion de types entre l'API Java de Netlogo et OpenMOLE en Scala) pour que son utilisation soit vraiment exemptes de bugs.

%SimpopLocal_h3_grille.nlogo qui tourne sur la grille en juin 2010, seule la seed est amené à varier pour le moment.

Dès juin 2010 et grâce à l'obtention d'un certificat autorisant l'accès à la VO des systèmes complexes, de tous premiers \textit{workflows} mettent en oeuvre le modèle SimpopLocal dans des premiers plans d'expériences très simples, mais s'exécutant officiellement sur environnement distribué. Cette première réussite marque aussi le début d'\textbf{une remise en cause du projet de plateforme} tel qu'il a été pensé à la fin de mon stage de Master 2 en 2009, voici pourquoi.

\paragraph {Une remise en cause du projet initial}

Dans un premier temps, il est seulement prévu de s'appuyer sur OpenMOLE pour construire et exécuter des plans d'expériences de nos modèles sur environnement distribué. Le schéma alors établi est le suivant \ref{figure_schema_initial}. Nous ne passerons pas trop de temps à le décrire ici car celui-ci est devenu rapidement obsolète. Le détail du projet reste disponible dans un compte rendu annuel et dans le projet de thèse initial \autocites{Rey2010, Rey2009}.

\begin{figure}[!htbp]
	\begin{sidecaption}[Ancienne ébauche de plateforme datée de 2010]{Ebauche de la plateforme telle qu'elle est réalisée et utilisée en 2010, intégrant OpenMOLE pour les aspects distribution sur grille de calcul.}[figure_schema_initial]
	 \centering
	 	\includegraphics[width=1.0\linewidth]{oldplatform2.png}
 \end{sidecaption}
\end{figure}

Dans le projet initial, une interface logicielle doit permettre aux géographes de piloter la construction, l'exécution, et la sauvegarde des expérimentations, mais aussi la sélection et la construction de rapports automatiques à exécuter. Cela revient à construire un générateur de workflow capable d'envoyer les modèles de simulation sur grille de calcul et de stocker automatiquement les résultats dans une base de données, exploitable de différentes façons : construction de rapports automatiques, consultation des rapports sur un wiki, visualisation des données dans un SIG, etc.

Au début de l'année 2011, juste après la première campagne de calibrage du modèle SimpopLocal utilisant des algorithmes évolutionnaires (on reviendra par la suite sur cette période), il devient de plus en plus évident que non seulement l'intégration ou l'utilisation d'OpenMOLE de façon externe dans une plateforme n'est pas possible, mais qu'en plus cette solution n'est pas viable, pour plusieurs raisons :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item OpenMOLE est encore en plein développement, il ne sera pas stable avant plusieurs mois, notamment le DSL permettant d'écrire les \textit{workflows}.
\item Deux ingénieurs à plein temps travaillent sur le logiciel, dont le code source change tous les jours, il devient très vite difficile de suivre toutes les activités en parallèle (SimpopLocal, réalisation des workflow, réalisation des scripts pour traiter les résultats, etc.).
\item Une grande partie des outils déjà développés de mon côté sont en Python : génération de \textit{workflows} automatique, intégration des données dans une base de données, requetage de la base de données postgis, construction des rapports graphiques automatique en utilisant R et Python, etc.. La plateforme OpenMOLE est en Java/Scala, ce qui limite le dialogue entre les deux outils.
\item L'interface graphique pour la construction de \textit{workflows} contient un générateur de plans d'expériences complet et interactif. Celui-ci est - au même titre que le reste de l'interface graphique - développé par Mathieu Leclaire à plein temps, donc pourquoi redévelopper un outil similaire de mon côté ?
\end{enumerate}

Une autre évidence remet très rapidement en cause notre démarche de construction de modèle. Les premières expériences pour l'exploration du modèle SimpopLocal produisent un volume de données déjà très important, ne serait-ce qu'à cause de la stochasticité, qui nécessite de nombreuses réplications du modèle. En soit ce n'est pas vraiment un problème technique car les bases de données mises en place à cette occasion peuvent largement supporter un tel poids (quelques Gigas de données). Le problème est plutôt d'ordre méthodologique car à quoi cela sert-il de stocker un tel volume de données sur des simulations qui pour le moment ne sont pas calibrées, et sont donc destinées à être très rapidement supprimées des bases de données ? La fouille de données \textit{a posteriori} est-elle une si bonne idée dans ce cas précis ?

Prenons un exemple plus concret. Dans le cadre de SimpopLocal \autocites{Schmitt2015,Schmitt2014} un des fait stylisés recherchés par les géographes correspond à une description de la hiérarchisation du système de peuplement à un temps $t_{final}$ égal à $4000$ pas de temps (1 pas de temps = 1 année), une durée historique retenue par les experts (voir figure \ref{fig:rtstylisee}).

\begin{figure}[h]
	\begin{sidecaption}[Emergence d'une loi rang-taille dans un système de peuplements stylisé]{Emergence d'une loi rang-taille dans un système de peuplements stylisé}[fig:rtstylisee]
		\centering
		\includegraphics[width=0.7\linewidth]{rangtaille_generique.pdf}{
		}
  \end{sidecaption}
\end{figure}

Dans une exploration \textit{a posteriori} (fouille de données), la seule façon de qualifier si la reproduction de ce fait stylisé est correcte ou pas est de soumettre aux experts les graphiques rang-taille générés. Si l'on réalise un plan complet sur les $5$ paramètres retenus pour varier dans le modèle, avec une discrétisation en $10$ valeurs sur chacun d'entre eux, et $30$ réplications, cela représente vite quelques centaines de milliers d'exécutions du modèle pour cette seule expérimentation : $10^{5} * 30 = \num{3000000}$.

Admettons que l'on puisse résumer chaque groupe de $30$ réplications dans un seul graphique comportant la moyenne, la médiane, l'écart-type, etc. cela nous laisse quand même $\num{100000}$ évaluations visuelles à expertiser. Tout cela en n'ayant aucune certitude sur la capacité de cette maille ($10$ valeurs pour chacun des paramètres, c'est très peu pour couvrir un tel espace, on en reparlera dans la section \ref{sssec:Optimisation}) à détecter la ou les zones de valeurs de paramètres qui génère correctement ce fait stylisé.

Si chaque fichier de données récupéré fait $1$ Mo, une fois multiplié par $\num{100000}$ chaque expérimentation génère environ $100$ Go de données qu'il faut récupérer, stocker, puis interroger avec les outils adéquats. Sans aucune garantie de résultat. En prenant moins de points de mesures pendant la simulation on peut revenir à des chiffres plus raisonnables, mais il faut quand même prendre en compte l'importance de cet argument.

On s'apercoit très vite que ce schéma d'expérimentation n'est pas le plus adéquat pour calibrer un modèle, même d'apparence assez simple, comme SimpopLocal. C'est un problème que nous avions déjà envisagé de façon théorique avec Thomas Louail, en reconnaissant la nécessité d'avoir des outils à notre disposition pour calibrer de façon plus automatique les modèles. Seulement en 2009, nous n'avions aucun outil fonctionnel à disposition, le stage de trois mois de A. Monzi pour développer une interface aux Algorithmes Evolutionnaires de B.Calvez n'ayant malheureusement pas abouti sur un prototype utilisable \autocite[140-141]{Louail2010}

Il se trouve que l'équipe d'ingénieurs responsable d'OpenMOLE a déjà utilisé des Algorithmes Evolutionnaires sur grille de calcul pour une problématique en agroalimentaire \autocite{Mesmoudi2010}. Or les Algorithmes Evolutionnaires sont - comme on en parlera plus en détail dans la section \ref{sec:MGO} - une des familles de techniques métaheuristiques les plus efficaces pour encadrer des problèmes d'optimisation multi-critères. Le calibrage se définit assez naturellement comme un problème d'optimisation dont l'objectif est la découverte du meilleur ajustement possible entre un jeu d'hypothèses et un jeu de critères fournis par le modélisateur. Ce type d'optimisation étant très consommateur de puissance de calcul, le couplage déjà réussi entre MGO et OpenMOLE présage d'une infrastructure technique adéquate pour la mise en oeuvre d'un projet d'évaluation systématique resté jusqu'alors très théorique (voir section \ref{ssec:evaluation_construction}).

Comme l'avaient indiqué en leur temps Diplock et Openshaw \autocite{Diplock1996}, cette possibilité de calibrer - et plus généralement d'explorer avec toutes les nouvelles méthodes HPC à notre disposition - de façon systématique et avec très peu d'efforts les modèles de simulation permet de mieux se concentrer sur la construction des modèles et des critères. Après la réalisation du premier prototype utilisant OpenMOLE, SimpopLocal et MGO, nous évoquerons comment cette approche d'évaluation systématique a continué d'inspirer des changements dans notre façon de concevoir les modèles et la modélisation. % dans le courant de l'année 2013, avec le démarrage d'une nouvelle dynamique et d'un nouveau projet de modèle de simulation.

Poursuivre le développement de cette librairie logicielle, encore instable, est très vite reconnu par les deux équipes comme un objectif d'intérêt commun. Car d'un côté il y a cette possibilité de développer un prototype de méthodologie permettant de construire et d'explorer (calibrer dans un premier temps) SimpopLocal avec des outils adaptés, réactifs, et jamais utilisés jusque là au laboratoire Géographie-cités. Et d'un autre côté, il y a la possibilité quasi-simultanée de mutualiser et donc de généraliser ces outils et cette méthodologie à n'importe quel autre modèle de simulation utilisant Netlogo et OpenMOLE.

%est  pour en faire un catalogue évolutif d'algorithmes évolutionnaires utilisables très facilement dans OpenMOLE. SimpopLocal sert de cas d'utilisation concret, mais l'horizon final est bien de permettre l'utilisation de ces outils pour n'importe quel autre modèle de simulation intégré dans un \textit{workflow}.

%Un des premier avantage à voir dans l'application de cette méthodologie réside dans l'incitation à formuler des critères qu'elle engendre chez les modélisateurs. Pour pouvoir apprendre de la dynamique des modèles mis en oeuvre, il n'y a pas d'autre choix que de formaliser les questions, les critères, que l'on veut s'attend à voir ou ne pas satisfait lors du calibrage. Cette approche à été longuemment décrit dans la section \ref{ssec:evaluation_construction}, mais aucune technique n'avait alors été évoqué pour son support.

Comme on vient de le décrire, la coopération avec l'équipe d'OpenMOLE ne va aller qu'en se renforçant au cours de l'année 2010. Le maintien de deux projets indépendants alors même que ceux-ci ont en commun un grand nombre d'idées et d'objectifs n'a plus vraiment de sens. Il aura donc fallu se déposséder d'abord de l'idée de plateforme pour l'évaluation des modèles telle que nous l'avions imaginée, pour la reconstruire à nouveau au travers d'un co-développement étroit avec des ingénieurs spécialisés dans le HPC. Une ressource informatique qui reste à ce jour le seul moyen d'explorer efficacement les modèles de simulation, et cela depuis les années 1980, date où les géographes avaient déjà pressenti la nécessité d'un tel projet (\ref{p:experience_minuit}). Une ressource indispensable mais aussi inatteignable pour un développeur seul et non spécialisé dans cette discipline. En ce sens, continuer sur ce projet de façon autonome ne fait que freiner le développement d'une coopération qui pourrait s'avérer plus fructueuse encore, car basée sur l'expertise et le travail déjà réalisé par chacun. Sans compter qu'à ce moment là le développement d'OpenMOLE laisse entrevoir des possibilités qui se situent bien au delà de ce que l'on avait imaginé dans notre cahier des charges initial.

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item Les \textit{workflows} peuvent être sauvegardés et partagés.
\item A terme, il sera possible de \enquote{versionner}\Anote{versionnerwf} des \textit{workflows}, voire des données et des résultats \textit{associés} à ces \textit{workflows}.
\item Il existe la possibilité de proposer des \textit{workflows} de type \textit{PuzzleWorkflow}, prêt à être utilisés ceux-ci doivent d'abord être complétés ou paramétrés par l'utilisateur, comme un \textit{puzzle} dont il manquerait seulement quelques pièces pour être terminé.
\item Les concepteurs envisagent de proposer une collection de \textit{workflows} déjà prêts à l'emploi, où l'utilisateur n'aurait plus qu'à ajouter son modèle.
\item Il est possible d'utiliser et d'ajouter des tâches pour le langage R dans les \textit{workflows}, ce qui comble une partie des besoins des géographes sur le plan de l'analyse statistique.
\end{enumerate}

Tout comme son potentiel avait déjà dû apparaitre aux yeux d'Arnaud \textcite[19-26]{Banos2013} (\enquote{et si le vrai luxe c'était le calcul ?} ), il nous est  aussi paru évident qu'un tel logiciel pouvait transformer les usages d'une communauté de modélisateurs dont on a vu qu'elle était en attente de ce type d'outils depuis plus d'une decennie. Sachant que la plupart des modélisateurs actuels en sciences humaines et sociales se situent plutôt dans un entre-deux sur le plan technique (ni experts informaticiens, ni débutants), la proposition faite par les ingénieurs de l'ISC-PIF pourrait bien s'avérer être ce \enquote{chaînon manquant} qui permettrait d'enclencher une véritable démocratisation des usages du HPC pour la modélisation en géographie, et plus généralement pour la modélisation en SHS.

A partir de là, les aspects plus géomatiques prévus dans la plateforme initiale vont être mis entre parenthèses pour se concentrer principalement sur la construction d'un prototype de méthodologie d'exploration pour SimpopLocal appuyé par MGO et OpenMOLE. %De part la nature de la plateforme, dédié avant tout à la l'exploration des modèles de simulation, l'essentiel des activités de développements par se focaliser sur l'amélioration de ce processus, et va déboucher sur plusieurs innovations au croisement des différents domaines.

\paragraph {Vers la construction d'une plateforme intégrée}

Plutôt que de réinventer une nouvelle fois la roue, pourquoi alors ne pas \textbf{intégrer} les outils qui manquent aux géographes et aux modélisateurs en sciences humaines et sociales directement dans OpenMOLE, participant ainsi d'une démarche cumulative bénéfique à tous.

Si la suite de cette thèse se focalise pleinement sur la mise en oeuvre des outils et des méthodes susceptibles de fournir une évaluation plus systématique des modèles de simulations dans les \textit{workflows}, ce projet d'une plateforme intégrée d'orientation plus géomatique ne reste pas lettre morte.

L'intérêt d'une plateforme comme OpenMOLE c'est qu'elle propose une structure ouverte à l'intégration :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item Ajouter des connecteurs en entrées / sorties vers les bases de données. dans les \textit{workflows}
\item Ajouter une possibilité de traiter et de visualiser des données géographiques dans la plateforme (avec GeoTools, ou JTS par exemple).
\item Ajouter des tâches pour le traitement de données massives.
\item etc.
\end{enumerate}

Un autre projet d'intégration a été mené durant l'année 2013, et celui-ci a mobilisé deux stagiaires que j'ai encadrés sur 2 et 6 mois (Benjamin Bernard et Cyril Jayet). L'objectif était d'intégrer dans OpenMOLE un outil de \enquote{visualisation scientifique} pouvant être couplé de façon flexible avec les données circulant en sorties des tâches de \textit{workflows}, comme le propose la figure \ref{fig:visu_projet}. Devant les possibilités offertes par les librairies de visualisation javascript, le choix du support pour construire les visualisations (un simple graphique XY dans un premier temps) s'est porté sur un navigateur Internet. Ce projet était relativement complexe sur plusieurs aspects. Il fallait gérer l'intégration d'un navigateur Internet dans OpenMOLE, coupler un tracé des visualisations en Javascript dans un navigateur avec un flux de données en Scala, et enfin gérer l'affichage de données de façon synchrone ou asynchrone. En effet, les modèles de simulations s'exécutant sur un environnement distribué, les données sont effectivement récupérées de façon asynchrone par OpenMOLE.

\begin{figure}[htbp]
	\begin{sidecaption}[Plan d'expérience sur le modèle Netlogo Fire]{Le plan d'expérience propose de faire varier la densité du modèle de $0$ à $100$ par pas de $10$, avec $5$ réplications pour chacune de ces variations de densité. Le résultat est directement visible pendant l'exécution des simulations sur grille de calcul grâce au plugin implémenté dans OpenMOLE. Le bouton \foreignquote{english}{refresh} permet d'afficher les dernières données récupérées par OpenMOLE. Le \textit{mapping} permet de changer les variables en $x$ et $y$ en fonction des entrées et des sorties de tâches disponibles dans le \textit{workflow} : $seed$, $density$, $t_{end-of-fire}$ }[fig:visu_projet]
		\centering
		\includegraphics[width=0.9\linewidth]{leftdoenetlogo_visu.pdf}
  \end{sidecaption}
\end{figure}

Le prototype a bien été implémenté et testé avec l'aide de Robin Cura, l'auteur, et les deux stagiaires. Toutefois ce développement n'a pas pu être maintenu dans le temps. OpenMOLE disposant de sa propre autonomie et de sa propre feuille de route, les objectifs définis initialement peuvent changer très rapidement. L'interface graphique sur lequel notre prototype s'appuyait en 2013 a été abandonnée pendant plus d'un an et demi le temps de son redéveloppement et de son transfert dans un navigateur Internet. OpenMOLE devenant client/serveur, ce qui implique des changements importants d'architecture. De ce fait, les développements réalisés sur le prototype de visualisateur réalisé en 2013 doivent être revus intégralement au regard de ces changements. Ce point illustre aussi la difficulté du travail interdisciplinaire lorsque il y a un différentiel entre les équipes en termes de force de développement. Le logiciel OpenMOLE est en effet amendé tous les jours, voire plusieurs fois par jour, par deux développeurs qui opèrent sa construction à plein temps. Construire un plugin qui s'appuie sur les concepts de fond du logiciel comme on a tenté de le faire (interface graphique, implémentation interne des workflow, communication entre le coeur métier du logiciel et l'interface graphique, etc.) alors même que ceux-ci continuent d'évoluer est un travail qui peut se faire uniquement en se calant sur le rythme de développement des autres développeurs. La nouvelle interface est encore en cours de développement par Mathieu Leclaire, il faudra donc encore attendre l'officialisation et la stabilisation de celle-ci pour envisager plus sereinement la reconstruction de ce plugin de visualisation, dont l'intérêt reste entier. Plus de détails, en particulier techniques, peuvent être trouvés dans les rapports de stage de \textcite{Bernard2013} et \textcite{Jayet2013}.

\paragraph {Retour sur les étapes importantes d'une démarche prototype}

% Référence à Amblard Amblard2010 et Amblard2003 sur l'expérimentation au coeur du processus de modélisation.

Cette liste se présente comme un résumé des développements intégrant trois objets de recherche, le modèle SimpopLocal, la librairie d'algorithmes évolutionnaires MGO, et OpenMOLE.

De façon générale, les objectifs et les enjeux thématiques justifiant la création du modèle SimpopLocal sont décrits très en détail dans la thèse de Clara Schmitt \autocite{Schmitt2014} et dans un article écrit avec Romain Reuillon, Denise Pumain et Clara Schmitt \autocite{Schmitt2015}. Cette dernière publication est reproduite et complétée par quelques visuels en annexe \ref{sec:simpoplocal}.

\begin{itemize}[label=\textbullet]
%\item \textbf{1981 (CC P1)}

\item {\textbf{Janvier 2010}} Le projet de modéle multi-agents SimpopLocal commence, des réunions ont lieu tous les 15 jours environ. L'implémentation en Netlogo se fait de façon presque immédiate après quelques premières réunions.

\item {\textbf{Janvier 2010}} Rencontre avec Romain Reuillon, Mathieu Leclaire et Jérémy Fiegel à l'ISC-PIF.

\item {\textbf{Mars 2010}} Signature d'une convention devant l'intérêt commun des deux parties.

\item {\textbf{Mars 2010}} Suite aux expériences déjà réalisées de son côté avec les algorithmes génétiques, Romain Reuillon nous demande de réflechir à la formalisation des sorties de notre modèle. Autrement dit, la question \enquote{Qu'est-ce que vous voulez démontrer avec votre modèle ? } s'impose comme une question préalable à la compréhension entre les deux équipes.

\item {\textbf{Mars 2010}} Arnaud Banos demande l'inscription de Géographie-cités auprès de l'organisme de certification.

\item {\textbf{Juin 2010}} De premiers \textit{workflows} utilisant SimpopLocal sont exécutés sur la VO des systèmes complexes. Les recherches pour essayer de caractériser de façon plus automatique les sorties du modèle démarrent ce même mois.

\item {\textbf{Octobre 2010}} Première présentation du projet d'utilisation d'une grille de calcul pour le calibrage du modèle lors des journées Grid Day.

\item {\textbf{Novembre 2010}} Les premiers plans d'expériences pour explorer le modèle génèrent un grand volume de données. Même aidé par des résumés visuels, il est très difficile de faire un tri manuel à la recherche des solutions intéressantes. Il est proposé de basculer sur une évaluation des modèles de simulations a priori, en utilisant des critères quantitatifs (durée des simulations, seuil de population à atteindre) et qualitatifs (recherche d'une lognormalité dans la hiérarchisation des peuplements).

\item {\textbf{Novembre 2010} - \textbf{Janvier 2011} } Les différents objectifs pour évaluer SimpopLocal sont implémentés par Sébastien Rey dans une tâche indépendante qui peut être exécutée après la série de réplications.

\item {\textbf{Janvier 2011}} Le projet intègre l'ERC GeoDivercity obtenue par Denise Pumain. OpenMOLE devient la plateforme référente, et l'ancien projet est mis en suspens pour se concentrer uniquement sur la réalisation d'un prototype de démarche visant l'automatisation du calibrage sur le modèle SimpopLocal.

\item {\textbf{Avril 2011}} Un \textit{workflow} assez complexe (voir section \ref{p:prototype_fonctionel}) s'appuie sur la librairie d'algorithmes évolutionnaires développée à l'ISC-PIF (MGO, écrit en Scala) pour exécuter de premiers essais du calibrage du modèle SimpopLocal. A partir de cette date, l'exploration du modèle va être le moteur d'un cercle d'amélioration vertueux touchant à la fois OpenMole, MGO, et évidemment SimpopLocal dont le calibrage reste le premier objectif.

\item {\textbf{Avril 2011}} En stressant les différentes zones de valeurs de paramètres du modèle SimpopLocal, nous nous apercevons que des mécanismes de contrôle doivent être ajoutés pour limiter la saturation mémoire et processeur liée à certaines combinaisons de valeurs de paramètres. Le modèle expose une palette très variable de durée d'exécution en fonction des valeurs de paramètres, la réalisation des objectifs pouvant aller de quelques minutes à plusieurs dizaines de minutes. Le nombre d'\enquote{objet innovation} dans le modèle doit être limité par un nouveau paramètre.

\item{\textbf{Mai 2011}} Les critères retenus pour évaluer les simulations sont discutés en comité de thèse. Pour gérer l'agrégation des valeurs résultats sur les objectifs, il est choisi de privilégier l'écart absolu à la médiane (la \textit{Median Absolute Deviation} ou MAD).

\item {\textbf{Mai 2011}} La librairie pour les algorithmes évolutionnaires multi-objectifs MGO va être complètement redéveloppée en faveur d'une approche orientée composant, et de nouveaux algorithmes vont être ajoutés progressivement avec l'avancement de notre problématique d'exploration sur SimpopLocal. Ce travail est décrit dans la section \ref{sec:MGO}, il est motivé par Sébastien Rey, Romain Reuillon et Gabriel Cardoso.

\item {\textbf{Septembre 2011}} Robin Cura ajoute et expérimente un mécanisme pour déclencher des catastrophes dans SimpopLocal.

\item {\textbf{Février 2012}} Première campagne d'exploration plus conséquente du modèle, plus de $\num{50000}$ simulations sont exécutées, et les calculs durent plusieurs jours (6-7) consécutifs.

\item {\textbf{Mars 2012}} Pour essayer d'accélérer les exécutions des modèles, un plugin Netlogo est créé par Sébastien Rey et Romain Reuillon. Celui-ci tente d'exporter dans un autre langage les opérations d'échanges d'innovations entre les peuplements, une phase de tri complexe qui correspond au goulot d'étranglement limitant les performances d'exécution du modèle. Les résultats sont peu concluants et ne montrent pas une diminution des temps d'exécution satisfaisante.

\item {\textbf{Mars 2012}} Le modèle SimpopLocal est réécrit en Scala par Sébastien Rey, avec l'aide de Romain Reuillon. L'exécution du modèle est bien plus rapide (on passe de plusieurs minutes à quelques secondes) car le modèle est optimisé pour gérer les architecture multi-coeurs (ce qui n'était pas possible avec Netlogo).

\item {\textbf{Mars 2012}} Avec l'aide d'un nouveau type de \textit{workflow} paramétrable (\textit{PuzzleWorkflow}), le \textit{workflow} prototype utilisé pour calibrer SimpopLocal peut être intégré comme un \textit{workflow} générique paramétrable et donc réutilisable par tout le monde dans OpenMOLE. La définition de \textit{workflow} pour utiliser un modèle avec des algorithmes évolutionnaire passe de plusieurs centaines de lignes de script à seulement quelques dizaines de lignes.

\item {\textbf{Mars - Décembre 2012}} Comme le calibrage ne permet pas de connaître en détail la dynamique exprimable par chacun des paramètres, Romain Reuillon appuyé par Jean-Baptise Mouret (ISIR) propose une nouvelle méthode. En s'appuyant sur un objectif plus explicite de diversité, on pousse les algorithmes évolutionnaires à explorer de façon exhaustive le spectre de dynamique exprimable pour un paramètre sous contrainte des objectifs \autocite{Reuillon2015}. On obtient ainsi des profils de dynamique par paramètre, toujours en fonction d'objectifs à atteindre. La première implémentation est opérationelle en décembre 2012.

\item {\textbf{Juillet 2012}} Sébastien Rey ajoute une métrique de convergence basée sur une variante de calcul de l'Hypervolume dans MGO \autocite{Fonseca2006}. Cet algorithme nous renseigne sur l'état du volume occupé par les solutions évaluées en n-dimensions.

\item {\textbf{Juillet 2012}} Sébastien Rey ajoute l'algorithme SMS-EMOA dans MGO \autocite{Emmerich2005}. Cette variante de NSGA2 se base sur la métrique d'Hypervolume pour appuyer la sélection des meilleures solutions candidates (sur la base d'un calcul de contribution d'une solution candidate à l'expansion du volume occupé par les meilleures solutions).

\item {\textbf{Août 2012}} Romain Reuillon implémente des algorithmes évolutionnaires en \enquote{îlots} appuyé par MGO dans OpenMOLE \autocite{Whitley1997}. A la différence des méthodes classiques de distribution d'algorithmes évolutionnaires sur grille de calcul (\textit{Generational} et \textit{Steady State}), la répartition en îlot permet d'ajouter un niveau de parallélisme supplémentaire à l'exécution en faisant tourner sur chaque noeud de la grille de calcul, non pas une seule simulation, mais une population de simulations gérées par un Algorithme Evolutionnaire autonome. Les meilleurs candidats sont récoltés une fois la procédure d'évolution terminée sur chacun de ces noeud/iles. La durée des expérimentations diminue immédiatement.

\item {\textbf{Août - Décembre 2012}} La sensibilité d'une fonction objectif pose problème dans la construction d'un ensemble de solutions satisfaisant. L'epsilon dominance est ajouté en décembre dans MGO par Romain Reuillon (voir section \ref{sec:MGO} pour un exemple), ce qui règle le problème.

\item {\textbf{Octobre 2012 - Mars 2013}} Une nouvelle problématique se pose à l'équipe. Les résultats rapportés par les algorithmes évolutionnaires proposent des jeux de valeurs de paramètres de qualité très en dessous de ce qui est attendu d'une telle sélection. Après une campagne d'exploration (Sébastien Rey) dédiée à l'étude de la variabilité liée à l'aléa dans SimpopLocal, on s'apercoit que le nombre de réplications ($30$) est largement insuffisant pour que l'optimiseur puisse opérer une sélection fiable sur la base d'une médiane pour chaque objectif (voir section \ref{sec:MGO} pour un exemple). Certaines solutions ont pu être selectionnées par chance sur la base d'une bonne série de réplications, et comme l'optimiseur ne réévalue jamais les meilleures solutions selectionnées, ces solutions demeurent jusqu'à la fin de l'optimisation. $100$ réplications apparait comme un minimum, or ce n'est pas faisable, car cela multiplie la durée d'expérimentation par un facteur supérieur à deux. Une des solutions implémentées par Romain Reuillon en Mars 2013 est de rester à $30$ réplications, mais d'intégrer dans MGO une méthode de compensation de la stochasticité par une réévaluation aléatoire des meilleures solutions retenues par l'optimiseur \autocite{Pietro2004}.

\item {\textbf{Février 2013}} L'équipe présente les résultats obtenus sur SimpopLocal avec l'aide des deux méthodes d'explorations : calibrage par Algorithmes Evolutionnaires, et analyse de sensibilité en utilisant la méthode des profils.

\item {\textbf{Avril 2013}} Le modèle SimpopLocal est décomposé en briques réutilisables selon la même méthode qu'utilisée pour MGO et intègre ensuite la nouvelle plateforme de construction de modèle générique SimPuzzle. Celle-ci a pour vocation d'aide à la mutualisation des hypothèses implémentées pour les différents modèles, qui peuvent ensuite être réutilisées plus facilement dans de nouvelles compositions. L'idée d'un versionnement des hypothèses se concrétise un peu plus.

\item {\textbf{Juillet 2013}} Première soumission d'un article à \textit{Environment and Planning B} (EPB). Celui-ci fait le point sur le modèle et sur l'ensemble des développements mobilisés pour son exploration. Cette publication, reproduite en annexe (voir \ref{sec:simpoplocal}) et dans \textcite{Schmitt2015}, est l'aboutissement d'un tout premier prototype qui ouvre la voie à de nouveaux développements méthodologiques et techniques dans l'équipe ERC. On en parle plus ensuite.

\end{itemize}

Avec l'avancement des différents projets de l'ERC GeodiverCity, l'exploration des modèles devient le pilier central d'une réflexion commune qui semble rejaillir sur l'ensemble des développements, qu'ils soient thématiques, méthodologiques, ou techniques.

\subsubsection{Quatrième moment, au delà du prototype}
\label{sssec:quatrieme_moment}

\begin{figure}[t]
\begin{sidecaption}[Un schéma de synthèse pour les activités d'explorations des différents modèles de simulation dans l'ERC GeoDiverCity]{Ensemble des activités organisées autour de l'exploration des modèles de simulation. Les cercles de couleurs représentent des activités, et les flèches des relations entres les différentes activités.}[fig:S_explo_calendrier]
  \centering
 \includegraphics[width=.9\linewidth]{explorations_calendrier.pdf}
  \end{sidecaption}
\end{figure}


Le quatrième moment démarre avec la mise en route de nouveaux projets, de nouvelles dynamiques, et l'arrivée de nouvelles personnes dans l'équipe modélisation (Paul Chapron, Clémentine Cottineau, Elfie Swaerts, Guillaume Chérel). Il n'est pas possible de lister en détail l'ensemble des développements menés par cette équipe en 2014 tellement ceux-ci sont nombreux. On se contentera de nommer les projets en indiquant pour certains les avancées qu'ils représentent. Les enjeux et les challenges qui accompagnent cette interdisciplinarité vécue sont relatés plus en détail dans \textcite{Chapron2014}.

L'activité d'exploration continue d'être un catalyseur pour différents projets, c'est donc le centre de cette figure \ref{fig:S_explo_calendrier} choisie pour mettre en perspective les différentes activités passées et en cours.

\begin{myitemize2}

\item [1] \textbf{\#Exploration \#OpenMOLE} \\ Les échanges avec Romain Reuillon et Mathieu Leclaire autour de la construction de \textit{workflows} représentent plusieurs centaines d'heures de travail cumulées. Cette interaction a permis de tester, corriger, améliorer la syntaxe DSL utilisée pour décrire les \textit{workflows}, mais aussi de tester la robustesse du logiciel sur un cas d'utilisation concret, nécessitant des expérimentations de durées importantes.

\item[2] \textbf{\#OpenMOLE \#MGO \#Exploration} \\ Les différentes campagnes d'explorations menées vont avoir des retombées sur la construction de la librairie logicielle MGO, le couplage entre OpenMOLE et MGO, ce qui en retour améliore les explorations.

\begin{enumerate}
\item Intégration des algorithmes génétiques dans les \textit{workflows} OpenMOLE : \textit{EA Generational} (2011) $<$ \textit{EA Steady State} (2012) $<$  \textit{EA Island} (Août 2012) sont classés ici par degré d'utilisation du parallélisme. L'objectif est de rendre toujours plus efficientes les explorations
\item Grâce au concept de \textit{workflow puzzle}, l'écriture avec le DSL de \textit{workflows} utilisant les Algorithmes Evolutionnaires est grandement simplifiée (Mars 2012)
\end{enumerate}

\item[3] \textbf{\#Exploration \#Modeles} \\ L'exploration des résultats \textit{a posteriori} s'avère incertaine et trop coûteuse (durée d'exécution, volumes de données importants), ce qui nous renvoie à un questionnement \textit{a priori} des modèles en utilisant des méta-heuristiques.
\begin{enumerate}
	\item Réalisation du prototype de calibrage inversé utilisé sur SimpopLocal (Janvier 2010 - Juillet 2013 )
	\item Utilisation d'un nouveau type d'analyse de sensibilité avec la méthode CP-Profile (idée en Juillet 2012, réalisation Décembre 2012 , premiers résultats Février 2013 )
	\item L'exploration permet d'avoir un retour sur les hypothèses et la façon dont elles sont implémentées ( Exploration -> Modèlisation ), mais la construction modifie aussi la façon dont on va penser l'exploration : construction par incrément EBIMM \autocite{Cottineau2015}.
\end{enumerate}

\item [4] \textbf{ \#MGO \#Modeles \#Exploration} \\ Boucle liant l'activité de construction MGO, l'activité de construction du modèle, et l'activité d'exploration des modèles.

\begin{enumerate}
\item  Le développement de nouvelles méta-heuristiques pour l'exploration impacte notre connaissance des dynamiques du modèle, et donc modifie l'activité de construction des modèles.
 \begin{enumerate}
	\item Hypervolume et SMS-MOEA (Juillet 2012)
	\item CP-Profile (Décembre 2012)
	\item PSE (Janvier 2014)
	\item Epsilon Dominance (Décembre 2012)
	\item Re-Evaluation de l'archive (Mars 2013)
\end{enumerate}

\item Les modèles peuvent être amenés à être réécrits pour satisfaire des raisons de performance, ou de modularité :
\begin{enumerate}
	\item Réécriture de SimpopLocal (Mars-Avril 2012)
	\item Intégration de SimpopLocal dans SimPuzzle (Avril 2013)
	\item Réécriture de Marius pour Simpuzzle (Mai 2013)
\end{enumerate}

\item La création de SimFamilly (mai 2014) s'appuie sur MGO, OpenMOLE et SimPuzzle, et modifie profondément la façon de construire les modèles. Les modèles sont générés de façon automatique à partir des hypothèses entrées pour une famille de modèles dans SimPuzzle, l'optimiseur recherche les meilleures combinaisons d'hypothèses, et de paramètres accompagnant les hypothèses, pour ajuster les critères qu'on lui a transmis.

\item Les modèles sont également modifiés pour être utilisés par les métaheuristiques, et celle-ci mettent à l'épreuve en retour l'implémentation du modèle vis-à-vis des objectifs fixés. Ce processus intervient dans la validation interne des modèles en soulevant les erreurs, les comportements contre-intuitifs, les zones de valeurs de paramètres bloquant, ou les conditions d'arrêt trop ou pas assez strictes pour l'exploration. L'introduction d'objectifs sous forme de critères multiples pour évaluer les sorties de la simulation de façon automatique participe également de la formalisation et de la construction des connaissances, entrainant souvent pour les modélisateurs un retour sur l'empirie.
\begin{enumerate}
 \item SimpopLocal (Janvier 2010)
 \item Marius (Janvier 2013)
 \item Gugus (2014)
 \end{enumerate}
\end{enumerate}

\item[5] Boucle liant OpenMole et Mgo, le \textit{framework} est repensé pour être plus beaucoup modulaire, et donc plus facilement utilisable sous forme de tâche dans OpenMOLE (depuis Mai 2011)

\item[6] Intégration dans OpenMOLE d'un prototype d'outil permettant de construire des visualisations en temps réels sur les sorties de tâches OpenMOLE (Janvier - Septembre 2013)

\item[7]  Construction d'outils pour l'exploration des sorties de simulation des modèles réalisés
 \begin{enumerate}
 \item TrajPOP (Septembre 2011)
 \item Varius (début 2015)
 \end{enumerate}

\end{myitemize2}
