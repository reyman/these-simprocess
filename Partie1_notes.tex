% -*- root: These.tex -*-

\Anotecontent{versionnerwf}{?}

\Anotecontent{pom_realiste}{\foreigntextquote{english}[{\cite[315-316]{Railsback2012}}]{Let us warn you that no other platforms or ways to design and program ABMs are remotely as easy to use, well documented, and polished as Netlogo is. You can expect a serious increase in the effort and software knowledge required to produce working models, which could be a strong incentive to keep your models simple enough to stick with netlogo. However, if you do think you will outgrow NetLogo, we strongly recommend taking one or two serious courses in object-oriented programming, probably using the Java language.[...] Another alternative to consider seriously is finding a collaborator to provide the software expertise - either a paid programmer or an academic computer scientist (Chapter 8 of Grimm and Railsback 2005 provides advice on working with software professionals.)}}

\Anotecontent{uncertainty_grimm}{\foreigntextquote{english}[{\cite[255-256]{Railsback2012}}]{Parameterization is the word modelers used for the step of selecting values for a model's parameters.[...] The reason that parametrization is important is, of course, that quantitative results matter.[...] Calibration is a special kind of parameterization in which we find good values cause the model to reproduce patterns observed in the real system. Calibrating an ABM is part of POM because we calibrate \enquote{against} (i.e., to reproduce) patterns observed in the real system. Now, however, the patterns are typically quantitative instead of qualitative and more descriptive of the whole system, not its agents and parts. [...] Why do we include calibration as a part of pattern-oriented modeling separate from theory development, when they both include changing of adjusting a model until it adequately matches a set of observed patterns ? The major differences is that in theory development, we are focused on one particular part of a model: its traits for agent behavior. We often test theory against qualitative patterns so that we do not need to worry yet about how closely model reproduces observations from the real system. Calibration comes after we've identified theory for behavior and assembled the full model. Another way to think of the difference between this chapter and the rest of POM is that chapters 18 and 19 were about using patterns to reduce a model's \enquote{structural uncertainty} by finding good model designs and agent traits, and this chapter is about reducing \enquote{parameter uncertainty} by finding good parameter values.}}

\Anotecontent{evaludation}{\foreigntextquote{english}[\cite{Augusiak2014}]{Confusion about model validation is one of the main challenges in using ecological models for decision support, such as the regulation of pesticides. Decision makers need to know whether a model is a sufficiently good representation of its real counterpart and what criteria can be used to answer this question. Unclear terminology is one of the main obstacles to a good understanding of what model validation is, how it works, and what it can deliver. Therefore, we performed a literature review and derived a standard set of terms. ‘Validation’ was identified as a catch-all term, which is thus useless for any practical purpose. We introduce the term ‘evaludation’, a fusion of ‘evaluation’ and ‘validation’, to describe the entire process of assessing a model’s quality and reliability.}}

\Anotecontent{dsl}{Pour les modélisateurs, il faut imaginer que ce langage dédié met à disposition des utilisateurs différentes primitives spécifiques au logiciel pour construire et executer les workflows, un peu comme Netlogo le propose avec la manipulation des Tortues.}

\Anotecontent{holland_multi_utilisation}{ Holland a développé les GA avant tout pour leur capacité de \foreignquote{english}{robust adaptive systems} et pas seulement pour leur capacité d'optimisation comme le rapelle \textcite{DeJong1993a} : \foreignquote{english}{However, with all this activity, there is a tendency to equate GAs with function optimization. There is a subtle but important difference between \enquote{GAs \textbf{as} function optimizers} and \enquote{GAs \textbf{are} function optimizers}} ; L'investissement d'Holland dans l'étude des \foreignquote{english}{Complex Adaptive Systems} s'inscrit dans une trajectoire de recherche resté proche des thématiques de ce qui deviendra plus tard la méta-discipline \textit{Artificial Life}. Son investissement continue dans cette branche de développement est d'ailleur lisible au travers de deux plateformes successive sur ce thème : \textit{$\alpha$-universe} et \textit{Echo} dont on trouve une analyse dans les travaux de \autocites{Taylor1999, Taylor2001} }


\Anotecontent{difference_objective_heuristique}{Il n'est pas forcément évident de faire la différence entre ces termes très proches, dont le sens se recoupe parfois, voici donc une aide à la désambiguisation inspirée de celle de \textcite[36]{Weise2011} :

\begin{enumerate}[labelindent=\parindent,leftmargin=*]
\item La fonction objectif (\textit{objective function}) peut etre considérée comme une forme d'heuristique, à la différence que celle ci est une mesure forcément directe du potentiel d'un aspect de la solution, alors que l'heuristique peut être de mesure directe ou indirecte, en ne fournissant par exemple qu'une approximation de la distance séparant une mesure de l'optimum. En ce sens, la fonction objectif mobilise souvent plus d'expertise sur le système que l'heuristique.
\item Une fonction \textit{fitness} est une fonction d'utilité secondaire, conçu comme une combinaison possible de fonction objectifs, et/ou d'heuristiques. Celle-ci peut également être une mesure relative, pour quantifier par exemple la différence existante entre deux solutions.
\end{enumerate}}

\Anotecontent{barricelli_multi_utilisation}{ Tout comme les travaux de McMillan ont permis de voir plus clair dans les intentions de Von Neumman derrière la notion de \textit{self-reproduction automata} ..., les travaux de Dyson \Autocite{Dyson1997}, de Fogel \autocite{Fogel2006a} sur l'histoire de cette discipline a permis également de redécouvrir les recherches de Barricelli comme celle d'un véritable pionnier en ALife, mais également comme celui d'un pionnier dans l'idée d'utiliser l'évolution comme support à la résolution de problème.}

\Anotecontent{fraser_comment}{\foreignquote{english}{Fraser was one of the first to conceive and execute computersimulations of genetic systems, and his efforts in the 1950s and1960s had a profound impact on computational models of evo-lutionary systems. The simulation algorithms he used were im-portant not only in the simulation of genetical problems, but pro-vided a menu of techniques that enriched the entire simulationeffort in any problem that involved probability sampling amonga population of alternatives, the heart of Monte Carlo methods. }\autocite[429]{Fogel2002}}

\Anotecontent{note_pattee_semantic_closure}{ \foreignquote{english}{Additionnary, from an epistemological point of view, Pattee(1995b) points out taht symbolic information (such as that contained in an organisms's genes) has \enquote{no instrinsic meaning outside the context of an entire symbol system as well as the material organization that constructs(writes) and interprets(reads) the symbol for a specific function, such a classification, control, construction, communication ...}. He argues that a necessary condition for an organism to be capable of creative open-ended evolution is that it encapsulates this entire self-referent organisation (Pattee refers to this condition as semantic closure). From this it follows that organisms should be constructed \enquote{with the parts and the laws of an artifical physical world} Pattee (1995a)(p.36). In other words, the interpretation (phenotype) of the symbolic information (genotype) of an artificial organism should be constructed and act within the artificial physical environment of the system. Additionally, if the system is to model the \enquote{origin} of genetic information, then the genotype itself must also be embedded within the environment; that is, the complete semantically-closed organisation -- the \enquote{entire organism} -- must be completely embedded within the physical environment.} \autocite{Taylor2001}}

\Anotecontent{np_complet_def}{ \foreignquote{english}{Identifying which combinatorial problems are easy to solve and which are hard is an important and challenging task, which has occupied theoretical computer scientists for many years. In order to translate the everyday expression \enquote{easy to solve} to mathematical theory the concept of polynomial time algorithms has been introduced. An algorithm is said to run in polynomial time if there is a polynomial $p$ such that the algorithm applied to an input of size $n$ always finds a solution in time $p(n)$, that is after performing $p(n)$ simple instructions. Note that we measure the worst case complexity, that is the time in which we are sure that the algorithm ends regardless of which input of size $n$ we have fed it. The execution time of a polynomial time algorithm grows slowly enough with increasing input size to be able to be run on a computer, but if the execution time grows exponentially the algorithm is useless for all but the smallest inputs. One of the most accepted ways to prove that a problem is hard is to prove it NP-complete. If an optimization problem is NP-complete we are almost certain that it cannot be solved optimally in polynomial time.} \autocite[1]{Kann1992}}

\Anotecontent{texte_csunplugged}{Le \href{http://csunplugged.org/}{@site} officiel présente le projet ainsi : \foreignquote{english}{CS Unplugged is a collection of free learning activities that teach Computer Science through engaging games and puzzles that use cards, string, crayons and lots of running around. The activities introduce students to Computational Thinking through concepts such as binary numbers, algorithms and data compression, separated from the distractions and technical details of having to use computers. Importantly, no programming is required to engage with these ideas! CS Unplugged is suitable for people of all ages, from elementary school to seniors, and from many countries and backgrounds. Unplugged has been used around the world for over twenty years, in classrooms, science centers, homes, and even for holiday events in a park!}}

\Anotecontent{traduction_unplugged}{Le livre a été traduit par une partie de l'équipe d'\href{https://interstices.info/}{@Interstices}, et de l'EPI, il est téléchargeable à cette  \href{https://interstices.info/jcms/c_47072/enseigner-et-apprendre-les-sciences-informatiques-a-lecole}{@adresse}.}

\Anotecontent{pensee_informatique}{Le terme original \textit{Computational Thinking} vient assez logiquement de Seymour Papert et son travail sur LOGO (inspiré par quatre années passé avec Jean Piaget, l'enseignant mathématicien George Polya qui a marqué de nombreux chercheurs par sa méthode pédagogique, la rencontre avec Minsky, l'école Bourbaki, etc. \autocite{Catlin2014}) mais il a été remis au gout du jour par Jeannette Wing, directrice et professeur du département Informatique du Carneggie Melon.  \href{https://interstices.info/jcms/c_43267/la-pensee-informatique}{@traduction} en français de l'article de \textcite{Wing2006} est proposé par \href{https://interstices.info/}{@Interstices}, dont voici un extrait :
C'est le terme employé pour désigner le socle de connaissance lié à la discipline informatique, indépendamment des langages de programmation : \enquote{ La pensée informatique constitue pour nous tous un savoir fondamental, pas seulement pour les informaticiens. Au même titre que la lecture, l'écriture ou l'arithmétique, nous devrions la transmettre à nos enfants. Alors que l'imprimerie a permis la diffusion des trois premiers savoirs (lire-écrire-compter), les technologies numériques véhiculeront cette pensée informatique. Mais la situation est particulière, car c'est précisément cette pensée informatique qui a servi à développer ces technologies. Adopter un mode de pensée informatique conduit à résoudre des problèmes, à concevoir des systèmes et à comprendre le comportement humain différemment, en s'appuyant sur les concepts fondamentaux de la discipline informatique et en y incluant une panoplie d'outils intellectuels qui reflètent l'étendue de la science qu'est l'informatique.} Dans ce socle de connaissance, on trouve évidemment les principes universel d'algorithmie, de décomposition, d'abstraction, de reconnaissance de forme; mais la notion de pensée informatique va au delà, et mobilise ces concepts dans un contexte éducatif. Interviennent alors les capacités de créativité, la découverte, l'apprentissage, l'interactivité, la simulation tel qu'ils sont mobilisés dans les réflexions et les outils activateurs de cette synergie entre concept et utilisateurs tel que développés depuis les années 1970 par Papert, puis Restnick, etc.}

\Anotecontent{billet_weise}{Voir le \href{http://blog.it-weise.de/p/309}{@billet} daté de juin 2014.}

\Anotecontent{note_pengouin}{Que faut il penser par exemple d'un algorithme bio lorsqu'il est nommé \foreignquote{english}{Pengouin Search Optimization Algorithm} (PSOea) \autocite{Gheraibia2013} ? }

\Anotecontent{stochastic_note}{Si l'optimisation stochastique (\textit{stochastic optimization}) ou approche probabiliste de l'optimisation (\textit{probabilistic approaches} apparait comme un autre chapeau susceptible de pouvoir englober l'ensemble de ces techniques, le schéma \ref{fig:S_OverviewOptimisation} de Weise contredit ce constat. Il existe en effet dans cette vaste catégorie tout un ensemble de techniques (\textit{Hill Climbing}, \textit{Simulated Annealing}, etc.) qui diffèrent très fortement dans leur structure, leur définition, ou leur inspiration, de la branche de techniques qui nous préoccupe ici, à savoir l'EC. }

\Anotecontent{equipe_mixite}{Suivant les travaux menés dans notre équipe par \textcite{Reuillon2015}, la validité de ce dernier paragraphe est clairement remise en question. L'originalité de ces derniers résident dans la mixité de ces deux objectifs. En intégrant \enquote{la capacité d'extension spatiale} dans l'exploration de ces espaces comme un critère d'optimisation supplémentaire aux objectifs plus classique de recherche de minima, une cartographie dirigée et plus exhaustive est devenue possible.}
%En intégrant l'\enquote{exploration de cette espace} des solutions (si on veut découvrir une carte  exhaustive des solutions optimisés), ou de l'espace de recherche (si on veut cartographier l'espace de recherche menant à cet espace de solution optimisé) comme un objectif d'optimisation supplémentaire aux objectifs plus classique de recherche de minima, une cartographie dirigé et plus exhaustive de certaines zones de cet espace est devenu possible.}


\Anotecontent{amoral_reproductible}{La volonté des auteurs d'une démarche exemplaire ayant vocation à être reproduite et diffusé apparaît plusieurs fois dans le manuscrit \autocite{AMORAL1983}, en introduction \enquote{Conscient de l'intérêt que cette approche modélisée par Analyse Systémique pouvait apporter, le groupe a tenu à présenter toute l'information concernant le modèle AMORAL, allant bien au-delà de sa présentation et de son mode d'emploi. Nous espérons ainsi avoir ouvert une voie qui, semble-t-il, mérite d'être explorée et expérimentée pour l'aide à la prise de décisions au niveau micro-régional.}, ou bien en conclusion : \enquote{Pour en faciliter la pratique toute l'information le concernant est accessible directement dans ce rapport : le dictionnaire de toutes les variables, tables et coefficients [...], les tables fixées en fonction des types de relations intervenant dans le modèles, le graphe complet des relations [...], le listing du programme comportant toute les équations du modèle. Cette transparence totale du travail et de ses fondements assurera peut-être une meilleur diffusion de ce rapport qui contient à la fois une approche méthodologique nouvelle pour l'aménagement du territoire, et un outil pratique d'aide à la décision.}}


\Anotecontent{amoral_note}{ La préface du rapport \autocite{AMORAL1983} et les différentes publications \autocite{Guermond1984} sur ce modèle initié avant les années 1980 sont très explicites sur l'apport méthodologique, et théorique des Systèmes Dynamiques à la Forrester. A ce sujet Marie-Geneviève Durand, alors ingénieur de recherche, raconte \enquote{Très vite, cette recherche appliquée faisant appel à une méthodologie nouvelle en la matière, nous renvoyait à la recherche fondamentale. Par voie de conséquence, le présent rapport [...] s'enrichit d'une information nécessitée par le caractère innovateur de la méthode autant que par l'utilisation du modèle. [...] Il a fallu aussi s'habituer à manipuler cette dynamique intrinsèque au système modélisé. Il en résulte un nouvel état d'esprit et une toute autre compréhension des phénomènes. Ce travail s'est donc révélé fécond, nous conduisant à réflechir sur notre propre discipline, à tester et affiner la méthode.} \autocite{AMORAL1983}}

\Anotecontent{def_meta_sorensen}{\foreignquote{english}{A metaheuristic is a high-level problem-independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic optimization algorithms (Sörensen and Glover, 2013). [...] A problem-specific implementation of a heuristic optimization algorithm according to the guidelines expressed in a metaheuristic framework is also referred to as a metaheuristic. The term was coined by \textcite{Glover1986} and combines the Greek prefix meta- (metá, beyond in the sense of high-level) with heuristic (from the Greek heuriskein or euriskein, to search)} \autocites{Sorensen2013a, Sorensen2013b} }

\Anotecontent{def_meta_weise}{C'est également ainsi que \textcite[36, 225]{Weise2011} comprend ce terme \foreignquote{english}{A metaheuristic is a method for solving general classes of problems. It combines utility measures such as objective functions or heuristics in an abstract and hopefully efficient way, usually without utilizing deeper insight into their structure, i. e., by treating them as black box-procedures}}

\Anotecontent{greedy_description}{Un \enquote{choix optimal local} est réalisé à chaque itération durant l'optimisation, ce qui produit en général des solutions viables mais très rarement optimales.}

\Anotecontent{q_ppr}{Questions tirés du wiki \textit{Portland Pattern Repository} (\href{http://c2.com/cgi/wiki?MetaHeuristic}{@PPR}), qui est au passage un des premier wiki sur le web (1995)}

\Anotecontent{paysage_cumule}{En ce sens, la figure \ref{fig:spacePspaceOmultimodal} peut représenter tout autant \begin{enumerate*}[label=(\alph*)]
\item un ensemble de population de solutions candidates désignées en amont par un plan d'expérience, évaluées en une seule passe par l'optimiseur, puis projetté dans l'espace des objectifs
\item ou, un cumul de population candidates évaluées représentatif du fonctionnement de l'optimiseur étalé sur plusieurs passes, celui-ci manipulant entre chaque itération non pas une population, mais un seul individu solution candidate. \end{enumerate*} }

\Anotecontent{test_fonction_surutilisation}{Sous l'impulsion remarquée de quelques chercheurs \autocite{Zitzler1999a, Zitzler1999b, Fonseca1996}, différentes approches ont étés formalisés ces dernières années pour mieux mesurer et comparer les performances de ces différents algorithmes d'optimisation. Ces approches et ces mesures sont régulièrement comparées, et utilisées de façon complémentaire dans la littérature informatique pour confronter au mieux les nouveaux algorithmes dont on rapelle qu'ils sont stochastique, et donc plus difficile à évaluer les uns rapport aux autres \autocite{Coello2006}. Cette évolution passe par la création d'un certain nombre de \textit{benchmark}, avec par exemple l'évaluation par les algorithmes de set de fonctions mathématiques (comme ceux de Schwefel et DeJong)dont on connait la forme du front de Pareto \autocites[580]{Weise2011}[138]{Back1996}, en utilisant diverses mesures (sur le paysage \autocite[163]{Weise2011}, sur les opérateurs de comparaisons utilisés pour différencier les algorithmes \autocite{Zitzler2003, Zitzler2007}, etc.) et leur mise en oeuvre automatisée par le biais de plateformes dédiées, comme la plateforme COCO (COmparing Continuous Optimisers) \autocite{Hansen2011} utilisée depuis 2009 pour les conférences prestigieuses de GECCO. Nécessaire dans l'établissement théorique de la discipline, un point de vue réflexif et critique des chercheurs dans cette discipline ont pour effet d'engager de nouvelles réflexions, et paradoxalement, parce qu'il faut bien trouver un moyen de comparer ces algorithmes, invente de nouvelle fonctions tests et de nouvelle mesures : \foreignquote{english}{Zitzler et al. 2000 stated that, when assessing performance of an MOEA, one was interested in measuring three things: 1. Maximize the number of elements of the Pareto optimal set found. 2. Minimize the distance of the Pareto front produced by our algorithm with respect to the global Pareto front (assuming we know its location). 3. Maximize the spread of solutions found, so that we can have a distribution of vectors as smooth and uniform as possible. This, however, raised some issues. First, it was required to know beforehand the exact location of the true Pareto front of a problem in order to use a performance measure. This may not be possible in real-world problems in which the location of the true Pareto front is unknown. The second issue was that it is unlikely that a single performance measure can assess the three things indicated by Zitzler et al. 2000.} \autocite{Coello2006} Différents auteurs, dont certains pionniers créateurs de ces même fonctions, se sont récemment adressés à la communauté sur ce sujet, en demandant si possible d'arreter d'utiliser ces fonctions types. \hl{ref à rajouter}}

\Anotecontent{remarque_section_metaheuristique}{Ce constat n'est pas forcément évident avec les exemples utilisés jusqu'à présent, mais il faut imaginer que l'optimiseur va jouer avec les valeurs $x$ et $y$ en fonction de leur espace respectif et des contraintes possiblement associés, et \textbf{non pas en se déplacant physiquement} sur le plan 2D $(x,y)$, que l'on a utilisé ici avant tout pour des facilité de représentation. Les qualités topologiques de cet espace (dans quel cluster de solution candidates je me situe ? ou se situe les prochains clusters voisins intéressant à explorer ? les clusters de valeur $v$ sont il très homogènes, très hétérogènes etc.) qui pourraient effectivement permettre de dégager des informations utiles dans la sélection des futures solutions candidates ne sont généralement pas pris en compte de façon initiale par la plupart des métaheuristiques que nous allons étudier, à moins qu'on ne leur en donne les moyens. L'expertise de l'espace des solutions candidates évalués, ou espace des objectifs, est bien plus souvent mobilisé pour motiver les nouvelles solutions candidates à évaluer, comme on va pouvoir le découvrir dans la section suivante. Il existe donc de nombreuses possibilités pour intégrer diverses connaissances améliorant les choix de l'optimiseur, la prospection intelligente des différents espace à sa disposition en fait partie.}

\Anotecontent{note_knapsack}{Dans le problème du sac-à-doc, ou \textit{Knapsack problem}, il s'agit de trouver la combinaisons idéale d'objets disposant d'une masse et d'une valeur, en essayant de maximiser la somme calculée à partir de la valeur des objets que l'on arrive à entrer dans le conteneur. C'est un problème d'optimisation discret NP-Complet, difficile à résoudre lorsque le nombre d'éléments pris en compte augmente, au même titre que le voyageur de commerce.}

\Anotecontent{notation_dominance}{On trouve également cette notation sous la forme inverse dans la littérature. C'est par exemple le cas dans les écrits de \textcites{Deb2000a,Deb2002}, créateur de l'algorithme multi-objectifs très utilisé et très connu nommé NSGA2. La relation de domination entre deux éléments est écrite en utilisant le symbole inverse $x_1 \prec x_2$, signifiant $x_1$ domine $x_2$. Une des explications possible est la suivante : \foreignquote{english}{$x^* \succ x$ to indicate that $x^*$ dominates $x$.This notation can be confusing because the symbol $\succ$ looks like a \enquote{greater than} symbol but since we deal mainly with minimization problems, the symbol \enquote{$\succ$} means the function values of $x^*$ are less than or equal to those of $x$. However this notation is standard in the literature, so this is the notation that we use.} Le signe serait donc couramment retourné pour éviter une possible confusion. \hl{ref à ajouter : A Tutorial on the Performance Assessment of Stochastic Multiobjective Optimizers et Zitzler2003 ... A voir dans goldberg1989 puisque c'est lui le premier qui l'a utilisé si on en croit certain papier} }

\Anotecontent{formation_informatique}{\enquote{La question de la formation sérieuse des géographes aux outils de l’informatique est centrale pour une meilleure utilisation des SIG en géographie et une meilleure participation des géographes au projet de la géomatique. Pour en être triviale, elle n’en est pas moins fondamentale.}\autocite[480]{Joliveau2004}}

\Anotecontent{aquoicelasert}{A quoi cela sert d'apprendre les formules d'un modèle gravitaire, de modèle d'auto-organisation, et plus généralement de modèle spatio-temporel dynamique si on est incapable de le mobiliser en dehors de la feuille de papier !?}

\Anotecontent{essouflement_genet}{ On trouve une interview vidéo de Jean-Philippe Genet lors du séminaire Fichet-Heynlin sur le \href{http://www.reseau-terra.eu/article1309.html}{@site} du réseau Terra, accompagné de commentaires mettant en perspective ce témoignage par rapport à deux acteurs (Frédéric Clavert et Jean-Philippe Genet) pratiquant le numérique à des époques différentes, et aux avancées actuelles chez les historiens et l'informatique.}

\Anotecontent{joliveau_peur}{\enquote{Régulièrement, des appels ont été lancés pour prévenir les géographes du risque d’être exclus du prochain train de l’informatique géographique. 1969, S. Rimbert : « Faut-il laisser aux ingénieurs, aux architectes, aux sociologues, le soin de multiplier des expériences qui pourraient tout aussi bien être dans leur domaine. Les géographes ont-ils une place ? » (Rimbert et Lengellé 1969). 1994, Y. Guermond : « Est-ce que ça ne va pas se passer en dehors de nous ? Est-ce qu’on ne va pas devoir ramasser les miettes ? (Guermond 1994b). 2001, M. Thériault : « Les géographes peuvent-ils se permettre d’être virtuellement exclus de tous ces domaines d’application parce qu’ils n’ont pas acquis les habiletés techniques et les connaissances fondamentales nécessaires ? » (Thériault 2001)} \autocite[481-482]{Joliveau2004}}

\Anotecontent{histoire_informatique}{Outre la littérature légère déjà citée auparavant, le lecteur intéressé par la thématique des rapports de l'histoire à l'informatique dans ses développements anciens ou plus récents pourra se référer au site du \href{http://www.menestrel.fr/}{@Menestrel}, au \href{http://histnum.hypotheses.org/}{@blog} de l'historien Frédéric Clavert, mais également dans les publications suivantes de \textcite{Deuff2014}, de \textcite{Soulet2003}, et de \textcite{Genet1988,Genet1993, Genet2011,Genet2011}}

\Anotecontent{def_convergence}{\foreignquote{english}{An optimization algorithm has converged (a) if it cannot reach new candidate solutions anymore or (b) if it keeps on producing candidate solutions from a “small” subset of the problem space.} \autocite[251]{Weise2011}}

\Anotecontent{remarque_resolution}{Une remarque qui ouvre la possibilité d'autres questionnements, comme celui par exemple de la résolution et de la sensibilité à partir desquels on identifie deux solutions comme différentes, alors même que les algorithmes sont entrainés à faire la différence entre deux évaluations en tenant compte d'écarts infimes ? Cette remarque est également valable pour l'échantillonage des valeurs de $x$, si l'optimiseur est contraint de sélectionner les valeurs selon un seuil de résolution fixé par un seuil (un pas de 0.1 par exemple), ne prend-t-on pas le risque important de passer à coté d'optimum plus intéressants ?}

\Anotecontent{note_weak}{\foreignquote{english}{[...] In fitness landscapes with weak (low) causality, small changes in the candidate solutions often lead to large changes in the objective values, i. e., ruggedness. It then becomes harder to decide which region of the problem space to explore and the optimizer cannot find reliable gradient information to follow. A small modification of a very bad candidate solution may then lead to a new local optimum and the best candidate solution currently known may be surrounded by points that are inferior to all other tested individuals. The lower the causality of an optimization problem, the more rugged its fitness landscape is, which leads to a degeneration of the performance of the optimizer [1563].} \autocite[162]{Weise2011}}

\Anotecontent{note_strong}{\foreignquote{english}{During an optimization process, new points in the search space are created by the search operations. Generally we can assume that the genotypes which are the input of the search operations correspond to phenotypes which have previously been selected. Usually, the better or the more promising an individual is, the higher are its chances of being selected for further investigation. Reversing this statement suggests that individuals which are passed to the search operations are likely to have a good fitness. Since the fitness of a candidate solution depends on its properties, it can be assumed that the features of these individuals are not so bad either. It should thus be possible for the optimizer to introduce slight changes to their properties in order to find out whether they can be improved any further. Normally, such exploitive modifications should also lead to small changes in the objective values and hence, in the fitness of the candidate solution.} \\ La définition donnée par Weise pour une \textit{strong causality} est donc la suivante \foreignquote{english}{Strong causality (locality) means that small changes in the properties of an object also lead to small changes in its behavior.} \autocite[161]{Weise2011}}

\Anotecontent{note_elitisme}{Les stratégie d'élitisme visent à s'assurer que les meilleures solutions ne seront jamais perdues, et cela quelque soit le déroulement de l'algorithme : \foreignquote{english}{No matter how elitism is introduced, it makes sure that the fitness of the population-best solution does not deteriorate. In this way, a good solution found early on in the run will never be lost unless a better solution is discovered. The absence of elitism does not guarantee this aspect.} Une propriété qui n'est pas garantie dans les premières générations d'algorithmes EA d'optimisation multi-objectifs. On trouvera plus de détail sur ce terme dans les pages d'ou cette précédente définition a été tiré \autocite[239-240]{Deb2001}. L'introduction de ce concept, bien que daté de \hl{(ref De Jong, 1975)}, est attribué à Zitzler dans sa version multi-objectif si on en croit \textcite{Coello2006}, qui en donne la définition suivante \foreignquote{english}{In the context of multi-objective optimization, elitism usually (although not necessarily) refers to the use of an external population (also called secondary population) to retain the nondominated individuals found along the evolutionary process. The main motivation for this mechanism is the fact that a solution that is nondominated with respect to its current population is not necessarily nondominated with respect to all the populations that are produced by an evolutionary algorithm. Thus, what we need is a way of guaranteeing that the solutions that we will report to the user are nondominated with respect to every other solution that our algorithm has produced. Therefore, the most intuitive way of doing this is by storing in an external memory (or archive) all the nondominated solutions found. If a solution that wishes to enter the archive is dominated by its contents, then it is not allowed to enter.}}

\Anotecontent{martin_fowler}{L'auteur informaticien Martin Fowler, qui s'est intéressé dans plusieurs articles à l'étymologie de ce principe d'inversion de contrôle, et à sa relation proche avec l'injection de dépendance, fournit un élément de réponse sur la différence entre le concept de librairie logicielle et celui de \textit{framework} : \foreignquote{english}{Inversion of Control is a key part of what makes a framework different to a library. A library is essentially a set of functions that you can call, these days usually organized into classes. Each call does some work and returns control to the client. A framework embodies some abstract design, with more behavior built in. In order to use it you need to insert your behavior into various places in the framework either by subclassing or by plugging in your own classes. The framework's code then calls your code at these points.} L'article complet est accessible sur le \href{http://martinfowler.com/bliki/InversionOfControl.html}{@site} de l'auteur.}

\Anotecontent{sean_luke_mason}{\foreignquote{english}{In 1998, after using a variety of genetic programming and evolutionary computation toolkits for my thesis work, I decided to develop ECJ, a big evolutionary computation toolkit which was meant to support my own research for the next ten years or so. ECJ turned out pretty well: it’s used verywidely in the evolutionary computation field and can run on a lot of machines in parallel. [...] One common task (for me anyway) for evolutionary computation is the optimization of agent behaviorsin large multiagent simulations. ECJ can distribute many such simulations in parallel across simultaneousmachines. But the number of simulations that must be run (often around 100,000) makes it fairly important to run them very efficiently. For this reason I and my students cooked up a plan to develop a multiagentsimulation toolkit which could be used for various purposes, but which was fast and had a small and cleanmodel, and so could easily be tied to ECJ to optimize, for example, swarm robotics behaviors.} \autocite[8]{Luke2014}}

\Anotecontent{basic_histoire}{Le langage BASIC, qui a initié des milliers d'étudiants à la programmation sur différentes plateforme, a fêté en 2014 ses cinquante ans. On peut trouver des informations, et des témoignages des créateurs sur le \href{http://www.dartmouth.edu/basicfifty/}{@site} spécial de l'université de Darthmouth.}

\Anotecontent{sean_luke_ecj}{\foreignquote{english}{ECJ is an evolutionary computation framework written in Java. The system was designed for large, heavy- weight experimental needs and provides tools which provide many popular EC algorithms and conventions of EC algorithms, but with a particular emphasis towards genetic programming. ECJ is free open-source with a BSD-style academic license (AFL 3.0). ECJ is now well over ten years old and is a mature, stable framework which has (fortunately) exhibited relatively few serious bugs over the years. Its design has readily accommodated many later additions, including multiobjective optimization algorithms, island models, master/slave evaluation facilities, coevolution, steady-state and evolution strategies methods, parsimony pressure techniques, and various new individual representations (for example, rule-sets). The system is widely used in the genetic programming community and is reasonably popular in the EC community at large. I myself have used it in over thirty or forty publications.} \autocite[7]{Luke2014b}}

\Anotecontent{sean_luke_masondifficile}{\foreignquote{english}{MASON is not an easy toolkit for Java beginners. MASON expects significant Java knowledge out of its users. If you are a rank beginner, allow me to recommend NetLogo, a good toolkit with an easy-to-learn language. [...] Finally MASON does not have plug-in facilities for Eclipse or NetBeans, though it can be used quite comfortably with them. If you’re looking for a richer set of development tools, you might look into Repast. }\autocite[8]{Luke2014}}

\Anotecontent{sean_luke_ecjdifficile}{\foreignquote{english}{A toolkit such as this is not for everyone. ECJ was designed for big projects and to provide many facilities, and this comes with a relatively steep learning curve.} \autocite[7]{Luke2014b}}

\Anotecontent{coello_note}{Le chercheur \textcite{Coello2015} est un auteur régulier d'états de l'art \autocite{Coello1999, Coello2000, Coello2007} ou d'articles \autocite{Coello2006} sur l'historique de cette branche spécifique de l'EC concentré sur la résolution de problèmes multi-objectifs maintient également sur son \href{http://www.lania.mx/∼ccoello/EMOO/}{@site} une base de données bibliographiques riche à ce jour de plus de 9000 entrées.}

\Anotecontent{reflexion_DeJong}{\foreignquote{english}{By understanding the role each element plays in the overall behavior and performance of an EA, it is possible tomake informed choices about how the elements should be instantiated for a particular application. At the same time, it is clear that these components interact with each other so as to affect the behavior of simple EAs in complex, nonlinear ways. This means that no one particular choice for a basic element is likely to be universally optimal. Rather, an effective EA is one with a co-adapted set of components.}\autocite[70]{DeJong2006a}}

\Anotecontent{mcts_go}{C'est le cas par exemple de la classe d'heuristiques dites de \textit{Monte-Carlo Tree Search} (MCTS) \autocites{Browne2012, Bouzi2014} dont le perfectionnement successif a permis l'émergence de quelques programmes \autocite{Coulom2006} aux réussites notables lors de compétitions mondiales de GO, assurant un rayonnement plus large à cette technique en intelligence artificielle, avec des bénéfices dans le domaine des méta-heuristiques qui nous intéresse ici \autocite[4]{Wang2012}. On trouvera plus d'informations sur ces récents exploits éléctroniques dans les articles de \href{http://www.wired.com/2014/05/the-world-of-computer-go/}{@Wired}, du \href{http://rfg.jeudego.org/item/122-la-guerre-sainte-electronique}{@New-York Times} et d'\href{https://interstices.info/jcms/c_43860/le-jeu-de-go-et-la-revolution-de-monte-carlo}{@Interstices}}

\Anotecontent{tromp_appel_calcul}{Tromp a lancé un \href{http://tromp.github.io/go/legal.html}{@site} internet pour collecter la ressource disponible nécessaire pour ce calcul. Celui-ci estime la charge de travail à fournir pour obtenir un résultat à 10 à 13 serveurs, possédant au moins 8 processeurs et 512 Gb de RAM, pendant 5 à 9 mois.}

\Anotecontent{odersky_note_cake}{\foreignquote{english}{We argue that, at least to some extent, the lack of progress in component software is due to shortcomings in the programming languages used to define and integrate components. Most existing languages offer only limited support for component abstraction and composition. This holds in particular for statically typed languages such as Java, and C\# in which much of today's component software is written.} \autocite{Odersky2005}}

\Anotecontent{note_informatique_mixin}{Cette footnote est plus à destination d'un public informaticien. On apelle \keywordmin{mixin} cette abstration informatique qui permet de composer différents \keywordmin{trait} en Scala. Si un \keywordmin{trait} apparait de prime abord similaire à une interface ou une classe abstraite supportant comme ces deux dernières l'héritage, le \keywordmin{mixin} inclut bien cette dernière propriété mais s'avère \textit{in extenso} beaucoup plus puissante. En effet, un \keywordmin{trait} est abstrait, supporte l'héritage multiple, ne tient pas compte de l'ordre d'association, et supporte une mixité du niveau d'abstraction associé à la déclaration des types, des méthodes, et des variables implémentées. Comme on l'apercoit dans \ref{fig:principe_mixin}, il est possible de décorer dynamiquement les classes par le biais de cette composition, sans se soucier d'un ordre quelconque. Associé au \textit{self-type annotation} les traits supportent également des références cycliques entre composants (A dépend de B, B dépend de A). L'utilisation plus ou moins cumulatives de ces abstractions permet une grand flexibilité, il n'y a donc pas une technique de \textit{Cake Pattern}, comme pourrait le sous entendre le nom, mais de multiples variations. Plus de détail sur ces abstractions peuvent être trouvés dans le papier original d'\textcite{Odersky2005}}

\Anotecontent{cas_utilisation_wfom}{\hl{a raffiner}Le cas d'utilisation étant inédit sur la plateforme openMOLE, de très nombreuses heures ont été nécessaire pour réaliser et tester les premiers workflow organisant l'execution d'un algorithme génétique sur une grille de calculs. De part les toutes nouvelles limites imposés par ce travail (puissance nécessaire, durée d'execution, complexité du worklow présenté) ce cas d'utilisation organisé autour de la calibration d'un modèle de simulation a permis aux différents acteurs du projet de progresser sur plusieurs fronts à la fois, sur la grammaire de composition de workflow dans openMOLE, sur la conception de MGO, sur les nouvelles possibilités permise par leur couplage, mais également sur la définition des fonctions objectifs, et sur l'analyse des résulats.}

\Anotecontent{borillo_note}{Il est a noter que Mario Borillo est recruté à Marseille au CADA par J.C. Gardin, et reprend en 1971 ce laboratoire d’archéologie pionnier sur les usages computationel en Archéologie. On notera que le congrès de Marseille en 1969,1971 organisé par Gardin sur les usages computationels en Archéologie est un moment important dans l’archéologie au niveau national mais également international \autocite{Whallon1972}. Après plusieurs années à diplore le réseau LISH, il va à Toulouse ou il rejoindra l’IRIT dès sa fondation, un laboratoire d’informatique à Toulouse qui joue plus ou moins indirectement toujours un rôle important dans l’accompagnement technologique des SHS, via les productions d’outils et les interactions très fortes de certains de ses membres informaticiens avec les sciences humaines et les géographes (L’équip SMAC par exemple) \autocite{Aurnague2014}}

\Anotecontent{sylvain_deuxiemephase}{\enquote{Par exemple, le stage de formation ayant eu lieu à Rouen en 1982 introduit cette deuxième phase et porte sur l’analyse des systèmes. Il ne s’agit plus de la seule acquisition de méthodes statistiques ou  mathématiques. [...] L’évolution entre ces deux périodes réside également dans le fait que les géographes, formés dans les années 1970 par des statisticiens ou des informaticiens, prennent eux-mêmes en charge la formation des nouvelles générations à partir des années 1980 mais surtout 1990. Le mouvement théorique et quantitatif devient donc auto-suffisant et peut se reproduire par lui-même. Dans les années 2000, de nouvelles générations prennent en charge les stages, accompagnées des pionniers du mouvement théorique et quantitatif en géographie.} \autocite[321]{Cuyala2014}}

\Anotecontent{note_amoral_difficulté}{Le premier contact avec la modélisation par Analyse Systémique a été celui du rapport Meadows présenté par le Club de Rome sous le titre \enquote{Halte à la Croissance ?}, et le premier intérêt méthodologique a été suscité par F. Renchenman de l'IRIA et P. Uvietta de l'IMAG qui ont considérablement contribué à dédramatiser une approche qui paraissait hors de nos possibilités.\autocite[p129]{Guermond1984}}

%Voir pour ajouter : La lettre d'Histoire Moderne et Contemporaine et Informatique, la lettre d'information du groupe Histoire et informatique,
 % ISHA Paris 4 'Institut des Sciences Humaines Appliquées qui publie “Informatique et Science humaines” publié % http://www.paris-sorbonne.fr/presentation-3133

\Anotecontent{litterature_legere}{On trouve plus d’informations dans la littérature grise de cette époque, la \enquote{feuille d’avis du LISH}, les Bulletins de la MSH relatant les activités du LISH, mais également dans les compte rendus et les activités de certaines disciplines formées à l’informatique à cette période, comme les lettres, l’histoire, l’archéologie, la sociologie. On retrouve ainsi des informations pratiques et techniques (tutoriels, programmes, conseils) dans les journaux comme \enquote{Le médiéviste et l’ordinateur} de l’IHRT 1979-2003, \enquote{archéologie et ordinateur} publié de 1982 à 1995 ,  \enquote{Informatique et sciences humaines} publié par GEMAS/Paris 4  (? -- ?) , \enquote{Programmation et sciences de l'homme} édité par l'ENS à partir de 1980, les cahiers spéciaux de l'AFCET  etc. Il est d’ailleurs étonnant de ne pas trouver plus de discussions abordant ces aspects “techniques” d’accès à la ressource, de programmation, de manipulation techniques; bref de “bidouilage” il faut bien le dire, dans la littérature grise des années 1970, comme par exemple celle des “Brouillons Dupont”.}

\Anotecontent{centre_formation}{Il faut savoir que ces centres nationaux, aujourd’hui connus sous le nom d’IDRIS (ex-CIRCE) et du CINES (ex-CNUSC), dispensent toujours des formations à destination des scientifiques, quelque soient leurs disciplines de rattachement; il n’est donc pas trop tard pour apprendre le Fortran.}

\Anotecontent{presentation_cnusc}{Pour en savoir plus sur l'activité, et les services matériels et logiciels de ce centre, on pourra se référer à la présentation faite par \textcites{Lelouche1982, Lelouche1982b} dans le journal du \enquote{médiéviste et l'ordinateur} }

 \Anotecontent{consommateur_data}{Une étape qu'il convient de rapeller aujourd'hui chez les consommateurs aveugle de Data. D'une part de telles bases se construisent, ce qui implique un temps de collecte, et un temps de construction menant à des choix d'interprétation, de conceptualisation, et de traitements des données parfois irreversible. La Data n'est pas donc jamais totalement neutre, sauf peut être dans quelques circonstances et dans quelques disciplines.}

\Anotecontent{massonie_texte}{\enquote{En 1969, la Faculté des Lettres et Sciences Humaines de Besançon créait un poste de mathématique. Un an plus tard un poste d'assistant était a son tour créé. Les premiers clients furent les géographes, puis vinrent les historiens, les sociologues et enfin les littéraires. L'utilisation de l'analyse des données et donc de l'ordinateur devint non pas une mode, mais un instrument de plus dans l'arsenal des différentes disciplines. Il ne s'agissait pas de faire faire une thèse qui utilise les méthodes nouvelles, mais de faire une thèse de géographie ou d'histoire.}\autocite{Massonie1986}}


\Anotecontent{batty_code}{\foreignquote{english}{Ok I started programming in 1966 using Atlas Autocode that was a forerunner to Algol which was then merged into Pascal. Its was a declarative language where you had to define all the variables you used. I then moved to Fortran in 1969 when I moved from manchester to reading - in those days all the auto code stuff was based on punched tape but when i starred fortran we used punched cards. I also began with Dartmouth basic in 1971 at reading which was an interpretative language - it compiled as one went along. I stuck with Fortran until 1990 even beyond a bit - and the version i used on the PC was Waterloo Fortran 77 I think - actually the melbourne 1986 model is an avi file and won't run on mac but the earlier 1982 model is in VAX Fortan 77 and here is a picture of it all you can find a bit on these \href{http://www.complexcity.info/media/movies/early-computer-movies-1967-86/}{@pages}

If you go to \href{http://www.casa.ucl.ac.uk/movies-weblog/Melbourne-1982-Movie.mov}{@[site]} you can load the model from 1986 that we ran and see this as movie - not code as we were getting into graphics then I then learnt some UNIX and C but this was archaic and I managed to run some Fortran programs on Sun workstations. I basically then left Fortran completely and in 2003 went to Visual Basic - actually very powerful within Visual Studio and I still use this occasionally - I am debating about what to use next - My programmer Richard Milton with our big spatial interaction models uses C sharp i think - actually all these languages are sort of the same except for the object orientation that i don't find very intuitive for aggregate spatial interaction} (Correspondance privé daté du 21 mars 2014)}

%ACITER
\Anotecontent{pumain_main_cambouis}{\enquote{J’ai en partie réécrit un programme, en \enquote{mettant les mains dans le cambouis }, parce que les sorties à l’époque étaient vraiment très élémentaires. Je vous parle d’expérimentations conduites au début des années 1980 ; Patrice Langlois a connu, lui aussi, ces époques héroïques du calcul. On avait en sortie des listings... il fallait les envoyer au Circe... attendre assez longtemps des retours, pour s’apercevoir que l’on avait oublié dans la carte perforée une parenthèse ou un point-virgule... et recommencer. C’était un processus lent qui permettait de bien réfléchir entre deux envois de simulations. Et nous n’avions pas de sortie cartographique : on avait un format d’impression qui nous donnait une visualisation approximative de la carte de la ville telle qu’elle ressortait à l’issue des simulations. C’était il y a vingt ans, ce n’est pas si loin.} \autocite[154]{Mathieu2014}}

\Anotecontent{description_laurini_algo}{Il est à noter que cette école d'été en novembre 1982 dédiée aux mathématiquess de l'analyse des systèmes, dirigée par Guermond et regroupant 56 personnes sur quelques jours \autocites[320-321]{cuyala2014}{Guermond1983}, va donner lieu à un des rares livres \autocites{CGR1983,Guermond1984} contenant la description d'\textbf{algorithmes} de modèles urbains dynamiques, sous la plume de Laurini.}

\Anotecontent{esprit_micro_jeu}{ Pour \textcite[193]{Massonie1986}, \enquote{Ce n'est qu'avec l'apparition des micro-ordinateurs et surtout avec l'esprit micro, que l'utilisation des méthodes nouvelles va se répandre. Les orientations ont été prises suivant quelques principes simples:\begin{enumerate*}[label=(\alph*)]
\item travailler au plus faible coût possible,
\item ce sont les résultats obtenus dans une discipline qui sont importants,
\item un utilisateur doit investir dans sa formation de telle manière que cela ne nuise pas à son travail dans sa discipline d'origine,
\item les grands projets ambitieux demandent des crédits énormes que nous n'avions pas et trop souvent deviennent une fin, en oubliant les buts initiaux
\item un logiciel se construit avec les utilisateurs; l'informaticien n'est pas un véritable utilisateur
\item un logiciel aussi performant soit-il ne peut répondre à toutes les questions, il doit pouvoir être adapté à chaque cas
\item un logiciel ne fait que traduire pour la machine des idées de travail de l'utilisateur qui doit done rester le maitre-d'oeuvre
\end{enumerate*}}}

\Anotecontent{note_documentation_ccalcul}{Il existe une littérature dédiée à l'utilisateur, mais celle-ci semble très difficile à se procurer aujourd'hui, car probablement peu considérée sur le plan scientifique, et techniquement obsolète. Celle-ci contient néammoins une trace des usages qu'il conviendrait de sauvegarder. On notera par exemple la présence d'un \textit{Guide du nouvel utilisateur CIRCE} \autocite{LISH1980}, de listes normalisées de logiciels \textit{LISTLOG} présentes au CNUSC, d'un groupe d'utilisateurs comme celui géré par Bernard Gaulle, etc. }


\Anotecontent{massonie_1978}{Toujours sur son \href{http://jean-philippe.massonie.pagesperso-orange.fr/science/informatique.html}{@site} personnel, voici ce que dit Jean-Philippe Massonie sur l'acquisition d'un AppleII en 1978 par rapport à l'IRIS alors en place : \enquote{ AppleII : En 1978, mon labo ayant quelque argent, on achète un AppleII avec une télévision couleur et une imprimante et deux lecteurs de disquettes. Le prix ? On pourrait s'acheter maintenant deux PowerPC et une imprimante laser. Honnêtement, on voulait faire joujou.

Mais il y avait un Basic. Bon d'accord, Basic ...Alors on a programmé pour jouer. Et puis X. Luong a traduit de Fortran en Basic, le programme d'analyse des correspondances que nous utilisions. Et là, surprise on pouvait traiter des tableaux aussi grands qu'avec l'Iris. Certes le micro était moins rapide que l'Iris, mais comme on travaillait directement dessus, on arrivait à faire 3 analyses dans son après-midi, contre une seule avec l'Iris. A coté de l' analyse des correspondances, naissait une famille de programmes qui était l'ancêtre de HyperPatate, logiciel d'analyse de vocabulaire.}}

\Anotecontent{ca_simd_avantage}{\foreignquote{english}{The advantages of an architecture optimized for cellular automate (CA) simulations are so great that, for large-scale CA experiments, it becomes absurd to use any other kind of computer [...] In 1981, the frustrating inefficiency of conventional computer architectures for simulating and displaying cellular automata became a serious obstacle to our experimental studies of reversible cellular automata.} ( 1987 Physica D, Cellular Automata Machines, Normam Margolus, Tommaso Toffoli)}

\Anotecontent{CA_physical}{
voir l'essai sur Feynman par David Hillis : \href{http://longnow.org/essays/richard-feynman-connection-machine/}{@[site]}
}
\Anotecontent{relation_france}{relation avec Pomeau}

\Anotecontent{simd_def}{SIMD pour \textit{Single Instruction Multiple Datastream} est une classe de la taxonomie de Flynn's : une instruction par processeur est exécutée de façon simultanée sur de multiples données, ce qui suppose souvent un contrôle en amont.}

\Anotecontent{mimd_def}{MIMD pour \textit{Multiple Instruction Multiple Datastream} est une classe de la taxonomie de Flynn's : \foreignquote{english}{The MIMD class of parallel architecture consists of multiple processors (of any type) and some form of interconnection. From the programmer’s point of view each processor executes independently but to cooperatively execute to solve a single problem although some form of synchronization is required to pass information and data between processors.[...]The primary characteristic of a large MIMD multi-processor system is the nature of the memory address
space. If each processor element has its own address space (distributed memory), the only means of communication between processor elements is through message passing. If the address space is shared (shared memory), communication is through the memory system.} \autocite[696]{Flynn2012}}

\Anotecontent{hiebeler_parcours}{Le travail de Hiebeler avec Langton, suite à une correspondance privée :  during the summer 1989 to 1990 on Cell Sim, then after you go to Thinking Machines, and you return to Santa Fe in Oct 1992 to fall 1993 to work on SWARM C, then summer 1994 to work on SWARM objC}

\Anotecontent{ouppy_marseille}{Ajouter ref sur proocedings des Houches, France 1989 ; Hiebeler 1990}

\Anotecontent{top_500_note}{\foreignquote{english}{The most commonly known ranking of supercomputer installations around the world is the TOP500 list. It uses the equally well-known LINPACK benchmark as a single figure of merit to rank 500 of the world’s most powerful supercomputers. The often-raised question about the relation between the TOP500 list and HPCC can be addressed by recognizing the positive aspects of the former. In particular, the longevity of the TOP500 list gives an unprecedented view of the high-end arena across the turbulent era of Moore’s law [] rule and the emergence of today’s prevalent computing paradigms. The predictive power of the TOP list is likely to have a lasting influence in the future, as it has had in the past.} \autocite[845]{hum}}

\Anotecontent{DMM}{A Distributed-Memory Multiprocessor (DMM) is built
by connecting nodes, which consist of uniprocessors or of shared memory multiprocessors (SMPs), via a network, also called Interconnection Network (IN) or Switch. While the terminology is fuzzy, Cluster generally refers to a DMM mostly built of commodity components, while Massively Parallel Processor (MPP) generally refers to a DMM built of more specialized components that can scale to a larger number of nodes and is used for large, compute-intensive tasks - in particular, in scientific computing.}

%standard MPI qui marque le début d'une fin de reigne des architectures couteuse de type SIMD pour la diffusion d'architecture MIMD, dont on va voir qu'elle sont par la suite totalement démocratisé par des approches comme Beowulf.

%MPI standard à expliquer (page 1184 - 1190)

\Anotecontent{loosely_sync}{Pour simplifier, un noeud ne peut pas écrire sur le noeud de destination du message tant que celui-ci n'est pas prêt à recevoir ce message.}

\Anotecontent{sanders_couplage_spirale}{\textcite{Sanders2013} propose d'observer dans la discipline l'apparition de modèle opérant la synthèse de deux vagues d'innovations successives, Equation différentielle et ABM : \enquote{Sur cette dernière décennie cependant, on enregistre un nombre croissant de travaux
combinant, comparant, couplant des modèles issus des deux vagues de modélisation
discutées dans cette contribution.} Sanders apelle l'image d'une \enquote{avancée en spirale} pour imager \enquote{Les allers-retours que l’on peut constater entre les différentes familles de modèles, les essais de couplages, amènent à l’image de la spirale pour rendre compte de l’évolution de ce champ de recherche.} Les modèles intégrant ces différents formalismes en fonction de leur capacité à représenter au mieux les objets, ou les processus vis à vis d'un questionnement donné, la différence de point de vue entre les différents acteurs de ce champ de recherche menant à une spirale de progression qui n'est pas sans rapeller l'importance de la discussion, l'acceptation et la réutilisation des modèles au sein de la communauté comme d'un processus participant à la Validation de ceux-ci, tel que supporté par \textcite{Rouchier2013} ou \textcite{OSullivan2004}.}

\Anotecontent{exemple_simd_mimd}{ \textcite[91-92]{Openshaw2000} propose de prendre un exemple simple pour mesurer la différence de fonctionnement qu'implique l'utilisation de chacune des architectures. On ne traite ici que les plus courantes. Imaginons un problème nécessitant l'examen de 100 feuilles d'examen, chacune avec 5 questions. On dispose d'une ensemble de correcteurs pour traiter ces questions de façon parallèle.
\begin{enumerate}
	\item{\textbf{SIMD - Data Parallel}} (CM-1 par exemple) : Un superviseur envoie à chaque correcteur un lot de feuilles à traiter. Il attend que toutes les feuilles soient distribuées, puis il annonce le traitement à réaliser par chacun des correcteurs : \enquote{Tout le monde traite la première question de la feuille d'examen !}, puis une fois que tout le monde a fini et que le résultat est rapporté au superviseur, celui-ci annonce \enquote{ Tout le monde traite la deuxième question de la feuille d'examen}, etc. Si un correcteur met plus de temps à traiter une feuille, alors tout le monde doit l'attendre. Si il y a moins de feuilles disponibles à traiter que de correcteurs, alors ceux-ci ne feront rien durant la durée des traitements, ce qui représente un certain gachis de ressources.

	\item{\textbf{MIMD - Shared Memory}} Il n'y a aucun superviseur. Chaque correcteur se voit attribuer un ensemble unique de feuilles à traiter, si possible en tenant compte de sa rapidité ( on parle alors de load-balancing). Les feuilles sont disponibles sur un seul et unique tas disposé au fond de la salle, partagé par tous les correcteurs.  Le temps de complétion de la tâche correspond alors au temps pris par le plus lent des correcteurs pour corriger le tas de feuille qui lui a été attribué.

	\item{\textbf{MIMD - Distributed Memory}} Deux types de corrections alternatives sont possibles a) Chaque correcteur reçoit par le biais d'un courrier (équivalent à un message sur une interconnection réseau) une attribution de parcelle contenant un ensemble de feuilles à corriger, qu'il traitera en fonction de son propre emploi du temps. Les feuilles sont donc corrigées en parallèle par chacun des correcteurs, qui renvoie ses corrections au service central par courrier une fois seulement sa tâche terminée. b) Si le service de courrier est très rapide, alors il est plus intéressant de récupèrer chaque feuille dès qu'elle est corrigée, et on en renvoie une autre au correcteur. Ainsi, que le correcteur soit lent ou rapide, les feuilles à corriger plus ou moins longues, ou un mélange de ces différents comportements, personne n'est plus amené à perdre de temps dans ce cas.
\end{enumerate}}

\Anotecontent{idris}{Informations tirées de la page de description des deux calculateurs disponibles sur le site du laboratoire CNRS de l'\href{http://www.idris.fr/}{@IDRIS}.}

\Anotecontent{gibson}{Comme le dit pourtant l'écrivain inventeur du CyberEspace William Gibson, la pratique de l'anticipation reste un exercice en définitive beaucoup plus facile que d'imaginer le fonctionnement de sociétés vidées de ces technologies si largement acceptées et répandues aujourd'hui. \foreignquote{english}{It’s harder to imagine the past that went away than it is to imagine the future. What we were prior to our latest batch of technology is, in a way, unknowable. It would be harder to accurately imagine what New York City was like the day before the advent of broadcast television than to imagine what it will be like after life-size broadcast holography comes online. But actually the New York without the television is more mysterious, because we’ve already been there and nobody paid any attention. That world is gone.} \href{http://www.theparisreview.org/interviews/6089/the-art-of-fiction-no-211-william-gibson}{@Interview} de The Art of Fiction numéro 211}

\Anotecontent{openshaw_virus}{ \foreignquote{english}{Most geographers involved in GIS (and elsewhere social scientists) are already the hapless but seemingly willing victims of a virulent form of \enquote{let others do the programming for us} form of computer escapism. As a result, they are likely to be forever restricted to software packages they have little or no control over and which more or less determine what they can and cannot do. Others, who perhaps should now better, seem to have been lulled into complacency by the increased computing power offered by PCs. They ask \enquote{What is the point of high-performance computing when with a bit of Fortran or Pascal or C programming I can do all I want on my PC?} Indeed, some others will tell you that \enquote{what they did in 1991 on a mainframe they can now do on a PC}, while some really clever folk can do it in Unix with awk! This is all true. The point is that, sadly, thus is a very negative and backward-looking perspective. What these people are doing today is more or less what they first did, albeit with considerably greater difficulty, five or ten or twenty or more years ago, and they appear to think that what was good for them when they did research is also good for you when you do your research. This is not progress but regress! It is both simultaneously very understandable and an
unfortunate neglect of the immense potential that HPC systems have to offer.} \autocite[2]{Openshaw2000}}

\Anotecontent{bdmp_package_strasbourg}{Le package BDMP par exemple permettait de faire des analyses factorielles et des classifications, un logiciel déjà utilisé à Northwestern par les géographes pionniers américains des années 1960 \autocite{Marble2010}}

\Anotecontent{puissance_collective}{Sachant qu'1 PFlops vaut 1000 TFlops, la mise en réseau d'ordinateurs de particuliers au service du calcul scientifique , même si elle est soumis à plus d'aléa de services du fait de la nature de celle-ci, est loin d'être une ressource informatique négligeable.}

\Anotecontent{note_equipe}{Sur ce point, nous avons eu la chance, dans notre équipe, de bénéficier des pratiques cumulés de scientifiques ayant depuis les années 1970-80 toujours misé pour l'activité de modélisation sur cette double ouverture à la fois vers les autres disciplines, et vers l'innovation informatique, au moins sur les aspects logiciels.}

\Anotecontent{openshaw_revolution}{\foreignquote{english}{The world of computing underwent a quiet revolution of profound long-term significance in the early 1990s. Hillis (1992) argues that it was then in the throes of a major technological change characterised by the development of highly parallel super-computing hardware which was about to change significantly how science is done. This process is now complete. Faster supercomputers have stimulated new ways of doing science in areas that are just too complex to be handled by any other means or where there is need to analyse large volumes of data or where there is need for real-time analysis and modelling. Computer-based experimentation and simulation is increasingly being regarded as a cost-effective and very useful means of creating new knowledge. Computation has become a scientific tool of equal importance to theory and experimentation. HPC is also widely acknowledged as one of the key information technologies of the future. But so far HPC has had a minimal impact on geography and most of the social sciences.} \autocite{Turton1998}}

\Anotecontent{bulettin_intergeo_a}{L'enquête de 1981 citée par Le Carpentier est la suivante : \textit{ Groupe de travail micro-informatique de la Commission de Géographie théorique et quantitative enquête de juin 1981}. La référence donnée s'est avérée pour le moment insuffisante pour trouver trace physique de cette enquête. Il n'est pas impossible que cette enquête soit la même que celle citée en 1984 par Faugieres.}

\Anotecontent{bulettin_intergeo}{Je n'ai pu accéder directement à cette enquête pour le moment, celle-ci se trouvant probablement dans une lettre Intergeo entre 1981-1982, accessible seulement à l'institut de géographie. }

\Anotecontent{remarque_informaticien_roue}{Avec l’augmentation graduelle et rapide de la puissance disponible sur les micro-ordinateurs dans les années 1980-1990, grand nombre de programmes tournant dix ans plus tôt sur des \textit{mainframes} ou des \textit{superordinateurs} vont être remplacés par des logiciels tout à fait fonctionnels sur des PC de puissance modeste. L'émergence de ces logiciels, parfois importés de l'étranger, se fait (comme toujours aujourd'hui dans la communauté du libre) selon un sélection quasi-darwinienne dans la profusion de logiciels créés courant des années 1980. Combien de réussites peut-on compter par rapport aux efforts engendrés, un peu partout, parfois sûrement en parallèle ? Ne dit-on pas régulièrement chez les informaticiens qu'il est inutile de sans cesse vouloir recréer la roue ? }

\Anotecontent{informations_colette_cauvin}{Ces informations sont pour la plupart issues d'un document réponse à une interview par mail de Colette Cauvin, daté du 5 mai 2015.}

\Anotecontent{collette_ccsc_centre}{\enquote{Nos fiches perforées demeuraient au centre de calculss et nous avions grâce à Anne, accès à un bureau au sous-sol où nous pouvions tout laisser. On préparait nos données et les instructions de contrôle propres aux analyses que nous souhaitions dans une grande salle au sous-sol, et nous montions les entrer dans le lecteur de cartes au rez-de-chaussée. En attendant nos résultats (cela pouvait durer entre 10 mn et 1 heure) selon le nombre de chercheurs présents au centre), nous pouvions préparer d’autres données ou, merveille, faire des parties de ping-pong !  Excellente détente calmante dans certains cas où l’attente se prolongeait pour aboutir à constater une erreur de perforation qui nous faisait recommencer le circuit pour une bêtise.} (interview par mail de Colette Cauvin, daté du 5 mai 2015)}

\Anotecontent{calcul_curri}{En 2007 le CURRI sera ensuite intégré/fusionné dans le projet de méso-centre de l'Université de Strasbourg UdS. Si des travaux sont toujours en cours avec le méso-centre au niveau du laboratoire Image et Ville, cette intégration a probablement modifié la nature et les modalités d'accès à cette ressource informatique, du fait entre autre de l'élargissement des publics. Sur ce point des recherches restent à faire.}

\Anotecontent{lena_geopoint}{Une régression multiple peut être exploratoire ou confirmatoire, idem pour un modèle agent (Léna Geopoint2000)}

\Anotecontent{rupture_openshaw}{Même si ce modèle est critiquable en plusieurs points, il nous aura fallu presque trente ans, une équipe inter-disciplinaire rodée, et l'appui de divers partenaire institutionel pour produire une expertise technique similaire au prototype réalisé par Openshaw en 1988. Il va s'en dire que même au royaume-uni Openshaw et son équipe ont du rester quelque temps pionniers, voire peut être même incompris chez leur contemporains géographes anglais. REF. On retrouve aussi cette logique de construction de modèle par famille dans }

\Anotecontent{rq_depassement_shs}{Rejoignant le constat du chapitre 1, cela prouve aussi que les sciences humaines et les géographes ont été capables d'apprendre l'informatique et de surmonter des obstacles bien plus importants que ceux pouvant se dresser devant nous aujourd'hui pour accéder au HPC (ce qui explique aussi le manuel d'Openshaw sur le HPC \autocite{Openshaw2000}, qui croyait vraiment à cette possibilité de dépassement dans la discipline), preuve que lorsque les enjeux scientifiques sont à la hauteur, rien n'est impossible.}

\Anotecontent{comparaison_tianhe_idris}{


}

\Anotecontent{appui_academie_science}{ Extrait de l'avis donné par la CNN en juin 2013 : \enquote{[...] Le Conseil National du Numérique s'associe donc à cette initiative de l'académie des sciences et compte s'appuyer sur les analyses et les conclusions de ce rapport dans des travaux futurs. [...] Le Conseil National du Numérique propose de contribuer à une réflexion focalisée sur la méthode qui permettra d'atteindre un objectif simple : généraliser d'ici trois ans l'enseignement de l'informatique depuis l'École jusqu'au lycée.}}

\Anotecontent{humanite_digitale_histoire}{La revue \textit{Computer in the Humanities} date par exemple de 1966 ... Par ailleurs, il n'est pas étonnant de retrouver parmis les acteurs de cette cartographie, les historiens de Paris 1, dont le rapport avec l'informatique prend racine dans un mouvement et dans des travaux entamés au début des années 1970 \autocites{Deuff2014, Genet1988}, et qui s'ancre rapidement dans une réflexion plus internationale de ce mouvement \autocite{Genet1993}.}

\Anotecontent{joliveau_pgeorge}{A propos des arguments donnés par George1972, Joliveau remarque : \enquote{Ce qui est saisissant dans ce texte de P.George est que les arguments pour ou contre l'information géographique n'ont pas changé en trente ans - le débat anglo-saxon l'a prouvé et on l'entend tous les jours dans les couloirs des universités - alors que le niveau d'informatisation de la société, les techniques informatiques et les outils géographiques n'ont strictement plus rien à voir avec ceux d'alors. On a l'impression que le rapport à l'informatique d'une majorité de géographes s'est en quelque sorte figé, et ne se corrige guère malgré le rajeunissement des cadres. On pourra en conclure que c'est simplement P.George qui avait raison: la pratique de l'informatique est accessoire en géographie, ce que les géographes prouvent tout les jours. Nous nous inquiéterions nous plutôt de la cécité de ceux qui peuvent tenir imperturbablement pendant 30 ans un discours sur un objet en aussi forte évolution.} \autocite[479]{Joliveau2004}}

\Anotecontent{joliveau_texte}{\enquote{Pour de nombreux géographes français, les SIG sont essentiellement un domaine technique dans lequel les étudiants trouvent des débouchés professionnels. Pour d’autres, ils constituent un outil utile pour stocker les données nécessaires à l’analyse spatiale, la modélisation ou la cartographie. Pour peu d’entre eux, ils constituent un objet de questionnement, pour ne pas dire de recherche pleinement géographique. Le nombre de géographes français impliqués dans la géomatique reste faible et n’a guère tendance à augmenter. Les études de la géomatique n’intéressent pas beaucoup les géographes au-delà d’un petit noyau. Les géographes semblent peu lire d’articles de géomatique, et ceux qui le font sont essentiellement les spécialistes intéressés par les questions de formalisation et de modélisation présents à “Géopoint94”.

Pour le coup, c’est cette indifférence méfiante de la majorité qui risque finalement d’être la cause de la dérive technologique soi-disant redoutée. La coupure entre géomaticiens et géographes risque en effet de s’étendre. Les géographes non spécialistes vont se trouver incapables de suivre le développement des techniques géomatiques. Et les liens des géographes- géomaticiens avec leur discipline d’origine se distendront. La pratique des SIG par les géographes se trouvera déconnectée à la fois des travaux théoriques en géomatique et de l’avancée théorique, conceptuelle et critique de la géographie. Les systèmes d’analyse et les bases de données spatialisées se feront sans esprit géographique. Quant aux bonnes questions et aux bonnes interprétations à faire avec ces outils, ce seront les spécialistes d’autres disciplines ou des équipes interdisciplinaires sans géographes qui les poseront et les donneront.} \autocite[481]{Joliveau2004}}

\Anotecontent{definition_complexe}{En ce sens, on peut dire qu’il s’agit d’un mouvement, et non plus seulement d’un moment. Ce mouvement a produit ses manifestes. Ils font entendre la voix d’une minorité et expriment un sentiment d’oppression – et probablement, dans le même temps, le sentiment de distinction d’une avant-garde clairvoyante. Si je mentionne cette double motivation, c’est que l’on peut opposer deux modèles. D’un côté, le manifeste américain issu de l’université de Californie, publié pour la première fois en 2008, révisé de façon collaborative et actualisé en 2009. Je le décrirais comme pamphlétaire, utopique et artistique, car il me semble pertinent de le rapprocher du surréalisme, du dadaïsme ou du futurisme. De l’autre, le manifeste élaboré à Paris en 2010 par les organisateurs et les participants du premier THATCamp européen. Ce texte a davantage valeur de déclaration ; il invite les lecteurs à signer une pétition et à rejoindre un mouvement en définissant une orientation constructive. Il s’agit de favoriser une culture numérique dans l’ensemble de la société, d’établir des programmes, des diplômes, des carrières pour ceux qui se consacrent à ces études, de définir également une « compétence collective » au service d’un bien commun. D’une façon générale, il est question d’oeuvrer à une réforme et non pas à une révolution, à travers le partage de bonnes pratiques, à travers un consensus au sein des communautés et à travers le développement de cyberinfrastructures, c’est-à-dire d’équipements et d’institutions spécifiques. La prise en compte de ce dernier besoin est d’ailleurs l’une des raisons pour lesquelles les humanités numériques ne peuvent pas constituer une nouvelle tour d’ivoire : il leur faut des moyens, des équipes et une collaboration intense. \autocite{Berra2012}}

\Anotecontent{human_num_note}{\enquote{Huma-Num offre aux utilisateurs l'accès à ces deux types de ressources (grille et calculateur), l'emploi de l'un ou de l'autre se faisant en fonction des besoins. Il convient de signaler que l'utilisation d'une grille de calculs nécessite des connaissances particulières en termes de distribution de job de traitement. [...] La grille de calculs de la TGIR Huma-Num correspond à un droit d'usage pour les Sciences Humaines et Sociales, des fermes de calculs du CC-IN2P3. La distribution de job de traitement se fait via le système Grid Engine de Sun/Oracle. L'utilisation de cette grille s'opère via la création d'un compte AFS validé par la direction de la TGIR. L'utilisateur sera donc rattaché au groupe de traitement de la TGIR Huma-Num. Les supercalculateurs sont des machines propres à Huma-Num et sont utilisables de façon interactive via ssh.} \href{http://www.huma-num.fr/servicegrille/service-de-traitement-des-donnees}{Extrait du texte de présentation récupéré en 2015 sur le site web @Huma-Num}}

\Anotecontent{human_num_notecout}{Le centre de calculs de l'IN2P3 facturait à l'ex-TGIR Adonis 2000€ le million d'heures CPU (normalisé HES06). Le projet Geodivercity a utilisé pour ses simulations 4 à 6 millions d'heures HES06 par mois depuis novembre, soit l'équivalent d'environ 60K€ de CPU sur 6 mois. Un chiffre qui semble-t-il s'avère largement au dessus des quota prévus pour \textbf{l'ensemble des projets nécessitant du calcul en sciences humaines.}}

\Anotecontent{huma_num}{ Huma-Num est une très grande infrastructure (TGIR) visant à faciliter le tournant numérique de la recherche en sciences humaines et sociales.
Pour remplir cette mission, la TGIR Huma-Num est bâtie sur une organisation originale consistant à mettre en œuvre un dispositif humain (concertation collective) et technologique (services numériques pérennes) à l’échelle nationale et européenne en s’appuyant sur un important réseau de partenaires et d’opérateurs.

La TGIR Huma-Num favorise ainsi, par l’intermédiaire de consortiums regroupant des acteurs des communautés scientifiques, la coordination de la production raisonnée et collective de corpus de sources (recommandations scientifiques, bonnes pratiques technologiques). Elle développe également un dispositif technologique unique permettant le traitement, la conservation, l'accès et l'interopérabilité des données de la recherche. Ce dispositif est composé d'une grille de services dédiés, d'une plateforme d'accès unifié (ISIDORE) et d'une procédure d'archivage à long terme.}


\Anotecontent{extrait_CNN}{\enquote{L'enseignement de l'informatique doit se développer au sein de l’Education nationale et à tous les niveaux : à l’école primaire avec la pensée informatique, au collège par le biais d’un cours de programmation en troisième, et au lycée par la généralisation déjà prévue du cours  d’ISN  à  toutes  les  terminales  générales  et  technologiques.  Afin  de  créer  des  citoyens  en capacité d'agir dans une société numérique,  maîtrisant plutôt que subissant les transformations liées au numérique, l'informatique doit être enseignée à tous.} \autocite{CNNum2014}}

\Anotecontent{historique_EPI}{L'association \enquote{Enseignement Public et Informatique} (EPI) créé en 1971 tient à jour un \href{http://www.epi.asso.fr/revue/histosom.htm}{@historique} très complet sur plus de 40 ans de rapports chaotiques entre la politique, l'éducation et l'informatique.}

\Anotecontent{simplon}{Le texte d'ouverture sur le site est le suivant \enquote{Simplon.co est une école, qui propose des formations intensives de six mois pour apprendre à créer des sites web, des applications web/mobile, et en faire son métier. La formation s’adresse prioritairement aux jeunes de moins de 25 ans, non diplômés ou peu diplômés, issus des quartiers populaires, des diasporas et des milieux ruraux, aux demandeurs d’emploi, aux allocataires du RSA, ainsi qu’aux femmes qui sont insuffisamment représentées dans les métiers techniques. Elle est gratuite, ouverte à tous, pourvu que la motivation soit au rendez-vous !}}

\Anotecontent{openshaw_intuition}{\foreignquote{english}{The dream is of some kind of model-crunching machine which could be persuaded to search for interesting model specifications in the universe of all possible models relevant to a particular purpose} \autocite{Openshaw1983}}

\Anotecontent{pincette_oshaw}{Ces raisons bien qu'intéressante, sont aussi à prendre avec prudence, car elles sont évoqués selon un référentiel anglo-saxon, certe ayant traversé les frontières, mais ne devant pas échapper à une mis en perspective critique regard de l'historique particulier de la discipline géographique française. Un travail qui reste à faire.}

\Anotecontent{prevision_osaw}{ Comment ne pas penser au fameux \textit{big data}, tant à la mode ces dernières années, lorsqu'on lit la prose suivante ? \foreignquote{english}{

}\autocite{Openshaw1998}}

\Anotecontent{projet_beowulf}{Depuis 1994, et le projet de mise en grappe de \enquote{machine standard} nommé de façon générique \textit{cluster Beowulf}, cette forme de HPC est régulièrement renouvellé par tout les bidouilleurs adepte de la partie technologique de ce mouvement de plus en plus gros du \textit{Do It Yourself}. L'utilisation aujourd'hui de micro-pc à peine plus gros qu'une carte bancaire permet une mis en parallèle rapide et relativement peu couteuse de plusieurs centaines de ces systèmes autonome, dont le moins cher d'entre eux C.H.I.P est à moins de 10\$ (On connait aussi les systèmes Arduino, Raspberry, Edison, etc. déjà très usité par les \textit{makers}). On observe également ce type de pratiques avec l'achat de cartes graphiques (\textit{Graphic Processing Unit} GPU), dont la forte densité de processeurs initialement dédiés au calcul parallèle de rendu graphique gourmand que l'on trouve classiquement dans les usages du grand public (jeu vidéo, rendu 3D, etc.), est réutilisé pour effectuer du calcul scientifique performant à moindre cout, via des \textit{clusters GPU}.}

\Anotecontent{projet_parallela}{Voir par exemple le site et les objectifs du projet \href{http://supercomputer.io/}{@parallela}}

\Anotecontent{reproduire_repliquer}{Pour \textcite{Stodden2011} On parle de \textit{replicability} (réplication) pour une tentative de re-génération de résultats publiés à partir du code source et des données de l’auteur. La \textit{reproductibility} (reproductibilité) est un terme plus général qui inclut à la fois cette idée de réplication, mais également la possibilité de générer des résultats similaires de façon en partie indépendante du code, ou des données associées à la publication originale; c’est en quelque sorte une mise à l’épreuve des résultats originaux qui rejoint l’idée de robustesse. La différence entre les deux termes peut varier selon les communautés employant les termes. Par exemple, la définition de \textit{replicability} chez \textcite{Wilensky2007a} semble à cheval par rapport aux deux définitions précédentes \foreignquote{english}{Though many conceptions of replication may exist, for the purposes of this paper, we will define replication as the implementation (replicated model) by one scientist of group of scientists (model replicaters) of a conceptual model described and already implemented (original model) by a scientist or group of scientists at a previous time (model builders). The implementation of the replicated model must differ in some way from the original model, and, per our definition, the implementation of the replicated model must be executable, not another formal conceptual model.}}

\Anotecontent{hirtzel}{ Assumé par l'auteur de l'étude, il faut noter que cette simplification n'empêche en aucun cas la création de nouvelles connaissances sur les dynamiques complexes du modèle dont on rapelle qu'elles sont inconnues initialement. Le travail d'analyse réalisé ici par \autocite{Hirtzel2015} sur Mobisim est déjà remarquable compte tenu des difficultés techniques entourant l'exécution de ce dernier. Voici toutefois quelques remarques extraites de son travail.

A propos du calibrage du module Mobosim-Demo (p171-172) : \textit{Le paramétrage mériterait également d’être complété et amélioré. Réalisé manuellement, par essai-erreur, il permet d’atteindre un objectif, sans aucune garantie sur son caractère optimal dans l’ensemble de l’espace de valeurs des paramètres (Evans et Unsworth, 2012). Le jeu de paramètres finalement identifié n’est en aucun cas unique, ni optimal. Il est le meilleur possible compte tenu des moyens (techniques et données de référence) à disposition.}

Dans le cas de l'analyse de sensibilité de Mobisim-MR (incertitude sur les valeurs de paramètres), il faut savoir que celui-ci \textit{[...] est composé d’un grand nombre de paramètres et de variables, en interactions les uns avec les autres (cf. chapitre 3). Compte tenu de la dynamique implémentée dans le modèle et au regard de tout ce qui a été évoqué dans le paragraphe 7.1.1, une exploration globale du modèle paraît de prime abord la plus appropriée pour répondre au premier objectif, afin de prendre en compte ces interactions et leur impact sur les résultats du modèle.[...] Cependant, ces variables et paramètres sont de natures diverses et interviennent à différents niveaux dans la formalisation du modèle, et notre priorité est avant tout d’identifier des profils de comportements pour chacun d’entre eux, indépendamment des interactions dont ils dépendent. De plus, le second objectif se focalise sur certains paramètres seulement ce qui pencherait plutôt en faveur d’une analyse plus localisée, dans un premier temps du moins [...] Ces contraintes techniques freinent la mise en place d’analyses globales pour Mobisim-MR : étant donné le nombre de paramètres, il faudrait tester un trop grand nombre de combinaisons de valeurs pour que l’analyse soit pertinente, ce qui rendrait l’expérience très coûteuse en temps de calcul. [...] Ces différents constats nous ont conduit à procéder à des analyses de sensibilités locales, avec la méthode OAT, en modifiant les valeurs de chacun des paramètres les unes après les autres, toutes choses égales par ailleurs, c’est-à-dire tous les autres paramètres étant fixés à leur valeur par défaut (cf. chapitre 5).}

Même si il y a deux objectifs en jeu (analyse sensibilité globale \textit{a posteriori}, exploration locale de pré-calibrage), le choix de la méthode \textit{OAT (One At a Time)} semble quand même être un choix par défaut, car le deuxième objectif d'exploration locale/ciblé est inclus et dépendant des résultat du premier objectif. En effet juste avant l'auteur rapelle que (p 250) l'analyse globale \textit{[...] paraît incontestablement la plus performante pour étudier rigoureusement la sensibilité d’un modèle, et plus particulièrement d’un modèle aux dynamiques non-linéaires [...]} et cela même si \textit{ [...] , certains auteurs soulignent néanmoins la complémentarité des deux approches (Ginot et Monod, 2007), l’analyse locale permettant de préparer le protocole de l’analyse globale.}

La limitation des ressources informatiques conduit malheureusement ici les auteurs à opérer une double simplification, d'une part dans le choix de l'unique méthode \textit{OAT} - car si elle permet bien de préparer l'analyse globale, elle ne remplace pas cette dernière pour autant - , d'autre part, car l'analyse OAT n'échappe pas non plus à la problématique de couverture d'espace comme la \textit{Curse Dimensionality} (voir section \ref{sssec:heuristique} pour une explication). Le nombre de valeurs de paramètres jalons retenus pour l'analyse étant de nouveau restreinte \textit{Dans le cas de Mobisim-MR, les valeurs à tester ont été choisies différemment pour chaque paramètre, et leur sélection résulte de plusieurs compromis. Tout d’abord, nous avons dû restreindre le nombre total de tests à effectuer en raison des contraintes de temps de calcul et de traitement des données évoquées précédemment.} (p 255)

L'auteur est tout à fait consciente de ces limitations, comme en témoigne un extrait de la conclusion (p 384) \textit{L’analyse de sensibilité telle que nous l’avons menée ne permet pas de quantifier le rôle de chaque paramètre dans la variance des résultats, ni de tester la sensibilité des résultats de simulation aux interactions entre les paramètres, ce qui la rend partielle au regard de la formalisation sur laquelle repose le modèle. Ce choix à la fois nous est imposé par des contraintes techniques liées à la simulation avec Mobisim et est assumé pour procéder dans un premier temps à une analyse simple où il est possible de saisir les tenants et aboutissants des résultats. Elle est d’autant plus partielle que les résultats de simulation sont considérés de manière agrégée (pour l’ensemble des ménages), ne nous permettant pas de déceler si la modification d’un des paramètres a un impact local sur l’un d’entre eux : la modification d’un paramètre peut affecter de manière plus accentuée le comportement d’un type de ménages en particulier.}

Ce sont ces problématiques (volumétrie des données, combinatoire elevé, et couverture de l'espace des paramètres très limités) qui nous ont invariablement poussés vers l'établissement d'un raisonnement inverse, en opposant à l'exploration incertaine, couteuse et \textit{a posteriori} des modèles, l'établissement d'objectif questionnant la dynamique du modèle \textit{a priori}. Cette dernière n'empêche pas l'exploration des modèles, contrairement à ce que l'on pourrait penser, mais impose malgré tout une formulation plus directe et formalisée des \enquote{questions à poser au modèle}, ce qui limite forcément l'exploration aux critères d'évaluation imaginés par les modélisateurs \autocites{Schmitt2015}. La création de nouveaux algorithmes de la classe des méta-heuristiques permet toutefois d'aller au-delà, soit en s'appuyant sur les objectifs existants pour établir des profils renseignant le modélisateur sur la dynamique associée à chacun des paramètres (objectifs à optimiser, 1 paramètre fixe, les autres varient librement)\autocite{Reuillon2015}, soit en explorant cette fois ci la diversité de dynamiques des comportements des modèles sans objectif a priori (objectif de diversité à optimiser, tout les paramètres libres). \autocite{Cherel2015}}

\Anotecontent{code_humanite}{\enquote{Au sein de ces nouveaux environnements de recherche, l'enjeu ne se situe pas tant dans le fait que tous les chercheurs doivent savoir coder, mais davantage dans la garantie des conditions de l'interdisciplinarité. Il s'agit en effet de faire en sorte que ces différents profils atteignent un niveau de compréhension réciproque suffisant pour pouvoir travailler ensemble [...] Dans les deux cas, il ne s'agit pas de demander aux uns de faire le travail des autres, mais davantage de permettre à chacun de comprendre l'épistémologie de l'autre, et ainsi de traduire ses besoins et objectifs dans une forme compréhensible pour l'échange.} \autocite[74]{Plantin2014}}

\Anotecontent{genci}{Société de droit civil détenue à 49\% par l'Etat représentée par le Ministère de la Recherche et l'Enseignement Supérieur, 20\% par le CEA, 20\% par le CNRS , 10\% par les Universités et 1\% par l'INRIA}

\Anotecontent{naive_eject_shs}{La question semble naïve tant la présence des sciences humaines dans les projets d'avenir concernant le calcul intensif a été évacuée ... Ce qui a aussi pour effet de mettre en lumière un paradoxe, car au moment ou l'Europe dépense des milliards (!!) pour financer un projet de simulateur mondial et social à l'échelle micro comme FuturICT, on peut se demander à quel moment et avec le soutien de quels acteurs institutionnels les sciences humaines, (et la géographie ??) jusqu'alors majoritairement absentes sur ce terrain du HPC vont réapparaître dans les rapports d'activités d'organismes dans le but de justifier les fonds ainsi distribués ?

De façon tout à fait générale et pragmatique, et dans la lignée de cette dernière réflexion, on pourrait après tout se dire que c'est une ressource informatique comme une autre, alors pourquoi ne pas l'utiliser ? Celle-ci ayant prouvé son utilité dans bien d'autres disciplines scientifiques faisant partie de la famille des systèmes complexes, alors pourquoi ne pas l'utiliser également dans nos pratiques de simulation, dont on martèle depuis longtemps leur appartenance à cette famille ? Une nouvelle façon de de réaffirmer que la simulation des systèmes sociaux vaut bien la complexité des systèmes physiques ou biologiques, dans une conjoncture plutôt favorable où de grands challenges et des financements européens fleurissent justement sur ces thèmes ?}

\Anotecontent{kych_notecalcul}{Quant on voit l'absence des SHS dans les rapports d'activités des organismes tels que GENCI, ou le projet de réintégration des SHS ne semble pas à l'ordre du jour, les modes d'attributions de calculs et l'absence de comité thématiques sont des arguments rédhibitoires dont on se demande en définitive si ils ne seraient pas là justement pour se protéger des sciences \enquote{inexactes} ...}

\Anotecontent{note_autocensure}{Retrouverait on ici un argument similaire à celui des quota d'utilisation fixé très bas pour le TGIR Huma-Num, résultat d'une politique d'auto-censure lié aux tarifs d'utilisation des centres de calculs du CNRS, impossible à financer pour des SHS, mais pas pour d'autres disciplines !? }

\Anotecontent{remarque_denise_centrecalcul}{\enquote{Le centre de calculs était en libre service pour les enseignants de Paris I, avec des machines à perforer et un bac où on déposait les paquets de cartes à traiter au CIRCE pour revenir chercher les listings le lendemain. Deux dames assuraient pour les travaux de recherche la préparation des paquets de cartes de données (on leur apportait des bordereaux de perforation pour cette saisie) et en général nous perforions nous-mêmes les cartes de contrôle nécessaires à l'envoi des programmes, ou les quelques cartes de subroutines en Fortran permettant d'adapter nos données aux logiciels de traitement.} (Remarque de Denise Pumain sur le texte, mai 2015)}

\Anotecontent{pionnier_genet}{Ce type de collaboration avec les informaticiens sera poursuivi par la suite dans cette UFR toujours en pointe dans la formation des historiens à l'informatique. Si cette UFR n'a probablement pas été la seule à se battre pour la formation en informatique des historiens, elle mérite en tout cas largement sa place dans la liste des pionniers lorsqu'on parle aujourd'hui d'histoire francaise des humanités digitales.

Ce point est vérifié tant sur les aspects totalement innovants des modalités d'enseignements de l'informatique, que sur les aspects des collaborations inter-disciplinaires, des programmes de recherches menés par les chercheurs et étudiants, mais aussi des publications supportées : l'histoire et la mesure, le médiéviste et l'ordinateur. Je renvoie le lecteur curieux de cette histoire à la publication en annexe du témoignage de Jean-Philippe Genet, ainsi qu'à la lecture .}

\Anotecontent{denise_extrait_ccparis}{\enquote{Il me semble que Genet n'a pas vu passer l'épisode de terminaux, non pas individuels, mais micro et partagés dans la petite salle du centre de calculs, qui ont un temps remplacé les perforeuses et qui permettaient encore d'envoyer des travaux en \enquote{batch} au CIRCE, au lieu de perforer les cartes on tapait les instructions. La date exacte d'apparition doit se situer vers 1981, je me souviens avoir traité là-dessus les données de migrations interrégionales en France (des matrices sur plusieurs périodes intercensitaires qu'on a d'abord traitées pour les étudiants et que j'ai traitées ensuite à partir de 1984 avec le modèle de la synergétique, chez Weidlich et Haag, d'où le livre de 1988), et aussi à peu près toutes nos applications du modèle de Peter Allen sont passées comme ça, j'ai commencé fin 1981 et je me souviens de très longues soirées passées au centre de calculs, en compagnie de Lena après l'été 1982 quand elle a décidé de faire sa thèse sur ce modèle, on a donc essayé de faire marcher une première version jusque vers la fin de 1982 puis une meilleure, et ce doit être en 1983 que j'ai testé au collège de France le programme Minuit pour estimer simultanément quelque 24 paramètres de ce modèle, sans succès, avec Bertrand Roehner, un physicien du labo de physique théorique de Paris 6 (qui était venu me contacter au début des années 1980 car il s'intéressait à la loi de Zipf). Bref pour en revenir aux terminaux on y apportait ses données, sur disquette souple ou petite dure (assez vite venues), on tapait les quelques lignes de cartes de contrôle et on passait les programmes au CIRCE, retour des listings papier le lendemain parfois encore, je ne suis pas sûre qu'on ait eu les résultats lisibles sur un micro. en tout cas on ne pouvait rien laisser de personnel dessus}  (Extrait d'une correspondance daté de mai 2015)}

\Anotecontent{note_micro_infographie}{\enquote{C’est un peu par hasard que, en 1978, je suis \enquote{tombé} dans la géographie, par l’intermédiaire d’un collègue, Bernard Lannuzel, qui intervenait à la fois en informatique et en géographie dans un laboratoire de \enquote{micro-infographique} créé par Philippe Lecarpentier dans le département de géographie de Rouen. Le laboratoire comprenait à l’époque plus de mathématiciens que de géographes. J’y ai travaillé au développement d’un logiciel de cartographie statistique, d’abord en chercheur associé car en 1980 j’avais obtenu une charge de cours d’informatique à Jussieu, puis à plein temps quand un an après j’ai été nommé sur un poste de maître de conférences de géographie qui s’est dégagé à l’université de Rouen, pour un enseignement de statistiques et d’utilisation des outils informatiques. Depuis cette date, je me suis ancré dans ce groupe de recherche au nom changeant mais à forte identité : « Laboratoire micro-infographique », devenu « groupe Image » (informatique et mathématiques appliquées à la géographie), puis, avec Yves Guermond comme directeur, qui s’est fondu dans « l’équipe MTG » (modélisation et traitement graphique en géographie) avec le statut de jeune équipe CNRS en 1983, puis d’unité de recherche associée en 1988, équipe MTG qui s’est à son tour fondue, sans perdre son identité, dans l’unité mixte de recherche Idees (Identités et différentiations de l’environnement des espaces et des sociétés).}\autocite[190]{Mathieu2014}}

\Anotecontent{denise_extrait_ruefour}{\enquote{en 1986 je reçois à l’INED mon premier PC micro personnel, peu de temps après le laboratoire de la rue du Four en est équipée aussi, donc nous abandonnons la fréquentation des micros du centre de calculs. La mise en réseau de nos ordinateurs est effectuée de façon très pionnière parmi les labos de SHS du CNRS par notre ingénieur Ky Nguyen. Il installe des messageries Internet dès 1995 et m’aidera à créer la revue électronique Cybergeo (première française, europénne et peut-être mondiale pour les SHS, à l’époque en France il n’y avait qu’une revue de maths) en avril 1996.} (Extrait d'une correspondance daté de mai 2015)}

\Anotecontent{dependent_application}{\textcite[26-28]{Openshaw2000} fait une typologie générique d’applications pouvant tirer partie du HPC : \foreignquote{english}{a) legacy modelling applications, b) classical but implicit HPC applications c) new HPC-dependent methodologies}}

\Anotecontent{formation_idris}{En 2012, ce sont pas moins de 24 sessions de formations qui ont eu lieu, dont 7 à l'extérieur, suivies par 338 personnes sur presque 60 journées. En 2013, les formations suivantes sont disponibles : programmation MPI, MPI/OpenMP, Fortran 95, Fortran 2003, Utilisation Ada/Turing \autocite{Idris2013}}

\Anotecontent{pumain_slocal}{\enquote{On est au début des années 1990, et la formule de collaboration choisie est celle de l’accueil d’un doctorant en informatique dans un laboratoire de géographie, pour réaliser un modèle de simulation multi- agents, la thèse étant financée par les géographes. La collaboration d’effectue selon une séparation des tâches, l’étudiant informaticien effectuant la programmation en suivant les indications de l’équipe de géographes qui tente d’assimiler les rudiments de l’ontologie informatique : agents, attributs, règles, classes et héritages des propriétés, échanges d’informations entre les agents, afin de spécifier un modèle qui sera appelé Simpop. Au passage, se réalise une \enquote{ innovation } pour les deux parties, puisque les \enquote{ agents } sont des entités collectives, des villes, pour lesquelles ce type de modèle informatique n’avait pas encore été réalisé.} \autocite{Pumain2014}}

\Anotecontent{note_integration_rouen}{Une intégration qui permet de maintenir un certain niveau technique dans le laboratoire, d’introduire de nouvelles compétences comme par exemple la manipulation d'Automates Cellulaires courant des années 1980, tout en assurant une certaine continuité dans l’enseignement de la programmation et des statistiques aux Géographes dans ces universités.}

\Anotecontent{pumain_simpop}{\enquote{[...] quand France Guérin, qui avait fait sa thèse sous ma direction sur la croissance des villes françaises, nous a proposé de travailler avec Jacques Ferber27, spécialiste des systèmes multi-agents (SMA), pour construire un modèle de dynamique des villes fondé sur cette méthodologie, nous n’avons pas hésité. On en a eu une très grande satisfaction, que l’on peut souligner aujourd’hui parce qu’à l’époque, ce n’était pas du tout évident que ce type de modélisation aurait le succès qu’il a connu dans les sciences sociales. Nous nous sommes lancées dans une application et nous avons publié en 1996 le premier article portant sur l’application des modèles multi-agents en géographie, avec le modèle SIMPOP. Là encore, nous avons rencontré quelques difficultés dans la collaboration interdisciplinaire : nous ne maîtrisions pas du tout ce nouveau langage de programmation. L’informaticien était un thésard qui n’en faisait qu’à sa tête et ne nous laissait pas entrer dans sa conception du modèle. Il pensait avoir compris aussi bien que nous les mécanismes spatiaux et économiques que l’on essayait d’introduire. Reconnaissons, à sa décharge, que fabriquer une interface entre un système multi-agents et une carte de géographie n’était pas chose facile avec les moyens de l’époque. Il travaillait avec un programme small-talk qui essayait déjà de faire de la programmation par objets un petit peu simplifiée, mais il fallait, tout de même, écrire beaucoup de codes pour y arriver. Et puis, tout cela n’est pas non plus toujours très aisé à communiquer à des non-spécialistes de l’informatique. Nous n’étions pas dans des conditions de travail extrêmement aisées.} \autocite[155-156]{Mathieu2014}}

\Anotecontent{scattrplot}{Le mieux pour comprendre est encore de se rendre sur le site ayant permis de faire cette analyse. Robin Cura à redéveloppé une version interactive de ce plot en utilisant Shiny et R, avec pour avantage la possibilité de charger les données csv que l'on souhaite. Les données au format csv et le workflow ayant permis d'arriver à cette archive contenant le front de pareto utilisé dans ces analyses sont accessibles sur le  \href{http:\\sebastienreycoyrehourcq.fr/these/}{@site}}
