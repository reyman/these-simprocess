% -*- root: These.tex -*-

\Anotecontent{holland_multi_utilisation}{ Holland a développé les GA avant tout pour leur capacité de \foreignquote{english}{robust adaptive systems} et pas seulement pour leur capacité d'optimisation comme le rapelle \textcite{DeJong1993a} : \foreignquote{english}{However, with all this activity, there is a tendency to equate GAs with function optimization. There is a subtle but important difference between \enquote{GAs \textbf{as} function optimizers} and \enquote{GAs \textbf{are} function optimizers}} ; L'investissement d'Holland dans l'étude des \foreignquote{english}{Complex Adaptive Systems} s'inscrit dans une trajectoire de recherche resté proche des thématiques de ce qui deviendra plus tard la méta-discipline \textit{Artificial Life}. Son investissement continue dans cette branche de développement est d'ailleur lisible au travers de deux plateformes successive sur ce thème : \textit{$\alpha$-universe} et \textit{Echo} dont on trouve une analyse dans les travaux de \autocites{Taylor1999, Taylor2001} }


\Anotecontent{difference_objective_heuristique}{Il n'est pas forcément évident de faire la différence entre ces termes très proche, dont le sens se recoupe parfois, voici donc une aide à la désambiguisation inspirée de celle de \textcite[36]{Weise2011} :

\begin{enumerate}[labelindent=\parindent,leftmargin=*]
\item La fonction objectif (\textit{objective function}) peut etre considérée comme une forme d'heuristique, à la différence que celle ci est une mesure forcément directe du potentiel d'un aspect de la solution, alors que l'heuristique peut être de mesure directe ou indirecte, en ne fournissant par exemple qu'une approximation de la distance séparant une mesure de l'optimum. En ce sens, la fonction objectif mobilise souvent plus d'expertise sur le système que l'heuristique.
\item Une fonction \textit{fitness} est une fonction d'utilité secondaire, conçu comme une combinaison possible de fonction objectifs, et/ou d'heuristiques. Celle-ci peut également être une mesure relative, pour quantifier par exemple la différence existante entre deux solutions.
\end{enumerate}}

\Anotecontent{barricelli_multi_utilisation}{ Tout comme les travaux de McMillan ont permis de voir plus clair dans les intentions de Von Neumman derrière la notion de \textit{self-reproduction automata} ..., les travaux de Dyson \Autocite{Dyson1997}, de Fogel \autocite{Fogel2006a} sur l'histoire de cette discipline a permis également de redécouvrir les recherches de Barricelli comme celle d'un véritable pionnier en ALife, mais également comme celui d'un pionnier dans l'idée d'utiliser l'évolution comme support à la résolution de problème.}

\Anotecontent{fraser_comment}{\foreignquote{english}{Fraser was one of the first to conceive and execute computersimulations of genetic systems, and his efforts in the 1950s and1960s had a profound impact on computational models of evo-lutionary systems. The simulation algorithms he used were im-portant not only in the simulation of genetical problems, but pro-vided a menu of techniques that enriched the entire simulationeffort in any problem that involved probability sampling amonga population of alternatives, the heart of Monte Carlo methods. }\autocite[429]{Fogel2002}}

\Anotecontent{note_pattee_semantic_closure}{ \foreignquote{english}{Additionnary, from an epistemological point of view, Pattee(1995b) points out taht symbolic information (such as that contained in an organisms's genes) has \enquote{no instrinsic meaning outside the context of an entire symbol system as well as the material organization that constructs(writes) and interprets(reads) the symbol for a specific function, such a classification, control, construction, communication ...}. He argues that a necessary condition for an organism to be capable of creative open-ended evolution is that it encapsulates this entire self-referent organisation (Pattee refers to this condition as semantic closure). From this it follows that organisms should be constructed \enquote{with the parts and the laws of an artifical physical world} Pattee (1995a)(p.36). In other words, the interpretation (phenotype) of the symbolic information (genotype) of an artificial organism should be constructed and act within the artificial physical environment of the system. Additionally, if the system is to model the \enquote{origin} of genetic information, then the genotype itself must also be embedded within the environment; that is, the complete semantically-closed organisation -- the \enquote{entire organism} -- must be completely embedded within the physical environment.} \autocite{Taylor2001}} 

\Anotecontent{np_complet_def}{ \foreignquote{english}{Identifying which combinatorial problems are easy to solve and which are hard is an important and challenging task, which has occupied theoretical computer scientists for many years. In order to translate the everyday expression \enquote{easy to solve} to mathematical theory the concept of polynomial time algorithms has been introduced. An algorithm is said to run in polynomial time if there is a polynomial $p$ such that the algorithm applied to an input of size $n$ always finds a solution in time $p(n)$, that is after performing $p(n)$ simple instructions. Note that we measure the worst case complexity, that is the time in which we are sure that the algorithm ends regardless of which input of size $n$ we have fed it. The execution time of a polynomial time algorithm grows slowly enough with increasing input size to be able to be run on a computer, but if the execution time grows exponentially the algorithm is useless for all but the smallest inputs. One of the most accepted ways to prove that a problem is hard is to prove it NP-complete. If an optimization problem is NP-complete we are almost certain that it cannot be solved optimally in polynomial time.} \autocite[1]{Kann1992}}

\Anotecontent{billet_weise}{Voir le \href{http://blog.it-weise.de/p/309}{@billet} daté de juin 2014.}

\Anotecontent{note_pengouin}{Que faut il penser par exemple d'un algorithme bio lorsqu'il est nommé \foreignquote{english}{Pengouin Search Optimization Algorithm} (PSOea) \autocite{Gheraibia2013} ? }

\Anotecontent{stochastic_note}{Si l'optimisation stochastique (\textit{stochastic optimization}) ou approche probabiliste de l'optimisation (\textit{probabilistic approaches} apparait comme un autre chapeau susceptible de pouvoir englober l'ensemble de ces techniques, le schéma \ref{fig:S_OverviewOptimisation} de Weise contredit ce constat. Il existe en effet dans cette vaste catégorie tout un ensemble de techniques (\textit{Hill Climbing}, \textit{Simulated Annealing}, etc.) qui diffèrent très fortement dans leur structure, leur définition, ou leur inspiration, de la branche de techniques qui nous préoccupe ici, à savoir l'EC. }

\Anotecontent{equipe_mixite}{Suivant les travaux menés dans notre équipe par \textcite{Reuillon2015}, la validité de ce dernier paragraphe est clairement remise en question. L'originalité de ces derniers résident dans la mixité de ces deux objectifs. En intégrant \enquote{la capacité d'extension spatiale} dans l'exploration de ces espaces comme un critère d'optimisation supplémentaire aux objectifs plus classique de recherche de minima, une cartographie dirigée et plus exhaustive est devenue possible.}
%En intégrant l'\enquote{exploration de cette espace} des solutions (si on veut découvrir une carte  exhaustive des solutions optimisés), ou de l'espace de recherche (si on veut cartographier l'espace de recherche menant à cet espace de solution optimisé) comme un objectif d'optimisation supplémentaire aux objectifs plus classique de recherche de minima, une cartographie dirigé et plus exhaustive de certaines zones de cet espace est devenu possible.}


\Anotecontent{def_meta_sorensen}{\foreignquote{english}{A metaheuristic is a high-level problem-independent algorithmic framework that provides a set of guidelines or strategies to develop heuristic optimization algorithms (Sörensen and Glover, 2013). [...] A problem-specific implementation of a heuristic optimization algorithm according to the guidelines expressed in a metaheuristic framework is also referred to as a metaheuristic. The term was coined by \textcite{Glover1986} and combines the Greek prefix meta- (metá, beyond in the sense of high-level) with heuristic (from the Greek heuriskein or euriskein, to search)} \autocites{Sorensen2013a, Sorensen2013b} } 

\Anotecontent{def_meta_weise}{C'est également ainsi que \textcite[36, 225]{Weise2011} comprend ce terme \foreignquote{english}{A metaheuristic is a method for solving general classes of problems. It combines utility measures such as objective functions or heuristics in an abstract and hopefully efficient way, usually without utilizing deeper insight into their structure, i. e., by treating them as black box-procedures}}

\Anotecontent{greedy_description}{Un \enquote{choix optimal local} est réalisé à chaque itération durant l'optimisation, ce qui produit en général des solutions viables mais très rarement optimales.}

\Anotecontent{q_ppr}{Questions tirés du wiki \textit{Portland Pattern Repository} (\href{http://c2.com/cgi/wiki?MetaHeuristic}{@PPR}), qui est au passage un des premier wiki sur le web (1995)}

\Anotecontent{paysage_cumule}{En ce sens, la figure \ref{fig:spacePspaceOmultimodal} peut représenter tout autant \begin{enumerate*}[label=(\alph*)]
\item un ensemble de population de solutions candidates désignées en amont par un plan d'expérience, évaluées en une seule passe par l'optimiseur, puis projetté dans l'espace des objectifs
\item ou, un cumul de population candidates évaluées représentatif du fonctionnement de l'optimiseur étalé sur plusieurs passes, celui-ci manipulant entre chaque itération non pas une population, mais un seul individu solution candidate. \end{enumerate*} }

\Anotecontent{test_fonction_surutilisation}{Sous l'impulsion remarquée de quelques chercheurs \autocite{Zitzler1999a, Zitzler1999b, Fonseca1996}, différentes approches ont étés formalisés ces dernières années pour mieux mesurer et comparer les performances de ces différents algorithmes d'optimisation. Ces approches et ces mesures sont régulièrement comparées, et utilisées de façon complémentaire dans la littérature informatique pour confronter au mieux les nouveaux algorithmes dont on rapelle qu'ils sont stochastique, et donc plus difficile à évaluer les uns rapport aux autres \autocite{Coello2006}. Cette évolution passe par la création d'un certain nombre de \textit{benchmark}, avec par exemple l'évaluation par les algorithmes de set de fonctions mathématiques (comme ceux de Schwefel et DeJong)dont on connait la forme du front de Pareto \autocites[580]{Weise2011}[138]{Back1996}, en utilisant diverses mesures (sur le paysage \autocite[163]{Weise2011}, sur les opérateurs de comparaisons utilisés pour différencier les algorithmes \autocite{Zitzler2003, Zitzler2007}, etc.) et leur mise en oeuvre automatisée par le biais de plateformes dédiées, comme la plateforme COCO (COmparing Continuous Optimisers) \autocite{Hansen2011} utilisée depuis 2009 pour les conférences prestigieuses de GECCO. Nécessaire dans l'établissement théorique de la discipline, un point de vue réflexif et critique des chercheurs dans cette discipline ont pour effet d'engager de nouvelles réflexions, et paradoxalement, parce qu'il faut bien trouver un moyen de comparer ces algorithmes, invente de nouvelle fonctions tests et de nouvelle mesures : \foreignquote{english}{Zitzler et al. 2000 stated that, when assessing performance of an MOEA, one was interested in measuring three things: 1. Maximize the number of elements of the Pareto optimal set found. 2. Minimize the distance of the Pareto front produced by our algorithm with respect to the global Pareto front (assuming we know its location). 3. Maximize the spread of solutions found, so that we can have a distribution of vectors as smooth and uniform as possible. This, however, raised some issues. First, it was required to know beforehand the exact location of the true Pareto front of a problem in order to use a performance measure. This may not be possible in real-world problems in which the location of the true Pareto front is unknown. The second issue was that it is unlikely that a single performance measure can assess the three things indicated by Zitzler et al. 2000.} \autocite{Coello2006} Différents auteurs, dont certains pionniers créateurs de ces même fonctions, se sont récemment adressés à la communauté sur ce sujet, en demandant si possible d'arreter d'utiliser ces fonctions types. \hl{ref à rajouter}}

\Anotecontent{remarque_section_metaheuristique}{Ce constat n'est pas forcément évident avec les exemples utilisés jusqu'à présent, mais il faut imaginer que l'optimiseur va jouer avec les valeurs $x$ et $y$ en fonction de leur espace respectif et des contraintes possiblement associés, et \textbf{non pas en se déplacant physiquement} sur le plan 2D $(x,y)$, que l'on a utilisé ici avant tout pour des facilité de représentation. Les qualités topologiques de cet espace (dans quel cluster de solution candidates je me situe ? ou se situe les prochains clusters voisins intéressant à explorer ? les clusters de valeur $v$ sont il très homogènes, très hétérogènes etc.) qui pourraient effectivement permettre de dégager des informations utiles dans la selection des futures solutions candidates ne sont généralement pas pris en compte de façon initiale par la plupart des métaheuristiques que nous allons étudier, à moins qu'on ne leur en donne les moyens. L'expertise de l'espace des solutions candidates évalués, ou espace des objectifs, est bien plus souvent mobilisé pour motiver les nouvelles solutions candidates à évaluer, comme on va pouvoir le découvrir dans la section suivante. Il existe donc de nombreuses possibilités pour intégrer diverses connaissances améliorant les choix de l'optimiseur, la prospection intelligente des différents espace à sa disposition en fait partie.}

\Anotecontent{note_knapsack}{Dans le problème du sac-à-doc, ou \textit{Knapsack problem}, il s'agit de trouver la combinaisons idéale d'objets disposant d'une masse et d'une valeur, en essayant de maximiser la somme calculée à partir de la valeur des objets que l'on arrive à entrer dans le conteneur. C'est un problème d'optimisation discret NP-Complet, difficile à résoudre lorsque le nombre d'éléments pris en compte augmente, au même titre que le voyageur de commerce.}

\Anotecontent{notation_dominance}{On trouve également cette notation sous la forme inverse dans la littérature. C'est par exemple le cas dans les écrits de \textcites{Deb2000a,Deb2002}, créateur de l'algorithme multi-objectifs très utilisé et très connu nommé NSGA2. La relation de domination entre deux éléments est écrite en utilisant le symbole inverse $x_1 \prec x_2$, signifiant $x_1$ domine $x_2$. Une des explications possible est la suivante : \foreignquote{english}{$x^* \succ x$ to indicate that $x^*$ dominates $x$.This notation can be confusing because the symbol $\succ$ looks like a \enquote{greater than} symbol but since we deal mainly with minimization problems, the symbol \enquote{$\succ$} means the function values of $x^*$ are less than or equal to those of $x$. However this notation is standard in the literature, so this is the notation that we use.} Le signe serait donc couramment retourné pour éviter une possible confusion. \hl{ref à ajouter : A Tutorial on the Performance Assessment of Stochastic Multiobjective Optimizers et Zitzler2003 ... A voir dans goldberg1989 puisque c'est lui le premier qui l'a utilisé si on en croit certain papier} }

\Anotecontent{def_convergence}{\foreignquote{english}{An optimization algorithm has converged (a) if it cannot reach new candidate solutions anymore or (b) if it keeps on producing candidate solutions from a “small” subset of the problem space.} \autocite[251]{Weise2011}}

\Anotecontent{remarque_resolution}{Une remarque qui ouvre la possibilité d'autres questionnements, comme celui par exemple de la résolution et de la sensibilité à partir desquels on identifie deux solutions comme différentes, alors même que les algorithmes sont entrainés à faire la différence entre deux évaluations en tenant compte d'écarts infimes ? Cette remarque est également valable pour l'échantillonage des valeurs de $x$, si l'optimiseur est contraint de selectionner les valeurs selon un seuil de résolution fixé par un seuil (un pas de 0.1 par exemple), ne prend-t-on pas le risque important de passer à coté d'optimum plus intéressants ?}

\Anotecontent{note_weak}{\foreignquote{english}{[...] In fitness landscapes with weak (low) causality, small changes in the candidate solutions often lead to large changes in the objective values, i. e., ruggedness. It then becomes harder to decide which region of the problem space to explore and the optimizer cannot find reliable gradient information to follow. A small modification of a very bad candidate solution may then lead to a new local optimum and the best candidate solution currently known may be surrounded by points that are inferior to all other tested individuals. The lower the causality of an optimization problem, the more rugged its fitness landscape is, which leads to a degeneration of the performance of the optimizer [1563].} \autocite[162]{Weise2011}}

\Anotecontent{note_strong}{\foreignquote{english}{During an optimization process, new points in the search space are created by the search operations. Generally we can assume that the genotypes which are the input of the search operations correspond to phenotypes which have previously been selected. Usually, the better or the more promising an individual is, the higher are its chances of being selected for further investigation. Reversing this statement suggests that individuals which are passed to the search operations are likely to have a good fitness. Since the fitness of a candidate solution depends on its properties, it can be assumed that the features of these individuals are not so bad either. It should thus be possible for the optimizer to introduce slight changes to their properties in order to find out whether they can be improved any further. Normally, such exploitive modifications should also lead to small changes in the objective values and hence, in the fitness of the candidate solution.} \\ La définition donné par Weise pour une \textit{strong causality} est donc la suivante \foreignquote{english}{Strong causality (locality) means that small changes in the properties of an object also lead to small changes in its behavior.} \autocite[161]{Weise2011}}

\Anotecontent{note_elitisme}{Les stratégie d'élitisme visent à s'assurer que les meilleures solutions ne seront jamais perdues, et cela quelque soit le déroulement de l'algorithme : \foreignquote{english}{No matter how elitism is introduced, it makes sure that the fitness of the population-best solution does not deteriorate. In this way, a good solution found early on in the run will never be lost unless a better solution is discovered. The absence of elitism does not guarantee this aspect.} Une propriété qui n'est pas garantie dans les premières générations d'algorithmes EA d'optimisation multi-objectifs. On trouvera plus de détail sur ce terme dans les pages d'ou cette précédente définition a été tiré \autocite[239-240]{Deb2001}. L'introduction de ce concept, bien que daté de \hl{(ref De Jong, 1975)}, est attribué à Zitzler dans sa version multi-objectif si on en croit \textcite{Coello2006}, qui en donne la définition suivante \foreignquote{english}{In the context of multi-objective optimization, elitism usually (although not necessarily) refers to the use of an external population (also called secondary population) to retain the nondominated individuals found along the evolutionary process. The main motivation for this mechanism is the fact that a solution that is nondominated with respect to its current population is not necessarily nondominated with respect to all the populations that are produced by an evolutionary algorithm. Thus, what we need is a way of guaranteeing that the solutions that we will report to the user are nondominated with respect to every other solution that our algorithm has produced. Therefore, the most intuitive way of doing this is by storing in an external memory (or archive) all the nondominated solutions found. If a solution that wishes to enter the archive is dominated by its contents, then it is not allowed to enter.}}

\Anotecontent{martin_fowler}{L'auteur informaticien Martin Fowler, qui s'est intéressé dans plusieurs articles à l'étymologie de ce principe d'inversion de contrôle, et à sa relation proche avec l'injection de dépendance, fournit un élément de réponse sur la différence entre le concept de librairie logicielle et celui de \textit{framework} : \foreignquote{english}{Inversion of Control is a key part of what makes a framework different to a library. A library is essentially a set of functions that you can call, these days usually organized into classes. Each call does some work and returns control to the client. A framework embodies some abstract design, with more behavior built in. In order to use it you need to insert your behavior into various places in the framework either by subclassing or by plugging in your own classes. The framework's code then calls your code at these points.} L'article complet est accessible sur le \href{http://martinfowler.com/bliki/InversionOfControl.html}{@site} de l'auteur.}

\Anotecontent{sean_luke_mason}{\foreignquote{english}{In 1998, after using a variety of genetic programming and evolutionary computation toolkits for my thesis work, I decided to develop ECJ, a big evolutionary computation toolkit which was meant to support my own research for the next ten years or so. ECJ turned out pretty well: it’s used verywidely in the evolutionary computation field and can run on a lot of machines in parallel. [...] One common task (for me anyway) for evolutionary computation is the optimization of agent behaviorsin large multiagent simulations. ECJ can distribute many such simulations in parallel across simultaneousmachines. But the number of simulations that must be run (often around 100,000) makes it fairly important to run them very efficiently. For this reason I and my students cooked up a plan to develop a multiagentsimulation toolkit which could be used for various purposes, but which was fast and had a small and cleanmodel, and so could easily be tied to ECJ to optimize, for example, swarm robotics behaviors.} \autocite[8]{Luke2014}}

\Anotecontent{sean_luke_ecj}{\foreignquote{english}{ECJ is an evolutionary computation framework written in Java. The system was designed for large, heavy- weight experimental needs and provides tools which provide many popular EC algorithms and conventions of EC algorithms, but with a particular emphasis towards genetic programming. ECJ is free open-source with a BSD-style academic license (AFL 3.0). ECJ is now well over ten years old and is a mature, stable framework which has (fortunately) exhibited relatively few serious bugs over the years. Its design has readily accommodated many later additions, including multiobjective optimization algorithms, island models, master/slave evaluation facilities, coevolution, steady-state and evolution strategies methods, parsimony pressure techniques, and various new individual representations (for example, rule-sets). The system is widely used in the genetic programming community and is reasonably popular in the EC community at large. I myself have used it in over thirty or forty publications.} \autocite[7]{Luke2014b}}

\Anotecontent{sean_luke_masondifficile}{\foreignquote{english}{MASON is not an easy toolkit for Java beginners. MASON expects significant Java knowledge out of its users. If you are a rank beginner, allow me to recommend NetLogo, a good toolkit with an easy-to-learn language. [...] Finally MASON does not have plug-in facilities for Eclipse or NetBeans, though it can be used quite comfortably with them. If you’re looking for a richer set of development tools, you might look into Repast. }\autocite[8]{Luke2014}}

\Anotecontent{sean_luke_ecjdifficile}{\foreignquote{english}{A toolkit such as this is not for everyone. ECJ was designed for big projects and to provide many facilities, and this comes with a relatively steep learning curve.} \autocite[7]{Luke2014b}}

\Anotecontent{coello_note}{Le chercheur \textcite{Coello2015} est un auteur régulier d'états de l'art \autocite{Coello1999, Coello2000, Coello2007} ou d'articles \autocite{Coello2006} sur l'historique de cette branche spécifique de l'EC concentré sur la résolution de problèmes multi-objectifs maintient également sur son \href{http://www.lania.mx/∼ccoello/EMOO/}{@site} une base de données bibliographiques riche à ce jour de plus de 9000 entrées.}

\Anotecontent{reflexion_DeJong}{\foreignquote{english}{By understanding the role each element plays in the overall behavior and performance of an EA, it is possible tomake informed choices about how the elements should be instantiated for a particular application. At the same time, it is clear that these components interact with each other so as to affect the behavior of simple EAs in complex, nonlinear ways. This means that no one particular choice for a basic element is likely to be universally optimal. Rather, an effective EA is one with a co-adapted set of components.}\autocite[70]{DeJong2006a}}

\Anotecontent{mcts_go}{C'est le cas par exemple de la classe d'heuristiques dites de \textit{Monte-Carlo Tree Search} (MCTS) \autocites{Browne2012, Bouzi2014} dont le perfectionnement successif a permis l'émergence de quelques programmes \autocite{Coulom2006} aux réussites notables lors de compétitions mondiales de GO, assurant un rayonnement plus large à cette technique en intelligence artificielle, avec des bénéfices dans le domaine des méta-heuristiques qui nous intéresse ici \autocite[4]{Wang2012}. On trouvera plus d'informations sur ces récents exploits éléctroniques dans les articles de \href{http://www.wired.com/2014/05/the-world-of-computer-go/}{@Wired}, du \href{http://rfg.jeudego.org/item/122-la-guerre-sainte-electronique}{@New-York Times} et d'\href{https://interstices.info/jcms/c_43860/le-jeu-de-go-et-la-revolution-de-monte-carlo}{@Interstices}}

\Anotecontent{tromp_appel_calcul}{Tromp a lancé un \href{http://tromp.github.io/go/legal.html}{@site} internet pour collecter la ressource disponible nécessaire pour ce calcul. Celui-ci estime la charge de travail à fournir pour obtenir un résultat à 10 à 13 serveurs, possédant au moins 8 processeurs et 512 Gb de RAM, pendant 5 à 9 mois.}

\Anotecontent{odersky_note_cake}{\foreignquote{english}{We argue that, at least to some extent, the lack of progress in component software is due to shortcomings in the programming languages used to define and integrate components. Most existing languages offer only limited support for component abstraction and composition. This holds in particular for statically typed languages such as Java, and C\# in which much of today's component software is written.} \autocite{Odersky2005}}

\Anotecontent{note_informatique_mixin}{Cette footnote est plus à destination d'un public informaticien. On apelle \keywordmin{mixin} cette abstration informatique qui permet de composer différents \keywordmin{trait} en Scala. Si un \keywordmin{trait} apparait de prime abord similaire à une interface ou une classe abstraite supportant comme ces deux dernières l'héritage, le \keywordmin{mixin} inclut bien cette dernière propriété mais s'avère \textit{in extenso} beaucoup plus puissante. En effet, un \keywordmin{trait} est abstrait, supporte l'héritage multiple, ne tient pas compte de l'ordre d'association, et supporte une mixité du niveau d'abstraction associé à la déclaration des types, des méthodes, et des variables implémentées. Comme on l'apercoit dans \ref{fig:principe_mixin}, il est possible de décorer dynamiquement les classes par le biais de cette composition, sans se soucier d'un ordre quelconque. Associé au \textit{self-type annotation} les traits supportent également des références cycliques entre composants (A dépend de B, B dépend de A). L'utilisation plus ou moins cumulatives de ces abstractions permet une grand flexibilité, il n'y a donc pas une technique de \textit{Cake Pattern}, comme pourrait le sous entendre le nom, mais de multiples variations. Plus de détail sur ces abstractions peuvent être trouvés dans le papier original d'\textcite{Odersky2005}}

\Anotecontent{cas_utilisation_wfom}{\hl{a raffiner}Le cas d'utilisation étant inédit sur la plateforme openMOLE, de très nombreuses heures ont été nécessaire pour réaliser et tester les premiers workflow organisant l'execution d'un algorithme génétique sur une grille de calcul. De part les toutes nouvelles limites imposés par ce travail (puissance nécessaire, durée d'execution, complexité du worklow présenté) ce cas d'utilisation organisé autour de la calibration d'un modèle de simulation a permis aux différents acteurs du projet de progresser sur plusieurs fronts à la fois, sur la grammaire de composition de workflow dans openMOLE, sur la conception de MGO, sur les nouvelles possibilités permise par leur couplage, mais également sur la définition des fonctions objectifs, et sur l'analyse des résulats.}

\Anotecontent{lab_mis}{Ce laboratoire a été le support dans la décennie 1980 d'un secteur de recherche (AMMSIOSHE) groupant plusieurs laboratoires (Géographie, Histoire, Littérature, Philosophie, etc.), pilotant également un GIS \enquote{Techniques nouvelles en sciences de l'homme} et également porteur d'un DEA pluridisciplinaire \enquote{Méthodes et Techniques nouvelles en sciences de l'Homme et de la Société)} \autocite{TSH1984}. Le laboratoire MIS deviendra en 1997 le MTI@SHS (Méthodologie et Technologies de l'Information appliquées aux Sciences de l'Homme et de la Société) \autocite{Girardot2004} et regroupera dans son projet l'ambition d'être à la fois une plate-forme technologique, un lieu d'interdisciplinarité pour les sciences de l'homme et de la société, un partenaire pilote dans la formation à l'innovation mélant informatique et sciences humaines (ISTI). Après le départ à la retraite de Jean-Philippe Massonie, ce projet inter-disciplinaire construit sur 30 ans se poursuit également avec l'émergence d'un nouveau projet scientifique pilotant la MSHE C. N. Ledoux (2002), structure commune aux deux universités de la région \autocites{Favory2003, Favory2009}. Le MTI@SHS est aujourd'hui intégré comme équipe de recherche technologique (ERT) pour l'intelligence territoriale au sein de l'UMR multi-site Théma créé en 1999, qui participe également dans certains pôles de la MSH de Dijon et de la MSHE Ledoux de Besançon. Dans cette optimique mutualiste, elle est un acteur important pour les plateformes informatiques de ces dernières, notamment par la mise à disposition de services humaines et matériels appartenant à Théma \textit{in situ}. La stucture de la MSHE Ledoux dispose en effet en 2015 de ses propres calculateurs de taille moyenne (PowerEdge R815, Dell Précision 7500) destinés aux traitements locaux de données lourdes, preuve aussi que cette \enquote{culture du centre de calcul} reste une condition importante dans la marche d'un environnement inter-disciplinaire. Une remarque à mettre en perspective du tout décentralisé (huma-num), et de l'absence d'équipements dans d'autres MSH en sciences humaines. Il existe également un partenariat entre ce laboratoire et le Mésocentre (centre de calcul régional) de Franche-Comté, sur lequel par exemple tourne la plupart des simulations du modèle LUTI Mobisim.\autocites{Thema2010, Hirtzel2015} }

\Anotecontent{borillo_note}{Il est a noter que Mario Borillo est recruté à Marseille au CADA par J.C. Gardin, et reprend en 1971 ce laboratoire d’archéologie pionnier sur les usages computationel en Archéologie. On notera que le congrès de Marseille en 1969,1971 organisé par Gardin sur les usages computationels en Archéologie est un moment important dans l’archéologie au niveau national mais également international \autocite{Whallon1972}. Après plusieurs années à diplore le réseau LISH, il va à Toulouse ou il rejoindra l’IRIT dès sa fondation, un laboratoire d’informatique à Toulouse qui joue plus ou moins indirectement toujours un rôle important dans l’accompagnement technologique des SHS, via les productions d’outils et les interactions très fortes de certains de ses membres informaticiens avec les sciences humaines et les géographes (L’équip SMAC par exemple) \autocite{Aurnague2014}}

\Anotecontent{note_amoral_difficulté}{Le premier contact avec la modélisation par Analyse Systémique a été celui du rapport Meadows présenté par le Club de Rome sous le titre \enquote{Halte à la Croissance ?}, et le premier intérêt méthodologique a été suscité par F. Renchenman de l'IRIA et P. Uvietta de l'IMAG qui ont considérablement contribué à dédramatiser une approche qui paraissait hors de nos possibilités.\autocite[p129]{Guermond1984}}

%Voir pour ajouter : La lettre d'Histoire Moderne et Contemporaine et Informatique, la lettre d'information du groupe Histoire et informatique,
 % ISHA Paris 4 'Institut des Sciences Humaines Appliquées qui publie “Informatique et Science humaines” publié % http://www.paris-sorbonne.fr/presentation-3133

\Anotecontent{litterature_legere}{On trouve plus d’informations dans la littérature grise de cette époque, la \enquote{feuille d’avis du LISH}, les Bulletins de la MSH relatant les activités du LISH, mais également dans les compte rendus et les activités de certaines disciplines formés à l’informatique à cette période, comme les lettres, l’histoire, l’archéologie, la sociologie, la sociologie. On retrouve ainsi des informations pratiques et techniques (tutoriels, programmes, conseils) dans les journaux comme \enquote{Le médiéviste et l’ordinateur} de l’IHRT 1979-2003, \enquote{archéologie et ordinateur} publié de 1982 à 1995 ,  \enquote{Informatique et sciences humaines} publié par GEMAS/Paris 4  (? -- ?) , \enquote{Programmation et sciences de l'homme} édité par l'ENS à partir de 1980, les cahier spéciaux de l'AFCET  etc. Il est d’ailleur étonnant de ne pas trouver plus de discussions abordant ces aspects “techniques” d’accès à la ressource, de programmation, de manipulation techniques; bref de “bidouilage” il faut bien le dire, dans la littérature grise des années 1970, comme par exemple celle des “Brouillons Dupont”.}

\Anotecontent{centre_formation}{Il faut savoir que ces centres nationaux, aujourd’hui connu sous le nom d’IDRIS (ex-CIRCE) et du CINES (ex-CNUSC), dispensent toujours des formations à destination des scientifiques, quelque soit leurs disciplines de rattachement; il n’est donc pas trop tard pour apprendre le Fortran.}

\Anotecontent{presentation_cnusc}{Pour en savoir plus sur l'activité, et les services matériels et logiciels de ce centre, on pourra se référer à la présentation faite par \textcites{Lelouche1982, Lelouche1982b} dans le journal du \enquote{médiéviste et l'ordinateur} }

\Anotecontent{kuntzmann}{Le pôle d'informatique grenoblois, et notamment l'IMAG, est historiquement lié à l'émergence de la discipline informatique comme science du même nom en France. Une des impulsions importantes est donné par l'universitaire Grenoblois Jean Kuntzmann dans les années 1950, ce mathématicien et supporteur précoce de l'analyse numérique va militer pour la création, la formation, et la diffusion de ce qui va devenir par la suite l'informatique. Celui-ci est le fondateur et dirigeant de l'IMAG jusqu'en 1977, un des premiers laboratoire associé au CNRS, disposant d'un rayonnement international y compris sur le plan non-académique. Au milieu des années 1960, le laboratoire emploi 140 personnes, dont 60 doctorants. Si le Centre InterUniversitaire de Calcul de Grenoble (1972) n'est vraiment officialisé qu'en 1972, de nombreux ordinateurs analogiques (1951), puis digitaux(Gamma ET, Marchant 10FA en 1957, IBM 7044 et 1401 en 1963, IBM 360-67 en 1968) seront amenés à passer bien avant dans ce premier Centre de Calcul. En 1977, cet effectif se porte à 300 personnes. Celui-ci est également le créateur de l'Association Française de Calcul (AFCAL) en 1957 qui deviendra par la suite AFCALTI, AFIRO, puis l'AFCET (Association Française pour la Cybernétique), dont les colloques et conférences ont constitués un des canal importants de diffusion de la systémique dans le milieu scientifique et technique, y compris en géographie quantitative \autocite{Pumain2003} \hl{autre ref ?}. Il n'est donc pas étonnant de voir l'utilisation appliqués de systèmes dynamique émerger de ce centre en particulier, car Kuntzmann lorsqu'il créé l'Ecole Nationale Supérieure d'Informatique et de Mathématiques Appliquées de Grenoble (ENSIMAG) il lui insuffle cette volonté d'une mathématique, et d'une informatique résolument tourné et enrichit en retour par des applications, car \enquote{Il ne faut pas séparer une science de ses applications; à longue échéance, les mathématiques coupées des applications deviendraient stériles.} \autocites[422]{Mounier2012}{Chiffres1977}}

 \Anotecontent{consommateur_data}{Une étape qu'il convient de rapeller aujourd'hui chez les consommateurs aveugle de Data. D'une part de telles bases se construisent, ce qui implique un temps de collecte, et un temps de construction menant à des choix d'interprétation, de conceptualisation, et de traitements des données parfois irreversible. La Data n'est pas donc jamais totalement neutre, sauf peut être dans quelques circonstances et dans quelques disciplines.}  

\Anotecontent{massonie_texte}{\enquote{En 1969, la Faculté des Lettres et Sciences Humaines de Besançon créait un poste de mathématique. Un an plus tard un poste d'assistant était a son tour créé. Les premiers clients furent les géographes, puis vinrent les historiens, les sociologues et enfin les littéraires. L'utilisation de l'analyse des données et donc de l'ordinateur devint non pas une mode, mais un instrument de plus dans l'arsenal des différentes disciplines. Il ne s'agissait pas de faire faire une thèse qui utilise les méthodes nouvelles, mais de faire une thèse de géographie ou d'histoire.}\autocite{Massonie1986}}


\Anotecontent{batty_code}{\foreignquote{english}{Ok I started programming in 1966 using Atlas Autocode that was a forerunner to Algol which was then merged into Pascal. Its was a declarative language where you had to define all the variables you used. I then moved to Fortran in 1969 when I moved from manchester to reading - in those days all the auto code stuff was based on punched tape but when i starred fortran we used punched cards. I also began with Dartmouth basic in 1971 at reading which was an interpretative language - it compiled as one went along. I stuck with Fortran until 1990 even beyond a bit - and the version i used on the PC was Waterloo Fortran 77 I think - actually the melbourne 1986 model is an avi file and won't run on mac but the earlier 1982 model is in VAX Fortan 77 and here is a picture of it all you can find a bit on these \href{http://www.complexcity.info/media/movies/early-computer-movies-1967-86/}{@pages}

If you go to \href{http://www.casa.ucl.ac.uk/movies-weblog/Melbourne-1982-Movie.mov}{@[site]} you can load the model from 1986 that we ran and see this as movie - not code as we were getting into graphics then I then learnt some UNIX and C but this was archaic and I managed to run some Fortran programs on Sun workstations. I basically then left Fortran completely and in 2003 went to Visual Basic - actually very powerful within Visual Studio and I still use this occasionally - I am debating about what to use next - My programmer Richard Milton with our big spatial interaction models uses C sharp i think - actually all these languages are sort of the same except for the object orientation that i don't find very intuitive for aggregate spatial interaction} (Correspondance privé daté du 21 mars 2014)}

\Anotecontent{description_laurini_algo}{Il est à noter que cette école d'été en novembre 1982 dédié aux mathématique de l'analyse des systèmes, dirigé par Guermond et regroupant une quarantaine de personnes sur quelques jours \autocite[320-321]{cuyala2014}, va donner lieu à un des rares livres \autocite{Guermond 1984} contenant la description d'\textbf{algorithmes} de modèles urbains dynamiques, sous la plume de Laurini.}

\Anotecontent{esprit_micro_jeu}{ Pour \textcite[193]{Massonie1986}, \enquote{Ce n'est qu'avec l'apparition des micro-ordinateurs et surtout avec l'esprit micro, que l'utilisation des méthodes nouvelles va se répandre. Les orientations ont été prises suivant quelques principes simples:\begin{enumerate*}[label=(\alph*)]
\item travailler au plus faible coût possible,
\item ce sont les résultats obtenus dans une discipline qui sont importants, 
\item un utilisateur doit investir dans sa formation de telle manière que cela ne nuise pas à son travail dans sa discipline d'origine,
\item les grands projets ambitieux demandent des crédits énormes que nous n'avions pas et trop souvent deviennent une fin, en oubliant les buts initiaux
\item un logiciel se construit avec les utilisateurs; l'informaticien n'est pas un véritable utilisateur
\item un logiciel aussi performant soit-il ne peut repondre à toutes les questions, il doit pouvoir être adapté à chaque cas
\item un logiciel ne fait que traduire pour la machine des idées de travail de l'utilisateur qui doit done rester le maitre-d'oeuvre
\end{enumerate*}}}

\Anotecontent{note_documentation_ccalcul}{Il existe une littérature dédié à l'utilisateur, mais celle-ci semble très difficile à se procurer aujourd'hui, car probablement peu considéré sur le plan scientifique, et techniquement obsolète. Celle-ci contient néammoins une trace des usages qu'il conviendrait de sauvegarder. On notera par exemple la présence d'un \textit{Guide du nouvel utilisateur CIRCE} \autocite{LISH1980}, de listes normalisées de logiciels \textit{LISTLOG} présente au CNUSC, d'un groupe d'utilisateur comme celui géré par Bernard Gaulle, etc. }


\Anotecontent{massonie_1978}{Toujours sur son \href{http://jean-philippe.massonie.pagesperso-orange.fr/science/informatique.html}{@site} personnel, voici ce que dit Jean-Philippe Massonie sur l'acquisition d'un AppleII en 1978 par rapport à l'IRIS alors en place : \enquote{ AppleII : En 1978, mon labo ayant quelques argents, on achète un AppleII avec une télévision couleur et une imprimante et deux lecteurs de disquettes. Le prix ? On pourrait s'acheter maintenant deux PowerPC et une imprimante laser. Honnêtement, on voulait faire joujou.

Mais il y avait un Basic. Bon d'accord, Basic ...Alors on a programmé pour jouer. Et puis X. Luong a traduit de Fortran en Basic, le programme d'analyse des correspondances que nous utilisions. Et là, surprise on pouvait traiter des tableaux aussi grands qu'avec l'Iris. Certes le micro était moins rapide que l'Iris, mais comme on travaillait directement dessus, on arrivait à faire 3 analyses dans son après-midi, contre une seule avec l'Iris. A coté de l' analyse des correspondances, naissait une famille de programmes qui étaient l'ancêtre de HyperPatate, logiciel d'analyse de vocabulaire.}}

\Anotecontent{ca_simd_avantage}{\foreignquote{english}{The advantages of an architecture optimized for cellular automate (CA) simulations are so great that, for large-scale CA experiments, it becomes absurd to use any other kind of computer [...] In 1981, the frustrating inefficiency of conventional computer architectures for simulating and displaying cellular automata became a serious obstacle to our experimental studies of reversible cellular automata.} ( 1987 Physica D, Cellular Automata Machines, Normam Margolus, Tommaso Toffoli)}

\Anotecontent{CA_physical}{
voir l'essai sur Feynman par David Hillis : \href{http://longnow.org/essays/richard-feynman-connection-machine/}{@[site]}
}
\Anotecontent{relation_france}{relation avec Pomeau}

\Anotecontent{simd_def}{SIMD pour \textit{Single Instruction Multiple Datastream} est une classe de la taxonomie de Flynn's : Une instruction par processeur est executé de façon simultané sur de multiples données, ce qui suppose souvent un controle en amont.}

\Anotecontent{mimd_def}{MIMD pour \textit{Multiple Instruction Multiple Datastream} est une classe de la taxonomie de Flynn's : \foreignquote{english}{The MIMD class of parallel architecture consists of multiple processors (of any type) and some form of interconnection. From the programmer’s point of view each processor executes independently but to cooperatively execute to solve a single problem although some form of synchronization is required to pass information and data between processors.[...]The primary characteristic of a large MIMD multi-processor system is the nature of the memory address
space. If each processor element has its own address space (distributed memory), the only means of communication between processor elements is through message passing. If the address space is shared (shared memory), communication is through the memory system.} \autocite[696]{Flynn2012}}

\Anotecontent{hiebeler_parcours}{Le travail de Hiebeler avec Langton, suite à une correspondance privé :  during the summer 1989 to 1990 on Cell Sim, then after you go to Thinking Machines, and you return to Santa Fe in Oct 1992 to fall 1993 to work on SWARM C, then summer 1994 to work on SWARM objC}

\Anotecontent{ouppy_marseille}{Ajouter ref sur proocedings des Houches, France 1989 ; Hiebeler 1990}

\Anotecontent{top_500_note}{\foreignquote{english}{The most commonly known ranking of supercomputer installations around the world is the TOP500 list. It uses the equally well-known LINPACK benchmark as a single figure of merit to rank 500 of the world’s most powerful supercomputers. The often-raised question about the relation between the TOP500 list and HPCC can be addressed by recognizing the positive aspects of the former. In particular, the longevity of the TOP500 list gives an unprecedented view of the high-end arena across the turbulent era of Moore’s law [] rule and the emergence of today’s prevalent computing paradigms. The predictive power of the TOP list is likely to have a lasting influence in the future, as it has had in the past.} \autocite[845]{hum}}

\Anotecontent{DMM}{A Distributed-Memory Multiprocessor (DMM) is built
by connecting nodes, which consist of uniprocessors or of shared memory multiprocessors (SMPs), via a network, also called Interconnection Network (IN) or Switch. While the terminology is fuzzy, Cluster generally refers to a DMM mostly built of commodity components, while Massively Parallel Processor (MPP) generally refers to a DMM built of more specialized components that can scale to a larger number of nodes and is used for large, compute-intensive tasks - in particular, in scientific computing.}

%standard MPI qui marque le début d'une fin de reigne des architectures couteuse de type SIMD pour la diffusion d'architecture MIMD, dont on va voir qu'elle sont par la suite totalement démocratisé par des approches comme Beowulf.

%MPI standard à expliquer (page 1184 - 1190)

\Anotecontent{loosely_sync}{Pour simplifier, un noeud ne peut pas écrire sur le noeud de destination du message tant que celui-ci n'est pas prêt à recevoir ce message.}

\Anotecontent{exemple_simd_mimd}{ \textcite[91-92]{Openshaw2000} propose de prendre un exemple simple pour mesurer la différence de fonctionnement qu'implique l'utilisation de chacune des architectures. On ne traite ici que les plus courantes. Imaginons un problème nécessitant l'examen de 100 feuilles d'examen, chacune avec 5 questions. On dispose d'une ensemble de correcteurs pour traiter ces questions de façon parallèle. 
\begin{enumerate}
	\item{\textbf{SIMD - Data Parallel}} (CM-1 par exemple) : Un superviseur envoie à chaque correcteurs un lot de feuilles à traiter. Il attend que toutes les feuilles soient distribués, puis il annonce le traitement à réaliser par chacune des correcteurs : \enquote{Tout le monde traite la première question de la feuille d'examen !}, puis une fois que tout le monde a fini et que le résultat est rapporté au superviseur, celui-ci annonce \enquote{ Tout le monde traite la deuxième question de la feuille d'examen}, etc. Si une correcteur met plus de temps à traiter une feuille, alors tout le monde doit l'attendre. Si il y a moins de feuille disponible à traiter que de correcteurs, alors ceux-ci ne feront rien durant la durée des traitement, ce qui représente un certain gachis de ressources.

	\item{\textbf{MIMD - Shared Memory}} Il n'y a aucun superviseur. Chaque correcteur se voit attribuer un ensemble unique de feuilles à traiter, si possible en tenant compte de sa rapidité ( on parle alors de load-balancing). Les feuilles sont disponibles sur un seul et unique tas disposé au fond de la salle, partagé par tout les correcteurs.  Le temps de complétion de la tâche correspond alors au temps pris par le plus lent des correcteurs pour corriger le tas de feuille qui lui a été attribué.

	\item{\textbf{MIMD - Distributed Memory}} Deux types de corrections alternatives sont possibles a) Chaque correcteur reçoit par le biais d'un courrier (équivalent à un message sur une interconnection réseau) une attribution de parcelle contenant un ensemble de feuilles à corriger, qu'il traitera en fonction de son propre emploi du temps. Les feuilles sont donc corrigé en parallèle par chacun des correcteurs, qui renvoie ses corrections au service central par courrier une fois seulement sa tâche terminée. b) Si le service de courrier est très rapide, alors il est plus intéressant de récupèrer chaque feuille dès qu'elle est corrigée, et on en renvoie une autre au correcteur. Ainsi, que le correcteur soit lent ou rapide, les feuilles à corriger plus ou moins longues, ou un mélange de ces différents comportements, personne n'est plus amené à perdre de temps dans ce cas.
\end{enumerate}}

\Anotecontent{idris}{Informations tirés de la page de description des deux calculateurs disponible sur le site du laboratoire CNRS de l'\href{http://www.idris.fr/}{@IDRIS}.}

\Anotecontent{gibson}{\foreignquote{english}{It’s harder to imagine the past that went away than it is to imagine the future. What we were prior to our latest batch of technology is, in a way, unknowable. It would be harder to accurately imagine what New York City was like the day before the advent of broadcast television than to imagine what it will be like after life-size broadcast holography comes online. But actually the New York without the television is more mysterious, because we’ve already been there and nobody paid any attention. That world is gone.} \href{http://www.theparisreview.org/interviews/6089/the-art-of-fiction-no-211-william-gibson}{@Interview} de The Art of Fiction numéro 211}

\Anotecontent{openshaw_virus}{ \foreignquote{english}{Most geographers involved in GIS (and elsewhere social scientists) are already the hapless but seemingly willing victims of a virulent form of \enquote{let others do the programming for us} form of computer escapism. As a result, they are likely to be forever restricted to software packages they have little or no control over and which more or less determine what they can and cannot do. Others, who perhaps should now better, seem to have been lulled into complacency by the increased computing power offered by PCs. They ask \enquote{What is the point of high-performance computing when with a bit of Fortran or Pascal or C programming I can do all I want on my PC?} Indeed, some others will tell you that \enquote{what they did in 1991 on a mainframe they can now do on a PC}, while some really clever folk can do it in Unix with awk! This is all true. The point is that, sadly, thus is a very negative and backward-looking perspective. What these people are doing today is more or less what they first did, albeit with considerably greater difficulty, five or ten or twenty or more years ago, and they appear to think that what was good for them when they did research is also good for you when you do your research. This is not progress but regress! It is both simultaneously very understandable and an
unfortunate neglect of the immense potential that HPC systems have to offer.} \autocite[2]{Openshaw2000}}

\Anotecontent{note_equipe}{Sur ce point, nous avons eu la chance, dans notre équipe, de bénéficier des pratiques cumulés de scientifiques ayant depuis les années 1970-80 toujours misé pour l'activité de modélisation sur cette double ouverture à la fois vers les autres disciplines, et vers l'innovation informatique, au moins sur les aspects logiciels.}

\Anotecontent{openshaw_revolution}{\foreignquote{english}{The world of computing underwent a quiet revolution of profound long-term significance in the early 1990s. Hillis (1992) argues that it was then in the throes of a major technological change characterised by the development of highly parallel super-computing hardware which was about to change significantly how science is done. This process is now complete. Faster supercomputers have stimulated new ways of doing science in areas that are just too complex to be handled by any other means or where there is need to analyse large volumes of data or where there is need for real-time analysis and modelling. Computer-based experimentation and simulation is increasingly being regarded as a cost-effective and very useful means of creating new knowledge. Computation has become a scientific tool of equal importance to theory and experimentation. HPC is also widely acknowledged as one of the key information technologies of the future. But so far HPC has had a minimal impact on geography and most of the social sciences.} \autocite{Turton1998}}

\Anotecontent{remarque_informaticien_roue}{Avec l’augmentation graduelle et rapide de la puissance disponible sur les micro-ordinateurs dans les années 1980-1990, grand nombre de programmes tournant dix ans plus tôt sur des \textit{mainframes} ou des \textit{superordinateurs} vont être remplacés par des logiciels tout à fait fonctionnels sur des PC de puissance modeste. L'émergence de ces logiciels, parfois importés de l'étranger, se fait (comme toujours aujourd'hui dans la communauté du libre) selon un selection quasi-darwinienne dans la profusion de logiciels crées courant des années 1980. Combien de réussite peut on compter par rapport aux efforts engendrés, un peu partout, parfois surement en parallèle ? Ne dit on pas régulièrement chez les informaticiens qu'il est inutile de sans-cesse vouloir recréer la roue ? }

\Anotecontent{informations_colette_cauvin}{Ces informations sont pour la plupart issues d'un document réponse à une interview par mail de Colette Cauvin, daté du 5 mai 2015.}

\Anotecontent{collette_ccsc_centre}{\enquote{Nos fiches perforées demeuraient au centre de calcul et nous avions grâce à Anne, accès à un bureau au sous-sol où nous pouvions tout laisser. On préparait nos données et les instructions de contrôle propres aux analyses que nous souhaitions dans une grande salle au sous-sol, et nous montions les entrer dans le lecteur de cartes au rez-de-chaussée. En attendant nos résultats (cela pouvait durer entre 10 mn et 1 heure) selon le nombre de chercheurs présents au centre), nous pouvions préparer d’autres données ou, merveille, faire des parties de ping-pong !  Excellente détente calmante dans certains cas où l’attente se prolongeait pour aboutir à constater une erreur de perforation qui nous faisait recommencer le circuit pour une bêtise.} (interview par mail de Colette Cauvin, daté du 5 mai 2015)}

\Anotecontent{calcul_curri}{Un Centre de Ressources Informatiques CRI créé en 1994 à l'université Louis Pasteur, qui après une restructuration des ressources de calculs en 2008-2009 intègre/fusionne dans le projet de méso-centre de l'Université de Strasbourg UdS. Preuve qu'un lien entre les géographes et ce type de structure a perduré encore un certain temps, au moins jusqu'à cette grande politique de mutualisation du calcul ayant opéré en France au travers de la création de meso-centre, ce nouveau maillage ayant probablement modifié la nature et les modalités d'accès de cette ressource, du fait entre autre de l'élargissement des publics.}

\Anotecontent{lena_geopoint}{Une régression multiple peut être exploratoire ou confirmatoire, idem pour un modèle agent (Léna Geopoint2000)}

\Anotecontent{rupture_openshaw}{Même si ce modèle est critiquable en plusieurs points, il nous aura fallu presque trente ans, une équipe inter-disciplinaire rodée, et l'appui de divers partenaire institutionel pour produire une expertise technique similaire au prototype réalisé par Openshaw en 1988. Il va s'en dire que même au royaume-uni Openshaw et son équipe ont du rester quelque temps pionniers, voire peut être même incompris chez leur contemporains géographes anglais. REF. On retrouve aussi cette logique de construction de modèle par famille dans }

\Anotecontent{comparaison_tianhe_idris}{
	

}

\Anotecontent{appui_academie_science}{ Extrait de l'avis donné par la CNN en juin 2013 : \enquote{[...] Le Conseil National du Numérique s'associe donc à cette initiative de l'académie des sciences et compte s'appuyer sur les analyses et les conclusions de ce rapport dans des travaux futurs. [...] Le Conseil National du Numérique propose de contribuer à une réflexion focalisée sur la méthode qui permettra d'atteindre un objectif simple : généraliser d'ici trois ans l'enseignement de l'informatique depuis l'École jusqu'au lycée.}}

\Anotecontent{humanite_digitale_histoire}{La revue \textit{Computer in the Humanities} date par exemple de 1966, .. blabla voir aussi la section centre calcul et shs. LAMOP !

Il n'est pas étonnant de retrouver parmis les acteurs de cette cartographie, le laboratoire de paris1, dont l'histoire avec l'informatique est assez riche.}

\Anotecontent{joliveau_texte}{\enquote{Pour de nombreux géographes français, les SIG sont essentiellement un domaine technique dans lequel les étudiants trouvent des débouchés professionnels. Pour d’autres, ils constituent un outil utile pour stocker les données nécessaires à l’analyse spatiale, la modélisation ou la cartographie. Pour peu d’entre eux, ils constituent un objet de questionnement, pour ne pas dire de recherche pleinement géographique. Le nombre de géographes français impliqués dans la géomatique reste faible et n’a guère tendance à augmenter. Les études de la géomatique n’intéressent pas beaucoup les géographes au-delà d’un petit noyau. Les géographes semblent peu lire d’articles de géomatique, et ceux qui le font sont essentiellement les spécialistes intéressés par les questions de formalisation et de modélisation présents à “Géopoint94”.}}

\Anotecontent{definition_complexe}{En ce sens, on peut dire qu’il s’agit d’un mouvement, et non plus seulement d’un moment. Ce mouvement a produit ses manifestes. Ils font entendre la voix d’une minorité et expriment un sentiment d’oppression – et probablement, dans le même temps, le sentiment de distinction d’une avant-garde clairvoyante. Si je mentionne cette double motivation, c’est que l’on peut opposer deux modèles. D’un côté, le manifeste américain issu de l’université de Californie, publié pour la première fois en 2008, révisé de façon collaborative et actualisé en 2009. Je le décrirais comme pamphlétaire, utopique et artistique, car il me semble pertinent de le rapprocher du surréalisme, du dadaïsme ou du futurisme. De l’autre, le manifeste élaboré à Paris en 2010 par les organisateurs et les participants du premier THATCamp européen. Ce texte a davantage valeur de déclaration ; il invite les lecteurs à signer une pétition et à rejoindre un mouvement en définissant une orientation constructive. Il s’agit de favoriser une culture numérique dans l’ensemble de la société, d’établir des programmes, des diplômes, des carrières pour ceux qui se consacrent à ces études, de définir également une « compétence collective » au service d’un bien commun. D’une façon générale, il est question d’oeuvrer à une réforme et non pas à une révolution, à travers le partage de bonnes pratiques, à travers un consensus au sein des communautés et à travers le développement de cyberinfrastructures, c’est-à-dire d’équipements et d’institutions spécifiques. La prise en compte de ce dernier besoin est d’ailleurs l’une des raisons pour lesquelles les humanités numériques ne peuvent pas constituer une nouvelle tour d’ivoire : il leur faut des moyens, des équipes et une collaboration intense. \autocite{Berra2012}}

\Anotecontent{human_num_note}{\enquote{Huma-Num offre aux utilisateurs l'accès à ces deux types de ressources (grille et calculateur), l'emploi de l'un ou de l'autre se faisant en fonction des besoins. Il convient de signaler que l'utilisation d'une grille de calcul nécessite des connaissances particulières en termes de distribution de job de traitement. [...] La grille de calcul de la TGIR Huma-Num correspond à un droit d'usage pour les Sciences Humaines et Sociales, des fermes de calcul du CC-IN2P3. La distribution de job de traitement se fait via le système Grid Engine de Sun/Oracle. L'utilisation de cette grille s'opère via la création d'un compte AFS validé par la direction de la TGIR. L'utilisateur sera donc rattaché au groupe de traitement de la TGIR Huma-Num. Les supercalculateurs sont des machines propres à Huma-Num et sont utilisables de façon interactive via ssh.} \href{http://www.huma-num.fr/servicegrille/service-de-traitement-des-donnees}{Extrait du texte de présentation récupéré en 2015 sur le site web @Huma-Num}}

\Anotecontent{human_num_notecout}{Le centre de calcul de l'IN2P3 facturait à l'ex-TGIR Adonis 2000€ le million d'heures CPU (normalisé HES06). Le projet Geodivercity a utilisé pour ses simulations 4 à 6 millions d'heures HES06 par mois depuis novembre, soit l'équivalent d'environ 60K€ de CPU sur 6 mois. Un chiffre qui semble-t-il s'avère largement au dessus des quota prévu pour \textbf{l'ensemble des projets nécessitant du calcul en sciences humaines.}}

\Anotecontent{huma_num}{ Huma-Num est une très grande infrastructure (TGIR) visant à faciliter le tournant numérique de la recherche en sciences humaines et sociales. 
Pour remplir cette mission, la TGIR Huma-Num est bâtie sur une organisation originale consistant à mettre en œuvre un dispositif humain (concertation collective) et technologique (services numériques pérennes) à l’échelle nationale et européenne en s’appuyant sur un important réseau de partenaires et d’opérateurs.

La TGIR Huma-Num favorise ainsi, par l’intermédiaire de consortiums regroupant des acteurs des communautés scientifiques, la coordination de la production raisonnée et collective de corpus de sources (recommandations scientifiques, bonnes pratiques technologiques). Elle développe également un dispositif technologique unique permettant le traitement, la conservation, l'accès et l'interopérabilité des données de la recherche. Ce dispositif est composé d'une grille de services dédiés, d'une plateforme d'accès unifié (ISIDORE) et d'une procédure d'archivage à long terme.}


\Anotecontent{extrait_CNN}{\enquote{L'enseignement de l'informatique doit se développer au sein de l’Education nationale et à tous les niveaux : à l’école primaire avec la pensée informatique, au collège par le biais d’un cours de programmation en troisième, et au lycée par la généralisation déjà prévue du cours  d’ISN  à  toutes  les  terminales  générales  et  technologiques.  Afin  de  créer  des  citoyens  en capacité d'agir dans une société numérique,  maîtrisant plutôt que subissant les transformations liées au numérique, l'informatique doit être enseignée à tous.} \autocite{CNNum2014}}

\Anotecontent{historique_EPI}{L'association \enquote{Enseignement Public et Informatique} (EPI) créé en 1971 tient à jour un \href{http://www.epi.asso.fr/revue/histosom.htm}{@historique} très complet sur plus de 40 ans de rapports chaotique entre la politique, l'éducation et l'informatique.}

\Anotecontent{simplon}{Le texte d'ouverture sur le site est le suivant \enquote{Simplon.co est une école, qui propose des formations intensives de six mois pour apprendre à créer des sites web, des applications web/mobile, et en faire son métier. La formation s’adresse prioritairement aux jeunes de moins de 25 ans, non diplômés ou peu diplômés, issus des quartiers populaires, des diasporas et des milieux ruraux, aux demandeurs d’emploi, aux allocataires du RSA, ainsi qu’aux femmes qui sont insuffisamment représentés dans les métiers techniques. Elle est gratuite, ouverte à tous, pourvu que la motivation soit au rendez-vous !}}

\Anotecontent{openshaw_intuition}{\foreignquote{english}{The dream is of some kind of model-crunching machine which could be persuaded to search for interesting model specifications in the universe of all possible models relevant to a particular purpose} \autocite{Openshaw1983}}

\Anotecontent{pincette_oshaw}{Ces raisons bien qu'intéressante, sont aussi à prendre avec prudence, car elles sont évoqués selon un référentiel anglo-saxon, certe ayant traversé les frontières, mais ne devant pas échapper à une mis en perspective critique regard de l'historique particulier de la discipline géographique française. Un travail qui reste à faire.}

\Anotecontent{prevision_osaw}{ Comment ne pas penser au fameux \textit{big data}, tant à la mode ces dernières années, lorsqu'on lit la prose suivante ? \foreignquote{english}{
	
}\autocite{Openshaw1998}}

\Anotecontent{projet_beowulf}{Depuis 1994, et le projet de mise en grappe de \enquote{machine standard} nommé de façon générique \textit{cluster Beowulf}, cette forme de HPC est régulièrement renouvellé par tout les bidouilleurs adepte de la partie technologique de ce mouvement de plus en plus gros du \textit{Do It Yourself}. L'utilisation aujourd'hui de micro-pc à peine plus gros qu'une carte bancaire permet une mis en parallèle rapide et relativement peu couteuse de plusieurs centaines de ces systèmes autonome, dont le moins cher d'entre eux C.H.I.P est à moins de 10\$ (On connait aussi les systèmes Arduino, Raspberry, Edison, etc. déjà très usité par les \textit{makers}). On observe également ce type de pratiques avec l'achat de cartes graphiques (\textit{Graphic Processing Unit} GPU), dont la forte densité de processeur initialement dédié au calcul parallèle de rendu graphique gourmand que l'on trouve classiquement dans les usages du grand public (jeu vidéo, rendu 3D, etc.), est réutilisé pour effectuer du calcul scientifique performant à moindre cout, via des \textit{clusters GPU}.}

\Anotecontent{projet_parallela}{Voir par exemple le site du \href{http://supercomputer.io/}{@projet}}

\Anotecontent{reproduire_repliquer}{
	\hl{blabla}
}