
%\graphicspath{{Figure1/}}


\chapter{Construire et Évaluer des modèles en géographie}

\startcontents[chapters]
\Mprintcontents

Pour mieux comprendre quel sont les enjeux liés à la construction et à l'évaluation de modèle en géographie aujourd'hui, il est proposé dans ce chapitre d'adopter un point de vue historique pour comprendre en quoi la simulation est un outil incontournable et tout à fait compatible avec des critères dits "scientifique" guidant l'évaluation et la création de connaissance en géographie dès lors que l'on a affaire à des systèmes complexes.

Avant d'aller beaucoup plus loin dans la lecture des transferts entre épistémologues et géographes, il faut savoir que la nature relativement "lâche" de cette percolation s'exprime autant dans les discours que dans la méthodologie scientifique, et empêche par avance une quelconque lecture homogène de cette transition.

Deux courants viennent à influencer la géographie dans les années 1960-70. De la juxtaposition de ces deux courants naît une richesse d’interprétation qui permet tout autant l'expression renouvelé d'une point de vue individualiste (biographie d'Hägerstrand) que le point de vue holistique plus saillant dans le paradigme systémique en cours de transfert. 

La première influence qui marque la fondation de cette "nouvelle géographie" est probablement à chercher dans les répercussions dans le milieu scientifique du manifeste épistémologique néo-positiviste visant la reconstitution d'une unité des sciences, et la quasi juxtaposition lié à l'émergence de son successeur critique introduisant la pensée Hypothético-Déductive dans les méthodes des géographes à savoir Popper et Hempel; 

Le second courant naît à l'ombre de l'érosion d'un autre dogme, celui du déterminisme scientifique hérité de la pensée "classique" qui plonge de nombreuses disciplines en "crises" \autocite[20-23]{Pouvreau2013}. Une transition que l'on peut observer au travers de différents travaux majeurs, comme la statistique des lois de Boltzman pour la thermodynamique, ou le principe d’indétermination d'Heisenberg pour la mécanique quantique. Cette remise en cause peut également être associé à l'émergence d'une pensée "holiste" (ou pensée de la "totalité") qui se construit en confrontation avec la pensée réductionniste historique. 

Hérité d'un long processus de définition en partie cumulatif, il est encore aujourd'hui difficile de donner une définition générale et hors contexte de la plupart des concepts issue du paradigme systémique : "système"; "auto-organisation"; "équifinalité"; "émergence" ; etc.. Ceux ci naissent et s'enrichissent dans l'enceinte des premiers mouvements inter-disciplinaire \ref{subsec:systemique}, dont on peut avec le recul isoler au moins deux branches distinctes  un peu avant le milieu du XXème siècle : la Cybernétique de Wiener, et de façon quasi parallèle la théorie "organismique" de Bertalanffy qui deviendra un peu plus tard par extension la "General System Theory" (GST).

Ainsi, si c'est bien la Cybernétique ( \ref{ssubsec:cybernetic} ) qui la première remet en cause le schéma déterministe classique par l'introduction (ou plutôt la formalisation, car les origines du concept sont biologiques, avec Canon et Handerson) d'un nouveau type de causalité, les biais mécaniste et réductionniste \footnote{A ce titre, Von Neumman par exemple avec la théorie des automates reproducteurs, développe à l'époque une approche très réductionniste de la complexité biologique \autocite[783]{Pouvreau2013}; ce qui n’empêchera pas des développement théorique fructueux par la suite avec le développement des automates cellulaires, preuve que le réductionnisme n'est pas forcément à bannir !} empêche l'ouverture indispensable à l'expression de ces schèmes dans les sciences sociales.\autocite[783-784]{Pouvreau2013} Il faudra attendre la critique de Bertalanffy et la médiation d'Ashby pour que de divergence en convergence, ces courants fusionnent et établissent avec le temps un catalogue de lois isomorphes qui sert de base à une connaissance trans-disciplinaire encore féconde aujourd'hui, à condition de respecter le principe de l'"analogie prudente" entre disciplines. 

L'accent sera donc principalement mis sur les travaux de Bertalanffy ( \ref{ssubsec:gst} ) car son projet "systémique" bien qu'inachevé constitue le point de départ marquant la diffusion de ces concepts dans de nombreuses disciplines, l'interaction avec la physique étant de prime abord la plus fructeuse (voir l'inspiration réciproque de Bertalanffy et Prigogine). En référence à la crise de sa discipline, Bertalanffy annonce en 1932 qu'il est  "[...] un fait généralement reconnu que la biologie se trouve actuellement dans un état critique" \autocite[21]{Pouvreau2013}. C'est dans ce contexte difficile, que celui ci amène dans son premier traité de biologie théorique en 1932 les bases d'une pensée qui refuse déjà ce dualisme stérile opposant "holisme" et "réductionnisme", et pré-figure par là sa pensée systémique. Bertalanffy apparaît aujourd'hui comme une figure majeure de ce mouvement qui sera amené à dépasser et réconcilier les points de vue "vitaliste" et "mécaniciste / réductionnistes" \footnote{Pour \autocite[47-54]{Pouvreau2013}, il ne fut dans aucune mesure radical dans ses points de vue émergentistes ou holistiques, l'émergentisme étant pour Pouvreau un produit dérivé de l'holistique on pourra alors se référer à son manuscrit pour mieux saisir les nuances de ces différents concepts} grévant la formation d'une nouvelle Biologie au début du XXème siècle\autocite[20]{Pouvreau2013}. Sa théorie "organiciste" fonde la possibilité d'une biologie théorique qui s'appuie sur les influences et les débats qui anime la communauté entre 1900-1930, tant dans la mouvance des auteurs qui participe à l'émergence d'une vision holistique de la biologique, que dans les tentatives de dégager des premier isomorphisme faisant état d'un rapprochement possible entre physique, mathématique et biologie (Lotka, etc.) 

Au coeur de la théorie des "système ouverts" les concepts d'équifinalité, de hierarchisation de statistique sont dans leur opérationalisation \ref{subsec:operationaliser_concept} autant d'incitation à utiliser les récents progrès de l'informatique des années 1950-60 pour explorer un univers, non pas tant complexe dans sa description (comme en témoigne Simon, des problèmes complexes peuvent très bien être dérivé de règle simple) mais dans la multiplicité d'approche (trajectoire, échelles, interactions) qu'elles permettent.

Toutefois, il est difficile de donner des détails sur la nature et le degré de percolation de la GST dans les sciences humaines et sociales, pour plusieurs raisons. La première tient du fait que la GST est un "projet" plus qu'une théorie, et que celle ci est resté largement inachevé \autocite{Pouvreau2013}; la deuxième raison se pose en conséquence de la première, le flou conceptuel de ce "projet" a débouché sur une multitude de lecture et d'application des concepts inter et intra-discipline, en fonction des courants théoriques qui ont transférés tout ou part des concepts : Boudon, Barel, Lemoigne, Morin, etc. La lecture d'Orain sur la percolation de la GST dans la géographie française des années 1970 est un bon exemple de cette fragmentation dans les approches \autocite{Orain2001}. Un argument a double tranchant donc,  la faiblesse conceptuelle du "projet" qui a mené à des dérives aussi importantes témoigne également à la fois de la capacité heuristique des concepts une fois intégré par la discipline \autocite{Lallement2014}, ce qui nous intéresse plus particulièrement dans le cas de la géographie.

La notion de hiérarchie statistique qui suppose l’emboîtement de systèmes, largement développé par Bertalanffy en biologie, est tout à fait applicable de façon plus générale à tout systèmes dont on ne maîtrise pas l'ensemble du fonctionnement. Il parait ainsi possible de formuler/ énoncer des loi et d'appliquer la méthode Hypothetico-Déductive (oui mais comment?) à différents niveaux d'organisation. Autrement dit il est tout à fait possible de s'appuyer sur les lois déterministes qui découle des loi statistique d'un niveau inférieur pour fournir des explications, même partielles. Un constat partagé par Simon lorsqu'il affirme dans \textit{Les sciences de l'artificiels} que "cette construction de l'édifice de la science du toit vers ses fondations encore inachevées a été rendue possible par le comportement du système: chaque niveau ne dépend que d'une façon très approximative, simplifiée, abstraite, des caractéristiques du système au niveau inférieur.!" et d'ajouter par une analogie très concrète "Sinon la sécurité d'un pont ou d'un avion pourrait dépendre des particules élémentaires qui ne sont pas encore éprouvées." \autocite[48-49]{Simon2004} Dès lors, et pour continuer avec les propos éclairant de Simon, il est évident que la simulation est l'outil idéal pour la simulation des systèmes que l'on ne comprend que partiellement ! 

Dans ce cadre, le discours de Simon est considérée par certain comme celui d'un philosophie "pragmatiste"  (ref) évoquant de façon implicite mais aussi explicite la logique abductive Peircienne. En effet, Herbert Simon, prend rapidement le contrepied de Popper sur la thématique de la logique de découverte scientifique \autocite{Simon1973}, et propose une contre proposition qui passe par une relecture de l'abduction de Peirce, qu'il nomme rétroduction. Un constat cohérent avec les développement de théories des tenants du paradigme systémique en géographie, comme le montre l'introduction de la théorie évolutive des villes de Denise Pumain \autocite{Pumain91}, qui fait référence explicite dans son introduction aux concept de rétroduction.

Même si dans les premier temps seule une petite partie des lois issues de la GST percole en géographie, des notions comme celle de hierarchie ou d'équifinalité dans le cadre de système ouvert va être très largement repris par plusieurs domaines en géographie, on pensera notamment en géomorphologie avec les travaux de Chorley,  en géographie régionale ou Haggett souligne déjà en 1965 que "considérer la région comme un système ouvert présente l'avantage d'attirer l'attention sur les liens entre forme et processus, et de mettre la géographie au rang d'autres sciences" (extrait de la traduction française \autocite{Hagget1973}, et plus tardivement en géographie humaine. \autocite[91]{Dauphine2003} 

Témoin de cette volonté d'application des concepts de la GST, on trouve dès les années 1950-60, des universitaires qui vont s'avérer pionnier dans l'opérationalisation de ces principes, usant de tout les nouveaux outils à leur dispositions, la simulation y compris, pour fonder ce qui sera la révolution méthodologique associé à la révolution théorique, qui constitue en réalité deux face de la même révolution \autocite[p27]{Claval1977} . \ref{subsec:simulation_langage} 

Les pionniers Suédois de l'école de Lund et Américains de l'école de Washington \ref{ssubsec:courant_pionniers} qui vont porter les prémisses de cette révolution saisissent rapidement cette opportunité d'accélérer la résolution de modèles explicatifs déjà éprouvé avec du papier et du crayon en usant des tout premiers ordinateurs; car c'est à cette époque que sont justement développés les premiers langages informatiques génériques, et même si ceux ci sont d'abord réservé à quelques élites pionnières ayant accès à du matériel et aux multi-compétences adaptés, très vite de jeunes chercheurs formé à l'interdisciplinarité vont permettre la diffusion de ce savoir faire (Marble, Tobler, etc.). L'occasion donc pour les géographes de se frotter à ce qui correspond dans la discipline géographique à de toutes nouvelle méthodes de résolution mathématique, comme la programmation linéaire (avec le transfert des modèles économique redécouvert et introduit aux géographes par Isard), ou la méthode probabiliste de Monte-Carlo (introduite par l'école Suédoise), et un peu plus tard les automates cellulaires (en lien direct avec les travaux de Von Neumman).

Cette percolation de l'approche systémique se fait principalement par le biais de la GST, et permet de diffuser la notion de système ouvert à différents objets et niveau géographiques, Chorley avec la géomorphologie en 1962 \autocite{Chorley1962}, Berry avec les villes en 1964 \autocite{Berry1964}, etc. A ce titre \autocite{Berry1964} parle de \textit{ symbolic models} \autocite{Ackoff1961} pour désigner ces modèles  "which provide idealized representations of properly formulated and verified scientific theories relating to cities and sets of cities perceveid as spatial system"

L'approche conjointe re-introduisant l'individu comme paramètre explicatif (Hägerstrand et les biographies individuelles guidant les parcours et les rencontres des individus) vient enrichir et modérer la réintroduction d'une perspective holiste en géographie, deux éléments qui marque un premier pas irréversible de la géographie dans une complexité qui ne s'épuise pas dans ce débat stérile opposant deux instance extrémiste caricaturale entre holisme et individualisme méthodologique. Les premiers modèles de simulation spatialisés réalisés dans ce cadre souffre très rapidement des limitations technologiques de l'époque, et il faudra attendre une nouvelle génération d'ordinateurs capable d’appréhender cette dimension supplémentaire qu'est l'information spatiales avant d'envisager une possible diffusion de ces techniques \autocite{Marble1972}. \ref{ssubsec:echec_simulation}

Mais il est en général admis que c'est le  modèle de ville simplifié \textit{Urban Dynamics} mis au point par l'équipe de Forrester et l'introduction de la "dynamique des systèmes" (via le langage Dynamo) qui a largement polarisé le débat sur la véritable complexité des systèmes urbains \autocite{Batty2001} \ref{ssubsec:rebond_geo}, et cela malgré sa vision aspatiale d'une ville opérant isolé dans un système fermé aux interactions (ce qui peut dans un premier temps peut paraitre paradoxal)

D'un point de vue interne, le modèle \textit{Urban Dynamics} n'introduit pas tant d'originalité par rapport aux éléments acquis par la rencontre entre la vision d'Hagerstrand et les pionniers universitaires systémistes 10 ans auparavant, les premiers modèles de simulation qui implémente la dimension temporelle, stochastique, et la non linéarité des interactions sont implémentés en Fortran (le seule langage scientifique de l'époque) et datent d'avant 1965.

Le modèle de simulation et la techniques des systèmes dynamiques de Jay Forrester apparait comme un fait plus marquant dans le paysage scientifique, probablement pour plusieurs raisons. D'une part a) la thématique du modèle est très polémique dès sa sortie, ce qui lui permet d'assoir une grande visibilité médiatique autant auprès du grand public que des universitaires, b) il propose avec son modèle le premier langage graphique et informatique générique et accessible qui opérationalise tout une partie des concepts forts de la systémique, c) il n'utilise pas de données en dehors du calibrage des paramètres, c'est un modèle à visé explicatif et pédagogique, d) il donne à voir la supériorité des dynamiques non-linéaires pour décrire des effets contre intuitif dans les politiques publiques, un résultat certe non exploitable dans un cadre prévisionel (\textit{forecasting}), mais qui démontre la capacité explicative de modèles plus parcimonieux, dans un contexte où les modèles déterministes statiques extremement couteux pilotés par la RAND depuis 10 ans sont largement pris en défaut, e) il est amené à participer d'un éclairage universitaire international, avec sa participation au club de Rome, ou de nombreux scientifique vont faire la découvertes de ces techniques innovantes.

Le point sur la validation est particulièrement important car il propulse dans le débat public - parfois avec violence comme en témoigne les écrits de Forrester -   le problème de la validité de tels modèles théoriques, et cela dans une décennie à venir où la simulation va être confronté tantôt à un certain désengouement comme c'est le cas dans certaines sciences sociales (archéologie, etc.), tantôt à une reconversion comme c'est le cas en géographie. Forrester donne à voir avec son langage graphique la structure causale de son modèle, une première qui démontre aussi la puissance d'un outil dans la démocratisation et la diffusion des techniques mais aussi des théories (systémique). Car Forrester ne fait que mettre en lumière avec un langage graphique adapté des éléments de vocabulaire de la cybernétique, retranscrit ensuite dans un langage DYNAMO qui s'appuie sur la discipline des systèmes dynamiques, en pleine construction, avec les apports des tenants physiciens de la systémique, entre autre l'école de Prigogine et l'école de Haiken.

Cette démonstration, comme on pourrait presque dire, ne fait que mettre encore plus en valeur l'échec de l'application brute des modèles importés de l'économie géographique à la réalité, et cela malgré les investissement pharaonique de la RAND. Se dessine alors une double constat qui marque aux yeux du public (et des universitaires, qui ne participe pas tant que cela à ces constructions) \autocite{Lee1973} l'incapacité des modèles statiques d'équilibre pour prédire (et expliquer!) à une échelle macro cet objet qui s'avère beaucoup plus complexe qu'on ne l'imaginait, la ville et les systèmes urbains. Le retour à la parcimonie qui s'ensuit, l'exploitation des plus belles réussite de la RAND comme le modèle de Lowry,  ou l'extension des isomorphismes explorés par les pionniers universitaires (modèles gravitaire, modèle Lotka, etc.) Américains, mais aussi Européen, du fait de la diffusion des concepts systémique opérant dès 1970 via les physiciens européens; tout cela va permettre par la suite de reconsidérer la démarche de construction de modèle dans un cadre épistémologique de construction de connaissance qui cette fois-çi est pleinement conscient des contraintes (mais aussi des richesses) fortes imposés par les concepts issue du paradigme systémique, notamment l'équifinalité.

Suite , le temps passe, les problèmes restent similaires :
> Les modèles agents, nouveau mode d'expression, mais problèmes globalement identiques --
> Nécessité d'un cadre épistémologique adapté (appuyé par un historique des méthodes à géocités)
	>> Le problème d'une séparation en deux temps a) construction b) expérimentation 
	>> Retour sur des débats en cours, proposition de Machamer sur mécanismes (au sens opérationel, pas philosophique mécanistique)
	>> Dimension historicisation construction des modèles
	>> Favoriser isomorphisme loi/modèles, mais aussi expérimentation
	
%Introduction du temps rendu possible avec la simulation remet en cause à la fois la construction mais aussi la validation des modèles. L'évolution des individus mais aussi des structures et des interactions à différentes échelles rend plus difficile la justification et la mise en cohérence entre des mesures d'adéquation sur des lois statistiques ou des séries de données temporelles, et les hypothèses impliqués dans la construction des modèles ... 


%Jay Forrester introduit en réalité avec la dynamique des systèmes un support graphique et informatique reprenant "des schemes systémiques" bien connu, permettant ce faisant une opérationalisation (et une diffusion/démocratisation) beaucoup plus aisé des systèmes dynamiques non linéaire \footnote{C'est en ces termes que la société des systèmes dynamiques définit la discipline : \textit{System dynamics is a computer-aided approach to policy analysis and design.  It applies to dynamic problems arising in complex social, managerial, economic, or ecological systems — literally any dynamic systems characterized by interdependence, mutual interaction, information feedback, and circular causality.} source : http://www.systemdynamics.org/what-is-s/ }, une branche des mathématiques à l'époque en pleine extension, et en liaison directe avec les travaux récents de Prigogine, Haken, etc. Le langage Dynamo sous jacent, remis à jour plusieurs fois, permet tout à fait de gérer des systèmes dynamiques classique à la fois linéaire / non linéaire déterministes, mais aussi des processus stochastique. 

%En parallèle de la dynamique des systèmes, une autre école se développe et se concentre sur l'autre aspect important des systèmes ouverts, à savoir la notion de bifurcation propre à l'évolution des systèmes ouvert éloignés de l'équilibre. L'application des outils informatiques développés sous l'influence de l'école européenne de Prigogine et de l'école de Haken, les mieux placés pour transférer avec l'aide des géographes les concepts de bifurcations dans des système éloignés de l'équilibre observés en physique des systèmes ouverts.  

%Deux écoles se développent et marquent le début d'une autre vague de diffusion des techniques (après celle de la micro-simulation donc, un mouvement qui va continuer de se diffuser par la suite au travers de grands projets à l'international) de modélisations dynamiques et spatiales à l'international : l'école de Bruxelle mené par Peter Allen, et l'école de Leeds mené par Wilson. \autocite[30]{Louail2010}



%Une double inspiration pour Forrester, celle des servomécanismes, et celle de la Cybernétique, avec une nuance dans la perception de la notion de feedback, qui semble-t-il est perçu non pas comme une régulation contrant l'aggression extérieure, mais plutôt comme un outil de description des relations interne, ce qui suppose construction de modèle dynamique permettant de capturer/retenir uniquement dans un ensemble minimal d'élément et d'interactions entre les éléments, en favorisant les interactions entre celle ci. 


-- // --


%Ce cadre de pensée a permis la constitution d'un corpus de concept innovant, pour la plupart toujours d'actualité au travers de ce que l'on peut appeler "paradigme de la complexité", et qui nous sert de référentiels pour justifier d'outils tels que la simulation de modèle. A l'heure où cette technique est encore minoritaire et critiqué dans certaines disciplines des SHS, il est intéressant de renverser la question, et de faire un détour par les origines de ces concepts, et leur translation dans des outils pour essayer de comprendre pourquoi l'usage de ceux-çi a pu dans certaines branches des SHS a pu constituer tout autant un espoir qu'une déception, encore visible aujourd'hui comme en témoigne un certain nombre de publications. 



Critiqué par les géographes du fait de sa fermeture causale justement ... (a definir pourquoi ? )

Est ce que cela permet l'évolution des structures ? L'acquisition de fonction, de nouveauté, etc ? 
Ca rejoint le discours de Denise dans la complexité je pense, et Dauphiné, avec la notion d'évolution des structures qui n'est pas forcément permises par 


En un certain sens, Jay Forrester "donne à voir et a manipuler" la structure causale de son modèle

Importance de l'outil dans la démocratisation, un fil rouge dans notre propos.

 ouvent reconnu à tort comme une technique de simulation déterministe, il est tout à fait possible d'obtenir des comportements imprévisibles en partant de condition initiales identiques.

  couplé à l'absence de réelle données empiriques pour comparaison renforce cette idée qu'il existe un gradient opposant  compromis possible entre prédiction et explication.

 plus parcimonieuse mais aussi plus robustes que la plupart des autres modèles réalisés entre 1960 et 1970 sous pilotage de la RAND, alors justement touché par l'absence de résultat (somme toute relatif, les universitaires ayant été largement inspirés par les travaux des instituts de plannification  (ref Batty)) de la plupart des applications de modèle théoriques économiques statiques à l'équilibre à des cas réels. 

De plus le modèle de Forrester donne à voir de façon explicite la chaîne causale qui anime les systèmes;

Mais son approche reste finalement plus centré une vision mécanistique de la Cybernétique, et plus focalisé sur les boucles de rétro-action que sur l'explication des interactions probabiliste qui animent les systèmes. Autrement dit, la modélisation à base d'équation différentielle telle que présenté par Forrester ne permet pas l'expression spontané et dynamique des bifurcations qui animent les systèmes ouverts stationaire éloigné de l'équilibre, elle permet seulement leur étude au travers de l'exploration de l'espaces des paramètres via les analyses de sensibilités. Même si par la suite nombre d'auteur on travaillé, publié sur la compatibilité de Dynamo avec ce type de projets.


la véritable prise de contact de cette complexité se fait au travers des premiers langages "spécifiques" à la simulation , à la fois plus facile à mettre en oeuvre, mais aussi plus focalisé sur l'expression informatique (opérationalisation) des concepts issues de la GST, qui a fortement impacté les SHS et la géographie. 



La construction de cette nouvelle "pensée complexe" \autocite{Morin1990} se veut alors à l'opposé d'une "pensée simple" réductionniste jusqu'alors dominante, et parfois intolérante. 


 point de vue curieux nous pousse alors à analyser cette période pour en dégager le fondement de ses principes, et en quoi ils accompagnent la révolution à l'oeuvre dans 

Ainsi avoir identifié ce contact entre GST et science sociale au travers du prisme de son opérationalisation par la simulation, il est intéressant de relever un pattern qui même si il semble moins toucher la géographie, témoigne encore de la méfiance des autres disciplines des SHS envers cet outils. Peut on voir dans les difficultés que les SMA éprouvent encore aujourdh'ui pour se diffuser un certain écho qui n'est pas sans rapeller des difficultés déjà rencontré par le passé ? C'est le point de vue que nous essaierons de défendre par la suite, est ce que le problème de l'évaluation des simulation en SHS, un problème originel et finalement indépendant des techniques usités, n'est pas également celui des moyens mis en oeuvre pour répondre réellement aux problématiques levés par les concepts sous entendus par la complexité en SHS (équifinalité, auto-organisation, etc.)  ? Autrement dit, n'est il pas plus intéressant de se pencher sur la problématique "des moyens" pour l'évaluation pour la construction et l'évaluation des modèles plutôt que sur la finalité de l'évaluation en elle même, largement contextuelle, et finalement difficilement compatible avec une approche qui se voudrait universalisante. Pour cela il faut donner une vision plus précise de cette période charnière que sont les années 1960-70 pour la géographie. 


%Autrement dit la question de la "validation" est quasiment hors de notre propos, car impossible du fait de l'équifinalité, reste celle des moyens qui sont mis en oeuvre pour l'évaluation, autrement dit la construction du modèle et donc son exploration, qui ne forme qu'une seule et même chose.

L'informatique est largement inspiré des systèmes biologiques, et la théorie des systèmes ouverts, ainsi Von Neumman ne s'y trompera pas lorsqu'il développera seulement armé d'un papier et d'un crayon le concept d'auto-reproduction qui donnera lieu un peu plus tard aux automates cellulaires, et Hewitt qui développera sa propre vision des systèmes ouverts ("open systems") en informatique, fondant ainsi les prémisses d'une intelligence distribué à l'origine des développements des modèles agents.

Les bénéfices de la simulation sont nombreux, et pour n'en citer que quelqu'un elle permet a) de projeter dans le temps b) de multiples hypothèses et c) organisés en de multiples niveaux d'abstraction d) potentiellement emboités les uns des autres. D'abord mobilisé comme simple machine à calculer plus perfectionné, des auteurs issues des sciences humaines, mais pas seulement, voient dans les capacités symboliques des ordinateurs une possibilité d'expressivité beaucoup plus grande.

L'introduction des processus irréversible à tout à voir avec la notion du temps, comme en témoigne les travaux de Prigogine (à détailler ?), et la simulation amène justement cette formidable capacité à projeter des hypothèses dans le temps 

% Ordinateur, liaison entre technique et méthodologie

Si l'usage de la simulation en science sociale va évidemment se construire autour d'échec et de succès qui ne pourront évidemment pas tous être évoqué dans la partie \ref{subsec:simulation_SHSGEO} qui en réfère. Toutefois afin de montrer toute la mesure de la percolation de ces techniques dans les sciences humaines il parait nécessaire de l'évoquer dans un cadre plus large que celui seul de la géographie, car l'emploi d'un même outil par de multiples disciplines entraine dans son sillage la construction d'un objet de recherche commun. Rapidement une communauté va se structurer autour d'ouvrages comme celui de Randall Schultz \autocite{Schultz1972} ,Thomas H. Naylor \autocite{Naylor1966}, etc. qui discutent des enjeux et des limites techniques associés à l'utilisation de ce nouvel outil qu'est la simulation dans un cadre largement inter-disciplinaire. 

Un détail qui a son importance quand on sais que la géographie en elle même s'est largement nourri des travaux novateurs issues de disciplines annexes pour construire sa propre révolution.  Ainsi par exemple pour Johnston, cette période de renouveau en géographie a tout à voir avec la mise en place de cette approche inter-disciplinaire, et pour lui il n'y a que deux innovations à retenir provenant réellement du champs historique de la géographie : les travaux de l'école de géographes allemands avec la théorie des places centrales d'une part remis au goût du jour par Isard, et les travaux d'Hagerstrand sur la diffusion des innovations d'autre part. La plupart des autres avancées théoriques provenant selon lui d'échange inter-disciplinaire, de sociologues(Von Thunen, Hoover, Losch, Weber), d'économistes (Zipfs, Stouffer), de nouvelle zones de friction vers lequel s'est tourné la géographie pour repositionner ses questionnements. Une direction appuyé par le témoignage de Paul Claval pour qui le mouvement de la "New Geography" \autocite[6]{Claval1977} "[...] ne tarda pas à s'enrichir dans deux directions : à cotés des modèles théoriques empruntés à l'économie, les chercheurs apprirent à utiliser ceux que proposer la sociologie, l'ethonologie, ou la psychologie, et se mirent à en construire eux même"

%Model Building => \autocite{Gullahorn1965a} interet pour la psycho
%Simon Herbert 1954 =>  Some Strategic considerations in the development of social science model Paul F Lazarfeld
%Archéologie / Sociologie / etc, quelques points d'entrées fondateurs dans la simulation ... A renforcer avec une argumentation qui relie tout ça.
% =>  Systémique permet d'introduire temps et chaine de causalité dans le déroulement d'un processus.. peut on dire qu'il s'agit d'un retour en force de la méthode hypothético - déductif dans les disciplines des sciences sociales ? Et parmis l'arsenal a disposition il y a la simulation.

Ces remises en causes dans l'ensemble des sciences au début du XXème siècle vont de paire avec l’avènement de nouveaux moyens techniques, une occasion ici de questionner ces rapports bilatéraux souvent fructueux qui se nouent entre outils et problématique scientifique. L'apparition de l'ordinateur, et son usage pour la simulation \autocite[50]{Zwirn2006} \autocite{Simon1996} apparaissent ainsi comme des outils indispensables à l'appréhension de la complexité.

L'usage de simulateurs apparaît alors comme une méthode idéale pour donner corps à tout un pan de cette révolution méthodologique qui anime la plupart des disciplines des sciences humaines à cette période. 



% Application aux SHS 

Soulève la question de la validité des résultats compte tenu de l'équifinalité ...

% l'une pouvant devancer l'autre. Ce pattern qui apparaît dans les années 1960 / 70 avec la démocratisation de l'ordinateur, est probablement amené à se répéter à une autre échelle, suivant un autre objectif, qui est non plus celle de l’exécution des modèles, mais celle de l'exploration des modèles ?

% => Reintroduction argument de la précédente introduction, limites déjà révélé historiquement, et qui se reproduisent pas la suite ?

% Opérationalisation et application en géographie

% En gros la géographie semble mieux s'en tirer ... 


Il est important d'essayer de capter, même si c'est de façon partielle, ces surfaces d'interactions qui naissent du recouvrement entre la sphère des idées et celle de leur opérationalisation, l'une et l'autre étant intimement relié dans une boucle vertueuse.

 entre la mise au points des concepts ou abstraction propre à la manipulation des idées, et d'autres part les outils qui peuvent prendre la forme de méthodologie, ou d'outils executant celle ci.

Un point à mettre en regard de l'acceptation actuelle de la simulation de modèle en SHS, un outil/méthodologie qui peine encore à s'imposer en SHS alors que sa fonction heuristique et la logique "abductive" \autocite[3;10]{Banos2013} qu'elle sous tend est absolument nécessaire pour appréhender la complexité des systèmes sociaux, et cela de façon complémentaire et non opposé à l'étude classique empirique qualitative ou quantitative. Fort de problématiques qui date maintenant du milieu du XXème siècle, il est intéressant de voir par un bref aller-retour dans le temps certaines critiques liés à la scientificité de la méthode persister encore aujourd'hui. \autocite{Waldherr2013}  Des raisons qu'il faut chercher à comprendre, pour produire certes un contre discours, mais également des outils à même de légitimer et porter ce contre discours.

++ Le modèle est souvent vue comme "ajout", "apport" à la théorie, pourtant il aussi intéressant dans sa démonstration négative, de remise en cause de l'existant, de l'acquis, de l'effet pervers possible ++

++ information sur l'équifinalité, son origine, et le fait qu'elle n'a rien à voir avec la technique, mais avec le paradigme de la complexité, a voir avec historicisation du processus dans la construction du modèle et de la simulation ++ 

Étonnant de voir que la systémique, inspiré largement par la GST de Bertalanffy, accepte depuis longtemps de raisonner en terme statistique et non plus en trajectoire individuelle pour tenter de caractériser les lois qui mène à l'émergence de structure ou de dynamiques. En cela les systèmes sociaux ne sont pas moins complexes que les systèmes biologiques, au contraire.


++
Une métaphore échiquéenne illustre simplement la conception qu’avait Bertalanffy de la totalité. Au jeu d’échecs, la dame combine en elle la tour et le fou. Elle peut se déplacer comme l’une ou l’autre de ces deux pièces, et contrôle donc toutes les cases correspondant au contrôle permis par chacune d’entre elles : en apparence, sa puissance est donc celle de la somme des puissances de la tour et du fou. Néanmoins, la potentialité de choix dans son déplacement signifie que s’instaure en elle une relation entre tour et fou, leur unité, qui s’exprime par des possibilités combinatoires « émergentes » et implique que la puissance de la dame est en fait supérieure à cette somme : le fait est que son équivalent « matériel » est en général (selon la configuration de l’ensemble des pièces de l’échiquier) une tour, un fou et un ou deux pions – autrement dit, le « matériel » étant égal par ailleurs, une dame blanche dominera en général une paire de pièces noires formée d’une tour et d’un fou, l’équilibre nécessitant en général un ou deux pions noirs supplémentaires. Ainsi la « totalité » (la dame) est-elle plus que la « somme » de ses « parties » (la tour et le fou) ; mais elle n’est pour autant rien de plus que la « somme » de ses « parties » et de leur relation.
++ \autocite[52]{Pouvreau2013}

-------

La tentation serait grande de se placer directement dans un cadre d'étude au plus proche de nos expérimentation, en s'appuyant sur un savoir aux larges épaules pour évoquer la communauté et ses débats qui depuis bientot 20 ans ne cesse de s'aggréger autour des problématiques de la simulation de modèle agent en SHS, et dans notre cas en géographie. 

Le choix est fait ici d'un oeil un peu épistémologique, qui tient à l'écart sans toutefois l'oublier, ces avancées techniques qui ont bouleversés, chacune en son temps, la géographie dans son rapport à la connaissance (année 90 pour les modèles multi agent, 70 pour les Automate cellulaire). Cette prise de recul est avancé au profit d'une étude plus concentré sur un des aspects (parmis d'autres) motivant la construction d'un modèle, la volonté d'établir de nouvelles connaissances ou plutot de préciser celle déjà existantes.

Si on se réfère aux ouvrages et aux écrits des auteurs reconnus comme étant pioniers de la discipline (Chorley et Hagget, etc.) la volonté nomothétique de la géographie semble d'abord s'être concrétisé dans l'établissement d'une vaste science des modèles et de la modélisation qui a donné lieu par la suite à de nombreux exercices de classification.

 Ce sport encore largement pratiqué aujourd'hui du fait de la nature complexe et de la polysémie caractéristique de l'objet "modèle", qui a suivi ces 50 dernières années l'évolution des pratiques scientifique, et des nombreuses avancées technologiques. Ces 20 dernières années ont représentés en ce sens des changements technologiques si énormes que l'épistémologie des modèles et de la modélisation ne saurait être encore un terrain stable, et reste au contraire un domaine largement en construction. \autocite[12]{Varenne2012} \autocite[9]{Varenne2013}
 
 ...

Les années 1950-70 sont bien connu des géographes pour être porteuses d'un courant de renouveau dans la discipline géographique. Comme il serait tout à fait impossible de faire une lecture exhaustive des influences en amont de cette transformation, ce travail nécessitant à lui seul une thèse, la lecture proposé par l'auteur propose de faire une synthèse non exhaustive au travers de quelques axes qui nous paraissent marquants, transformants dans les pratiques de formation et d'évaluation des modèles (et des connaissances) en géographie, cela en regards d'une critère censé qualifier la valeur scientifiques des modèles ainsi créés, plus souvent évoqué comme la problématique épineuse de la "validation des modèles".

\section{Modéliser en géographie}

\subsection{Le modèle, définitions }

% Définition générale
% Spécificité en géographie

\paragraph{Définitions générales}

La première définition généraliste et aussi la plus couramment rencontré dans la littérature est probablement celle de Marvin Minsky établit en 1965 \autocite{Varenne2008} \autocite[15]{Varenne2013}  : « Pour un observateur B, un objet A* est un modèle d’un objet A, dans la mesure où B peut utiliser A* pour répondre à des questions qui l’intéressent au sujet de A » \autocite{Minsky1965}

a partir de cette définition, Franck Varenne \autocite{Varenne2008} relève les 5 points suivants : 
\begin{enumerate}
  \item Le modèle n'est pas nécessairement une représentation
  \item Le modèle doit son existence à l'existence d'un observateur subjectif, et d'un questionnement lui aussi subjectif
  \item Le modèle est un objet qui a une vie propre, une existence autonome
  \item L'existence du modèle est justifié par l'existence d'une "fonction de facilitation"
  \item Cette caractérisation minimale permet l'établissement d'une typologie
\end{enumerate}

Franck Varenne propose dans des travaux plus récents \autocite{Varenne2013} d'associer à cette définition les travaux de Mary S. Morgan et Margaret Morrison qui replace et caractérise le modèle dans une enquete de connaissance par la fonction de médiation (point 4 de la liste) qu'il investit, une facon de faire écho à la "problématique" motivant la construction de modèle établit dans la définition de Minsky.

Un modèle est ainsi définit comme " \textit{un objet médiateur qui a pour fonction de faciliter une opération cognitive dans le cadre d'un questionnement orienté}, opération cognitive qui peut être de cognition pratique (manipulation, savoir-faire, apprentissage de geste, de techniques, de conduites, etc.) ou théorique (récolte de données, formulation d'hypothèse, hypothèse de mécanismes théoriques, etc.) " \autocite{Varenne2013}

Cette typologie qui dénombre pas moins de 5 familles pour un total de 20 grandes fonctions permet de situer efficament la ou les problématiques - rien n'empeche les fonctions de se recouper - qui motivent la construction d'un modèle. 

\paragraph{Le modèle en géographie}


%Spécificité de l'objet d'étude "Le spatial et le temporel", objet d'étude des géographes
 
Partant de la grille proposé par Varenne \autocite{Varenne2013} il est possible de proposer un positionnement du modèle tel qu'on l'emploi le plus souvent aujourd'hui en géographie humaine quantitative; et de préciser le substrat sur lequel nous greffons différentes fonctions de médiations.

%Definition de base%
Etonnament, et alors que dans les faits beaucoup de choses ont changés sur le plan des pratiques, des techniques, des institutions, la référence à des définitions datant de 1965 reste encore aujourd'hui tout à fait possible. (ajout de papier récent citant Haggett, par ex \autocite{Antony2013}, \autocite{Dastes2001} etc.)

Le modèle est avant tout un construit; comme nous le rapelle dès 1965 Peter Haggett, s'appuyant sur la typologie et la réflexion d'Ackoff, et définit dans \textit{l'analyse spatiale en géographie humaine} je cite : «En construisant un modèle (model building), on crée une représentation idéalisée de la réalité afin de faire apparaître certaines de ses propriétés » [30]\autocite{Haggett1965}. Une définition toujours d'actualité dans la discipline; du moins en France \autocite{Brunet2000} \autocite[295]{Bailly1995} \autocite{Dastes2001}

A la différence de la première définition proposé par Varenne \footnote{Franck Varenne propose un panorama beaucoup plus complexe de la notion de modèle dans son ouvrage \texit{Théorie,Réalité, Modèle} paru en 2012. \autocite{Varenne2012}, celle de Haggett porte en 1965 l'emphase sur l'activité même de modélisation, qui si on consulte les propos d'Ackoff \autocite{Ackoff1962} (déjà cité par Ackerman en 1958) et de Kemeny-Snell \autocite{Kemeny1962}, les deux sources d'inspirations d'Haggett \autocite[106]{Berry1963} pour sa discussion sur les modèles, est à mettre en relation avec la démarche de production de connaissance scientifique.

% Le rapport avec démarche construction connaissance %

Pour Besse \autocite{Besse2000}  il existe "un \textit{projet} de la science, qui cherche à rendre compte rationnellement d'une réalité vécue d'abord comme opaque, et qui pour cela élabore des explications qui sont ensuite soumises à des tests destinés à en controler la validité." Toujours selon Besse, il ne s'agit pas de "viser le réel" mais de "viser ce réel \textit{à travers} des procédures théoriques et expérimentales", une "\texit{médiation discursive} qui est significative, et qui désigne la présence au coeur de l'activité scientifique d'un projet d'objectivité qui la définit de façon essentielle".

La science s'appuyant avant tout sur la normalisation d'un discours scientifique pris dans son hétérogeineité, et il existe donc toute une histoire qui accompagne différentes versions de cette opération explicative. \autocite{Besse2000} Sans compter qu'il existe un fossé entre l'idéal des pratiques scientifiques comme objet d'étude des épistémologues et la pratique réelle des scientifiques, et on pourra citer en exemple les tentatives courageuse \autocite{Harvey1969} mais au final malheureuse \autocite{Gale1972} de certains géographes qui ont tenté de se placer entre les deux approches ..

Dès la montée en puissance de cette volonté nomothétique au coeur de la "révolution quantitative" on comprend que c'est le modèle, au centre d'un dialogue entre théorie et expérience, qui dès lors endosse comme dans les sciences naturelles, la responsabilité de cette \texit{médiation discursive}.

L'ampleur de cette "révolution" des modèles qui s'ensuit \autocite{Wilson1972} \autocite{Varenne2014} se mesure quasiment à l'aune de cette collecte massive et quasi-exhaustive des modèles réalisés entre entre 1950 et 1970, publiés dans "Models in Geography" de Chorley et Hagget. Pour Golledge \autocite{Golledge2006}, ce livre marque un tournant dans la discipline et efface l'image d'une géographie humaine jusqu'à présent avare de théories, de lois, de modèles en donnant à voir la mise en place d'une véritable infrastructure scientifique support de cette diversité, appuyé par un language commun  mathématique et statistique.  %Paradoxalement il donne aussi à voir les limites des approches proposés pour létude de l'homme dans son environnement, et offre ainsi le matériel idéal pour appuyer la formulation critique des géographes radicaux marxiste, un mouvement qui s'amplifie dès le début des années 1970 en parallèle avec la conjoncture politique nationale et mondiale. \autocite{Golledge2006}

%Focus sur 2 types % 
Deux classes peuvent être extraites de nos pratiques de la modélisation actuelle et passés, les modèles statistiques et les modèles de simulations. Deux outils qui aparaissent et supportent largement la transformation d'une discipline démarré après 1950 aux Etats Unis et en Suède, et de façon brutale en 1970 \autocite[127]{Pumain2002} en France. 

%Ancrage historique%
Les innovations successives qui vont toucher ces deux outils, dans leur acception cumulative ou plus disruptive, se forme dans le cadre de boucle de rétro-action positive ou interragissent en s'enrichissant à la fois progrès de l'informatique, mathématisation de la discipline, transformation des enseignements, ouverture disciplinaire. 

De là se concrétisent deux trajectoire entremélés d'outils dont on admet toujours aujourd'hui largement la complémentarités \autocite{Sanders} \autocite{Manzo} \autocite{Cotinneau} \autocite{}. 

%Un exemple de méta modèle à l'explication : Lena Sanders
Pour éclairer un peu plus ce rapport du modèle à l'explication en géographie,le témoignage de Léna Sanders sur \textit{l'explication en analyse spatiale} \autocite{Sanders2000} est particulièrement éclairant. A partir d'une décomposition des pratiques à l'oeuvre dans une démarche de recherche, trois grandes étapes mobilisant l'une ou l'autre des techniques (modèle statistique, modèle simulation) sont identifiés : (a) identifier le phénomène à expliquer, (b) affiner les représentations pour éliminer les explications trop banales, (c) produire des explications. \autocite{Sanders2000}

% Du modèle au méta modèle %
La démarche introduite par Léna fait indirectement référence à l'existence d'un méta-modèle, ou modèle de modèle qui guide toute démarche de modélisation en géographie. Loin d'être un isolat, cette approche à déjà été suivi de nombreuses fois, et on pourra citer de façon non exhaustive quelques modèles de modèles bien connus tel que celui de Chorley \autocite{Hagget1965}, ou plus proche de nous en France, les différentes propositions de Durand Dastès \autocite{Dastes1992} \autocite{Dastes2001b}. Chacune de ces propositions consiste en l'établissement d'un point de vue original sur la mobilisation et l'interaction qui existent entre les modèles dans une démarche de construction des connaissances. Comme tout modèle, cette originalité est fonction du niveau d'abstraction, et de l'objectif fixé par ce modèle. ce qui nous amène porte à interroger le processus de construction d'un modèle comme résultat d'une expérience dont le succès est relié à la fois à la qualité du dispositif technique disponible, à la qualité des éléments qui rentrent en interaction avec le modèle à construire, et à la qualité des raisonements qui les ont mobilisés; mais aussi à définir de la qualité des interactions opéré durant tout le temps de la construction, et de leur conséquence sur la .

De fait la modélisation est concu comme un processus de construction purement contextuel, 

Il est ainsi tout à fait possible d'utiliser des modèles statistiques pour vérifier dans une démarche hypothètico-déductive des hypothèses \textit{a priori} afin de d'expliquer \footnote{Au sens de l'explication statistique} un corpus de données (ou inversement en utilisant les même outils dans une démarche exploratoire) Hypothèses qui définissent par la suite un support de réflexion pour l'experimentation dans un modèle de simulation : variations des conditions initiales, des valeurs de paramètres, des mécanismes générateurs dans un modèle de simulation. 

Un modèle de simulation pourrait être au coeur d'un plateforme intégré permettant de mobiliser les outils de façon dynamique, fonction de la démarche à mettre en oeuvre.

 capable de mobiliser des la flexibilité des  environnement dynamique ou les outils   en interaction avec un environnement dynamique se nourrit des interactions avec d'autres modèles.  donc déroulé sur un axe temporel, intègre complétement l'alternance et la complémentarité d'usage entre ces deux briques modèles : démarche exploratoire partant des données, ou top-down hypothético-déductive partant des hypothèses. 



Autre définition, cette fois ci issue du travail de Yves Guermond, qui propose de revenir sur la spécificité de la modélisation en science sociales.


Dimension à observer dans le processus de construction ? 
-
-

%Les outils qui participe à la démarche ne sont pas limités dans leurs fonctions. 

Les modèles de simulation \autocite{Banos2013a} ou statistiques \autocite{Sanders2000} qui nous intéressent dans cette étude consistent "ainsi davantage à "éclairer" qu'à "démontrer" et identifier des jeux de causalités bien stricts" \autocite{Sanders2000}, une démarche d'exploration des systèmes humains principalement marqué par une démarche abductive. Banos et Sanders positionne la simulation de modèle dans la grille proposé par Varenne, avec l'objectif "de décrire, de comprendre et/ou d'expliquer un phénomène donné" \autocite[833]{Banos2013a} 

En dehors du fait qu'il y a plusieurs échelles, plusieurs visions, et plusieurs observations mobilisables; il n'y a évidemment aucun modèle de simulation qui ne puissent etre construit en alternant phase de conception et phase de réalisation. Les inconnu sont beaucoup trop nombreuses : niveau de mesure, niveau de mécanisme, niveau des paramètres; et cela du fait de la variabilité opéré sur l'abstraction du phénomène à représenter, mais aussi du fait de l'abstraction opéré sur la chaine explicative à mobiliser (niveau de parcimonie), mais il y aussi le fait qu'on ne connait pas les sortie sans avoir expérimenter le modele seul, puis en condition de réplication si il est stochastique; et cela serait nié toute la phase reflexive opéré dans la construction du modèle que de dire l'inverse. Autrement dit tout les modèle que l'on essaye de construire aujourd'hui possède une dimension explicative sur le réel qui est au delà du modèle de simulation lui meme. L'explication opère donc par ce biais aussi, en matérialisant de nouvelle hypothèses qui doivent dans le cas de la simulation etre vérifié par l'empirie.

% Meme écueil explicatif
Il est important de noter que jusqu'à présent il n'a été fait aucune mention de techniques informatiques particulières, et cela car les problématiques posés par l'évaluation de connaissance tiré d'un modèle est en partie indépendante des techniques utilisés. A ce titre, les modèles de simulation agents, en permettant l'augmentation de la surface possible des représentations qu'il est possible de formaliser dans un modèle informatique, à ouvert la voie à une complexité qui n'a finalement fait que cristalliser un peu plus la problématique de la validation des modèles.(a reformuler)

Dans les deux cas, la limite entre description et explication semble difficile à délimiter \autocite{Sanders2000}, car dans la mise en marche des deux méthodes, on se heurtent au problème de sous détermination propre à l'étude des systèmes humains, dont la complexité n'est plus à démontrer. En analyse statistique exploratoire, la limite entre causalité et corrélation ne permet pas d'apposer une explication définitive sur l'observation d'un phénomène, il en est de même avec les simulations de modèles, et cela depuis un certain temps. Un problème qui n'en est pas un en réalité, car c'est la diversité des points de vues qui enrichit et pose l'interdisciplinarité pour être effective.

Donc tout comme il y a une famille d'hypothèses qui peut etre mobilisé en statistique pour éclairer un jeu de donnée, il y a une famille de modèle explicatif qui peut être mobilisé en modèle de simulation pour expliquer le même jeu de données (multi modelling)



En quoi sommes nous dans une démarche d'expérimentation ? 

Si il est commun de placer la création du modèle dans un schéma d

% Du modèle en géographie au modèle décrit comme expérimentation, cf calibration = exploration, injection variable explicative = idem, etc. remettre emphase sur la démarche d'expérimentation dans la construction du modèle , pour repondre à la phrase "?

En dehors de ces typologies à visée générale, pratique pour replacer nos démarches de modélisation courante, la définition rattaché à ce méta-modèle qui prend pour objectif l'explication témoigne aussi d'une trajectoire historique tout à fait singulière dans la discipline géographique. A modèle emboité, trajectoire historique emboité, ainsi la prise en compte de l'évolution du support de l'objet méta-modèle (modification des outils, des démarches) redonne indirectement un certain poids à l'instrumentation et à l'expérience qui en découle. Une prise de position qui vise à retablir un équilibre dans un rapport entre expérience - théorie trop souvent favorable à la dominance des seules théories.

Il n'est pas question ici de traiter l'une ou l'autre de ces trajectoires dans son exhaustivité, ce travail ayant été fait de façon exemplaire par bien d'autres auteurs dans la littérature. \autocite{Louail2010} \autocite{Banos2013a} \autocite{Pumain2002}, mais il est intéressant de se plonger dans les évolutions du modèle dans ce qu'il y a de vivant et d'autonome, et des transformations qu'à subit la démarche de construction des modèles au regard du paradigme de la complexité.


un objet géographique qui a subit, et continue de subir des mutations au regard de l'évolution des usages 


Reprenant la définition générale donné par Varenne, l'étude de cette trajectoire permet de préciser quels sont en réalité les activités de cognition qui ont motivé l'acceptation du modèle en géographie, mais aussi son évolution en tant que construit. 

en rapport avec Car si le modèle est fonction de médiation pour un questionnement, qu'apporte son autonomie, son évolution sur un axe temporel, sinon une remise en cause de ce même questionnement ? Car le questionnement qui mobilise les hypothèses repose sur un cadre tout aussi conceptuel et subjectif, et c'est pétri d'un présuposé théorique que lui aussi est amené à évoluer en parallèle, et en fonction de l'objet modèle médiateur.


Car ce qui est intéressant, ce n'est finalement pas le modèle en tant qu'objet lui même, mais ce qui est dans le modèle, cf. le choix des hypothèses, leur nature, leur traduction; c'est à dire tout ce qui traduit les points de vues représentées et l'histoire de leur construction.

Car le modèle est tout autant une histoire faite d'échec que de réussite.

--

La tendance a ramener la modélisation à la production du seul produit fini serait un retour en arrière dans une discipline ou le modèle a tout de suite été percu comme résultat d'une activité.

%En relevant son histoire, on dévoile ainsi la dynamique sous jacente à sa construction, une dimension seulement évoqué de façon implicite dans les précédentes définition; au travers nottament de l'autonomie de l'objet modèle.

%a permis la définition au cours du temps de nombreuses typologies capturant chacune à leur manière les différenciations qui caractérisent les activités de modélisation dans leur mise en place mais aussi dans leur évolution.

%Dans un contexte institutionel ou les logiques de construction de modèles pousse à la publication de résultats valide, on a tendance à oublier que la science avance tout aussi bien par ses erreurs que par ses réussites.

% Modèle, une définition holistique ? 

Ackoff est un mathématicien philosophe, qui a fondé avec Churchmann la science de la "recherche opérationelle" aux Etats-Unis. Définit par Pouvreau \autocite{Pouvreau2013} comme  "une théorie générale de l’organisation des procédures d’étude des problèmes suscités par ces organisations [sociales]", autrement dit l'étude et la classification organisés des problèmes posés par ces même organisation. Les "systèmes de problèmes" qui en découlent mobilisent pour être correctement approché dans une perspective holistique toute la richesse inter-disciplinaire alors disponible sur le sujet. 

Il propose de classer les modèles en géographie en trois grandes cases, les modèles analogique, iconiques, symboliques. 

%Notion collective associé au modele 
Autre dimension intéressante et qui touche plus à notre propos, " Ils assumaient pleinement le fait que la procédure de modélisation est un art de l’interprétation qui requiert une construction définitoire du système (de problèmes) étudié, laquelle est fondée sur des choix du modélisateur, en particulier quant aux informations prises en compte et à la manière de le faire, quant aux techniques formelles retenues et quant aux variables jugées pertinentes : des choix à expliquer, car ils incorporent des biais relatifs à certains objectifs (et, auraient-ils dû ajouter, à des systèmes de valeurs) qui se retrouvent constitutifs du modèle." \autocite[806]{Pouvreau2013} Il est ici tentant de relever l'analogie, déjà mobilisé auparavant, qu'il peut y avoir avec l'activité artistique. Si on peut ne pas être d'accord avec l'appel à des arguments esthétiques, ou métaphysique motivant l'inspiration du chercheur, il n'en reste pas moins que le modèle, dès lors qu'il est extériorisé, échappe tout comme l'oeuvre d'art en partie à son créateur, ce qui ouvre là il semblerait une dimension collective propre à l'enrichissement collectif, faisant de l'amélioration du modèle une construction collective. 

Autrement dit, la classe "Faciliter la communication et la coconstruction des savoirs" de Varenne passe d'une famille de fonctions subjective voulue, à celle d'une famille objective imposé par la norme scientifique "Les modèles seront des modèles dit scientifiques à partir du moment ou leur exposition se fait à la lumière du savoir collectif" 

Ce rapprochement - qui fait aussi état de divergences propre à la spécificité de la discipline - avec le projet systémique mené par Bertalanffy \autocite[804-805]{Pouvreau2013} fait de l'apparition de cette référence dans la littérature géographique une autre point d'entrée intéressant vers la littérature systémique qui après vérification pourrait s'ajouter à celui bien connu de Chorley \autocite{Chorley1962} et de la RAND au travers des instituts de plannification qui ont permis la réalisation des premiers "large scale" modèles en géographie (\autocite[6]{Batty1976})



Objet c'est la construction de théorie en géographie , cad ? 


Rapport entre construction de théorie et explication ? 
% Il faut d'abord évoquer que le modèle en géographie se rapport à deux types avant tout ... modèle stat et modèle simulation (chercher la ref) Le modèle stat est apparu de facon conjointe avec le modèle de simulation, qui lui a beaucoup progresser ces dernieres années avec l'apparition de différentes technique individu centré.
% Il y a un lien a faire avec ce que dit Léna sur la modélisation, qui inclut la dimension exploratoire importante dans la conception du modèle en géographie, ce qui pose la question du questionnement dans une construction dynamique.

Semblerait qu'il y ai collision entre la naissance du questionnement et l'usage de certaines techniques : empirico inductive permettant de faire évoluer ce meme questionnement. Ce que dit léna, l'approche est double dans la modélisation des données, il y a certes la dimensions de compression, mais aussi celle d'exploration qui nait principalement en géographie de l'étude des résidus.

 et donc un construit dont la réussite réside en quelque sorte dans sa capacité à échapper à son créateur. En ce sens on pourra dire que le travail de collecte des modèles réalisés par Hagget et Chorley reflete très bien ce processus d'absorption, transformation, réémission opéré par les géographes autour des modèles créés ou empruntés aux autres disciplines.



Loi, modèles, théories, le distingo entre ces trois mots semble dans un premier temps difficile à évaluer dans certaines publications.

\subsection{L'activité de modélisation}

Il s'agit de porter un regard sur la dynamique qui anime la construction d'un modèle dans la réalisation

=3 Transition vers la démarche de construction ... 

\section{Les années 1970, un tournant dans l'acceptation de la complexité en géographie}

% A faire remonter : Cette analyse a pour vocation de montrer quels sont les concepts clefs qui ont donné à voir la complexité des modèles, et en quoi ceux ci, malgré les évolutions techniques qui ont traversés la disciplines, se placent dans un rapport à la construction de la connaissance en géographie qui pour le moment n'a pas d'équivalent ou de transposition technique.

Les deux premiers axes portent plus sur la reflexivité et au sursaut de la discipline dans une période de changement à la fois marqué par un retour à l'interdisciplinarité, et à une volonté d'unification des sciences qui passe par un renouveau et un regain des formalisations des méthodes quantitatives dans la perspective d'élaboration de "loi", rétablissant ainsi la scientificité de la discipline au regard des autres sciences. Le troisième axe porte plus sur la question des moyens techniques accompagnateur de cette réalisation, un force évidemment indissociable dans la construction des deux premiers axes. %(=Le modèle est mis en temporalité)

\subsection{Les revendications(s) épistémologique(s) complexes des pionniers }
\label{subsec:epistemo}

\paragraph{La forte influence du positivisme logique}

Il semblerait que la communauté des géographes soit en accord \autocite[15]{Louail2010} pour désigner l'article de Fred Schaeffer \autocite{Schaeffer1953} comme catalyseur des frustrations d'une génération de géographe envers les pratiques alors en cours dans leur discipline, en déclin tant d'un point de vue scientifique qu'institutionel (ref ullman)

D'origine autrichienne Schaeffer profite d'une solide formation inter-disciplinaire en Allemagne, qu'il fuit dès lors qu'il est apparenté à un terroriste par les Nazi. Après une courte exode en grande bretagne, il s'installe aux Etat-Unis où il participe à la diffusion de la géographie économique Allemande, dont il se fait fin lecteur, et parfois traducteur (Lösch). 

A la lecture de son fameux article méthodologique \textit{Exceptionalism in Geography} on ne peut que deviner la parole influente de son ami proche Gustav Bergmann, un philosophe lui aussi exilé, connu et reconnu dans le mouvement des positiviste logiques du Cercle de Vienne. 

Ce groupe d'intellectuel formé dans les années 1920 à Vienne connait paradoxalement sa plus large popularité dès lors que l'imminence de la guerre et les pressions du régime nazi va pousser de nombreux acteurs du mouvement à s'exiler aux Etats-Unis.(ref) 

Cette philosophie vient alors se greffer à une philosophie américaine pragmatique depuis longtemps installé (Peirce, Dewey), jusqu'à finalement quasiment l'eclipser. Ce transfert ne se fait pas on l'imagine sans aucune mutation du discour originel Viennois, dont on retrouve quelques traces dans la pensée positiviste chez des auteurs américains, sans que l'on puisse pour le moment en dire beaucoup plus sur ce transfert, les travaux sur le sujet étant relativement peu nombreux en géographie. \autocite{Orain}

William Bunge, l'autre grand théoricien de cette "révolution quantitative", enseigne également à l'université de l'Iowa. Celui ci va se positionner sur la meme ligne que son collegue Schaeffer,dont il est proche \autocite{Goodchild2001}, et affirmer dans un article fondateur \autocite{Bunge1962} sa volonté d'une géographie avant tout nomothétique, comme les autres "sciences".\autocite{Bunge1979} \autocite{Claval2003} \autocite[429-430]{Gregory2009}. Un programme qui passe avant tout par la mise en avant d'une nouvelle langue universelle pour le raisonnement, celle des mathématiques, et de la géométrie, et l'établissement de loi spatiale dérivable logiquement, sous entendu comme l'entend l'école néo-positiviste.

% Deuxième élément marquant, autre que l'introduction de cette nouvelle philosophie des sciences au géographes, est l'affirmation de la géographie comme science ayant pour principal objet d'étude l'identification  des interactions spatiales.

Une autre lecture du changement est réalisé plus \textit{a posteriori} par Harvey en 1969 \autocite{Harvey1969}. Bien que le travail d'Harvey soit reconnu comme un apport bénéfique à la géographie par de nombreux relecteurs (Amadeo, Gregory, Wolpert), ce travail à la fois très dense et écris sans réel public cible en tête touche finalement une audience relativement limité, nottament du coté des étudiants, qui dispose déjà de nombreux autres ouvrages déjà connus comme référence (Gregory 1963, Chorley et Hagget 1965, Abler 1971) \autocite{Johnston2008}.

Cet ouvrage constitue une tentative intéressante d'introduction de l'épistémologie des sciences au géographe avant tout motivé par la description d'une "démarche scientifique" plus que d'une lecture rigide de l'orthodoxie proné par les positivistes logiques. Remise dans son contexte, l'écriture de l'ouvrage en 1960 dans l'université provinciale de Bristol tient plus selon Barnes \autocite{Barnes} d'un manifeste énergique adressé à la discipline sinon au monde.

Ce travail réflexif qui relève plus finalement d'une position de recherche en restera plus ou moins là par la suite, car Harvey rompt brutalement avec sa précedente analyse dès 1973 \autocite[166-168]{Gould2004}. \autocite[30]{Johnston2008} et \autocite[37]{Barnes2005} nous indique que celui ci ira également jusqu'à critiquer en partie ses propres travaux, à plusieurs reprises, dans la préface du livre \autocite{Harvey1969}, puis plus tard encore \autocite{Harvey1972} dans une réponse à la très longue et très argumenté critique de Stephen Gale sur "Explanation in Geography" \autocite{Gale1972} paru en 1972 dans la nouvelle revue "fer de lance" de la géographie quantitative "Geographical Analysis". 

Dans cette très longue liste de critique évoqués par Gale on retrouve pèle-mêle : les très-trop nombreuses contradictions de l'auteur quand à la faisabilité d'une théorie exclusivement formelle en géographie alors même que c'est l'idéal qu'il défend au travers d'une application de l'orthodoxie positiviste; le flou de certaine propositions nottament sur le degré de relachement propre à l'application des concepts; le focus de l'auteur sur la seule démarche "standard hypothético-déductive" au mépris des autres démarches scientifiques possibles; etc. 

En conclusion Gale juge le livre utile dans une possible stimulation du débat en géographie, et éventuellement comme ouvrage d'introduction à la philosophie des sciences en géographie, mais par contre il considère que du point de vue du "[...] research geographer interested in objective standards for scientific investigation, \textit{Explanation in Geography} is probably a failure." \autocite[316]{Gale1972}

Parmis les différentes raisons à cela, \autocite{Barnes2006} évoque le contexte politique chargé de 1968, date à laquelle le document est soumis à l'éditeur. Les bouleversements dans l'ordre mondial sont nombreux, asssasinat de Martin Luther King, guerre du vietnam, sont autant d'événements qui peuvent justifier cette migration de l'auteur vers un autre "style de pensée" (au sens de Crombie et Hacking) \autocite{Barnes2006} et une relecture plus marxiste, plus explicitement engagé politiquement que ne l'est son premier recueil "Explanation in Geography" (dont l'engagement était réel, mais plus implicite selon \autocite{Johnston2008}). Le déclin puis l'é                                                                       chec reconnu du mouvement positiviste préssenti avant même la publication en 1969 du manuscrit en est une autre. \autocite[47]{Barnes2006}

Il est indégnable que ce livre, ainsi que celui de Bunge vont inspirer une longue série d'ouvrage du coté anglo-saxon. Il faut toutefois reconnaitre qu'on ne peut lire le mouvement de la nouvelle géographie au seul éclairage de ces manifestes, pour plusieurs raisons. 

La diffusion de cette école de pensée positiviste ne trouve pas de support à l'internationalisation, nottament en France courant des années 1970 ou Claval ne note pas de réelle diffusion de cette épistémologie, et provoque au contraire un certain rejet.. (retrouver la ref). Le débat épistémologique intéresse certes, mais selon lui les géographes francais sont alors bien trop occupé alors à intégrer les fascinantes et toutes dernières techniques quantitatives pour qu'une synthèse voit le jour sur le sujet.\autocite[27-29]{Claval2003}

La pensée systémique qui semblent cohabiter avec l'épistémologie positiviste semble être plus un cas isolé qu'une tendance lié à un enseignement généralisé; ainsi pour Ullman c'est probablement plus l'influence de Bergman et des positivistes installés à l'université d'Iowa qui va alimenter, bien au dela de l'article de schaeffer, finalement relativement isolé, le désir d'une jeunes génération de géographes transformés en théoriciens qui sont passés sur ses bancs (Bunge, Harvey, etc.) \autocite{Ullman (a verifier)} Les livres de ces grands noms de la géographies ont certes eu un retentissement, et ont probablement servit de référence par la suite pour la formation de nombreux géographes, mais il faut convenir que cet exercice difficile du rapprochement de la géographie avec le courant néopositiviste ne semble pas être concluant, tant la pensée systémique, qui offre un cadre méthodolique et instrumental assez rapidement s'est ensuite largement surimposé, englobant la démarche hypothético déductive dans une acceptation finalement beaucoup plus lache.

%Le fait est donc que cette épistémologie, en dehors du fait qu'elle a su endosser durant un court moment cette volonté nomothétique de la géographie, n'a pas réussi par la suite à concilier dans son formalisme ce dialogue nécessaire avec une géographie qui s'appuient avant tout sur l'observation des données pour formuler ses hypothèses. Afin de mieux comprendre pourquoi Harvey et ses contemporains n'ont pu faire qu'une lecture décevante de l'application d'une telle épistémologie à la géographie, il faut montrer en quoi celle ci s'est avéré incompatible avec ces nombreux développements.
(Trouver mieux)

\paragraph{Le modèle Hypothético Déductif H-D de "confirmation" et Deductif Nomologique (D-N) de Hempel et Oppenheim}

Les origines d'une telle méthode sont généralement attribué à Roger Bacon(1214–1294) et Robert Grossetête (1175–1253), inspiré eux même par la traduction des travaux de scientifiques arabe. Ces deux auteurs ont été parmis les premiers d'une longue liste à comprendre et amender la méthode scientifique inductive-déductive d'Aristote. (ajout figure ?) Parmis les différentes avancées on note l'apparition d'une nouvelle étape dans le cheminement intellectuel, qui se traduit par une "méthode de vérification" qui permet de tester la capacité prédictive d'une loi dans le cadre d'une expérience dérivée d'une loi,et une "méthode de falsification". Cette étape rétablit l'expérimentation (au contraire de la spéculation) sur les données comme une démarche absolument nécessaire pour vérifier, écarter et innover dans la liste des hypothèses. \autocite[31]{Losee1972} \autocite[162]{Gaugh2003} \autocite[242]{Gaugh2003}

Mais selon Ian Hacking, cette méthode est probablement dans sa version générique une démarche universelle de raisonnement, que l'on doit probablement mettre en perspective avec les mécanismes à l'oeuvre dans la cognition. En cela, les travaux des positivistes logiques constitue une tentative de mise en ordre par la logique formelle d'une démarche considéré dans son universalité, et transverse à toute les sciences. Le modèle hypothètico-déductif est donc souvent entendu comme allant de soit, mais il s'agit en réalité d'une démarche générique auquel peuvent se rattacher en épistémologie des sciences plusieurs définitions, plus ou moins compatibles (Hempel, Popper).

Il est donc difficile d'obtenir une réelle définition de cette démarche tant celle ci varie une fois replacé dans un contexte historique, et cela en partie du fait nombreuses subtilités qu'introduisent les philosophes des sciences à chacune des étapes du raisonnement.  Ces premieres définition ne sont pas appliqués, et il faudra attendre plus longtemps pour voir une démarche d'expérimentation controllé tel qu'on l'intuite aujourd'hui, que l'on attribue le plus souvent à Newton.

Ian Hacking a selon Orain très bien saisi ce qui fait les axes communs des différentes relectures du positivisme. Les positivistes logiques ont ceci de particulier qu'ils raisonnent sur des démonstrations logiques encapsulant les énoncés observationels décrit dans une logique formelle si possible non ambigue. Empiristes, ils ont avant tout pour volonté d'éliminer toute métaphysique dans ce raisonnement, et s'opposent ainsi à toute structure a priori (anti-kantien) dans le processus de raisonnement; en cela l'inférence déductive se fait seulement sur des observations qui sont \textit{a priori} vérifiable.

Dès 1940 Hempel, un des membres influents dans le cercle de Vienne, s'intéresse à la problématique de plus près à la "confirmation" dans le cadre du modèle hypothètico déductif, nommé par la suite H-D confirmatif. Il va alors être le premier à s'interroger "[...] non pas sur la formulation d'un hypothèse ou d'une loi universelle à partir de cas particulier, mais sur la \textit{confirmation} d'une hypothèse ou d'une loi donnée" \autocite{Lecourt2012}. Mais cette démarche va connaitre rapidement plusieurs difficultées, qui apparaissent nottament via plusieurs paradoxes qui viennent questionner les démonstrations logiques au coeur de la méthode, comme le paradoxe de Goodman, ou celui d'Hempel (Raven Paradox). Certains paradoxe seront résolus dans différentes déclinaisons du modèle H-D, mais d'autres resteront problématiques, amenant peu à peu à l'affaiblissement de cette approche cumulative. 

++ chalmers décrit très bien les nombreux biais posé par une telle démarche inductiviste ++

La méthode H-D de confirmation permet rejeter ou d'accepter des énnoncés observationels, mais elle ne constitue pas en elle même une méthode "explicative". La démarche explicative d'Harvey s'appuie sur la méthode Deductive Nomologique (D-N) formulé par Hempel et Oppenheim’s en 1945, qui est alors une tentative tout à fait originale pour créer une logique formelle centré sur l'explication.

\paragraph{Le faillibilisme de Popper}

De plus Popper est un rationaliste critique, opposé à l'empirisme qui anime les conceptions des néo positivistes. Ce qui lui permet entre autre de repenser l'introduction de la métaphysique dans la genese des hypothèses, contrairement aux positivistes qui s'était juré (sans succès) de la bannir.

Dans les convergences entre faillibilisme et positivisme logique, il existe des divergences aussi forte que ne peuvent être les convergences. Ainsi à méthode H-D quasi-similaire, l'hypothético déductivisme de Popper impose pourtant un raisonnement inverse pour la formulation des hypothèses. Il ne s'agit plus d'une formulation pour la construction incrémentale de loi ou de théorie, mais d'une formulation dont la fonction est avant tout de destabiliser une théorie ou une loi existante. Pour Popper la science avance dans une perspective critique, la théorie de la relativité d'Einstein fournissant un parfait exemple de situation ou le seul échec d'une expérimentation peut remettre en cause toute un pan de la théorie. Dans le langage de Popper, l'hypothèse devient conjecture, et la vérification est donc empreint d'un double sens : une corroboration en cas d'une confrontation positive, et une falsification en cas de confrontation négative. Avec cette particularité que lorsque la conjoncture est vérifié, celle ci est d'un apport beaucoup plus faible que dans le cas d'une vérification, du fait des nombreux "problèmes de l'induction", que Popper veut écarter définitivement du processus de démarcation entre science et non science. 


Dans les deux cas Hempel et Popper sont fortement opposés à toute "logique de la découverte". La seule méthode valide pour faire le tri parmis l'infininité de faits disponible doit se faire par la confirmation/infirmation (Hempel ou Popper) d'une conséquence d'une inférence déductive de cette hypothèse sur celui ci. La formulation de l'hypothèse se fait donc \textit{a priori}, ce qui évacue la question du processus de formation.

L

 scientific explanation of a fact is a deduction of a statement (called the explanandum) that describes the fact we want to explain; the premises (called the explanans) are scientific laws and suitable initial conditions. For an explanation to be acceptable, the explanans must be true. 

Tout porte alors sur la nature de la loi en question. Sur cette question du critère de démarcation entre ce qui fait science et non science, Harvey se borne à décrire l'approche de Popper en quelques ligne \autocite{Johnston2002}, l'importance n'étant pas tant le moyen que l'objectif poursuivi, ce qui est un argument de plus en faveur de la description d'une démarche scientifique découplé de l'application stricte des concepts des positivistes logiques.

Harvey sur ce point se dédouane largement, en citant dans son livre ""


On est plus proche de la logique de la découverte proposé par Simon

une methode de vérification basé sur l'expérience dérivé de loi  est séparer les méthodes dédiés à l'explication et les méthodes pour la confirmation/infirmation de lois. La méthode hypothético-déductive est en général présenté avec la méthode.


\paragraph{L'incapacité à formuler une "logique de la découverte" compatible avec les sciences humaines}

Echec prédiction ?
(+ analyse lena jm besse)

En quoi la théorie systémique est peu compatible avec la démarche hypothético déductive ? 
"les critiques « externes », au premier rang desquelles celles de Hempel et de Nagel, faisaient à cette époque exception". \autocite[963]{Pouvreau2013}


-------------------------------------------------


Pour mieux comprendre ce que les géographes de la révolution quantitative appellent méthode hypothético-déductive, et pourquoi ils ont aussitot adhéré à cette nouvelle façon de faire de la science, il faut remettre en contexte certaines définitions en s'appuyant sur les textes des auteurs eux mêmes.

Cette méthode pour être comprise dans le cadre d'u à pour principal intéret de proposer dans une démarche vérificationiste l'intégration des deux principaux type de raisonnement disponible pour le chercheur, l'inférence inductive et l'inférence déductive.

Renverser le plan pour permettre d'abord systémise, puis épistémologie, en montrant en quoi elle pose probleme dans l'operationalisation ?
-> H-D, mise sur déduction, double probleme, pas de logique de découverte, deduction = prédiction + assymétrie, = abduction comme solution lié a opérationalisation !


Le problème c'est que Popper rejette l'induction comme moyen sur de généralisation.

(Il faut que tu parle de Curry et de son impact dans la transition école géographique prédictive (application de loi économique au détour du déterminisme))

Sur l'inscription positiviste, Olivian Orain cite Ian Hacking pour la définir : 



Il en reste que la géographie se réfère souvent à la démarche hypothético-déductive, et qu'il nous faut un tant soit peu l'expliquer pour tenter de voir plus clair dans ce qui fait "explication" en géographie.

Cela dit il serait délicat de classer cette "nouvelle géographie" comme étant seulement inspiré du courant néo-positiviste ( ou positivisme logique) dicté par les seul épistémologues du cercle de Vienne, il est vrai très populaire à cette période. [A COMPLETER PAR UNE DEFINITION ? > ian hacking par exemple, cf cours d'Olivier Orain ?] 

 Il est toutefois important aujourd'hui de pouvoir justifier d'une épistémologie pour justifier à la fois de la validité de nos résultats, mais également de la démarche de construction ayant permis de mener à ces résultats.


Dès 1924, le monde scientifique subit l'influence croissante d'une groupe d'intellectuel regroupé sous l'étiquette néo-positiviste (ou positivisme logique) ou Cercle de Vienne. Inspiré à la fois du positivisme et de l'empiricisme ce courant a marqué principalement la géographie Américaine dans son tournant quantitatif. Du fait de sa volonté Empiriciste, ce mouvement réintègre au coté de la raison, l'observation comme connaissance scientifique. L'accumulation répété d'observation permet alors l'inférence inductive d'hypothèses résultat d'une généralisation. Alors que de plus en plus de géographes rejettent l'exceptionalisme jusque alors caractéristique de la démarche géographique, voilà de bons arguments pour tenter un passage du singulier au général par une quantification des régularités observables.

Autant dire que le terrain pour aborder la question de la percolation de ces concepts dans la géographie est difficile, et on ne sera pas étonné de voir David Harvey évoquant la méthode hypothético-déductive avec beaucoup de largesse dans l'acceptation des termes, la différence entre approche néo-positiviste et Popper/Hempel (pourtant largement instigateur de la méthodologie) tenant dans son discours sur quelques lignes seulement, l'auteur prétextant d'une différence sur le moyen (critère de vérification) et non sur l'objectif. \autocite{Johnston2002} Une lecture des influences des positivistes / néo-positiviste somme toute à relativiser, une intuition qui se confirme à la lecture de \autocite{Dauphine2003} ou d'\autocite{Orain} sur ce sujet..

Popper, fondateur avec Hempel de la méthode Hypothético-Déductive est une influence plus lisible, qui touchera étonnament diront certains (vu le peu de science pouvant appliquer réellement les critères de falsifiabilité ), une vaste gamme de discipline. La critique de l'empirisme logique se fait entre autre sur critère de vérification permettant la délimitation de science et non science, avec la mise à mal du processus d'inférence par induction nécessaire pour déterminer des lois (paradoxe de Hempel). La logique de découverte est exclut et ne sont plus autorisés que l'existence de loi \textit{a priori} dérivant soit d'un fait psychologique (intuition créatrice?) ou d'une généralisation accidentelle.

Si on comprend bien la démarche d'Harvey, qui dès 1965 a pu approcher les premier modèles lors de son voyage en Suède et saisir les implications d'un tel ancrage, même partiel, de cette "révolution" que présage la construction de modèles de simulation dans une épistémologie, il reste un certain nombre de points à éclaircir dans sa proposition de démarche scientifique, qui clairement ne peut être appliqué en l'état pour justifier des connaissances en sortie de modèle de simulation. On pourra toujours, comme le fait Harvey, relaxé la contrainte sur la définition de ce que peut être une "loi" en science sociale, mais reste le problème de la vérification des hypothèses. Ce n'est donc pas juste une simple "question de moyens", surtout si on s'en tient à une référence Popperienne sur la méthode H-D/D-N , car celle-çi a) exclut toute "logique de découverte" par l'inférence inductive, b) utilise un critère de faillibilité bien peu adapté aux sciences historiques et sociales si on en croit les critiques plus récentes, mais non moins acerbes de Passeron sur ce sujet, c) admet de base selon Hempel la symétrie entre prédiction et explication, ce qui n'est pas sans poser de problèmes la aussi, d) admet l'existence de "loi universelles" dérivables bien peu compatible avec la multiplicité de points de vues propre.

Si cette référence, toujours d'actualité dans la plupart des sciences, n'empêche pas comme le sous entend bien Harvey dès 1969 la construction de modèle, elle pose par contre d'énorme problème aujourd'hui lorsqu'il s'agit de justifier de la connaissance en sortie des modèles de simulation, et marque l'urgence de remettre en place un cadre réflexif encadrant la justification des connaissances, coupant ainsi court aux critiques toujours d'actualité sur la valeur scientifique de nos outils. D'autant plus lorsqu'il s'agit d'examiner ce problème en regard des apports d'un deuxième courant qui se développe en parallèle, et balaye plus ou moins ce débat épistémologique en révélant une nouvelle dimension à explorer, celle des multiples complexités \autocite{Dauphiné2003} qui fondent les systèmes géographiques.



Pour \autocite[162-165]{Claval2001}, restreindre le tournant vécu par la géographie à ce seul objectif visant la recherche de "loi spatiale" comme seul dérivé de cette nouvelle démarche scientifique serait réducteur. La déception qui résulte de l'application des modèles de l'économie spatiale à de plus grandes échelles, confronté à des données observés en 1960-1965, rend d'autant plus pertinent la remarque précédente de Claval, pour qui finalement ce tournant doit avant tout "[...] sa fécondité à une idée simple : celle que les relations économiques, sociales ou politiques à l'oeuvre dans une société sont affectés par la distance qui sépare les partenaires.[...]", une idée introduite par Ullman en 1954, et qui marque bien la diversité des questionnement à l'oeuvre dans la construction de ce tournant.

Equivalence logique entre capacité prédictive et sa scientificité => 

=> conclusion sont déjà dans les prémisses !



\paragraph{Rappel des termes}

Les \textbf{faits} sont les données immédiatement accessibles

Les \textbf{loi naturelles} (ou \textbf{loi de la nature}) sont l'expression mathématisé d'une corrélation répétable, d'un comportement constant, ou d'une fréquence statistique observé parmis un ensemble de \textbf{faits}. Elles peuvent être \textbf{déterministes} ou \textbf{statistiques}, et ils existent différentes écoles de pensées, opposant les \textbf{empiriste} (partisant du subjectif) au \textbf{réaliste} (partisan)

Les fait ne sont pas régis par des lois, les fait comportent des lois.

Une théorie est un concept engloblant à la fois des lois, des modèles et qui mobilise forcément des relation de dépendance avec d'autres théories.

En géographie on pourra citer la théorie évolutives des villes de Denise Pumain, nécessairement englobante de loi, comme par exemple la loi de Zipf, ou la loi Rang-Taille, qui mobilise à la fois des modèles de distribution statistiques, mais aussi des théories intégrés dans les modèles de processus générateur de cette loi : les théories de la diffusion spatiales, de la différenciation, et de l'interaction.

\paragraph{Methode \textit{deductive-nomological} D-N, Methode \textit{hypothetico-deductive} H-D et théorie de l'explication/confirmation }
\cite{Paterson : David Harvey's Geography}

Positionnement dans le cadre d'une science objective, c'est à dire qui ne se résume pas à la somme des croyances individuelles (individualisme). Entre autre conséquence, et pour reprendre l'exemple de Chalmers avec Maxmell, l'opportunités objective fait qu'une théorie scientifique contient toujours plus que ce que le chercheur à voulut y trouver, dans le cas de Maxwell, l'existence des ondes radio découverte après sa mort.

La nouvelle géographie est plus connu comme le passage d'une science de l'idiographique au nomothétique (basé sur des loi), établissant ainsi sa volonté d'un rapprochement vers les sciences de la nature marqué à cette période par le courant néo-positiviste du cercle de Vienne. 

La méthode hypothetico-déductive (def?) chère à la nouvelle géographie doit pour être mobilisé faire appel à la pré-existence de loi naturelle sur lequel s'appuie des théories qui sont construite de façon incrémentale par la dérivation d'hypothèses (déduction) qui pourront être falsifié par l'empirie. Avec cette démarche, la théorie, le problème précéde toujours la formulation d'hypothèse ayant donner lieu à la collecte de données (même si aujourd'hui les \textit{BigData} semble renverser cette tendance ..) Il n'y a pas de logique de découverte possible autre que psychologique ou historique. L'autre particularité de cette démarche est qu'elle ne permet en aucun cas de "Vérifier" une théorie, seulement de la falsifier, sans pouvoir non plus de certifier de celle ci (car il est impossible de qualifier à 100 \% cette faillibilité, de nombreux paramètre entrant en jeu), à moins d'être un faillibiliste naif. 

%Pour popper un phénomène est expliqué lorsque l'on peut décrire la logique de la connaissance scientifique c
%ADEPLACER Tout comme il faut relativiser la notion de système par son acceptation très différentes par les disciplines, il en est de même avec l'application de la démarche hypothetico-déductive, qui s'applique rarement comme un référentiel suivi à la lettre par les chercheurs (c'est d'ailleur une des critiques qui sera faite au faillibilisme poppérien!). Plusieurs points peuvent être développés en ce sens.

2) La difficile prédictibilité des sciences sociales (et donc géographiques)

La logique déductive formalisé par Hempel dans son modèle D-N impose une symétrie entre explication et prédiction; hors dans le cadre des systèmes humains, nous somme probablement très loin d'atteindre un tel objectif.

De plus l’existence de théories alternatives multiples est une constante dans l’histoire des sciences humaines. L'étude de l'objet social est un construit contextuel qui se nourrit d'une multiplicité des point de vues. C'est à ce titre que Jean-Claude Passeron \autocite{Passeron2006} nous met en garde contre une tentative de vérification des modèles qui serait décorrélée de tout contexte historique. Pour lui le faillibilisme poppérien qui se cache derrière la méthode hypothético-déductive ne peut pas s'appliquer à la construction de théorie dans le cadre des sciences humaines et sociales. L'équifinalité est à ce titre un moteur permettant de confronter nos théories sur un objet social  qu'il est impossible de tout façon impossible de voir dans son unicité. Le critère de démarcabilité entre ce qui fait science et non science n'est pas capable d'évaluer un concept tel que le marxism e, la psychologie, et fait de ces sciences des non sciences.

Que penser alors de cette application semble t il un peu biaisé de la méthode hypothético-déductive dans la construction de la nouvelle géographie ? Tout d'abord peu de monde semble s'être réellement penché sur la question avant le positionnement d'Harvey, souvent cité comme un des premiers en 1969 à introduire une dimension réflexive épistémologique sur l'"explication en géographie".\autocite{Johnston2002} qui décrit cet état de fait, reprend une phrase de celui-ci indiquant qu'il faut clairement relativiser la notion de "loi généralisé universellement vrai", car une telle interprétation ne permettrai à aucune loi d'exister dans aucunes des disciplines scientifiques.  

Le processus de modélisation apporte une dimension supplémentaire à l'analyse de chacun de ces points de vue. Car  il est hélas  impossible de prouver par les modèles qu'il n'y a pas un tout autre ensemble de fait stylisés ou d'interactions qui soit capable d'arriver à la même observation, enlevant de fait toute unicité d’une explication " scientifique" au point de vue représenté par le modèle. L'équifinalité est donc à ce titre une limitation indépassable à la connaissance qui peut être déduite de nos modèles.

Le terme " validation " quant à lui est souvent entendu pour définir un état qualifiant la correspondance entre des observations empiriques et les sorties de la simulation. Compte tenu de la notion d'équifinalité, cet état de correspondance ne suffit pas à prouver que le modèle représente bien la " réalité ", dans la mesure où l’unicité de  cette adéquation peut être remise en cause par le jeu de nouvelles hypothèses.

L'équifinalité se positionne à de multiples échelles, celle du chercheur dans son environnement, qui mobilise des hypothèses fonction de ces connaissances et de ces pratiques à un instant t, celle du modèle qui peut atteindre un même but en mobilisant des hypothèses différentes pour une problématique donnés ... (a developper, différence niveau d'echelle, etc.)

Pour autant doit on parler de relativisme, et invoquer farerbayend ? 

Le "theory-building" de la nouvelle géographie se concentre semble t il plus sur la méthode de construction de modèle, et peu importe que la démarche hypothético-deductive puissent faire alors référence à une école positiviste (on généralise une théorie a partir d'expériences réussies) ou rationaliste critique(on procède à une élimination par falsification), les deux ne se différenciant finalement pour Harvey que par une différence de moyens et non de fins. (à préciser)

En réalité c'est un peu ce que l'on retrouve avec l'importation de nombre de concepts tout au long de l'histoire de la géographie, qui ne nous apporte pas tant une explication qu'une observation à charge pour nous d'expliquer.

Plusieurs dimensions : Theory Building indique qu'il s'agit d'un construit; qui possède une dimension temporelle; hors souvent on ne nous donne à voir que le résultat final; l'introduction de la simulation change la donne de ce point de vue.

L'étude de la "finalité" en tant que telle perd son sens ! => 

Comme il est impossible de faire une comparaison des relecture de cette diversité intra-disciplinaire à l'échelle du monde, l'auteur propose de faire une relecture de cette diversité par le prisme des transferts entre GST et géographie par le prisme des transferts entre l'école Américain et Suédoise et son acceptation par l'école Française.

%--------------------------------%

=> Tient plus de la systémique que d'une épistémologie en particulier


% Plusieurs voies :  schéma fournissent logique, statistiques fournissent états, dynamique ? 

\paragraph{Quelques points de contact entre géographie et systémique/pré-systémique à la GST }

Une première entrée de la GST dans la géographie provient de la discipline géophysique par Chorley (Géophysicien britannique) \autocite{Chorley1962}.  L'adéquation entre système ouvert, et boucle de rétro-action s'avère tout à fait effective pour application à des systèmes physiques; un transfert qui se fait sous l'oeil d'un \textit{reviewer} attentif, comme en témoigne les \textit{Acknowledgments} de \autocite{Chorley1962} qui cite Bertalanffy lui-même. (A ce titre on peut citer les intuitions de DAVIS qui préfigure largement largement de future systémisme, cf Claval)  

Walter Isard, un économiste joue un grand rôle dans la fondation d'une nouvelle géographie, du point de vue des institutions mais aussi des apports scientifique. En effet durant la seconde guerre mondiale il remet au gout du jour les modèles de l'école Allemande en traduisant et diffusant ceux ci dans des publications largement reconnues par la suite comme fondatrices (ex?). Au début des années 1950, il réunit un ensemble dans une séries de conférences des économistes participant de son point de vue, et fonde la toute nouvelle discipline des sciences régionales (ou \textit{regional science}), une économie résolument tourné vers l'interdisciplinarité où dialogue économistes, géographes, etc. Ces conférences marque la diffusion chez les géographes des nouvelles techniques quantitatives computationelle (programmation linéaire, statistiques, etc.), des modèles, mais aussi d'un certain "esprit interdisciplinaire" qui sera largement repris dans les institutions géographique par Ullman, un géographe qui joeura un rôle très important dans la diffusion de ces nouvelles techniques aux universitaires américain, etc. D'autant plus que celui ci a déjà eu lui même accès aux théories de Christaller via Lösch lorsque celui ci vient en déplacement aux Etats-Unis, à l'aube de la seconde guerre mondiale. A la suite de cette rencontre il introduit en 1940-41 et pour la première fois les travaux de Christaller aux géographes Nord-Américain. C'est également à cette période que les travaux de Christaller sont en France avec les actes traduit du congrès d'Amsterdam en 1938, et le sacre confirmant l'importance de ces travaux aura lieu au fameux symposium de Lund, organisés par Hagerstrand en 1962 \autocite{Berry1970}  %History of Regional Science and the Regional Science Association International: The Beginnings and Early History

Les travaux de Christaller et Losch préfigure par leur nouveauté d'approche, sur tout plan (méthodologique, outil technique, etc.) l'arrivée de la systémique en géographie. Le recours importants aux mathématiques pour l'effort de généralisation, l'introduction d'une imbrication de système, le choix assumé d'une approche hypothético-déductive appliqué, et une vision de la plannification urbaine résolument "holiste" (dans le sens presque totalitaire du terme).

Mais c'est à tort que l'on cite ces travaux comme unique initiateur de la révolution quantitative, c'est ainsi que Marie Claire Robic \autocite{Robic1982} introduit dans son article de 1982; il existe une base bien antérieure dans une étude française datant de 1841 qui pose elle aussi les même exigences propre à l'application d'une démarche hypothético-déductive, et propose d'étudié les villes dans un systèmes de villes ouvert, et marque également l'importance dans la systémique d'agent holonique comme l'administration francaise qui encadre par ses règles les choix de l'individu dans un contexte sociétal. 





++ blabla ++

En ce sens la méthode dite d'abduction qui oeuvre par rétroduction, comme le propose Denise Pumain dans sa théorie évolutive des villes, parait plus réalistes.

fait appel à l'imagination du chercheur, laquelle s'applique aussi dans la formulation d'analogies, ou isomorphisme, un sport bien connu des géographes, et cela bien avant l'arrivée de Bertalanffy et son projet unifiant. 
Loin d'apporter seulement un nouveau vocabulaire, Bertalanffy catalyse ?


\subsection{Retour sur la fondation et les apports du "paradigme systémique" au début du XXème siècle}
\label{ssec:systemique}

De la même façon que les épistémologues des sciences comme ici Olivier Orain \autocite{Orain2001}, l'auteur ne détaillera pas ici une approche inter-disciplinaire de la notion \footnote{Au sens donné par Piaget, voir note de bas de page \autocite {Orain2001}} de "système", difficile à envisager dans un cadre global car sa diversité d'acceptation est fonction, d'une part de la rapide évolution de cette notion depuis les années 1940, et d'autre part la règle définissant l'acceptation de cette \textit{notion} dépend non seulement de la variabilité inter-disciplinaire, mais aussi intra-disciplinaire. Le terme "approche systémique" est alors proposé par \autocite{Orain2001} pour incarner cette diversité d'intégration par les disciplines des sciences sociales de la "théorie systémique" ou "systémique".

La complexité d'approche caractéristique de cette notion est pour Jean Louis Lemoigne grandement lié à la reconstruction épistémologique \textit{a posteriori} de ce qu'il appelle "paradigme systémique". Une acceptation qui parait d'autant plus justifié tant l'étude exhaustive de la ramification qui découle du concept est impossible, et sans rentrer dans les détails de querelles entre les différentes chapelles, il est acceptable de voir cette construction comme un processus de raffinement cumulatif. (a dire mieux)

\subsubsection{La Cybernétique}
\label{ssubsec:cybernetic}

\paragraph{Des outils pour penser une nouvelle causalité}

Une des branches communément admises comme fondatrice du mouvement tient dans l'organisation des conférences de Macy entre 1942 à 1953. Celle ci sont considérés comme un des tout premier regroupement interdisciplinaire et marque une période de changement profond dans l'histoire des sciences en général, et particulièrement en science sociale. Celles ci vont réunir pendant plusieurs années autour d'une même table des acteurs majeurs des sciences physiques et sociales pour discuter autour de régularités communément observés, avec pour idée la construction d'un savoir commun que l'on pourra alors qualifier de trans-disciplinaire. 

Les conférences naissent suite à la rencontre entre un mathématicien réputé au MIT N. Wiener, un neurobiologiste A. Rosenbluch, et un ingénieur électronicien J.Bigelow qui vont opérer un rapprochement entre l'homme et la machine entre 1942 et 1946 (pour rappel le premier ordinateur ENIAC est opérationel en 1946) par le biais de groupes inter-disciplinaires chargés d'explorer ce "\textit{no man's land}" à l'interface des deux disciplines. 

Plusieurs "outils" dérivent de ces premiers séminaires organisés dès 1942 à la Josiah Macy, Jr. Foundation : la notion de "boite noire" ou système téléologique fonctionel, et la notion de \textit{feedback} ou causalité circulaire, avec pour objectif principal l'étude de l'homéostasie introduite auparavant par les travaux pionniers du physiologiste Walter Cannon en 1926.

Si la notion d'homéostasie pour des organismes vivants apparaît pour la première fois cité par Claude Bernard 1865, celle ci est reprise et étendue par Walter Cannon en 1932 dans le livre \textit{The Wisdom of the Body} \autocite{Cannon1932} comme « l’ensemble des processus organiques qui agissent pour maintenir l’état stationnaire de l’organisme, dans sa morphologie et dans ses conditions intérieures, en dépit de perturbations extérieures ». Ainsi dans le cadre de son application biologique cette rétro-action permet de décrire un certain nombre de mécanisme à l'oeuvre dans une cellule en interaction avec son environnement qui tente de maintenir de façon stable dans son milieu la concentration d'éléments comme les ions, la glycémie, etc.

L'attention des discutants dans ces premier séminaire porte donc avant tout sur l'ubiquité du concept et la pertinence de son transfert hors des systèmes biologiques. Wiener fait alors un rapprochement décisif entre les problématiques de calcul de trajectoire en balistique et des maladies nerveuses ayant pour symptôme l'ataxie. De ces discussions émergent alors un même schéma explicatif qui semble à la fois convenir à ces problématiques, la "causalité circulaire". \autocite[774]{Pouvreau2013, Rosnay1975}

L'approche néo-béhavioriste retenue par les discutants "consiste à étudier un objet comme une "boite noire", par l'examen de l'extrant de l'objet [i.e tout changement produit dans son environnement] et des relations entre cet extrant et l'intrant [i.e tout événement externe qui modifie l'objet]" \autocite{Pouvreau2013} En adoptant cette approche, le "comportement" d'une entité est perçu "comme tout changement extérieur détectable de cette entité par rapport à son environnement" , et par téléologique il faut entendre un comportement "finalisé" c'est à dire déterminé par un mécanisme de "rétroaction" négative. De la connaissance de ces entrants et de ces sortants, on peut en déduire qu'il existe une retro-action négative ou positive, ou \textit{feedback} permettant de décrire progressivement le système de commande de la boite noire.

L'introduction de cette "causalité circulaire" est pour l'époque loin d'être anodine car elle remet en cause le schéma classique linéaire cause \textrightarrow conséquence, qui se traduit dans le temps par la relation avant \textrightarrow après, la cause étant irrémédiablement suivi d'une conséquence. La possibilité de causalité circulaire, positive ou négative, brise ce schéma, et ne permet plus d'isoler un ordre entre cause et conséquence, c'est le problème de "la poule et de l'oeuf". En réintroduisant la poursuite d'un but, on injecte une autonomie, une spontanéité, une dynamique entre objets qui était jusque là absente de la causalité linéaire déterministe.

Appliqué à un système servo-mécanique, la stabilité de celui-ci suppose la capacité à anticiper et à annuler les agressions extérieures par une capacité de régulation (flexibilité) qui repose plus alors sur la dynamique des interactions que sur la structure physique en place (rigidité), un mode de fonctionnement impossible si on se place dans le cadre de la "pensée classique" de l'époque. 

%Dans "Behavior, Purpose and Teleology", le terme téléologie est à ce titre utilisé comme un synonyme de "l'objectif controllé par la rétroaction".\footnote{wikipedia}

\paragraph{La réintroduction du concept de "téléologie"}

Avec la mise en place d'une classification de ces comportements, et en prenant distance du concept de "causalité finale" qui lui était rattaché, les auteurs espèrent ainsi redorer le concept de téléologie, renouant avec la reconnaissance de l'"importance du but" qui avait disparu avec la mise au ban de ce concept. Reprenant les explications de \autocite[776]{Pouvreau2013}, celui-ci cite \autocite[23-24]{Rosenblueth1943} "[...] Puisque nous considérons la finalisation comme un concept nécessaire afin de comprendre certains modes de comportement, nous suggérons qu'une étude téléologique est utile si elle évite les problèmes de causalité et se limite à s'attacher à l'étude du but [...] Le comportement téléologique devient synonyme de comportement contrôlé par une rétroaction négative et gagne donc en précision par une connotation suffisamment restreinte." La finalité est reintroduite via le concept de "téléologie", mais elle est libéré de la notion de "causalité" qui lui était autrefois associé. Elle redevient l'étude des comportement associé à un but, dont l'importance ne peut plus être nié, et redevient compatible avec le concept autrefois opposé de déterminisme.\footnote{Pour donner un exemple peut-être plus parlant, l'étude en biologie des comportement oeuvrant dans la formation d'un organisme par une méthode téléologique n'empêche pas l'usage d'un cadre de pensée déterministe  correspondant à la formation d'un même organisme à partir d'un même code initial (un déterminisme largement remis en cause depuis, voir par exemple \href{http://www.nytimes.com/2014/01/21/science/seeing-x-chromosomes-in-a-new-light.html?ref=science&_r=0}{New York Times} )}

De ces discussions deux articles fondateurs à la fois des sciences cognitives \autocite[23]{Dupuy2000} et de la cybernétique vont être publiés : \textit{Behavior, Purpose and Teleology}ou Rosenblueth, Wiener, et Bigelow " propose de déconstruire la distinction entre action volontaire et acte réflexe, en assimilant la volonté à un mécanisme de rétro-action (\textit{feedback})"; et \textit{A logical calculus of the ideas immanent in nervous activity} où McMulloch et Pitts donne "une base purement neuroanatomique et neurophysiologique au jugement synthétique \textit{à priori}, et de donner ainsi une neurologie de l'esprit"

\paragraph{ Les limites du transfert des concepts aux sciences sociales}

\subparagraph{Introduction aux sciences sociales}
Parmis les auteurs de ces premiers séminaires organisés entre 1942 et 1944 figurent deux représentant des sciences sociales, Gregory Bateson et Margaret Mead. Enthousiastes, il vont rapidement trouver dans l'étude des concepts développés dans ce premier séminaire (1942) un écho à leur propre travaux sur la dynamique sociale, la notion d'homéostasie n'étant qu'un nouveau mot permettant de rassembler des travaux existants déjà au fait de ces phénomènes. Cette mise au jour de problématiques commune entre le biologique et le mécanique permet d'envisager la construction d'un référentiel lui aussi commun; une prise de conscience qui va amener les auteurs du cercle de réflexion initial à envisager rapidement l'élargissement de celui-çi à l'ensemble des acteurs des sciences sociales.

La suite des conférences de Macy (1946-1952) sera organisés par Arturo Rosenbluch et son ami Warren McCulloch, un autre neurobiologiste. Cette ouverture vers les sciences sociales est timide dans un premier temps, et ce n'est qu'à la 2ème conférence en octobre 1946 sur une suggestion de Lazarsfeld que les conférences concrétise cette ouverture dans le cadre d'un sous séminaire intitulé \textit{Téléogical Mechanisms in Society}. La 4ème conférence acte cette ouverture et introduit pour la troisième fois de suite une modification de l'intitulé, avec cette fois ci l'adjonction d'une dimension sociale à un objet d'étude, qui apparaît encore à cette date difficile à définir : "la causalité circulaire et des mécanismes de \textit{feedback} dans les systèmes biologiques et sociaux". Le terme \textit{Cybernetics} est pour la première fois introduit dans les séminaires par Wiener en 1946. Il faut toutefois attendre 1949 et la septième conférence pour que sous l'influence d'un nouveau participant nommé H. Von Foerster, ce terme chapeaute de façon définitive les prochains intitulés de séminaires. Au final, ces dix séminaires vont participer de l'émergence de la "science cybernétique" en "permettant l'échange effectif de savoir et d'experiences, tant entre les disciplines qu'entre les sciences et la société", réalisant par là un des objectifs annoncé par Wiener et Rosenbluch dans leur classification, faisant de la cybernétique une "[...] science générale des systèmes à comportement finalisé ayant principalement pour objet ceux dont le comportements est "téléologique" " \autocite{Pouvreau2013}

\subparagraph{Des biais mécanisistes mettent en échec ce premier transfert}

Wiener mais aussi d'autre acteurs de la cybernétique ont vus assez tôt tout l'intérêt que pourrait apporter l'utilisation et le transfert d'outils comme "la boite noire", ou le principe de régulation par "rétro-action" une fois appliqué à l'étude des interactions dans les systèmes sociaux. Mais les difficultés d'applications et les critiques ont rapidement mis à mal cet objectif trans-disciplinaire, pour plusieurs raisons qui tiennent : d'une part à l'existence de restriction mathématiques remettant en cause la scientificité des résultats obtenus : (a) les statistiques sur le long terme étant difficile à obtenir (b) la difficulté à minimiser la distance entre observateur et phénomène observés, et donc le biais qui s'applique aux données dans un tel cadre; et d'autres part au réductionnisme et le biais mécanicistes touchant la vision de certains acteurs des conférences de Macy  : "[...] la vie était pensée comme un dispositif de réduction d'entropie ; les organismes et leur associations, en particulier les hommes et leurs sociétés, l'étaient comme des servomécanismes ; et le cerveau comme un ordinateur" \autocite[784]{Pouvreau2013}

\autocite[782]{Pouvreau2013} explique très bien les limitations qui font  de l'extension de la cybernétique au sciences humaines une simple "[...] ressemblance superficielle au niveau du formalisme. Ne serait-ce que parce que dans un système tel que conçu par la "première" cybernétique, par définition fermé à l'information, la téléologie ne peut qu'être confinée au cercle d'un but déterminé; et que pour cette raison, ce modèle ne permet pas de comprendre de quelle manière un système peut être amené à redéfinir ses buts à partir de ses interactions avec son environnement, la pertinence d'une téléologie relative à des buts \textit{intentionels} restant donc intacte en sciences humaines".

\subsubsection{La GST ou la théorie des "systèmes ouverts"}
\label{ssubsec:gst}

Cette incapacité de la première cybernétique à coller aux problématique des systèmes sociaux va trouver un écho plus positif dans un courant qui se développe en parallèle du mouvement cybernétique. Ce mouvement fondé par le biologiste Ludwig Von Bertalanffy en 1937 peut être considéré comme la deuxième branche venant enrichir le paradigme systémique. Tout en apportant de nouveaux concepts, celui ci va se positionner de façon critique par rapport à la "première cybernétique" tout en englobant par la suite les autres innovations qui proviendront de ce courant, Asbhy jouant le rôle important de médiateur entre ces deux courants.\autocite[]{Pouvreau2013} De cette prise de position va peu à peu découler la construction d'une théorie établissant une méthodologie logico-mathématique à vocation unifiante, accessible à n'importe quel champs disciplinaire pour décrire les lois de structure similaires (isomorphe). \autocite{LeMoigne2006a}. 

Ainsi rapporté par LeMoigne en 1977, cette "vision stupéfiante est celle d'une une théorie générale de l'univers, du système universel" \autocite[59]{Lemoigne1977}. Le mot "Vision" est ici quasi synonyme de "Révélation", car elle amène à voir une tout autre approche du réel pour qui s'en rapporte. Ainsi selon les mots même de Bertalanffy, "De tout ce qui précède se dégage une vision stupéfiante, la perspective d'une conception unitaire du monde jusque-là insoupçonnée. Que l'on ait affaire aux objets inanimés, aux organismes, aux processus mentaux ou aux groupes sociaux, partout des principes généraux semblables émergent" \autocite[59]{Lemoigne1977} \autocite[220]{Bertalanffy1949}. Une idée déjà existante dans la maxime célèbre de Claude Bernard en 1885, remise au gout du jour par \autocite{Lemoigne1977}, celle-ci résume toute la souplesse offerte par cette notion d'un point de vue de la modélisation :  "Les systèmes ne sont pas dans la nature mais dans l'esprit des hommes"

Cette théorie nommé \textit{General System Theory} (GST) est évoqué pour la première fois en public en 1937-38 par Bertalanffy, s'ensuit alors la rédaction d'une première ébauche en 1950, et il faudra attendre 1968 pour qu'un ouvrage titré \textit{General System theory: Foundations, Development, Applications} proposent une synthèse de toutes les avancées. La durée de développement de cette théorie n'est pas anodine, et si on en croit Pouvreau \autocite{Pouvreau2013} qui a analysé en détail la très vaste littérature associé à cette thématique, cette théorie n'en est pas vraiment une en réalité. En effet l'état inachevé du projet de Bertanlanfy laisse plus à penser qu'il s'agit là d'un "projet", et c'est à ce titre que Pouvreau préfère employer le terme de "systémologie générale" pour désigner ce qu'il définit alors comme "le \textit{projet} d'une \textit{science de l'interprétation systémique} du "réel" " \autocite[9]{Pouvreau2013}. L'hypothèse défendu par Pouvreau étant que cette "[...]science de l'interprétation systémique du "réel" se caractérise en fin de compte comme une herméneutique, au sens où elle a pour vocation d'élaborer à la fois les moyens de construire des interprétations systémiques d'aspects particulier du "réel" sous la forme de modèles théoriques spécifiques et les moyens d'interpréter à leur tour de tels modèles comme des déclinaisons de modèles systémiques théoriques d'un degré de généralité supérieur."\autocite[9-10]{Pouvreau2013}

Mais avant de même de fonder ce projet unifiant qui par la suite va rayonner et être absorbé (non pas sans déformation ..) dans un grand nombre de disciplines, dont la géographie, il est intéressant de rappeler comment la théorie biologique de Bertalanffy a participé de la formation de grandes notions comme l'"équifinalité" ou l'"auto-organisation", des notions aujourd'hui communément admises comme fondatrice du paradigme actuel de la "complexité".

Bertalanffy poursuivant depuis 1937 avant tout cet objectif de dépasser la compréhension des systèmes biologiques  englué jusque alors dans une dualité opposant les "vitalistes" et "mécanistes". La synthèse de ces travaux est organisé dans une "biologie organismique" qui fonde une troisième voie visant d'une certaine manière la réconciliation entre les deux approches \autocite[55-56]{Lemoigne1977} \autocite[258]{Bertalanffy1949}. Avec cette nouvelle biologie théorique il s'agissait donc d'incarner "l'avenir de la biologie" en établissant via la mobilisation de moyen scientifique (analyse et analogies physico-chimique et mathématique du vivant) écartant la métaphysique/psychiques, un programme de recherche des ""loi systémiques ou d'organisation à tous les niveaux de la nature vivante" entendues comme "l'explication de l'harmonie et de la coordination des processus à partir de la dynamiques des forces qui leur sont immanentes""\autocite[456]{Pouvreau2013}. Principalement "ordonnées en direction de la conservation de la totalité"\autocite[440-458]{Pouvreau2013} dans une "tendance à une complication croissante", cette "Gestalt organique" de la théorie "organismique" de Bertalanffy place "l'Organisation" des processus comme une véritable problématique de recherche, et met de coté la question de la "finalité" du vivant.\autocite[455-457]{Pouvreau2013}

Déjà tout à fait conscient que "le tout est plus que la somme des parties" Bertalanffy admet que l'étude des mécanismes physico-chimiques des processus vitaux tient plus d'une heuristique de recherche, une "méthode téléologique qui permet "d'examiner jusqu’à quel point le caractère de conservation de la totalité se manifeste dans les processus qui se déroulent en eux" sans jamais arriver à en donner une complète description.\autocite[464]{Pouvreau2013}

Cette "biologie théorique organismique" (également appelé de façon synonyme par Bertalanffy "théorie systémique du vivant") montre en bien des points toutes les prémisses d'une pensée systémiste et non réductionniste qui dépasse déjà largement le cadre seul de la biologie, et cela même avant 1937 et l'introduction de "systèmes ouvert" \autocite[499]{Pouvreau2013} qui ont fait la renommée de l'auteur.  Cette "biologie organismique" de Bertalanffy, bien évidamment construite sur les acquis et l'aide de bien d'autres de ces contemporains (voir \autocite{Pouvreau2013}, arrive à maturité en 1937 \autocite[14]{Pouvreau2013}, et présente déjà à ce stade tout les traits d'une première "systémologie restreinte", qui va servir d'"antichambre" à la formation de la future "systémologie générale" (la première évocation publique date de 1945, mais des traces indirectes de ses premiers discours semblent remonter à 1937).\autocite[670]{Pouvreau2013} de Bertalanffy.

% D'abord on fait le point sur les principes (ce qui suppose de faire une grosse parenthèse avec tout ce que l'on a décrit sur la thermodynamique) et ensuite on peut passer à la critique, évoquant l'équifinalité et la hierarchisation de processus qui permet de recentrer aussi l'étude des boites noires.

L'articulation entre les deux "principes organismiques" qui fondent sa théorie apparaît de façon très claire dans une première définition du vivant en 1932, ici cité dans sa version telle que raffinée par Bertalanffy en 1937, date à laquelle selon \autocite{Pouvreau2013} sa théorie arrive à maturation : "Un système organique quelconque n'est essentiellement rien d'autre qu'un ordre hiérarchique de processus qui se tiennent mutuellement en équilibre de flux [...] Un organisme vivant est un ordre hiérarchique de systèmes ouverts, qui se maintient sur la base de ses conditions systémiques par un changement de ses composants"

%Définition des deux principes organismiques !? 

Le premier principe théorique "organismique" de Bertallanfy s'appuie sur le principe biologique fondamental qu'il a énoncé dès 1929 avec la "conservation du système organique en équilibre dynamique". Un équilibre qui parait statique d'un point de vue extérieur, mais qui est en réalité dynamique car son existence même est basé sur la remise en jeu permanente d'une partie du travail effectué par la cellule pour maintenir le système organique loin de l'équilibre "vrai" (physique, c'est à dire celui qui correspond à une mort thermique, ou chimique qui ne peut pas produire non plus de travail à l'équilibre). Un "équilibre de flux" qui ne peut être réalisé que parce que l'organisme n'est ni un système fermé, ni un système statique, mais un système dont l'ordre et l'organisation (def à valider ici) est fondé sur un travail issue d'un "flux" de matière et d'énergie résultat d'une transaction à double sens avec son environnement. \autocite[472]{Pouvreau2013} Je me permettrai de citer ici Morin, qui reprenant Héraclite, évoque très bien cet antagonisme à l'oeuvre dans les systèmes organiques, mais aussi par extension sociaux "" Vivre de mort, mourir de vie " : ne vivons-nous pas de la mort de nos cellules qui vieillissent et se décomposent pour laisser la place à des cellules jeunes ? [...] La vie et la mort sont certes deux ennemies fondamentales, mais la vie lutte contre la mort en utilisant la mort. Néanmoins, il est tuant de se régénérer en permanence. C’est épuisant. Finalement, on meurt à force de rajeunir. On meurt de vie. " \autocite{MorinXX} 

% Critique cybernétique
Le principe d'"équilibre des flux", même si il peut être rapproché du concept d'"homéostasie" définit par les tenants de la "première Cybernétique" (en analogie avec les systèmes mécaniques) comme la "conjonction des processus par lesquels, nous autres, être vivants, résistons au courant général de corruption et de dégénérescence" est trop généraliste pour application en tant que tel à toute les notions de régulations organiques. \autocite[194]{Morin1977} \autocite{Wiener1950}. L'"homéostasie" tel que définit par Wiener dans le cadre de la Cybernétique s'avère en réalité être un mécanisme de régulation organique parmi tant d'autres, tous n'étant pas basé sur le schème de rétro-action. A ce titre, la notion d'"homéostasie pourtant quasi semblable dans sa définition à l'équilibre de flux dans un système ouvert, mobilise en réalité un tout autre fonctionnement que le schème de rétro-action Cybernétique, et tient plus de l'extension aux systèmes ouverts du principe dit de "Le Chatelier". De la même façon la régulation intervenant dans le processus de croissance des organismes qui nécessite la régénération, et l'évolution des structures dans le temps n'est pas compatible avec l'ordre structural pré-établi des machines et le scheme de rétro-action promis par la Cybernétique. La vision "machinaliste" limité/biaisé des premiers cybernéticiens n'est donc pas satisfaisante pour une application aux systèmes organiques, dès lors qu'il faut accepter la constance non pas des structures mais des interactions entre les structures. Bertalanffy développe une classification plus complète de ces régulations qu'il considère selon le type de leur téléologie, et introduit le concept d'"équifinalité" comme téléologie dynamique moteur dans la construction et le maintien des systèmes organiques. Dans ce contexte, le principe d'équifinalité \autocite[131]{Pouvreau2013}, est ainsi évoqué pour la première fois comme la possibilité d'atteindre le même état finalisé à partir de trajectoires quelconques, un processus impossible dans le cadre de système fermé où les condition initiales définissent par avance l'état final. Ce faisant, Bertalanffy introduit la primauté de l'ordre dynamique sur l'ordre structurel et fait de l'équifinalité un concept qui dérive de l'ouverture des systèmes. \autocite[489]{Pouvreau2013} \autocite[647]{Pouvreau2013} Un exemple illustrant les effets de l'équifinalité dans les organismes vivants peut être montré avec le processus de division embryonnaire. Ainsi un organisme a qui ont impose la fragmentation, la régénération, ou des blessures d'unités biologiques élémentaires comme les gènes ou les chromosomes va de façon constante s'organiser suivant un plan pré-établi menant à la "constitution d'un tout", autrement dit un organisme complet. 

%Il nécessite un autre mode d'explication de processus téléologique, celui de la cybernétique s'avérant incompétent au regard du principe d'équifinalité observé dans les systèmes organiques.
 
% Bertalanffy s'appuie dans sa critique à raffiner sa classification des téléologies, ce qui lui permet d'introduire le concept d'équifinalité comme sous-type de téléologie dynamique, un type de processus de régulation qui selon lui ne peut pas être expliqué par les schèmes cybernétique initiaux, seulement capable de mobiliser le concept de finalité en regard d'une explication basé sur un arrangement structural pré-établi (une machine faites de composants) et non pas l'ordre  dynamique propres au système en équilibre de flux.

La combinaison des deux principes "organismique" menant à la théorie des "système ouvert en équilibre de flux" deux heuristiques de recherches \autocite[481]{Pouvreau2013}:
\begin{itemize}
\item La subordination du "principe de hierarchisation " à celui du "système ouvert en équilibre de flux", autrement dit la genèse et le maintien de l’ordre hiérarchique d’un « système organique » est conditionné par l'existence d'un "système ouvert en équilibre de flux"
\item  La relation précédente est un principe ubiquitaire s’appliquant à tous ses niveaux
\end{itemize} 

Cet idée sera particulièrement fructueuses une fois articulé avec le principe d'un enboitement des systèmes, l'accroissement du degré de liberté dans un système résultant de l'équifinalité.
 \autocite[38]{Bertalanffy1973} \autocite[786-788]{Pouvreau2013}

%Developpement rendu possible uniquement par l'apport des théories de la thermodynamique ... l'expression d'une trajectoire indépendamment de l'état final, celui ci n'est qu'un processus de régulation parmis d'autres, car ce même système organique est non seulement capable de maintenir son état mais choses plus importante, il permet surtout de produire de l'organisation, de la complexification.

% Relation avec science sociale ??
% => entéléchie / 
Cette notion d'équifinalité reliant un niveau micro à un niveau macro pourra par la suite être transposé dans les système sociaux, le parallèle de l'individu comme acteur réflexif dans la société sera mobilisé par ?

De ce fait la Cybernétique n'est pour Bertalanffy qu'un cas particulier dans une systémologie dont il pense qu'elle peut être beaucoup plus universelle... ++ Homéostasie avec Ashby ? ++

Tel que définie, cette notion d'équilibre dynamique de Bertalanffy est bien différente de celle produites en physique et en chimie, qui se caractérise justement par l'absence de travail disponible, l'énergie disponible étant minimale. Pour que la permanence d'un ordre puisse être effective dans la théorie organismique, il faut qu'il y ai un échange, un flux d'énergie mais aussi de matière possible avec l'environnement; une différenciation qui amène Bertalanffy à développer dès 1937 une théorie des "systèmes ouverts", la seule capable de s'appliquer également à des systèmes sociaux par la suite.

% Sur l'ouverture des systèmes
Pour mieux comprendre en quoi cette ouverture est importante pour l'application du paradigme systémique aux sciences sociales, il faut revenir quelques décennies en arrière pour définir les limitations des premier systèmes issue de la thermodynamiques, limitations qui par la suite ont irrigués les réflexions initiales des cybernéticiens tout autant que les motivations de Bertalanffy pour les dépasser dans le cadre de sa théorie "organismique"

La seconde loi de la thermodynamique esquissé par Carnot et formulé par Clausius en 1850 montre que l'energie calorifique ne peut se reconvertir, elle se \textit{dégrade} et perd son aptitude à effectuer un \textit{travail}. Clausius nomme "entropie" cette diminution irréversible de l'aptitude à se transformer et à effectuer un travail, propre à la chaleur.\autocite[35]{Morin1977} Prigogine dans la \textit{fin des certitudes} écrit à propos de l'entropie qu'elle "[...] est l’élément essentiel introduit par la thermodynamique, la science des processus irréversibles, c’est-à-dire orientés dans le temps." 

C'est Boltzmann, Gibbs et Planck qui vont par la suite faire le lien entre le niveau micro des particules et la notion de chaleur. Parce que la chaleur est caractérisé par l'agitation désordonné des molécules dans un systèmes, l'entropie devient plus qu'une simple réduction du travail, c'est aussi l'ordre et le désordre des molécules qui en est la cause. Cette transformation s'effectue avec création d'entropie, une "quantité de désordre" qui ne peut que croître dans le temps et cela jusqu'à atteindre une valeur maximale équivalente à ce nouvel état d'équilibre. De ce fait et de façon générale celle-ci définit comme évolution irréversible toute transformation réelle dans un système isolé (Système où la frontière est totalement imperméable : l'Univers est par définition un tout englobant) ou fermé (Système ou la frontière est perméable aux flux entrant ou sortant d'énergie mais imperméable aux échanges de matière : La Terre reçoit de l'énergie du soleil) partant d'un état non stable et se dirigeant vers un nouvel état stable.  Ainsi si on considère l'univers comme un méta-système isolé englobant tout les autres, alors ce second principe a pour corollaire que l'entropie de l'univers augmente vers un état de désordre maximal qui se traduit en définitive par une mort thermique.

L'intuition de cette possible analogie entre loi gouvernant systèmes physiques et biologiques est issues des réflexions menés par Boltzman, qui comme ces contemporains du XIX siècle est admiratif pour la récente théorie évolutive de Darwin \autocite[27]{Prigogine1996}. Celui ci tente alors un parallèle avec ses propres travaux sur la seconde loi de thermodynamique, que l'on retrouve dans une des fameuses citations présente dans son livre "second law of thermodynamic" : " The general struggle for existence of living beings is therefore not a struggle for raw materials — the raw materials of all organisms in the air, water and soil are in abundance there — nor about energy, which in the form of heat, unfortunately, is contained abundantly [but unfortunately] [in]convertible in each body, but a struggle for entropy, which is available [disposable] by the transfer of energy from the hot sun to the cold earth."

% Le sys ouvert/fermé , de la thermodynamique à la biologie ?
Le point de vue de Boltzmann est repris et théorisé par Alfred J. Lotka, un mathématicien, chimiste et statisticien qui va largement influencé par la suite Bertalanffy dans la formation de sa "systèmologie générale" \autocite[178]{Pouvreau2013} par ces études de la démographie des populations et des flux de matières dans le monde biologiques \autocite[545-546]{Pouvreau2013} , toutes deux usant largement des équations différentielles (un premisse d'isomorphisme mathématique applicable à diverses disciplines pour qui quiconque tente de rentrer dans le formalisme de Lotka, et par la suite Lotka et Volterra \autocite[550]{Pouvreau2013}). De la même façon que Bertalanffy par la suite, celui ci ignore sciemment les débats entre "vitalistes" et "mécanicistes", et adopte un point de vue unificateur qui vise la réconciliation entre système physique et système biologique, et part à la recherche d'isomorphisme en s'appuyant sur le processus d'irréversibilité commun aux deux paradigmes : "[...] the law of evolution is the law of irreversible transformation; that the \textit{direction} of evolution [...] is the direction of irreversible transformations. And this direction the physicist can define or describe in exact terms. For an isolated system, it is the direction of increasing entropy.  The law of evolution is, in this sense, the second law of thermodynamics" \autocite[26]{Lotka1925}.

Dès 1922 \autocite{Lotka1922a} \autocite{Lotka1922b} Lotka une nouvelle théorie qui acte la capacité de capturer de l'énergie comme un optimum à atteindre guidant la sélection tel quel est décrite par l'évolution Darwinienne. Il est également l'un des premier à percevoir les limites des lois actuelle de la thermodynamiques pour expliquer les processus du vivants, ainsi "Tenant pour légitime de traiter les êtres vivants et leurs associations comme des systèmes physiques, Lotka insistait toutefois sur le fait qu’il s’agit de « systèmes ouverts » aux flux de matière et d’énergie (ainsi que Raymond Defay (en 1929) et Bertalanffy (en 1932) les qualifièrent plus tard), capables d’échapper à l’équilibre thermodynamique défini par un maximum d’entropie promis aux systèmes fermés par le Second Principe, et d’évoluer vers une structuration croissante." \autocite[179]{Pouvreau2013}

En effet pour un système vivant, l'état d'équilibre tel que décrit pour des systèmes clos ou isolé, correspond à un état de mort cellulaire. Hors, il est prouvé empiriquement à cette période que les systèmes vivants évolue dans un environnement chimique en perpétuel évolution loin de l'équilibre, et sont de fait capable de maintenir un haut niveau d'organisation par l'échange d'énergie et de matière avec l'environnement. Autrement dit, il n'est pas possible de concevoir l'équilibration permanente des systèmes vivants comme le résultat d'une évolution entropique croissante \autocite[248]{Lemoigne1977}. Des résultats énoncés sous forme de loi en 1929 par Bertallanfy, qui fait de "la conservation de système organique en équilibre dynamique" un "principe biologique fondamental", et qui deviendra plus tard dans sa théorie "organismique", le premier principe de  "système ouvert" en "équilibre de flux". \autocite[492]{Pouvreau2013} 

Mais en voulant faire l'analogie entre ces deux systèmes, une question va rapidement se poser aux scientifiques. "Comment la progression irréversible du désordre pouvait elle être compatible avec le développement organisateur de l'univers matériel, puis de la vie, qui conduit à homo sapiens ?", une question qui va engendrer la problématisation et un changement de point de vue radical. Comme le résume bien \textit{a posteriori} Morin dans son premier tome de \textit{La Méthode}, "A partir du moment où il est posé que les états d'ordre et d'organisation sont non seulement dégradables, mais improbables, l'évidence ontologique de l'ordre et de l'organisation se trouve renversée. Le problème n'est plus : pourquoi y a-t-il du désordre dans l'univers bien qu'il y règne l'ordre universel ? C'est : pourquoi y a-t-il de l'ordre et de l'organisation dans l'univers ? " \autocite[37]{Morin1977}

Avec de tel propos se pose alors rapidement la question des mécanismes à l'oeuvre dans le vivant qui permettrait en quelque sorte de rétablir l'universalité de la seconde loi thermodynamique. Bien qu'intuité par de nombreux chercheur comme Lotka ou Bertalanffy, il faudra attendre les années 1940 pour que s'amorce plus concrétement ce rapprochement entre paradigme évolutionniste et domaine de la thermodynamique, concrétisé par le partage des théories entre biologistes et physiciens, qui va se réaliser notamment sous le couvert des récents progrès de ce dernier, permettant l'émission de nouvelle hypothèses. 

Reprenant l'acceptation d'un système ouvert, c'est le livre \textit{What is Life} de Schrödinger (1944) qui va marquer le plus les esprits, et soulève le mieux ce paradoxe à la croisée des deux théories. Deux choses au moins fascine celui-ci \autocite{Foerster1959}, d'une part l'existence d'un code héréditaire qui définit au niveau micro la formation, l'organisation d'organisme au niveau macro (le principe "order-from-order"), d'autre part l'étonnante stabilité de ce code héréditaire immergé à 310 Kelvin \autocite[47]{Schrodinger1944}, et qui ne répond donc pas au fameux principe statistique "order-from-disorder" établit précédemment par Boltzmann.

En inscrivant comme nécessaire l'existence d'un code génétique comme un plan guidant l'évolution (tout comme Bertalanffy qui développe des théories similaires à la même époque), il introduit avec son concept de d'"entropie négative" un principe qui rend de nouveau compatible la seconde loi de thermodynamique avec l'évolution des systèmes biologiques : "le physicien attribuait le maintien de l’organisme dans un état « stationnaire » éloigné de l’équilibre vrai à sa capacité de se « nourrir » d’« entropie négative » grâce à son ouverture sur son environnement. Une « néguentropie » interprétée comme une « création d’ordre à partir d’ordre » -- l’organisme créant un ordre spécifique à partir de la matière déjà ordonnée, structurée d’une manière déterminée mais devant être transformée pour ses besoins énergétiques, qu’il trouve dans son environnement" \autocite[502]{Pouvreau2013} Autrement dit, le maintien de l'organisation est un équilibre dynamique, un jeu à somme nulle où la création d'entropie est annulé par la capacité des organismes à transformer l'énergie, l'ordre puisé dans l'environnement pour maintenir ce degré d'organisation, un processus qualifié de néguentropique. Ce concept, déjà difficile à accepter tel quel dans sa généralité \autocite[225]{Lemoigne1977} va par la suite être raccroché à théorie de l'information de Shannon après son introduction en 1948 dans le microcosme Cybernétique. L'introduction de cette théorie étant un autre moment fort (avec la thermodynamique) ayant inspiré de nombreux développement dans la cybernétique. Mais les tentatives d'unification entre les deux théories débouche sur deux rapprochement possible, avec d'une part la qualification d'une "information pensé comme quantité physique" ou d'autre part l'expression des "quantité physique pensé comme de l'information", selon que l'on adopte le point de vue de Wiener ou de Brilloin 1956 (auteur de la néguentropie qui associe qui associe "information" et principe de négentropie ). Ces points de vues font encore à l'heure actuelle l'objet de nombreux débats, certains voyant la physique de l'information comme un point de départ à creuser pour appeller une théorie de l'"organisation" \autocite[37-38]{Morin2005}, alors que d'autres n'y voient qu'un concept flou seulement basé sur la similitude des deux formules.  Autant de ramifications naissent de ces positions, et leur présentation dépassent de loin le seul cadre d'étude de cette thèse, mais le lecteur pourra se référer au travail de \autocite{Triclot2007} pour mieux comprendre le point de départ d'un malentendu qui dure toujours /footnote{Voir par exemple la différence de ton qui existe entre le site http://www.eoht.info/page/Information+theory, mais aussi les notes de bas de pages de \autocite[277]{Lemoigne1977} }. 

\autocite[482]{Pouvreau2013} Mais finalement plus que les idées développés par Shrödinger, la plupart étant déjà largement sous entendu dans les travaux des biologistes de l'époque, il semblerait plutôt que cela soit avant tout ce nouvel éclairage physiciste apporté à la biologie {REF}, et l'espoir déguisé (finalement non réalisé) de trouver de nouvelles lois physique à l'oeuvre dans la construction du vivant associé à la grande diffusion du petit livre dans le grand public qui amèna peut être de nombreux physiciens à ne plus ignorer les avancés dans ce domaine, notamment durant les années 1940 / 50, tel que Prigogine \autocite[77]{Prigogine1996}, Von Foerster, etc. \autocite[73]{Lemoigne1977} 

Mais conscient des manquements et des reproches faites à son approche, alors incomplète, car focalisé sur la cinétique, celle ci n'est pas relié à une théorie plus explicatives sur les mécanismes energétiques à l'oeuvre justifiant l'existence de ces propriétés des systèmes vivants dans le cadre des systèmes ouverts. C'est les récents développements sur la "Thermodynamique des processus irréversibles" qui va introduire a posteriori la possibilité d'une thermodynamique des systèmes ouverts compatible avec l'approche de Bertlanffy. Des physiciens ayant participé à ces travaux sur la thermodynamique des systèmes ouverts loin de l'équilibre (Osanger, etc.) c'est les travaux de Prigogine  en 1946 \autocite{Prigogine1946} qui vont le plus attirer l'attention de Bertalanffy. Lorsque celui ci découvre vers 1948 ces récentes avancées qui semble faire parfaitement écho à ces travaux ( Prigogine n'hésitant pas à citer Bertalanffy comme un de ses modèles d'inspiration \autocite{Prigogine1996}), le rapprochement se fait assez rapidement et Bertalanffy n'hésite pas à promouvoir cette nouvelle thermodynamique comme le parfait support physique justifiant des principes qu'il a établi dans sa propre théorie des système ouvert en équilibre des flux ! \autocite[653-658]{Pouvreau2013}

Pas étonnant donc de voir Bertallanffy s'appuie sur les écrits de Schrödinger pour re-formuler et préciser ses premières intuitions, 
+

Malheureusement le "théorème de Prigogine" de "minimum de production d'entropie" ne s'exprime que dans des conditions semblent il très drastiques \autocite[53]{Lebon2008} et limité à des systèmes très proche d'un état d'équilibre tel que le prouve les travaux de Denbigh : " It is possible that certain reactions in biological systems may be sufficiently close to equilibrium for the rate of entropy production due to them to be very small. But in general it seems that the notion of minimum entropy production has no real significance as applied to chemical reaction in open systems [...] it is incorrect to regard the tendency of an open system to approach a stationary state as being determined by thermodynamic factors. The stationary state may or may not coincide with a state of minimum entropy production, according to whether the rates of the individual processes are linear functions of thermodynamic variables. In the above we have assumed this to be the case for diffusion (eqn. (ll)), but it is known not to be true for chemical reaction." \autocite{Denbigh1952}

Hors l'état des systèmes biologiques est semble t il loin d'être proche d'un état d'équilibre thermodynamique.. Bertalanffy qui jusqu'à présent se contentait de relier les résultats à son programme organismique ne cache alors plus sa déception lorsque en 1953 il écrit '"Un minimun de production d'entropie ne caractérise donc pas l'équilibre des flux dans les systèmes ouverts [...]"; autrement dit "la thermodynamique [...] ne nous dit jamais ce qui peut se passer dans un système, ce qui est permis [...] Et le problème de l'organisation progressive, la tendance néguentropique de l'évolution des organismes simples aux organismes compliqués, reste à présent non résolu." Bien qu'ils n'abandonne pas l'idée de voir expliquer un jour sa théorie organismique par une théorie thermodynamique adapté, il abandonne en 1953 l'étude de la biophysique des systèmes ouverts et se consacre par la suite uniquement à la construction de sa théorie du système général.

Le fait est qu'il y a réduction d'entropie dans les systèmes en équilibre de flux, et qu'il y a maintient et augmentation du niveau d'organisation, sans que l'on sache pourquoi pour le moment dans le monde du vivant. Si l'analogie et le pont entre tissé entre physique et biologie semble donc encore soumis à questionnement, les travaux de Prigogine sur la thermodynamique des systèmes ouverts va continuer quand a elle à ouvrir bien d'autres perspectives, notamment dans les systèmes sociaux. 

%paragraphe dimension reflexive auto-orga ... 
Elle dépasser largement ce cadre, et appuie sur des bases physiques le concept d'"auto-organisation", une notion déjà introduite dans le mouvement cybernétique par Ashby, un homme clef dans la convergence des idées entre Cybernétique et GST.

Ashby, tout comme Von Foerster interviennent dans la création de la seconde cybernétique, et introduise une dimension réflexive aux débats.

Inspiré par Von Foerster, vont alors introduire un autre concept "d'order from noise", totalement différent du "order-from-disorder" de Schrodinger.

TODO : Partie plus axé sur les changements de causalité ? (vient avant ou apres ici ?)

L'équifinalité 

Un autre concept important est introduit par Ashby dans le mouvement Cybernétique, le concept d'auto-organisation, l'introduction du mot "auto" amorcant ainsi un virage réflexif qui annonce la seconde Cybernétique, piloté par Von Foerster.


%Des auteurs comme Prigogine en 1947 >> clairement inspiré par bertalanffy/ Schrodinger...  cf Pouvreau et internet
%Il fait le lien avec processus physique => 
%http://www.informationphilosopher.com/solutions/scientists/prigogine/
%http://www.informationphilosopher.com/solutions/scientists/schrodinger/

%http://en.wikipedia.org/wiki/Entropy_%28information_theory%29#Relationship_to_thermodynamic_entropy

C'est également à cette époque, que relayant les premiers travaux de Prigogine sur les systèmes dissipatifs, Bertalanffy va catalyser ainsi ces idées dans sa GST.

Ce procédé sera transféré au réel par Ashby, un autre cybernéticien qui travaillera dès 1946 à la mise au point d'une machine expérimentale capable de reproduire de façon mécanique cette dynamique de stabilisation face aux variations de son environnements. Nommé "homéostat" celle çi sera construite en 1948, et présenté aux conférences de Macy en 1952.

WIkipedia => L'implication de la cybernétique dans la systémique est historiquement plus liée au « deuxième mouvement cybernétique ». En effet, si selon Norbert Wiener la cybernétique étudie exclusivement les échanges d'information (car c'est « ce qui dirige » les logiques des éléments communicants d'où le mot cybernétique), dans son évolution qui engendrera la systémique, on réintègre les caractéristiques des composantes du système, et on reconsidère les échanges d'énergie et de matière indépendamment des échanges d'information.

La dégradation de l'énergie nécessaire pour maintenir une organisation implique l'irréversibilité des transformations.


The history of an open system is part of its structure, and Prigogine links open systems to irreversibility. Prigogine calls open systems dissipative. Put more simply, this means that matter does not tend to organise itself in a particular location unless there is some external energy source powering it. Evolution can be seen as matter organising itself.


The term "self-organizing" was introduced to contemporary science in 1947 by the psychiatrist and engineer W. Ross Ashby.[9] It was taken up by the cyberneticians Heinz von Foerster, Gordon Pask, Stafford Beer and Norbert Wiener himself in the second edition of his "Cybernetics: or Control and Communication in the Animal and the Machine" (MIT Press 1961).

Self-organization as a word and concept was used by those associated with general systems theory in the 1960s, but did not become commonplace in the scientific literature until its adoption by physicists and researchers in the field of complex systems in the 1970s and 1980s.[10] After Ilya Prigogine's 1977 Nobel Prize, the thermodynamic concept of self-organization received some attention of the public, and scientific researchers started to migrate from the cybernetic view to the thermodynamic view. WIKIPEDIA


Malgré les critiques soulevés de part et d'autres, du faite entre autre d'un objectif peut être un peu sur-évalué voire immodeste, celle ci aura un large écho auprès des sciences humaines, et notamment en géographie; d'abord anglo-saxonne \autocite{Haggett1965, Chorley1962}, puis par diffusion en France \autocite{Raymond}.



L'avénement de la deuxième cybernétique : 
La régulation apparaît en effet comme un phénomène majeur chez les organismes vivants, puisqu’elle « retarde la dégradation de l’énergie et donc l’augmentation de l’entropie » (p 129), et associée au retard d’entropie et à la computation, elles forment l’essence même de la cybernétique


%Dans l'ouvrage de vulgarisation de Joël de Rosnay construit un fil historique qui se concentre avant tout autour des intersections entre parcours de scientifiques et le MIT, qui pour lui a joué un grand rôle comme formidable catalyseur durant les premiers temps de construction.

Lien Forrester / Système dans science de gestion => Toutefois, les idées développés dans le cadre de ces conférences se borne dans un premier temps à des concepts sans réelle mise en application. Il faudra attendre les années 1960 et l'invention du transistor permettant la démocratisation des ordinateurs pour que la mise au point du langage DYNAMO en 1958 par Jay Forrester au MIT donne finalement corps à de nombreux concepts développés par la cybernétique.

 Mais la force des idées ainsi développés fait que celle ci n'attendent pas pour percoler déjà en réalité dans des disciplines qui en fait sont déjà tout à fait prompte à les accepter. En effet de nombreuses disciplines participe à cette période d'une révolution interne, et l'approche systémique développé par la cybernétique quand elle ne fait pas qu'apposer un nom commun sur des concepts déjà étudiés, fait alors écho à des révolution méthodologiques en attente d'être activés. \autocite[5]{Batty1976} résume la situation ainsi "The idea of systems being described in terms of structure and behaviour, in terms of input and output, and the notion of purposeful control of such systems in terms of negative and positive feedbacks, appeared to many social scientists an ideal description of their systems of interest and thus the approach has come to be used in more-or-less all of the social sciences". 
Une analyse reprise en l'état par certains acteur de la géographie française.

% ARG De la systémique à la simulation, il n'y a qu'un pas ...
"The intellectual revolution in geography since the middle and late ftities rests upon two main supporting pillars - men and machines." \autocite{Gould1970} Cette période de bouillonnement intellectuel s'appuie et accompagne les révolutions conceptuelles, l'usage renforcé de l'ordinateur pour l'enregistrement, le traitement et la visualisation des données est accompagné par le développement d'un nouvel usage, la simulation de modèle.

% Multiplicité des définitions à exposer quelque part par là ... histoire de resserrer le contexte sur la géographie par la suite ..

"Un système est un ensemble d'éléments en interaction dynamique, organisé en fonction d'un but." \autocite{Rosnay1975}

% Pivot causalité + temporalité , d'une approche seulement heuristique à un langage dédié, quel impact la construction des modèles !?
Les concepts de la cybernétique expose dans les termes la construction d'une chaîne de causalité, et de ce fait amène à rendre explicite les facteurs explicatifs à l’œuvre dans la construction des phénomènes à étudier, et d'autre part l'intégration de la flèche du temps dans des modèles jusque là exposé de façon statique. Il parait plus intéressant de situer ce débat dans un changement de perspective appuyé par un changement de paradigme dans la construction du concept de causalité en géographie. 

% Evolution outils va de paire avec l'évolution des questionnement méthodologique, la systémique à vocation à opréationalitsé, et c'est là que la simulation s'impose comme l'opérationalisation logique de ces questionnements, un trajet fait d'échec et de succès

% deux trucs de bati pour la nouvelle géographie : l'introduction de la systémique + révolution quantitative, l'usage de l'ordinateur.


% Introduire plan plus clair "géographie et concept issu de la complexité ? " devant l'opérationalisation ?
% Histoire de l'opérationalisation ? 
% Relire dauphiné pour écrire paragraphe sur la complexité
% plan ou je fait le lien entre méthode construction de la systémique de bertalanffy tel que appliqué en biologie par machamer finalement et méthode construction rapellé par Hedstrom, qui n'est qu'une réminiscence de celle ci ? 

%http://www.hypergeo.eu/spip.php?article426

\section{Un recouvrement des concepts}
\label{ssubsec:recouvrement_concept}

\paragraph{Exploration des isomorphismes}

\autocite[699]{Pouvreau2013} sur les isomorphismes.. / definition rapide /

- modèle gravitaire 
- loi rang taille
- lotka démographie (un usage aussi dans la biologie)
- structure dissipative

Le recours au modèle gravitaire inspiré des loi gravitationelle de Newton pour définir les attractions dans un espace isochrone, ou  

Une autre impulsion, de contour plus difficile à cerner, provient des spécialistes de la ville et des transports, que l'on suppose inspiré par les développement de la RAND un Think Thank américain qui pilote les grands programmes de plannification urbaines dès le début des années 1960. Même si les relations entre la RAND et les géographes paraissent unilatérales, celles ci employant peu d'universitaire \autocite[9]{Batty1994}, les retombés seront par la suite nombreuses. 

 apparaît que des géographes comme Ullman puis Garrison (Géographe Américain) bénéficie par leur inter-disciplinarité des retombées des modèles théoriques remis au goût du jour par Isard, lui même reconnu comme le fondateur de la science régionale, un mouvement largement inter-disciplinaire.

Enfin Hägerstrand, formé à l'université de Lund ou depuis les années 1950 la discipline géographique, comme dans beaucoup d'autres universités, ont été séparé en pole physique et humain. Une erreur selon Hägerstrand, fervant partisant de l'inter-disciplinarité, car lui même inscrit toute une partie de sa démarche dans la relation entre l'homme et son environnement. Ses premier travaux sur la diffusion de l'innovation vont l'amener à rencontrer les tenants géographes de l'école Américaine, avec qui il va opérationaliser ses modèles. Les échanges seront nombreux entre ces deux pays,  \autocite{Chardonnel1999}

Sa démarche semble selon les auteurs tantot résolument holistique pour certain, tantot tenant d'un individualisme méthologique pour d'autres. La réflexion de celui-ci semble pourtant se positionner dans un gradient entre les deux, l'importance de l'indivu comme acteur et penseur de sa propre vie ne semble pas dans la réflexion d'Hagerstrand incompatible avec l'approche multi-niveau et l'étude de l'individu ancré dans un territoire marque à la fois une approche holiste, qui même si elle est plus faible, est indéniablement présente dans la reflexion d'Hagerstrand. Edgar Kant (1902-1978) un géographe déjà rompu aux méthodes statistiques en Estonie \autocite{Chabot1937} , ou il a pu appliqué ses méthodes, est expatrié d'Allemagne avec dans ses bagages les travaux de Christaller, Lösch, etc. Tuteur d'Hagerstrand il le forme aux différentes méthodes qui vont se répercuter sur ses travaux de thèse.

Celui ci se posant pourtant comme anti-systémique (REF LENA A TROUVER), on trouvera quand même des écho troublant avec les concepts de la GST dans sa réflexion (interdisciplinarité de l'approche, importance des trajectoires spatio-temporelle, inscription multi-niveau, vision de l'individu probabiliste)
 
% Remarque Léna : Hagerstrand + individualisme méthodologique que holiste apparemment, elle l'a entendu dire ... surement lié au fait que c'est pas le meme niveau, individualisme méthodologique sur les aspects gst, mais holiste dans son approche générale (individu ancré dans le temps et l'espace).

L'intégration de la GST dans la géographie humaine semble être du fait des disciples de l'école de Garrisson; Ainsi, Peter Haggett, qui a joué un grand role dans la mise en application de ce transfert affirmait dès la première édition de \textit{L’analyse spatiale en géographie humaine} : "Au cours de la dernière décennie, la biologie et les sciences du comportement ont manifesté un intérêt croissant pour la théorie générale des systèmes (Bertalanffy, 1951). Quelques tentatives ont été faites (notamment par Chorley, 1962) pour introduire les concepts de cette théorie dans la géomorphologie et la géographie physique, et on ne voit pas pourquoi le concept de système ne pourrait pas être étendu à la géographie humaine." \autocite{Haggett1965} Alors que la région comme objet géographique se pose presque quasi-naturellement comme objet transférable dans le référentiel systémique\footnote{ Ce transfert parait tellement spontané que les géographes oublient bien souvent dans les années 1970 de justifier en quoi il fait "système", voir \autocite{Orain2001}}, Berry établit la définition de la ville comme sous système d'étude dès 1964, et pose ainsi la nécessité de penser les ville comme systèmes en interdépendance figurant l'étude de la ville comme objet évoluant dans un système résolument ouvert. 

On retrouve également avec la loi de Zipf-Auerbach une filiation indirecte avec la physique de la thermodynamique du début du XX ème siècle. C'est en effet le physicaliste Auerbac \footnote{ Si on en croit \autocite[87]{Pouvreau2013} Auerbac est convaincu que le progrès en biologie ne viendra que de l'explication entièrement physique des phénomènes biologiques} qui s’intéresse en premier à l'application sur des villes de l'effet d'inégalité soulevé par Pareto dans les population.\autocite[94]{Rosser} Il donne naissance à la loi Rank-Taille  qui montre que le produit de la population par le rang de la ville dans la hierarchie est une constante. Une analyse repris et développé par Zipf dans une étude lexicologique, et qui lui permet en conséquence d'assoir un peu plus son universalité, ce qui explique entre autre la confusion dans l'apellation. L'observation dans des espaces et des temporalités très différentes \autocite{Pumain1997} de cette distribution rang-taille comparable à une loi puissance ou une loi log-normale ouvre, à condition qu'on s'attache non pas à la finalité de l'objet lui même, un champs d'étude très vaste. On retrouve là une similarité avec l'étude systémique des organismes vivants, et on peut se demander si n'y a pas dans cette régularité sous forme de "finalité observable" une universalité qui pousse à formuler un rapprochement avec les moyens d'explicitation pour comprendre la directivité des processus développé par Bertalanffy. L'équifinalité comme téléologie dynamique est permises du fait de l'interaction dynamique des éléments, et non pas de la structure initiale en place, sinon comment expliquer sa complexification progressive dès lors qu'on admet l'irréversibilité des processus associé au temps ? On comprend mieux dès lors l'attrait des géographes pour tenter d'expliquer, tout comme les biologistes le font, les mécanismes à l'origine de cette "régularité".

\autocite[114]{Pouvreau2013}
Tout processus d’équilibre peut être formulé téléologiquement [...] Toutes les lois systémiques ont
la particularité que ce qui apparaît pour l’ensemble du système comme un processus causal
d’équilibre peut être formulé téléologiquement pour les parties 1 .
Ce qui correspond à un processus causal d’équilibre apparaît pour la partie comme un événement
téléologique, en ce que l’action de cette dernière semble dirigée vers le « but » consistant à prendre
sa place « convenable » dans le tout 2


On comprend mieux alors pourquoi le modélisateur est amené à imaginer lui même quel est le but du système lorsque celui ci est définit comme "[...] est un ensemble d'éléments en interaction dynamique, organisé en fonction d'un but". Car dans l'exemple d'un système auto-organisé comme peut l'être un système de ville, le but est inexpliqué et le restera, il n'est pas intéressant, mais par contre il est peut être décrit par le biais de mesures, qui le "donne à voir" dans son originalité. Ainsi l'étude "finaliste" de la distribution rang-taille en elle même n'a que peu d'intéret, et ne fait finalement que souligner l'originalité que représente la loi normale dans la nature. (ref denise)

L'interrogation sur la capacité du vivant "à remonter" l'entropie qui saisit la physique du début du XXème siècle amène Auerbac a proposer en 1910 le concept d'"ectropie"; préfigurant ainsi les débat à venir sur cette thématique dans les années qui suivent (néguentropie de Schrödinger en 1945, second principe de la théorie organismique de Bertalanffy en 1929 \autocite[475]{Pouvreau2013}, etc.) \autocite[80]{Pouvreau2013} Si Auerbac semble saisir le premier les irrégularités sur la capacité du vivant à déjouer semble t il la deuxième loi de la thermodynamique. qui précède celui de Zipf sur les loi allométriques, . En 1956 Narrow et Bertalanffy discute de la notion d'allométrie et reviennent sur son application dans les sciences sociales. qui fait en effet partie de ces loi va servir de base au travail de Berry pour son fameux "cities as systems within systems of cities"

Il n'est pas étonnant de voir les géographes se tourner de nouveau rapidement vers les physiciens ayant contribué à l'évolution de la discipline thermodynamique pour y chercher des isomorphismes entre systèmes physiques et systèmes sociaux.

réellement formaliser un transfert des concepts de la GST avec des intuitions qui agite la discipline depuis déjà de nombreuses années. En Suède, le porteur de cette ouverture inter-disciplinaire est Hägerstrand, avec la reintroduction d'un niveau individuel dans la démarche explicative, et de la dialectique entre dimension spatiale et temporelle, de nouveau associé. (voir papier a posteriori Hägerstrand 1969). Ces deux voies seront plus explorés dans la section suivante qui mettra en regard les progrès de l'informatique avec les percées réalisés par ces deux courants sur la voie explicative par la simulation.

Tout les éléments sont réunis pour que la vision systémique viennent renouvellé l'étude d'autres objets géographiques, comme la "région monographique".

 qui ne fait que donner le vocabulaire adapté à une discipline qui depuis quelques années déjà intuite les limites d'un déterminisme physique qui décrit la formation de paysage comme un processus exclusivement linéaire, dans la construction des paysages.

% %EQUIFINALITE en géomorphologie : Beven2012

\paragraph{Une transposition somme toute relativement floue, le cas de la France}

En France, Olivier Orain \autocite[26]{Orain2001}, l’adoption dès les années 1960 de cette théorie systémiste par la \textit{locational analysis} de l'école anglo-saxonne joue un grand rôle dans la diffusion de ces concepts auprès de l'école française de géographie. Ainsi 

Quand on connaît l'accueil qu'à reçu la traduction de ce livre en France, il est alors possible d'imaginer l'écho des paroles d'autres auteurs phare comme Berry, Haggett, Gould, Pred, etc. lorsqu'il développe ces concepts auprès des oreilles alors grande ouvertes des jeunes géographes français des années 1960 1970. \autocite[26]{Orain2001}

\paragraph{Le concept de "Boite noire" appliqué à la géographie}

Le géographe épistémologue Olivier Orain \autocite{Orain2001} donne à voir la nature d'une telle percolation dans la construction de la géographie française des années 1970 ou la notion de "système" est devenu le porte-étendard de cette pensée systémique hérité de la GST. Oui mais que sait-on de la nature de cet héritage ? A ce sujet, l'exposé d'Olivier Orain est précieux, et nous propose de lister dans les embranchements intellectuels d'une discipline en re-construction les convergences et divergences autour de l'acceptation des concepts de la GST; à commencer par le mot "système" qui sort alors de l'ornière du sens commun et se pare de nouvelles significations.

Olivier Orain pour qui les géographes français se sont très rapidement emparés de la notion de système émet alors comme hypothèse explicative principale "que les Nouveaux Géographes des années 1970-1980 ont trouvé dans l’idée de système un appareil conceptuel permettant à la fois de penser l’intégration de l’hétérogène et d’apporter une légitimité scientifique à l’étude de la région" \autocite[23]{Orain2001}

\autocite{Batty1976} Paul Claval pour qui la nouvelle géographie devient beaucoup plus soucieuse d'explication logique que de reconstitution historique, décrit très bien ce glissement méthodologique qui touche la discipline. Ainsi pour lui la nouvelle géographie " [...] essaie de proposer une interprétation théorique des phénomènes spatiaux : elle veut dégager des principes à partie desquels il est possible de comprendre leur articulation, de saisir leur fonctionnement et de reconstituer leur logique interne. Elle procède selon le modèle hypothético - déductif commun à toutes les disciplines scientifiques, et renonce au privilège jusque là attribué à l'induction" \autocite[p22]{Claval1977}

La notion de schema ne se suffit pas à elle seule, discursif celui n'est pas auto-suffisant, et ne constitue en rien un support scientifique rigoureux. Mais on ne peux pas lui enlever sa force heuristique, et il suffit de lire les pérégrinations mentales et graphiques de Christaller \footnote{http://cybergeo.revues.org/3153} pour en être convaincu 

Les cours de biologie mobilisent durant tout le secondaire le schéma comme mode d'explication privilégié, il n'est donc pas étonnant de retrouver là une correspondance. Orain à raison de dire que 

La démarche scientifique hypothético-déductive est appelé dans le cadre d'un nouveau rapport, cette fois ci plus nomothétique qu'idiographique construction théorique de la discipline géographique.


\paragraph{Les schéma systémique}

++ bla ++

Dans les années 1970 en France, la systémique en géographie se présente avant tout comme un langage graphique, schéma, diagramme sagital ou diagramme à la Forester. Durand Dastès nous rapelle les sages paroles de Peter Hagget à ce sujet , pour qui le modèle est avant tout "une représentation schématique de la réalité élaborée en vue d'une démonstration", et c'est dans cette entreprise de simplification qu'il est capable de mettre aussi bien en perspective le général dans le particulier. 

Le schéma apparaît très rapidement comme un outil de modélisation (par sa fonction forcément simplifiante) important dans la construction et la transmission d'une géographie spatiale théorique. Le livre de Peter Hagget fait figure de référence en ce sens avec un livre/catalogue de schéma argumenté qui traduit très bien cet effort de modélisation sur les structure spatialisés. 

La dimension intégrative du schéma comme support heuristique de pensée est tout à fait compatible avec une approche inter-disciplinaire, au contraire, elle permet de travailler le même objet à différentes échelles, et permet d'une certaine manière de se focaliser sur les interactions qui nourrissent le dialogue entre les niveaux.

Une importance également souligné dans le cadre de la biologie 

Le diagramme Sagital
Ce mode de représentation permet de figurer des dimensions de la complexité, ne serait ce que parcqu'il permet d'échapper à la description linéaire classique, et de représenter plus aisément les boucles d'interactions à l'oeuvre dans les système spatialisés. 

Dans le cadre du projet Transmondyn, le diagramme sagital et la description sous forme de schéma a été mobilisé avec succès comme médiateur indispensable entre les auteurs tenants de disciplines à tradition principalement herméneutique comme les historiens, archéologues, et des disciplines plus à l'aise avec une représentation schématique comme les géographes, les informaticiens.

Le diagramme Forrester est encore plus particulier, car celui ci non content de proposer un formalisme graphique, propose un langage informatique sous jacent qui permet l'opérationalisation. Le niveau de description des éléments nécessaire pour une réelle opérationalisation est évidemment plus contraint que dans le cadre d'une description par un diagramme sagital.

Pour Daniel Durand \autocite{Orain2001}, il y a deux types d'utilisation privilégiés des modèles systémiques, la voie heuristique, et la voie algorithmique. La première n'étant pas incompatible avec la seconde. 

En soulevant la principale limite à de telle production discursive Olivier Orain \autocite[47]{Orain2001}  fait référence de façon implicite à l'importance de cette "dynamique dans la construction de connaissance" qui présupose la création d'un schéma systémique rigoureux; Il ne suffit donc pas d'apposer la notion de "système" sur un schéma pour que celui ci le devienne par magie. La difficulté de l'évaluation d'un tel schéma tient au fait que non seulement le diagramme sagital ne "donne à voir" que le "système" résultat de la construction, et non la construction elle même; mais aussi que celui ci ne propose en l'état qu'une vision immédiate des états du systèmes, à charge du lecteur d'analyser avec rigueur la qualité des rétro-action à l'oeuvre ... un exercice délicat qui laisse à l'observateur l'intuition des mécanismes ayant mené à l'alternance de cet état si il n'y a pas derrière d'opérationalisation, ou d'observation quantifié de celle ci via la comparaison empirique ( un comportement contre-intuitif étant tout à fait possible )

Cette "mise en parenthèse du temps" permit par les schéma systémique comme l'indique Durand Dastès permet de mobiliser de façon complexe les temporalités dans un même schéma. 

L'heuristique du schéma systémique Durand Dastes identifie toutefois deux temporalités à l'oeuvre dans un schéma systémique : intra-systémique qui représente le temps de fonctionnement, cyclique, du système , et inter-systémique qui représente un degré de temporalité supérieur englobant. 

\paragraph{L'insufisance de l'approche statistique}

Pose les questions suivantes : 
Niveau d'abstraction ?
Mise en dynamique processus ? 

Tournant biologie moléculaire => biologie systémique ? Retour aux racines de Bertalanffy selon Pouvreau

Mobilisé un cadre de réflexion qui permet d'intégrer la temporalité d'un objet de recherche tel qu'un modèle, qui devient une famille de modèle + échapatoire à la problématique de la "cause" un reproche régulier fait à la simulation comme moteur de recherche.

La notion de mécanisme prend différentes formes dans la littérature en biologie, voir Nicholson2012

Nicholson2012 " It may refer to a philosophical thesis about the nature of life and biology (‘mechanicism’), to the internal workings of a machine-like structure (‘machine mechanism’), or to the causal explanation of a particular phenomenon (‘causal mechanism’)."

Biologie Moléculaire => Biologie systémique => Retour à Bertalanffy d'une certaine manière ...

\paragraph{La systémique algorithmique}

La modélisation sous forme de schéma fait d'une certaine manière écho aux outils développés par les biologistes pour 
 
Durand-Dastès principal propagateurs de la méthode systémique dans la géographie française. \autocite[11]{Orain2001}


On citera par exemple celle d'André Dauphiné en géographie. Celui-ci propose une relecture plus géographique des concepts de la GST, et s'appuie sur une relecture très algorithmique des concepts. ... XXX ettant ainsi en évidence autant l'analogie quasi-exacte que les manquements ou confusion des termes pouvant amener à la mise en place d'analogie erronés. Ainsi l'utilisation du mot "système" dans un référentiel rigide qui serait valable toute disciplines confondues ne serait pas viable tant l'écart peut être grand en terme d'interpretation. Il est clair par exemple que l'analogie entre ville et organisme vivant ne peut être assumé, car la téléologie associé semble ne pas etre de même nature, et si il y'a bien auto-organisation, celle ci ne porte pas sur la construction d'un ville finie morphologiquement. Les composantes micro n'agissant pas dans notre cas dans la formation ou l'émergence d'un "système intégrés ville" comme sous système clos aux frontières délimitées qui serait fonction d'un code inscrit au plus profond de nous. \autocite[]{Pumain1989} 

Il identifie 4 forme de complexité \autocite[45]{Dauphine2003} liés i) à la multiplicité et la nature des interactions entre grand nombre de de composantes(structurelles) , ii) à l'imbrication des échelles (spatiales), iii) à l'imbrication des niveaux (organisationnels), iv) à des comportements chaotique non linéaire. 

Pour Denise Pumain \autocite[]{Pumain1989} \autocite{Pumain2003} enfin les différents dépassement qui peuvent être retenu de l'application raisonné des concepts de la GST à la géographie : i) elle permet le dépassement de l'opposition entre idiographique et nomothétique, ii) la possibilité d'intégrer de la variabilité dans les trajectoires/bifurcations marquant l'évolution des entités géographiques, iii) la possibilité d'imaginer de nouvelle causalité à l’œuvre dans les temporalités qui forge l'explication des entités géographiques.

Démarche hypothético-déductive / nomothétique dans le cadre de l'équifinalité pose évidemment la question de "la validation d'une hypothèse" dans le cadre d'un système complexe. La propriété d'un système complexe étant justement son irréductibilité à ses composantes,

Si pour Olivier Orrain le systémisme en tant que posture intellectuelle explicite se limite aujourd'hui à quelques grand porteur de théorie  (Denise Pumain, Roger Brunet), il est également évident que le systémisme continu à exister de façon diffuse dans les objets géographiques, toujours porteur de cette complexité intrenseque. Il existe également dans sa dimension opérationalisante au travers de l'usage de la simulation.

=> Introduction de la dynamique 

% NOTE :  Comment introduire le retour à un cadre heuristique introduit déjà auparavant par Bertalanffy ? 

\paragraph{Interaction des composantes, le cas des villes}

\paragraph{Le passage à une causalité systémique}

\paragraph{Equifinalité}

Introduction de l'équifinalité se fait au niveau du modèle, dans sa dynamique d'execution, mais aussi, et de façon implicite dans sa construction, une dimension jusque là mise à l'écart (historisation des mécanismes, diversité d'approche rapport à une meme observation finale, on touche aux pratiques scientifiques et a la diversité/multiplicité des vision sur un même objet d'étude => systèmes complexes). Equifinalité existe au niveau de l'évolution de la structure du modèle. Pour la comparer il faut être capable de la mesurer.

% Différents types de complexité qui font écho concept

% Axe imbrication système

% Axe individu centré

% Axe équifinalité

% % transition vers Accompagnent d'une  nouvelle démarche ?

% manquant : spatiale 
% recoupe : échelle


\subsubsection{La nécessité d'une opérationalisation des concepts}
% changement de causalité ? écho a la gst/systémique ? => Passage de Dauphiné sur le sujet

"La conduite de la recherche moderne implique [...] l'utilisation d'un arsenal de démarche beaucoup plus sophistiquées et diverses que celles connues jusque-là. La nouvelle géographie ne serait pas possible sans une profonde révolution méthodologique" \autocite[p27]{Claval1977}. Il n'y aurait pas de méthodes si il n'y avait pas les outils qui les supportent, cf \autocite{Gould1970} \autocite{Gould2004} : "Mais il faut bien se rendre compte que la formulation de questions et les avancées méthodologiques sont les deux faces d'une même pièce".

Limite modèle hypothetico-deductif => Abduction  => Simulation 
http://fr.wikipedia.org/wiki/Abduction_%28%C3%A9pist%C3%A9mologie%29
=> Invention d'une nouvelle réalité 

L'opérationalisation des concepts ...

\paragraph{Encapsulation complexité}

\paragraph{Capacité symboliques}

\paragraph{Introduction de la temporalité}

Pumain2002 p137 :
Non seulement les formalisations mathématiques associées au développement des théories systémiques modifient la représentation des objets géographiques, mais elles permettent de renouveler la question de la causalité en géographie. À la représentation de la rencontre fortuite de séries indépendantes en un lieu, qui rend compte d’une combinaison locale spécifique d’éléments, se substitue celle d’une cohérence évolutive, soumise certes à la contingence selon les bifurcations qui jalonnent la formation d’un objet géographique. L’image parfois pesante de l’« inertie » attribuée aux structures héritées de l’espace géographique est remplacée par la notion d’adaptabilité, in duisant des ajustements permanents de ces structures au changement, selon une conception évolutive de la résilience

Le passage à une causalité systémique va de paire avec le développement des outils à même de traiter cette dernière. A DEVELOPPER

%Berthelot2001 [p100] voit deux composante qui doivent mener à  une tradition de base empiriste, et une tradition plus théorique.
Le processus d'absorption de l'approche systémique est communément abordé de façon conjointe avec une révolution quantitative, et fait référence à deux dimensions développés par Berthelot pour décrire ? :   %2 dimension de berthelot ?

La nécessité d'une relecture par le biais de l'opérationalisation des concepts, afin de voir lesquels sont satisfait et lesquels ne le sont pas ? 

\section{Le bilan d'une opérationalisation des concepts}

\subsection{Les années 1950-70, bilan d'une première vague d'opérationalisation }
\label{ssubsec:operationaliser_concept}

Dans le cadre d'un transfert inachevé entre les concepts de la GST et la géographie dans les années 1970, quels courants pionniers ont fait les premiers usages de la simulation pour expliquer ou prédire ? Ont-il réussi ? Quel est le poids du modèle de Forrester dans la remise en cause du modèle causal dominant jusque là ? 

%\subsubsection{Introduction boucle vertueuse "Concepts/Outils" en général }

% Introduction Individualisme / Holisme => Nécessité d'une double approche 
% Explication / Prediction => Nécessité d'une approche structurelle 

\subsubsection{L'informatique, une discipline en pleine effervescence}
\label{ssubsec:simulation_langage}

% discrete event vs continuous
% Timespan de l'exp : Horizon fini vs état d'équilibre (steady state simulation) 

Dans la typologie proposé par Nance \autocite{Nance1993}, le terme simulation se réfère historiquement à trois domaines d'application en informatique : (i) simulation événement discret, (ii) la simulation continue, (iii) les techniques dites de "Monte-Carlo".

La période 1955 - 1965 est une période de recherche caractérisé par la reconnaissance de la simulation pour résoudre un certain nombres de problèmes difficilement tractables mathématiquement.\autocite{Nance1993, Ackoff1961} Les programmes de développement visant à la mise en place de modèle de représentation, de description nécessaire et facilitant pour la construction de simulations se multiplie. Deux classes de langage informatiques vont voir le jour durant cette période, et vont continuer à se développer et à s'influencer chacune de leur coté jusqu'à encore aujourd'hui. D'une part, les langage de plus haut niveau qui apparaissent ont pour vocation de se positionner comme une alternative plus expressive que l'assembleur. Dans cette optique le premier compilateur FORTRAN apparait en 1957,  Algol en 1958, Cobol en 1959, et Lisp 1958. Ces langages et leur successeurs sont d'usages assez génériques, et permettent de décrire correctement les trois domaines d'applications. Toutefois à l'époque de leur apparition ils sont d'accès relativement difficile pour une personne non initié, ce qui nous amène au développements sur la même période d'une deuxième catégorie de langage, plus spécialisé dans la construction spécifique de modèle de simulation.. \autocite[239]{Naylor1966}

A la même époque donc des langage spécialisés dans l'expression des simulations apparaissent, et pour la plupart  s'appuie et évolue en parallèle des développement des langages classiques sur lequel ils s'appuient.. Ces SPL (Simulation Programming Langages) comme Simula en 1962, ou bien Dynamo en 1958 ont ceci d'intéressant qu'ils ont très largement accompagnés les formidables avancées conceptuelles de cet époque et cela au travers des différentes disciplines. Ainsi la première période 1955-1960 est marqué par la mise au point de GSP (General Simulation Program) \autocite{Tocher1960} par Owen et Tocher. Celui ci est considéré comme le tout premier langage mis au point pour faciliter la description de simulation sur ordinateur. Un effort que Tocher va accompagner d'une publication phare en 1963 dans le livre \textit{Art of Simulation} \autocite{Tocher1963} . Vient ensuite la première génération de langage en 1960-1965 avec entre autre GPSS (General Purpose System Simulator), Simscript (développé sous l'impulsion de la RAND corporation), et la première version du langage SIMULA, qui donnera naissance à la fin des années 1960 à Simula-67, un langage qui aura un impact dépassant largement la classe des SPL, et inspirera les créateurs des futurs langage objets comme Alan Kay auteur du premier langage objet SmallTalk. 

\subsubsection{Deux courants pionniers initiateur de la simulation en géographie}
\label{ssubsec:courant_pionniers}

\autocite{Haggett1969} nous indique alors qu'à cette période l'ordinateur intervient comme le support indispensable dans au moins quatres usages qui font écho aux méthodes modernes décrites par \autocite{Claval1977} : (i) statistiques multivariés, (ii) surface de tendances, (iii) graphismes, (iv) simulation. 

Afin d'illustrer l'importance de l'outil "simulation" dans la construction théorique, deux axes qui se juxtaposent dans l'espace et dans le temps peuvent être développés. D'une part il y a l'apparition et la rencontre début des années 1960 de deux pôles académiques innovants avec d'un coté les universitaires américains de la cote ouest dirigés par Garrison et de l'autre les universitaires Suèdois avec Hagerstrand, et d'autre part il y a cette montée en puissance simultanée (1959-68) des instituts de planning encadrés par le think-thank de la RAND corportation, qui va piloter la construction d'un ensemble de modèles de simulations urbains sous l'impulsion des récents succès de modèles d'économie urbaine remis au goût du jour par Isard, fondateur des récentes "sciences urbaines" avec ses disciples Alonso et xx de l'université de Pennsylvanie. \autocite{Batty1976}

La synthèse qui résulte de l'étude de ces deux axes pxermettra d'une part d'affirmer l'importance du niveau micro individuel comme facteur explicatif dans l'explication de l'organisation des systèmes sociaux, et d'autre part va permettre de lever un certain nombre de limitations à la fois institutionnelle, méthodologique et techniques qui vont avoir pour effet d'entamer rapidement cet optimisme un peu naïf envers les possibilités de la simulation,  caractéristique de cette période des années 1960-70. Ce mouvement en géographie est loin d'être isolé, les autres sciences sociales touchés par ce problème de la scientificité qui attrait à la construction et a la validation de modèle + limitation lié à l'abstraction mathématique (dont je n'ai pas parlé mais dont il faudra parler) %Il sera intéressant de faire un comparatif de cette situation avec les difficultés que rencontre aujourd'hui la discipline, et de voir comment il est possible de s'appuyer sur l'accès à de nouveaux outils pour tenter de dépasser ces limitations, de la même façon que des nouveaux outils ont permis de dépasser les problématiques de l'époque.

\paragraph{La simulation à visée explicative, les universitaires pionniers de l'école Américaine et Suédoise}

% Prévision => Rapport avec la démarche scientifique 
% Sur la causalité la conclusion p91 de "causalité et géographie" est très bien ! 
% p92-p102  + p109 => histoire de la geographie Claval 910.3 cla
% p168 -179 => p172 Sur la ville Claval2001 + systémisme dans le meme bouquin ! 
% Echo avec la systémique ? 

% Economie spatiale : Isard Ponsard 

Il est difficile de faire un récit linéaire de ce que l'on apelle aujourd'hui "la révolution quantitative", notamment du fait du caractère multi-site de cette contestation qui s'enracine dans la défiance progressive d'une jeune génération envers une ancienne école faite d'universitaires pour la plupart attachés à leurs façons de faire. \autocite{Gould2004}  propose de s'attarder en particulier sur deux foyer important de cette révolte. Le premier se situe dans quelques universités de la cote ouest des Etats-Unis \autocite{Gould2004} parmis lesquels Washington, Iowa et NorthWestern; le deuxième en Suède via l'université de Lund.

C'est à Washington, sous la direction de Ed Ullman et William Garrisson, considéré comme l'un des premiers à voir l'intérêt général de l'usage de l'ordinateur pour la géographie, qu'à la fin des années 1950 se forme un groupe d'étudiants qui va marquer le renouveau de la géographie.  L'innovation des thèmatiques abordés dans les publications ( catégories à lister), mais aussi des formations proposés va de pair avec l'entrainement mutuel qui anime cette équipe de jeunes doctorants que l'on appelera plus tard le groupe des "Space Cadets". Brian Berry, William Bunge, Richard Morril, Duane Marble, bientôt rejoint par Hägerstrand sont les premiers à mettre en pratique les techniques computationelles les plus récentes. 

% Déjà au courant des approches systémiques a cette période (voir olivier orrain)
Chorley qui dès le début des années 1960 les concepts de système  \autocite{Johnston2004} , et dans la même période enseigne les concepts de programmation linéaire et autres techniques \autocite{Haggett1969}

% Apport important de l'école suedoise, déjà largement au courant (voir olivier orrain + )
Inspiré principalement par l'économie spatiale, % a completer

Une deuxième percée va être impulser à la discipline suite au déplacement de Torsten Hägerstrand aux états Unis.  Deux années après sa première publication en anglais en 1957, Hägerstrand est aussitôt repéré et invité par Garrisson en 1959 à présenter ses travaux novateurs, dont la première publication date de 1952, une période ou la géographie est encore uniquement idiographique en Angleterre et aux Etats-Unis. La rencontre a lieu à Washington dans un séminaire intitulé "simulation modelling of the diffusion of innovation". Encore réalisé à la main lors de sa venu à Washington, les premières simulations Monte-Carlo impressionne les disciples de Garrisson, notamment Morril, qui à la suite de cette expérience va partir plusieurs mois en Suède\autocite{Morril2005}. Pour la petite histoire, c'est via un voyage aux Etats Unis que le physicien Karl Erik Frödberg, un ami d'enfance de Torsten Hägerstrand, recupère un texte polycopié présenté par John Von Neuman et Stanislas Ulam sur les méthodes de Monte-Carlo. Alors appliquées au calcul de l'épaisseur des chapes de béton pour les centrales nucléaires, la technique est utilisé pour pallier à une résolution impossible de ce problème via les approches mathématiques classiques. Hägerstrand ayant déjà travaillé à l'étude de l'émigration en 1949, trouvera dans cette technique un écho innovant à sa problématique d'alors, la propagation des idées et des innovations dans l'agriculture suédoise. \autocite[26-28]{Gould2004}]

Quatre innovations fondamentales sont relevés ainsi par Peter Gould : "First was the introduction (at least at the geography) of the idea of spatial and time-processes, that geographic development over time could be understood and modeled; second was the particular processes of spatial diffusion; third was the technique of Monte-Carlo simulation; and fourth was the idea that individual behavior, not just that of large groups, could be modelled"

% + sur école suedoise ?
En 1960, le symposium de Lund, alors en pointe dans ce domaine, attire de nombreux géographe urbains de par le monde, et aura toujours selon Morril un large impact sur la diffusion des méthodes et des théories dans la géographie urbaines. A son retour Morril contribuera lui aussi largement à diffuser cette méthode au travers d'un papier applicatif sur le ghetto \autocite{Morril1965}. 

Même si il faudra attendre 1967 \autocite{Hagerstrand1967} pour que les simulations soient effective sur ordinateur, pour Gould, cette première demonstration de simulation probabiliste au niveau micro consacre la "simulation" comme un outil désormais indispensable à la discipline, et cela au travers de deux constats : "First, that there are elements of chance in human spatial behavior which cannot be modelled in traditional deterministic form ; and, secondly, that processes acting over space by definition act overtime. Handling space and time simultaneously is a difficult business, and simulation, for all its current detractors, often appears to offer the only feasible way out" \autocite{Gould1970}

% En france : 910.3 CLA : Histoire de la géographie Francaise Paul Claval
% Choley en France 1970 / Pinchemel qui introduit en France traduction Berry + Hagget 
% Après 1968, et l'amorce du tournant dans la discipline, la plupart des géographes qui se lance dans l'aventure de la nouvelle géographie puisent leur inspiration dans des ouvrages anglo saxon de la fin des années 1960, le plus lu étant surement Models In Geography de Chorley et Hagget 1967. \autocite[342]{Claval1998}
% \autocite[354]{Claval1998} => Structure système causalité

D'un point de vue plus technique \autocite{Haggett1969} cite comme véritable point de départ dans la discipline la  démocratisation de l'accès à la ressource informatique issue de fait de la diffusion d'une deuxième générations d'ordinateurs (IBM7094 series) beaucoup plus accessible et performants. Dans une période ou les compétences informatiques nécessaires à la programmation se font encore très rare, des listes de programmes disponible dans les universités sont peu à peu publiés, et des pionniers comme Marble ou Tobler mettent à disposition dès la fin des années 1960 différente routines informatique en libre accès.

% Difficulté à trouver des réponses dans la communauté, elle aussi perplexe vis à vis des problématiques de validation qui se pose à l'interdisciplinaire (annexe sur autres disciplines)

Mais l'utilisation des programmes par des non initiés amène à des erreurs d'interprétation. 

En 1972 Duane Marble \autocite{Marble1972} publie un article qui nous renseigne bien sur l'avancement des simulations de modèles spatiaux au milieu des années 1970. Il apparait de façon assez claire que peu d'avancée ont pu être réalisé du fait des limitations techniques, le traitement de données spatialisés étant largement au dessus des moyens techniques de l'époque. Ainsi la quasi totalité de la recherche fondamentale réalisé durant cette période 1959 - 1968 sera lié aux efforts des organismes plannificateurs dans la mise en place de programme nationaux, les seules à l'époques à se pourvoir de moyen humains et techniques suffisament importants pour développer des simulations de ce type. Sur ce point, nous pouvons aussi citer Hagget1969, relayé par Batty1976 qui montrent à quel points les attentes vis à vis de ces programmes sont fortes\autocite{Haggett1969}, et à quel point les retombés seront importantes, cela malgré les constats d'échec d'un certains nombre d'auteur. 

%Ces réflexions théoriques se propagent très vite, Chorley et Hagget (55-65) en angleterre  

Transition => Attente très fortes vis à vis des modèles dirigés par les instituts de planning, qui dispose des moyens nécessaires.. 

\paragraph{Le ThinkThank de la RAND}

(B) Partie ThinkThank + construction Orcutt

% Explicativité se greffe à l'individu, travaux d'orcutt à la meme période vont dans le meme sens.

=> Echec de la de prédiction, retour à des modèles plus simple et prise en compte de la complexité au travers d'un retour vers la physique et les mathématique.

\subsection{Une mise en échec partielle de la simulation au début des années 1970}

=> Conclusion de la partie ? => L'opérationalisation amène la dimension temps, et donc casse les codes établis jusque là. 
Elle le fait de différente facon, en remettant en question les modèles théoriques statiques existant, mais aussi en introduisant la nécessité renforcé de l'équifinalité, qui permet de recouper ce que dit Denise Pumain dans geographie et complexité.

"Le modèle est un construit scientifique qui possède sa propre temporalité" => Implique qu'il peut etre repris par les autres, Implique que plusieurs publications peuvent avoir lieu sur le meme modele, Implique que diverses explorations peuvent etre réalisés sur celui ci, etc.

\paragraph{Echec de la simulation comme outil de prédiction}
La RAND corporation, un ThinkThank américain fondé juste après guerre, va avoir une large influence sur un large panel de discipline, dont la géographie fait partie.

L'application des modèles statiques à la réalité urbaine dans une perspective de prédiction, un axe qu'a précédemment choisi de développer l'école américaine en remettant au goût du jour des modèles économiques, va très rapidement s'avérer décevante.

Les programmes de plannification, exclusivement orienté vers la prédiction, 

- Limite prédiction vs explication => incapacité à produire des prédictions (un débat encore actif aujourd'hui)

Transition => Retour à des modèles plus simples, construction de modèle dynamique, 

- Introduction par Orcutt du niveau micro avec dynamique en opposition au large scale model, 
	=> Limitation technique, attente de nouveaux ordinateurs lui aussi ...

\paragraph{La simulation dans le reste des sciences sociales, histoire complexe d'une désillusion}

Du fait de l'ancrage inter-disciplinaires des acteurs guidant les conférences de Macy, et de ce formidable développement des langages marquant une nouvelle accessibilité pour developper des applications sur les ordinateurs, le rayonnement des concepts introduits par la cybernétique se diffuse rapidement dans de nombreuses disciplines des sciences sociales. 

% avec l'apparition de l'intelligence artificielle et des sciences cognitives,
Ainsi en psychologie les travaux pionnier de Newell, Shaw, et Simon à la fin des 1950 autour des tentatives pour la construction d'une machine universelle de résolution de problème (Logic Theorist program en 1957 et General Problem Solver en 1959) apparait pour \autocite{Gullahorn1965}  comme la toute première démonstration de la faisabilité de la simulation dans la discipline. Ce programme s'avère également être la première pierre posé de la l'intelligence artificielle, en formation à l'intersection de la naissance encore récentes des science cognitive et de l'informatique. Cette machine est concu pour mimer les capacités de résolutions de l'esprit humain, et permet enfin d'exprimer et de questionner les théories comportementales dans un langage informatique alors plus précis et moins ambigu que le langage naturel. Le programme est ainsi capable de résoudre des problèmes aussi différents que de jouer aux échec, de résoudre des problèmes mathématiques,  ou de retrouver des motifs dans des données. A ces travaux s'ajoute ceux répétés de Hovland en 1960 puis de Robert Abelson en 1968  qui encourage l'utilisation de la simulation pour la construction théorique de ce que Ostrom appellera a posteriori les « complex human processess » \autocite{Ostrom1988}. La simulation est utilisé en psychologie pour formuler et vérifier des théories sur les comportements sociaux\autocite{Gullahorn1965a}, comme par exemple le modèle Homonculus développé par Gullahorn pour tenter de mieux comprendre les stratégies de résolution de conflits avec la programmation de comportements au niveau individuel \autocite{Gullahorn1965} 

En anthropologie l'intégration de la simulation dans l'arsenal méthodologique prend part d'un mouvement qui démarre en démographie visant "\textit{[...] to better account for cultural and social constraints on human demographic processes in general}". Bennett Dyke situe le premier usage de l'ordinateur "\textit{in anthropology appears to have been the work of Kunstadter et al in 1963}", auquel on peut également associer les travaux de Gilbert and Hammel 1966 \autocite{Costopoulos2007} \autocite{Dyke1981}. Antony Wallace également, levy strauss 1955: les mathémztique de l'homme...
 
En sociologie, James Coleman "\textit{considered it as a half-way point between verbal speculative theory and formal theory, aiding in the development of such theory through concretizing the functioning of social processes.}" \autocite[36]{Schultz1972}

% A compléter 

Autre discipline, et même constat affiché par Ostrom en psychologie sociale. Ostrom \autocite{Ostrom1988}, lorsqu'il revendique en 1988 l'importance de la simulation comme un « third way system » pour faire de la science,  fait également un constat assez accablant sur la place aujourd'hui tenu par cette pratique de modélisation dans le courant mainstream de la psychologie. Ainsi dit il,  force est de constater en 1988 le peu de retours rapportés par la discipline face aux manifestes des pionniers tels que Gullahorn \autocite{Gullahorn1965} qu'Abelson \autocite{Abelson1968} : « Despite the clear relevance of these models to  social psychology, the simulation approach had not caught the imagination of main stream social psychologists. Very few simulations had appeared in the core journals of the field prior to the publication of Abelson's chapter. […] At the time of Abelson's writing, simulation models had not made much contact with the dominant empirical pursuits of the field. » \autocite[382]{Ostrom1988}

En archéologie également, l'arrivée des outils statistiques par transfert d'autres disciplines, comme la géographie, va introduire une rupture dans les pratiques de la disciplines. Des auteurs pionnier dans la description de modèles de simulation vont publier plusieurs articles fondateurs \autocite{Clarke1968} \autocite{Doran1970} 

Malheureusement, si on en croit les temoignages de \autocite{Aldenderfer1998} en archéologie, la pratique de la simulation ne concerne au final qu'un tout petit cercle de personnes : "During the 1980s, relatively few archaeologists continued to advocate whole-society modeling, the most prominent of them being James Doran, who has called for the "formal modeling" of societies, especially the inter- action of their political and sociological subsystems. While much of Doran's work has been widely cited within the rela- tively small community of mathematically inclined archaeologists, his work has had relatively little influence beyond this small circle " \autocite{Aldenderfer1998}

En anthropologie, les travaux de \autocite{Kunstadter1963} marque l'introduction de la simulation à la discipline. \autocite{Dyke1981} dresse un bilan en 1981 de la discipline à ce sujet et fournit des hypothèses supplémentaires pour expliquer un tel désavoeu de l'outil : 

« The first use of computer simulation in anthropology appears to have been the work of Kunstadter et al in 1963. Since that time there has been a considerable increase in the number of publications whose results have depended on simulation studies. Despite this increase, it is probably fair to say that simulation has received at best only a cautious acceptance in anthropology. »

« The initial enthusiasm for a newly acquired ability to model complex systems, characteristic of the early days of anthropological simulation, more often than not led to an exaggeration of the capabilities and usefulness of computer models. In retrospect it seems clear that much of this excess could have been avoided had more attention been paid to testing (particularly to validation). The literature of the past 4 or 5 years, however, gives ample evidence that the situation has changed. Those who continue to use simulation seem to have paid much more attention to the problem of validation and tend to be more modest in their claims of utility. »

En sociologie, sociologie politique ..\autocite{Padioleau1969}


Dans la très claire retrospective historique faite par Gary Lock en 2003\cite{Lock2003} sur l'histoire de l'archéologie computationelle, l'auteur s'attache à bien séparer au moins deux sinon trois époques aux méthodologies et aux outils différents. En adoptant une posture un peu simplificatrice on peut donc affirmer que si l'archéologie pre-années 1960 se base principalement sur la récolte de données empiriques et la mise en exergue de pattern dans ces même données pour générer la plupart de ces explications, une rupture dans la discipline se dessine dès les années 1960-70 avec l'avénement d'un courant d'archéologie proclamant une « new archeology » (ou \emph{processualism}). Rejettant un empirisme beaucoup trop subjectif, celle ci vante le retour à la seule « Méthode Scientifique » pour générer des explications. Selon \autocite{Kohler2011} \autocite{Flanery1968} s'appuie le premier sur les enseignements de la cybernétique pour penser les cultures comme des systèmes, l'exercices étant avant tout pour lui d'établir des règles qui auront vocation par la suite à être simulé. Clark publie en 1968 un premier livre ambitieux «Analytical Archeology» qui vante à son tour l'approche systémique et tente de raccrocher l'archéologie aux développement récents en géographie, en analyse statistique \autocite{Kohler2011}. Le potentiel que pourrait avoir la simulation stochastique en archéologie est également évoqué \autocite{Clarke1968}. Une analyse a posteriori confirme l'apport de la systémique dans la construction des modèles de simulation, comme en témoigne \autocite{Aldenderfer1998} "One of the theoretical hallmarks of the "New Archaeology" was the systems approach (Aldenderfer, 1991), and a result of its adoption was the use of computer simulation to model whole societies or significant portions of them." Une vision qui se concrétise en 1970 avec James Doran \autocite{Doran1970} , qui non content de confirmer ce changement de paradigme dans  «Systems theory, computer simulation and archaeology» paru dans \emph{World Archeology} en inscrivant de façon provocatrice sa rupture avec les méthodes de ces derniers années \footnote{Until comparatively recently, say fifteen years ago, interpretive archeaology was a discipline which made no use of mathematical or scientific techniques and concepts. Few suggested that it should.}, s'inscrit dans comme un des premiers à voir dans la simulation comme un "laboratory-like experiments". En utilisant la simulation non pas comme un solveur d'équation mais en utilisant la puissance des opérateurs symboliques à sa disposition pour la mise en temporalités de systèmes d'interaction dans des sociétés passés, Doran décrit une vision de la simulation qui n'est pas sans rapeller le multi-agent d'aujourd'hui. Une conception de la simulation reprise et concrétisé par DH Thomas en 1972. Pour anecdote, ce mouvement pionnier de l'archéologie et de l'anthropologie vers la simulation fait étonnant écho avec le retour de Doran sur le devant de la scène, en collaboration avec Nigel Gilbert, déclaré chef de file du mouvement inter-disciplinaire qui va reaffirmer l'importance de la simulation dans des disciplines ou elle avait été mis de coté jusque là après la crise de confiance des années 1970. Pour plus de référence sur ce sujet, le lecteur pourra se référer à une discussion sur ce sujet sur la mailling liste de SIMSOC \footnote {www.jiscmail.ac.uk/cgi-bin/webadmin?A2=ind04\&L=simsoc\&F=\&S=\&P=39083}

++ INFORMATION SUR DESILUSION ++

Quel écho avec la situations actuelle ?
Modèle agent / historique / enjeux en cours
Quel problème ?

** Approche Top -> Down, puis Bottom - Up **
=> D'un constat général à des pratiques localisés
=> De pratiques localisés à un constat général 

%Tjrs du point de vue de l'extension des concepts
\subsubsection{Le rebond de la géographie des années 1970}
\label{ssubsec:rebond_geo}
=> Forrester, Dynamo, succès formalisme par les géographes avec diffusion des modèles via club rome ?

(C) Forrester polarise le débat, changement dans la construction des modèles en géographie

Apport pour les géographes est important, il permet de se désolidariser encore un peu plus de la trajectoire historique classique ...  cf article de Denise sur la complexité !

Changement dans la structuration / construction des modèles, prise en compte de la complexité des systèmes urbains : auto organisation, bifurcation, etc. 

Pb absence spatialisation activité ?

+ premier modèle automate cellulaire .... ? .... version spatialisé .... Tobler 70

%Tjrs du point de vue de l'extension des concepts
\subsubsection{Le renouveau de la simulation multi-agent des années 1990 }

\paragraph{Historique modèles agents années 1990}

\paragraph{Les avancées }

\paragraph{Limites en}

\paragraph{Quel dépassement ?}

\subsection{Les limites soulevées résultat d'une opérationalisation partielle}

\subsubsection{La "Validation" des modèles, un terme mal choisi ..}

C'est justement parcqu'on ne peux pas saisir la complexité 

On ne cherche pas à valider le réel, on se heurte à la réminiscience de concept pourtant ancien, 
et bien connu : équifinalité

=> Equifinalité fait qu'il n'y a pas de validation possible

\paragraph{Le concept "historique" du terme "validation"}

Souligné l'absence de : 
=> REINTEGRATION DES PRINCIPES DE RECHERCHE
=> EXPLORATION EST UNE DIMENSION DE LA CONSTRUCTION
=> DIMENSION TEMPORELLE DE L'ÉVALUATION => Nécessité d'une épistémologie pour penser cette dimension : machamer
=> DIMENSION STRUCTURELLE DE L'ÉVALUATION


--- DRAFT  ----
%REINTRODUIRE TOUT LE TEXTE SUR historique dans chapitre1_v2.odt
%Il y a interdisciplinarité au niveau questionnement simulation / validation , largement reconnu comme un probleme par tout le monde !!!!
%VA avec potentiel simulation comme thoery building ! 
\section {Evaluer, un processus au coeur de la construction des modèles } 

Tout au long de ce chapitre, il serait fait régulièrement référence aux différents travaux et publications de Frédéric Amblard, car ils constituent à bien des égards des points d'entrées importants dans notre réflexion sur la validation dans la simulation de modèle en géographie.

\subsection{Un transfert de l'ingénierie à la simulation en science humaine et sociale}

Les références à Sargent, Balci,sont de par leur nature générale couramment reprise dans différents ouvrages ou publications.

Sur la question du transfert des épistémologues se sont déjà penchés sur la question ... voir Numo

Partir de Naylor pour évoquer la nécessité d'une prise de recul dès lors que l'on cherche à caractériser le retour sur investissement

Toutefois plusieurs autres auteurs viennent nous rapeller que le jeu joué par Naylor est dangereux, car sans tomber dans un relativisme qui se voudrait naïf et dangereux dans une perspective inter-disciplinaire, meme si la validation est avant tout problème qui doit être abordé à l'aube de la discipline qui la mobilise. Il ne s'agit donc pas de réduire la validation à un question sociologique ou psychologique lié aux individus ou groupes d'individu ici, mais simplement d'exposer la validation à une forme d'expertise plus large, à meme de comprendre les enjeux et les moyens mobilisés pour tenter de mener à bien une évaluation qui se veut un tant soit peu objective.

Pour prendre un exemple plus concret, si on considère la mobilisation dans nos modèle de simulation du  modèle gravitaire, qui est une forme stylisé et éprouvé par l'expérience de processus réel en jeu dans la réalité, alors il est admis que la connaissance produite par ce modèle puisse être jugé de façon objective compte tenu des théories ainsi injectés.

\subsection{Un transfert de l'ingénierie à la simulation en science humaine et sociale}

\section{Objectiver les processus à l'oeuvre dans le cadre d'une évaluation par les pairs}

\subsection{au regard de la littérature en géographie}

\subsection{au regard de la notion d'équifinalité}

=> Osullivan nous propose (dit de facon implicite) de se tourner  vers l'évaluation collective, la seule pour lui à même de dégager une connaissance. KISS en lui même {Axelrod1997} porte cet idéal de simplicité (qui ne renie pas en tant que telle la complexité et la richesse descriptive des mécanismes) pour favoriser l'échange, la diffusion des modèles. Hors de ce coté, le constat est maigre. Le mouvement M2M {Amblard, Rouchier},{Rouchier}, {M.Batty/P.Allen}

Sur ces points Hedstrom rejoint OSulivan et les autres, la Discussion de la scientificité des modèles se fait avant tout sur la qualité des mécanismes et des causalités à l'oeuvre (Entities / Activities de Machamer).  

Présentation du plan lié à ce paradoxe, il faut mobiliser des moyens techniques et humains important pour que puisse mettre  en place des outils standardisé à même d'externalisé le débat non plus sur la question de l'évaluation, mais sur l'objet de cette évaluation. Autrement dit pour pouvoir discuter de la connaissance apporté par le modèle, il faut être capable de produire des coupes fiables des multiples dynamiques à l'oeuvre dans nos modèles .: corrélation entre paramètres, 

Ces outils existent, non seulement car ils font l'objet de recherche en eux même, mais aussi parcqu'ils sont appliqués de façon systématique dans certaines disciplines. Pourtant en modélisation en géographie et dans d'autre disciplines des sciences humaines, il semble peu mobilisé, et cela de façon historique.

L'évaluation de par sa nature contextuelle doit être faite au préalable par les pairs, tout en restant ouverte à la critique interdisciplinaire. Toute la difficulté de tels modèles résidant donc dans le placement du curseur entre ces deux pôle que l'on pourrait qualifier d'attracteurs, de par les moyens qu'il faut mobiliser pour s'en rapprocher dans un cas ou dans un autre.

\subsection{Le processus de création intègre la validation}

Présentation concept de validation interne / externe.

De très/trop nombreux guides méthodologiques pointent trop souvent la construction de modèle comme la recherche d'un état fini, donné, qui n'est pas compatible, ni avec la notion d'équifinalié, ni avec l'idée qu'un modèle est souvent révisable continuellement.

\subsubsection{État des lieux, des moyens inadaptés à la création des modèles}

Complexification, Généralisation, et l'infernal aller retour entre les deux, voir concept théorie précédant à l'observation en épistémologie ?

=> Pour être à même de mesurer le retour de connaissance dans un modèle qui suit une courbe de complexification, deux concepts au moins doivent pouvoir être mobilisé : 

\begin{itemize}
\item La ou les briques ou incrément unitaire qui encapsule l'ajout de connaissance qui concrétise un différentiel avec le modèle précédent, que l'on peut résumer ainsi "En quoi ce nouveau modèle est un modèle différent vis à vis de la question posé par le modèle"
\item Le processus de mobilisation de cette brique unitaire de reflexion dans l'exploration de la dynamique, que l'on peut evoquer de la facon suivante "quel est l'impact d'une insertion, de la modification ou d'un retrait d'une brique, et qu'est ce que je peux en dire vis à vis de la question posé par le modèle"
\end{itemize}

\subsubsection{Le choix d'une unité de raisonnement aproprié}

Une prise de recul reflexive nécessaire pour comprendre ou se situe la création de connaissance vis à vis de la question posé.
Ce qui nous amène à nous intéresser autant aux processus de création de cette connaissance qu'à cette connaissance elle même.

Grille de lecture existante : 
> Guide de bonne pratiques
> Fer à cheval d'Arnaud, etc.

=> ok mais ca suffit pas, on a vu dans le cadre des questionnements originaux en géographie, la notion de mécanisme

\subsubsection{La spécificité du questionnement géographique}

Discussion / Remise en cause de la notion de mecanismes avec la géographie ? 
A regarder la littérature actuelle sur la relation micro/macro dans les modèles agents, il n'y a peu ou pas de prise en compte du spatial dans la construction des relations micro macro observé dans les modèles.

Le modélisateur doit toutefois être attentif sur au moins deux points : 
 a) La biologie n'est pas la géographie, et les analogies ont le sais peuvent rapidement être dangereuse (ex ville comme organisme vivant, etc.)
 b) Ne pas s'enfermer dans les modèles edictés par de tels raisonnements, cf exemple de René pumain sur le passage de l'influx nerveux, et le fait que la mitocondrie se met tout à coup à pariticper au passage des ions, alors qu'en fait on pensais ce truc complétement passif dans l'échange d'ions ... 

Equivalent pauvre en géographie du "schemata" evoqué par Machamer serait les chorème de Roger Brunet, toutefois ceux ci sont limités dans le sens ou ils sont statiques. En ce sens, en terme d'outil pour penser, la simulation accède au rang de "schemata", support dynamique à la réflexion.

\section{Une double contrainte sur la réalisation des modèles, explicativité et parcimonie}

Si on considère l'ajout de mécanismes au modèle, celui ci est contrainte au moins de deux façon : 
> La recherche d'une parcimonie minimale explicative du point de vue de la question
> La recherche de la meilleur estimation rapport à une série de données, ou à un ou plusieurs fait stylisés

Quels sont les cas possibles :
=> Il y a correspondance parfaite entre réel et simulé, le modèle est surdéterminé
=> il y a non correspondance entre réel et simulé, mais l'ajout de mécanisme apporte un gain
=> il y a non correspondance entre réel et simulé, mais l'ajout de mécanisme n'apporte pas de gain, du moins pas dans la dynamique actuelle

Schéma évoquant la double contrainte, permet d'évoquer les problématiques associés et de dérouler par la suite sur l'absence technique de support pour une telle exploration : algorithme génétique, description de la contrainte, objectif à réaliser, ajout mécanisme, etc.

\subsection{Le processus de diffusion}
\subsubsection{Des moyens inadaptés à la diffusion des modèles}
Absence de plateforme de publication
Pas de protocole standard ou de pratiques etablies dans la discipline

\section {Isoler les pratiques et des outils existant} 

Ce travail a été largement initié suivant des problématiques déjà très bien décrite par Thomas Louail dans sa thèse...
Ajouter tout le travail réalisé pour la présentation ecqtg2013

\subsection {La modélisation agent en géographie} 

Cette section est volontairement détaché du reste, car si la modélisation agent amène certe de nouvelles problématique à la notion d'évaluation, il n'empêche que cette problématique peut être traité séparément de par son ancienneté, et sa relative indépendance vis à vis des techniques employés.

Les technique de construction que nous présentons sont évidemment à rattacher aux pratiques actuelle de modélisation utilisant les agents, il n'en reste pas moins.

% Faire remonter l'analyse des pratiques existantes ?

\subsection{Proposition de formalisation des limites} 

En s'inspirant du travail de formalisation déjà exposé dans \autocite[120]{Louail2010}], voici une relecture des limites pour la construction de modèle agents telles qu'elles ont étés évoqués dans le chapitre 1.

%\textbf{< Numero de la limite : Catégorie, Support, Description courte > }

%\begin{itemize}
%
%Sur la construction / validation de modèle :
%
%\item \textbf{ < 1a : Methodologie, Publications, Critique sur la scientificité >  }
%
%La remise en cause de la scientificité des méthodes et des modèles soit un phénomène récurrent dans l'histoire de la simulation de modèles, cela bien avant les critiques plus récentes sur les ABM, a tel point que ces dernières années ce sont développés des publications oeuvrant pour la dotation d'un argumentaire commun \autocite{Waldherr2013} \autocite{Miller2007} justifiant cet approche pour la construction théorique. Cette limite s'inscrit bien évidemment dans un cercle vicieux qui impacte tout autant qu'elle découle de l'ensemble des autres limites exposés par la suite. Ainsi par exemple le peu de publication relevés exposant des méthodes pour construire et évaluer des modèles est tout autant un effet de ces critiques qu'une cause justifiant dès lors son abandon.
%
%\item \textbf{ < 1b : Methodologie, Publications, Absence de cas d'utilisation concret >  }
%
%C'est un point soulevés régulièrement dans la littérature des ABM {ref}
%
%\item \textbf{ < 1c : Methodologie, Formations, Absence de formation adaptés >  }
%
%De ce coté là les choses ont beaucoup évolués ces 5 dernières années, et en France des formations se mettent en place peu à peu, via des réseau comme MAPS, dans les écoles doctorales, des écoles d'étés, ou dans des Master spécialisé (Créteil, Erasmus Mondus.). C'est une pratique encore restreinte de la modélisation, plus sous la forme d'initiation, limité à un nombre restreint et motivé de personnes, et qui à l'heure actuelle ne permet pas de cumuler l'ensemble des compétences nécessaire à la construction et l'exploration de modèle dans un seul individu, surtout lorsque celui ci provient d'une formation en sciences humaines et sociales.
%
%\item \textbf{ < 1d : Methodologie, <Publications, Formations, Application>, Absences de methodes unifiés >  }
%
%Cette limite est particulière car impossible à résoudre. En effet l'appui inter-disciplinaire qui supporte la modélisation agent en science sociale associé a la nature contextuelle de la validation est une double contrainte qui limite la construction d'une méthodologie unifié. Tiraillé d'une part par l'ouverture du champ propre à l'exercice des système complexes, et d'autre part par la nécessité d'une validation par les pairs, le modélisateur est facilement pris en défaut lorsqu'il s'agit d'imaginer une méthodologie couvrant les besoins de l'ensemble des disciplines. Le protocole ODD par exemple, bien que proposant une grille commune pour la description de modèles, ne parvient pas à fourni un cadre suffisament précis pour contenter les géographes \autocite{Schmitt2013}. L'absence de méthode unifiés impacte ou du moins limite la création ou la diffusion de publications, de formations, et d'applications.
%
%\item \textbf{ < 1e: Methodologie, < Publications, Formations, Application>,  Absences des ressources informatiques nécessaires > }
%\item \textbf{ < 1f: Methodologie, < Publications, Application>,  Coûts temporels > }
%\item \textbf{ < 1g: Methodologie, < Publications, Application>,  Coûts financier > }
%\item \textbf{ < 1h: Methodologie, < Publications,  Application>,  Modèle est un objet de recherche non borné > }
%
%\end{itemize}
%
%\textbf{ < 2a : Outils, Formations, Absence de formations adaptés >  }
%\textbf{ < 2b : Outils, Application, Absence des ressources informatiques nécessaires >  }
%\textbf{ < 2c : Outils, Application, Absence des ressources humaines nécessaires >  }
%\textbf{ < 2d : Outils,  Applications, Couts temporels de développement }
%\textbf{ < 2e : Outils,  Applications, Couts financiers de développement }

%http://tex.stackexchange.com/questions/142410/using-biblatexmemoirrefsection-why-are-the-references-included-in-the-toc
\printbibliography[heading=subbibliography]

\stopcontents[chapters]

%\begin{table}
%\centering
%\subfloat[Source: élaboré à partir de Courel et al. (2005)]{
%    \includegraphics[width=160mm]{ClassificationMotifs.pdf}
%    }
%    \captionstyle{\centerlastline}
%    \caption{Typologie classique des motifs de déplacements}
%    \label{tab:classificationmotifs}
%\end{table}




Non adapté pour de multiples parts : 
1) L'equifinalité rend totalement obsolete la logique HD DN (Apppuie sur Passeron)
2) La logique HD n'est pas une logique de découverte. 
3) La théorie n'est pas la plus forte (cf le cours de la physicienne sur la démarche expérimentale en phys) + ian hacking
4) Il faut une méthode ouverte sur explication inter disciplinaire ! 

ian Hacking a un point de vue intéressant sur le sujet, il préfère parler de style de pensée (cf cours College France), jeter un oeil aussi a these Michel Bechet sur logique abductive page 341 (ou il dit logique similaire entre popper et peirce, largement auto inspiré l'un de l'autre), cela fait écho a ce que dit Hacking sur la construction a posteori des philosophes. (on pratique l'induction, deduction et autre depuis longtemps, on a juste mis des mots dessus, donc inutile d'en faire tout un plat)

La simulation a offert aux scientifique cette capacité d'induction dans le monde expérimental en fait ...
Historisation processus de modélisation en simulation doit devenir aussi un objet de recherche, au sens ou il expose tout à la fois nos erreurs et nos réussites au collectif.
