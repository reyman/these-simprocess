
\graphicspath{{Figure1/}}

\chapter{Construire et Évaluer des modèles en géographie}

\startcontents[chapters]
\Mprintcontents

\section{Une nouvelle forme d’explication en science sociale et en géographie avec les modèles de simulation}

%Il me parait intéressant d'avoir un regard croisé sur l'archéologie et la géographie, car elle partage en un sens une révolte commune qui va même jusque emprunter les même termes pour être désigné. Ainsi la "New Archeology" et la "New Geography" semble t elle vouloir s'éloigner des techniques uniquement descriptives, en réintégrant de l'explicatif dans leur méthodologie, et cela en se tournant vers des outils communs, l'analyse statistique spatiale, et la simulation de modèle pour tenter d'innover. Cette analyse croisé est aussi l'ocasion de mettre en lumière une désillusion commune et la levée de limitations conceptuelle autant que techniques qui va toucher l'ensemble des sciences humaines vis à vis de cet outil. La géographie elle aussi touché par cette crise semble toutefois s'en tirer mieux que les autres sur ce dernier point.

\subsection {L'introduction à la systémique dans les sciences sociales et en géographie}

% ARG groupe interdisciplinaire + va de paire avec introduction à la systémique / Cybernétique
De la même façon que les épistémologues des sciences comme ici Olivier Orain \autocite{Orain2001}, l'auteur ne détaillera pas ici une approche inter-disciplinaire de la notion \footnote{Au sens donné par Piaget, voir note de bas de page \autocite {Orain2001}} de "système", difficile à envisager dans un cadre global car sa diversité d'acceptation est fonction, d'une part de la rapide évolution de cette notion depuis les années 1940, et d'autre part la règle définissant l'acceptation de cette \textit{notion} dépend non seulement de la variabilité inter-disciplinaire, mais aussi intra-disciplinaire. Le terme "approche systémique" est alors proposé par \autocite{Orain2001} pour incarner cette diversité d'intégration par les disciplines des sciences sociales de la "théorie systémique" ou "systémique". 

Cette complexité d'approche autour de cette notion est pour Jean Louis Lemoigne grandement lié à la reconstruction épistémologique à postériori de ce qu'il appele préfère appeler de son coté le "paradigme systémique". Une acceptation qui parait justifié tant l'étude de la ramification qui en découle est complexe, et sans rentrer dans les détails de querelles entre les différentes chapelles, il est acceptable de voir cette construction comme un processus de raffinement cumulatif. (a dire mieux)

\subsubsection{L'apparition de nouveaux outils de réflexion pour penser la complexité}

Une des branches communément admises comme fondatrice du mouvement tient dans l'organisation des conférences de Macy entre 1942 à 1953. Celle ci sont considérés comme un des premier regroupement interdisciplinaire et marque une période de changement profond dans l'histoire des sciences en général, et particulièrement en science sociale. Celles ci vont réunir pendant plusieurs années autour d'une même table des acteurs majeurs des sciences physiques et sociales pour discuter autour de régularités communément observés, avec pour idée la construction d'un savoir commun que l'on pourra alors qualifier de trans-disciplinaire. 

Les conférences naissent suite à la rencontre entre un mathématicien réputé au MIT N. Wiener, un neurobiologiste A. Rosenbluch, et un ingénieur électronicien J.Bigelow qui vont opérer un rapprochement entre l'homme et la machine entre 1942 et 1946 (pour rappel le premier ordinateur ENIAC est opérationel en 1946) par le biais de groupes inter-disciplinaires chargés d'explorer ce "\textit{no man's land}" à l'interface des deux disciplines. 

Plusieurs "outils" dérivent de ces premiers séminaires organisés dès 1942 à la Josiah Macy, Jr. Foundation : la notion de "boite noire" ou système téléologique fonctionel, et la notion de \textit{feedback} ou causalité circulaire, avec pour objectif principal l'étude de l'homéostasie introduite auparavant par les travaux pionniers du physiologiste Walter Cannon en 1926.

Si la notion d'homéostasie pour des organismes vivants apparaît pour la première fois cité par Claude Bernard 1865, celle ci est reprise et étendue par Walter Cannon en 1932 dans le livre \textit{The Wisdom of the Body} \autocite{Cannon1932} comme « l’ensemble des processus organiques qui agissent pour maintenir l’état stationnaire de l’organisme, dans sa morphologie et dans ses conditions intérieures, en dépit de perturbations extérieures ». Ainsi dans le cadre de son application biologique cette rétro-action permet de décrire un certain nombre de mécanisme à l'oeuvre dans une cellule en interaction avec son environnement qui tente de maintenir de façon stable dans son milieu la concentration d'éléments comme les ions, la glycémie, etc.

L'attention des discutants dans ces premier séminaire porte donc avant tout sur l'ubiquité du concept et la pertinence de son transfert hors des systèmes biologiques. Wiener fait alors un rapprochement décisif entre les problématiques de calcul de trajectoire en balistique et des maladies nerveuses ayant pour symptôme l'ataxie. De ces discussions émergent alors un même schéma explicatif qui semble à la fois convenir à ces problématiques, la "causalité circulaire". \autocite[774]{Pouvreau2013, Rosnay1975}

De la connaissance de ces entrants et de ces sortants, on peut en déduire qu'il existe une retro-action négative ou positive, ou \textit{feedback} permettant de décrire progressivement le système de commande de la boite noire.

%Dans "Behavior, Purpose and Teleology", le terme téléologie est à ce titre utilisé comme un synonyme de "l'objectif controllé par la rétroaction".\footnote{wikipedia}

L'approche néo-béhavioriste retenue par les discutants "consiste à étudier un objet comme une "boite noire", par l'examen de l'extrant de l'objet [i.e tout changement produit dans son environnement] et des relations entre cet extrant et l'intrant [i.e tout événement externe qui modifie l'objet]" \autocite{Pouvreau2013} En adoptant cette approche, le "comportement" d'une entité est perçu "comme tout changement extérieur détectable de cette entité par rapport à son environnement" , et par téléologique il faut entendre un comportement "finalisé" c'est à dire déterminé par un mécanisme de "rétroaction" négative.

Avec la mise en place d'une classification de ces comportements, et en prenant distance du concept de "causalité finale" qui lui était rattaché, les auteurs espèrent ainsi redorer le concept de téléologie, renouant avec la reconnaissance de l'importance du but qui avait disparu avec la mise au ban de ce concept. Reprenant les explications de \autocite[776]{Pouvreau2013}, celui-ci cite \autocite[23-24]{Rosenblueth1943} "[...] Puisque nous considérons la finalisation comme un concept nécessaire afin de comprendre certains modes de comportement, nous suggérons qu'une étude téléologique est utile si elle évite les problèmes de causalité et se limite à s'attacher à l'étude du but [...] Le comportement téléologique devient synonyme de comportement contrôlé par une rétroaction négative et gagne donc en précision par une connotation suffisamment restreinte."

De ces discussions deux articles fondateurs à la fois des sciences cognitives \autocite[23]{Dupuy2000} et de la cybernétique vont être publiés : \textit{Behavior, Purpose and Teleology}ou Rosenblueth, Wiener, et Bigelow " propose de déconstruire la distinction entre action volontaire et acte réflexe, en assimilant la volonté à un mécanisme de rétro-action (\textit{feedback})"; et \textit{A logical calculus of the ideas immanent in nervous activity} où McMulloch et Pitts donne "une base purement neuroanatomique et neurophysiologique au jugement synthétique \textit{à priori}, et de donner ainsi une neurologie de l'esprit"

Parmis les autres auteurs de ces premiers séminaires organisés entre 1942 et 1944 figurent deux représentant des sciences sociales, Gregory Bateson et Margaret Mead. Enthousiastes, il vont rapidement trouver dans l'étude des concepts développés dans les premiers séminaire en 1942 un écho à leur propre travaux sur la dynamique sociale. Cette mise au jour de problématiques commune entre le biologique et le mécanique permet d'envisager la construction d'un référentiel lui aussi commun; une prise de conscience qui va amener les auteurs du cercle de réflexion initial à envisager rapidement l'élargissement de celui ci  à l'ensemble des acteurs des sciences sociales.

La suite des conférences de Macy (1946-1952) sera organisés par Arturo Rosenbluch et son ami Warren McCulloch, un autre neurobiologiste. Cette ouverture vers les sciences sociales est timide dans un premier temps, et ce n'est qu'à la 2ème conférence en octobre 1946 sur une suggestion de Lazarsfeld que les conférences concrétise cette ouverture dans le cadre d'un sous séminaire intitulé \textit{Téléogical Mechanisms in Society}. La 4ème conférence acte cette ouverture et introduit pour la troisième fois de suite une modification de l'intitulé, avec cette fois ci l'adjonction d'une dimension sociale à un objet d'étude, qui apparaît encore à cette date difficile à définir : "la causalité circulaire et des mécanismes de \textit{feedback} dans les systèmes biologiques et sociaux". Le terme \textit{Cybernetics} est pour la première fois introduit dans les séminaires par Wiener en 1946. Il faut toutefois attendre 1949 et la septième conférence pour que sous l'influence d'un nouveau participant nommé H. Von Foerster, ce terme chapeaute de façon définitive les prochains intitulés de séminaires. Au final, ces dix séminaires vont participer de l'émergence de la "science cybernétique" en "permettant l'échange effectif de savoir et d'experiences, tant entre les disciplines qu'entre les sciences et la société", réalisant par là un des objectifs annoncé par Wiener et Rosenbluch dans leur classification, faisant de la cybernétique une "[...] science générale des systèmes à comportement finalisé ayant principalement pour objet ceux dont le comportements est "téléologique" " \autocite[]{Pouvreau2013}

Wiener mais aussi d'autre acteurs de la cybernétique ont vus assez tôt tout l'intérêt que pourrait apporter l'utilisation et le transfert d'outils comme "la boite noire", ou le principe de régulation une fois appliqué à l'étude des interactions dans les systèmes sociaux. Mais les difficultés d'applications et les critiques ont rapidement mis à mal cet objectif trans-disciplinaire, pour plusieurs raisons qui tiennent d'une part à l'existence de restriction mathématiques remettant en cause la scientificité des résultats obtenus : (a) les statistiques sur le long terme étant difficile à obtenir (b) la difficulté à minimiser la distance entre observateur et phénomène observés, et donc le biais qui s'applique aux données dans un tel cadre; et d'autres part au réductionnisme et la vision mécanicistes touchant certains acteurs des conférences de Macy  : "[...] la vie était pensée comme un dispositif de réduction d'entropie ; les organismes et leur associations, en particulier les hommes et leurs sociétés, l'étaient comme des servomécanismes ; et le cerveau comme un ordinateur" \autocite[784]{Pouvreau2013}

\autocite[782]{Pouvreau2013} explique très bien les limitations qui font  de l'extension de la cybernétique au sciences humaines une simple "[...] ressemblance superficielle au niveau du formalisme. Ne serait-ce que parce que dans un système tel que conçu par la "première" cybernétique, par définition fermé à l'information, la téléologie ne peut qu'être confinée au cercle d'un but déterminé; et que pour cette raison, ce modèle ne permet pas de comprendre de quelle manière un système peut être amené à redéfinir ses buts à partir de ses interactions avec son environnement, la pertinence d'une téléologie relative à des buts \textit{intentionels} restant donc intacte en sciences humaines" 

\subsubsection{L'introduction des systèmes ouverts, un changement nécessaire pour penser les systèmes sociaux}

Cette incapacité de la première cybernétique à coller aux problématique des systèmes sociaux va trouver un écho plus positif dans un courant qui se développe en parallèle du mouvement cybernétique. C'est le biologiste Ludwig von Bertalanffy qui fonde dès 1937 ce mouvement que l'on peut considérer comme la deuxième branche venant enrichir le paradigme systémique. Celui ci va se positionner de façon critique sur un certain nombre de concepts tout en absorbant par la suite les autres innovations qui proviendront de ce courant. De cette prise de position va peu à peu découler la construction d'une théorie établissant une méthodologie logico-mathématique à vocation unifiante, accessible à n'importe quel champs inter-disciplinaire pour décrire les lois de structure similaires (isomorphe). \autocite{LeMoigne2006a}. 

Cette \textit{General System Theory} (GST) est nommé et évoqué pour la première fois en public en 1937-38 par Bertalanffy, s'ensuit en 1950 la rédaction d'une première ébauche, et c'est finalement en 1968 qu'un ouvrage nommé \textit{General System theory: Foundations, Development, Applications} proposera une synthèse de toutes les avancés. Un projet de "théorie" qui n'en sera jamais une si on en croit Pouvreau \autocite{Pouvreau2013} qui a analysé en détail la très vaste littérature qui gravite autour de ce sujet, un travail d'autant plus remarquable tant la notion parait floue. En effet l'état inachevé du projet de Bertanlanfy  laisse plus à penser qu'il s'agit d'un projet, et c'est à ce titre que Pouvreau préfère le terme de "systémologie générale" qu'il définit comme "le \textit{projet} d'une \textit{science de l'interprétation systémique} du "réel" " \autocite[9]{Pouvreau2013}. L'hypothèse défendu par Pouvreau étant que cette "[...]science de l'interprétation systémique du "réel" se caractérise en fin de compte comme une herméneutique, au sens où elle a pour vocation d'élaborer à la fois les moyens de construire des interprétations systémiques d'aspects particulier du "réel" sous la forme de modèles théoriques spécifiques et les moyens d'interpréter à leur tour de tels modèles comme des déclinaisons de modèles systémiques théoriques d'un degré de généralité supérieur."\autocite[9-10]{Pouvreau2013}

Bien qu'inscrit dans un cadre de réflexion similaire à la Cybernétique, Bertalanffy formule dès 1951 la première critique détaillés de ce que les épistémologues appellent \textit{a posteriori} la "première cybernétique". Non convaincu par le cadre explicatif fourni par la cybernétique et les lois énoncés par la physique conventionnelle pour décrire les interactions à l'oeuvre dans les systèmes biologiques (on trouvera plus de détail dans \autocite[787-789]{Pouvreau2013}), celui-ci défend dès 1937 une vision systémique innovante basé sur l'étude de "système ouvert". Pour mieux comprendre en quoi cette ouverture est importante pour l'application du paradigme systémique aux sciences sociales, il faut revenir plusieurs décenies en arrière pour définir quelques notions de thermodynamiques qui ont irrigués les réflexions initiales des cybernéticiens tout autant que les motivations de Bertalanffy pour la dépasser.

 Cette loi esquissé par Carnot  et formulé par Clausius en 1850 montre que l'energie calorifique ne peut se reconvertir, elle se \textit{dégrade} et perd son aptitude à effectuer un \textit{travail}. Il nomme "entropie" cette diminution irréversible de l'aptitude à se transformer et à effectuer un travail, propre à la chaleur.\autocite[35]{Morin1977} C'est Boltzmann, Gibbs et Planck qui vont par la suite faire le lien entre le niveau micro des particules et la notion de chaleur. Parce que la chaleur est caractérisé par l'agitation désordonné des molécules dans un systèmes, l'entropie devient plus qu'une simple réduction du travail, c'est aussi l'ordre et le désordre des molécules qui en est la cause. Cette transformation s'effectue avec création d'entropie, une "quantité de désordre" qui ne peut que croître dans le temps et cela jusqu'à atteindre une valeur maximale équivalente à ce nouvel état d'équilibre. De ce fait et de façon générale celle-ci définit comme évolution irréversible toute transformation réelle dans un système isolé (Système où la frontière est totalement imperméable : l'Univers est par définition un tout englobant) ou fermé (Système ou la frontière est perméable aux flux entrant ou sortant d'énergie mais imperméable aux échanges de matière : La Terre reçoit de l'énergie du soleil) partant d'un état non stable et se dirigeant vers un nouvel état stable.  Ainsi si on considère l'univers comme un méta-système isolé englobant tout les autres, alors ce second principe a pour corollaire que l'entropie de l'univers augmente vers un état de désordre maximal qui se traduit en définitive par une mort thermique.

Une définition qui allait vite devenir problématique, car "Comment la progression irréversible du désordre pouvait elle être compatible avec le développement organisateur de l'univers matériel, puis de la vie, qui conduit à homo sapiens ?", qui se traduit de façon plus générale par l'émergence par la problématisation de cette notion d'ordre et de désordre et le renversement d'un questionnement car "A partir du moment où il est posé que les états d'ordre et d'organisation sont non seulement dégradables, mais improbables, l'évidence ontologique de l'ordre et de l'organisation se trouve renversée. Le problème n'est plus : pourquoi y a-t-il du désordre dans l'univers bien qu'il y règne l'ordre universel? C'est : pourquoi y a-t-il de l'ordre et de l'organisation dans l'univers ? " \autocite[37]{Morin1977}

En s'appuyant sur les travaux de Clausius et Boltzmann, Alfred J. Lotka s'intéresse dès 1922 \autocite{Lotka1922} à la possible analogie entre système physique et biologique, et met à jour dans une nouvelle théorie la notion commune d'irréversibilité et l'importance de l'énergie tel que décrit par la seconde loi thermodynamique comme l'acteur invisible moteur dans la selection naturelle décrite par l'évolution Darwiniene  : "[...] the law of evolution is the law of irreversible transformation; that the \textit{direction} of evolution [...] is the direction of irreversible transformations. And this direction the physicist can define or describe in exact terms. For an isolated system, it is the direction of increasing entropy.  The law of evolution is, in this sense, the second law of thermodynamics" \autocite[26]{Lotka1925}. Il montre également en quoi cette seconde loi ne suffit pas evidemment à expliquer la selection naturelle ?

La dégradation de l'énergie nécessaire pour maintenir une organisation implique l'irréversibilité des transformations.

Bridgman et Shrodinger lèvent rapidement un paradoxe dans l'application de cette loi à des systèmes biologiques (et par extension sociaux) qui renvoie dos à dos désorganisation du méta-système clos Univers et organisation de plus en plus croissante des systèmes biologiques dans le temps, qui serait alors marqueur d'une entropie elle décroissante.

Bertalanffy qui va apporter une solution avec la notion de système ouvert

Boltzman fait dès le lien entre information et entropie, l'entropie devient alors le degré d'intermination du système, 

Bridgman et Schrodinger Hors 

A closed system in statistical thermodynamics is one in which the number of particles as well as the total energy are fixed by boundary conditions. Closed systems systems tend towards equilibrium and to increse in entropy.

For a living system, equilibrium corresponds to death. Living systems are open systems, thermodynamic systems persistently displaced from chemical equilibrium. 

The history of an open system is part of its structure, and Prigogine links open systems to irreversibility. Prigogine calls open systems dissipative. 

Put more simply, this means that matter does not tend to organise itself in a particular location unless there is some external energy source powering it. Evolution can be seen as matter organising itself.

 Erwin Shrodinger qui le premier impulse la reflexion sur l'insuffisance de la seconde loi de la thermodynamique p
Avec Ashby, 


The term "self-organizing" was introduced to contemporary science in 1947 by the psychiatrist and engineer W. Ross Ashby.[9] It was taken up by the cyberneticians Heinz von Foerster, Gordon Pask, Stafford Beer and Norbert Wiener himself in the second edition of his "Cybernetics: or Control and Communication in the Animal and the Machine" (MIT Press 1961).

Self-organization as a word and concept was used by those associated with general systems theory in the 1960s, but did not become commonplace in the scientific literature until its adoption by physicists and researchers in the field of complex systems in the 1970s and 1980s.[10] After Ilya Prigogine's 1977 Nobel Prize, the thermodynamic concept of self-organization received some attention of the public, and scientific researchers started to migrate from the cybernetic view to the thermodynamic view. WIKIPEDIA

Parmis les nombreux points qui font débat, Bertalanffy tente également de prendre ces distances avec le modèle d'explication de processus téléologique hérité de la cybernétique. Celui-ci s'appuie alors sur une nouvelle classification du concept de téléologie pour introduire le concept d'équifinalité comme sous-type de téléologie dynamique, un type de processus de régulation qui selon lui ne peut pas être expliqué par les schèmes cybernétique initiaux. Le principe d'équifinalité est ainsi évoqué pour la première fois comme la possibilité d'atteindre le même état final à partir d'états initiaux différents, par des itinéraires différents. \autocite[38]{Bertalanffy1973} \autocite[786-788]{Pouvreau2013}

Ashby joue un rôle important dans le rapprochement des deux mouvements.

Ce procédé sera transféré au réel par Ashby, un autre cybernéticien qui travaillera dès 1946 à la mise au point d'une machine expérimentale capable de reproduire de façon mécanique cette dynamique de stabilisation face aux variations de son environnements. Nommé "homéostat" celle çi sera construite en 1948, et présenté aux conférences de Macy en 1952.

Gregory Bateson + ashby passage de témoin entre les deux disciplines ! voir \cite{Pourveau}

WIkipedia => L'implication de la cybernétique dans la systémique est historiquement plus liée au « deuxième mouvement cybernétique ». En effet, si selon Norbert Wiener la cybernétique étudie exclusivement les échanges d'information (car c'est « ce qui dirige » les logiques des éléments communicants d'où le mot cybernétique), dans son évolution qui engendrera la systémique, on réintègre les caractéristiques des composantes du système, et on reconsidère les échanges d'énergie et de matière indépendamment des échanges d'information.

Malgré les critiques soulevés de part et d'autres, du faite entre autre d'un objectif peut être un peu sur-évalué voire immodeste, celle ci aura un large écho auprès des sciences humaines, et notamment en géographie; d'abord anglo-saxonne \autocite{Haggett1965, Chorley1962}, puis par diffusion en France \autocite{Raymond}. 



L'avénement de la deuxième cybernétique : 
La régulation apparaît en effet comme un phénomène majeur chez les organismes vivants, puisqu’elle « retarde la dégradation de l’énergie et donc l’augmentation de l’entropie » (p 129), et associée au retard d’entropie et à la computation, elles forment l’essence même de la cybernétique


%Dans l'ouvrage de vulgarisation de Joël de Rosnay construit un fil historique qui se concentre avant tout autour des intersections entre parcours de scientifiques et le MIT, qui pour lui a joué un grand rôle comme formidable catalyseur durant les premiers temps de construction.

Lien Forrester / Système dans science de gestion => Toutefois, les idées développés dans le cadre de ces conférences se borne dans un premier temps à des concepts sans réelle mise en application. Il faudra attendre les années 1960 et l'invention du transistor permettant la démocratisation des ordinateurs pour que la mise au point du langage DYNAMO en XX par Jay Forrester au MIT donne finalement corps à de nombreux concepts développés par la cybernétique.

 Mais la force des idées ainsi développés fait que celle ci n'attendent pas pour percoler déjà en réalité dans des disciplines qui en fait sont déjà tout à fait prompte à les accepter. En effet de nombreuses disciplines participe à cette période d'une révolution interne, et l'approche systémique développé par la cybernétique quand elle ne fait pas qu'apposer un nom commun sur des concepts déjà étudiés, fait alors écho à des révolution méthodologiques en attente d'être activés. \autocite[5]{Batty1976} résume la situation ainsi "The idea of systems being described in terms of structure and behaviour, in terms of input and output, and the notion of purposeful control of such systems in terms of negative and positive feedbacks, appeared to many social scientists an ideal description of their systems of interest and thus the approach has come to be used in more-or-less all of the social sciences". 
Une analyse reprise en l'état par certains acteur de la géographie francaise.

% ARG De la systémique à la simulation, il n'y a qu'un pas ...
"The intellectual revolution in geography since the middle and late ftities rests upon two main supporting pillars - men and machines." \autocite{Gould1970} Cette période de bouillonnement intellectuel s'appuie et accompagne les révolutions conceptuelles, l'usage renforcé de l'ordinateur pour l'enregistrement, le traitement et la visualisation des données est accompagné par le développement d'un nouvel usage, la simulation de modèle.

% Multiplicité des définitions à exposer quelque part par là ... histoire de resserrer le contexte sur la géographie par la suite ..

"Un système est un ensemble d'éléments en interaction dynamique, organisé en fonction d'un but." \autocite{Rosnay1975}

% Pivot causalité + temporalité , d'une approche seulement heuristique à un langage dédié, quel impact la construction des modèles !?
Les concepts de la cybernétique expose dans les termes la construction d'une chaîne de causalité, et de ce fait amène à rendre explicite les facteurs explicatifs à l’œuvre dans la construction des phénomènes à étudier, et d'autre part l'intégration de la flèche du temps dans des modèles jusque là exposé de façon statique. Il parait plus intéressant de situer ce débat dans un changement de perspective appuyé par un changement de paradigme dans la construction du concept de causalité en géographie. 

% Evolution outils va de paire avec l'évolution des questionnement méthodologique, la systémique à vocation à opréationalitsé, et c'est là que la simulation s'impose comme l'opérationalisation logique de ces questionnements, un trajet fait d'échec et de succès


% deux trucs de bati pour la nouvelle géographie : l'introduction de la systémique + révolution quantitative, l'usage de l'ordinateur.

%operationalisation donc passe par la simulation ?


\subsection{L'irruption de la simulation comme une méthodologie de dimension immédiatement inter-disciplinaire}

L'usage de simulateurs apparaît alors comme une méthode idéale pour donner corps à tout un pan de cette révolution méthodologique qui anime la plupart des disciplines des sciences humaines à cette période. 

Si l'usage de la simulation en science sociale va évidemment se construire autour d'échec et de succès qui ne pourront évidemment pas tous être évoqué ici. Toutefois afin de montrer toute la mesure de la percolation de ces techniques dans les sciences humaines il parait nécessaire d'évoquer à ce sujet un cadre plus large que celui seul de la géographie, car l'emploi d'un même outil par de multiples disciplines entraine dans son sillage la construction d'un objet de recherche commun. Rapidement une communauté va se structurer autour d'ouvrages comme celui de Randall Schultz \autocite{Schultz1972} ,Thomas H. Naylor \autocite{Naylor1966}, etc. qui discutent des enjeux et des limites techniques associés à l'utilisation de ce nouvel outil qu'est la simulation dans un cadre largement inter-disciplinaire. 

Un détail qui a son importance quand on sais que la géographie en elle même s'est largement nourri des travaux novateurs issues de disciplines annexes pour construire sa propre révolution.  Ainsi par exemple pour Johnston, cette période de renouveau en géographie a tout à voir avec la mise en place de cette approche inter-disciplinaire, et pour lui il n'y a que deux innovations à retenir provenant réellement du champs historique de la géographie : les travaux de l'école de géographes allemands avec la théorie des places centrales d'une part remis au goût du jour par Isard, et les travaux d'Hagerstrand sur la diffusion des innovations d'autre part. La plupart des autres avancées théoriques provenant selon lui d'échange inter-disciplinaire, de sociologues(Von Thunen, Hoover, Losch, Weber), d'économistes (Zipfs, Stouffer), de nouvelle zones de friction vers lequel s'est tourné la géographie pour repositionner ses questionnements. Une direction appuyé par le témoignage de Paul Claval pour qui le mouvement de la "New Geography" \autocite[6]{Claval1977} "[...] ne tarda pas à s'enrichir dans deux directions : à cotés des modèles théoriques empruntés à l'économie, les chercheurs apprirent à utiliser ceux que proposer la sociologie, l'ethonologie, ou la psychologie, et se mirent à en construire eux même"

Model Building => \autocite{Gullahorn1965a} interet pour la psycho
Simon Herbert 1954 =>  Some Strategic considerations in the development of social science model Paul F Lazarfeld
%Archéologie / Sociologie / etc, quelques points d'entrées fondateurs dans la simulation ... A renforcer avec une argumentation qui relie tout ça.
% =>  Systémique permet d'introduire temps et chaine de causalité dans le déroulement d'un processus.. peut on dire qu'il s'agit d'un retour en force de la méthode hypothético - déductif dans les disciplines des sciences sociales ? Et parmis l'arsenal a disposition il y a la simulation.

\subsection{Le point sur une effervescence technique derrière le terme simulation}

% discrete event vs continuous
% Timespan de l'exp : Horizon fini vs état d'équilibre (steady state simulation) 

Dans la typologie proposé par Nance \autocite{Nance1993}, le terme simulation se réfère historiquement à trois domaines d'application en informatique : (i) simulation événement discret, (ii) la simulation continue, (iii) les techniques dites de "Monte-Carlo".

La période 1955 - 1965 est une période de recherche caractérisé par la reconnaissance de la simulation pour résoudre un certain nombres de problèmes difficilement tractables mathématiquement.\autocite{Nance1993, Ackoff1961} Les programmes de développement visant à la mise en place de modèle de représentation, de description nécessaire et facilitant pour la construction de simulations se multiplie. Deux classes de langage informatiques vont voir le jour durant cette période, et vont continuer à se développer et à s'influencer chacune de leur coté jusqu'à encore aujourd'hui. D'une part, les langage de plus haut niveau qui apparaissent ont pour vocation de se positionner comme une alternative plus expressive que l'assembleur. Dans cette optique le premier compilateur FORTRAN apparait en 1957,  Algol en 1958, Cobol en 1959, et Lisp 1958. Ces langages et leur successeurs sont d'usages assez génériques, et permettent de décrire correctement les trois domaines d'applications. Toutefois à l'époque de leur apparition ils sont d'accès relativement difficile pour une personne non initié, ce qui nous amène au développements sur la même période d'une deuxième catégorie de langage, plus spécialisé dans la construction spécifique de modèle de simulation.. \autocite[239]{Naylor1966}

A la même époque donc des langage spécialisés dans l'expression des simulations apparaissent, et pour la plupart  s'appuie et évolue en parallèle des développement des langages classiques sur lequel ils s'appuient.. Ces SPL (Simulation Programming Langages) comme Simula en 1962, ou bien Dynamo en 1958 ont ceci d'intéressant qu'ils ont très largement accompagnés les formidables avancées conceptuelles de cet époque et cela au travers des différentes disciplines. Ainsi la première période 1955-1960 est marqué par la mise au point de GSP (General Simulation Program) \autocite{Tocher1960} par Owen et Tocher. Celui ci est considéré comme le tout premier langage mis au point pour faciliter la description de simulation sur ordinateur. Un effort que Tocher va accompagner d'une publication phare en 1963 dans le livre \textit{Art of Simulation} \autocite{Tocher1963} . Vient ensuite la première génération de langage en 1960-1965 avec entre autre GPSS (General Purpose System Simulator), Simscript (développé sous l'impulsion de la RAND corporation), et la première version du langage SIMULA, qui donnera naissance à la fin des années 1960 à Simula-67, un langage qui aura un impact dépassant largement la classe des SPL, et inspirera les créateurs des futurs langage objets comme Alan Kay auteur du premier langage objet SmallTalk. 

\subsection{L'introduction de la simulation dans les sciences sociales et en géographie plus particulièrement}

Du fait de l'ancrage inter-disciplinaires des acteurs guidant les conférences de Macy, et de ce formidable développement des langages marquant une nouvelle accessibilité pour developper des applications sur les ordinateurs, le rayonnement des concepts introduits par la cybernétique se diffuse rapidement à travers dans de nombreuses disciplines des sciences sociales. 

% avec l'apparition de l'intelligence artificielle et des sciences cognitives,
Ainsi en psychologie les travaux pionnier de Newell, Shaw, et Simon à la fin des 1950 autour des tentatives pour la construction d'une machine universelle de résolution de problème (Logic Theorist program en 1957 et General Problem Solver en 1959) apparait pour \autocite{Gullahorn1965}  comme la toute première démonstration de la faisabilité de la simulation dans la discipline. Ce programme s'avère également être la première pierre posé de la l'intelligence artificielle, en formation à l'intersection de la naissance encore récentes des science cognitive et de l'informatique. Cette machine est concu pour mimer les capacités de résolutions de l'esprit humain, et permet enfin d'exprimer et de questionner les théories comportementales dans un langage informatique alors plus précis et moins ambigu que le langage naturel. Le programme est ainsi capable de résoudre des problèmes aussi différents que de jouer aux échec, de résoudre des problèmes mathématiques,  ou de retrouver des motifs dans des données. A ces travaux s'ajoute ceux répétés de Hovland en 1960 puis de Robert Abelson en 1968  qui encourage l'utilisation de la simulation pour la construction théorique de ce que Ostrom appellera a posteriori les « complex human processess » \autocite{Ostrom1988}. La simulation est utilisé en psychologie pour formuler et vérifier des théories sur les comportements sociaux\autocite{Gullahorn1965a}, comme par exemple le modèle Homonculus développé par Gullahorn pour tenter de mieux comprendre les stratégies de résolution de conflits avec la programmation de comportements au niveau individuel \autocite{Gullahorn1965} 

En anthropologie l'intégration de la simulation dans l'arsenal méthodologique prend part d'un mouvement qui démarre en démographie visant "\textit{[...] to better account for cultural and social constraints on human demographic processes in general}". Bennett Dyke situe le premier usage de l'ordinateur "\textit{in anthropology appears to have been the work of Kunstadter et al in 1963}", auquel on peut également associer les travaux de Gilbert and Hammel 1966 \autocite{Costopoulos2007} \autocite{Dyke1981}. Antony Wallace également, levy strauss 1955: les mathémztique de l'homme...
 
En sociologie, James Coleman "\textit{considered it as a half-way point between verbal speculative theory and formal theory, aiding in the development of such theory through concretizing the functioning of social processes.}" \autocite[36]{Schultz1972}

% A compléter 

Dans la très claire retrospective historique faite par Gary Lock en 2003\cite{Lock2003} sur l'histoire de l'archéologie computationelle, l'auteur s'attache à bien séparer au moins deux sinon trois époques aux méthodologies et aux outils différents. En adoptant une posture un peu simplificatrice on peut donc affirmer que si l'archéologie pre-années 1960 se base principalement sur la récolte de données empiriques et la mise en exergue de pattern dans ces même données pour générer la plupart de ces explications, une rupture dans la discipline se dessine dès les années 1960-70 avec l'avénement d'un courant d'archéologie proclamant une « new archeology » (ou \emph{processualism}). Rejettant un empirisme beaucoup trop subjectif, celle ci vante le retour à la seule « Méthode Scientifique » pour générer des explications. Selon \autocite{Kohler2011} \autocite{Flanery1968} s'appuie le premier sur les enseignements de la cybernétique pour penser les cultures comme des systèmes, l'exercices étant avant tout pour lui d'établir des règles qui auront vocation par la suite à être simulé. Clark publie en 1968 un premier livre ambitieux «Analytical Archeology» qui vante à son tour l'approche systémique et tente de raccrocher l'archéologie aux développement récents en géographie, en analyse statistique \autocite{Kohler2011}. Le potentiel que pourrait avoir la simulation stochastique en archéologie est également évoqué \autocite{Clarke1968}. Une analyse a posteriori confirme l'apport de la systémique dans la construction des modèles de simulation, comme en témoigne \autocite{Aldenderfer1998} "One of the theoretical hallmarks of the "New Archaeology" was the systems approach (Aldenderfer, 1991), and a result of its adoption was the use of computer simulation to model whole societies or significant portions of them." Une vision qui se concrétise en 1970 avec James Doran \autocite{Doran1970} , qui non content de confirmer ce changement de paradigme dans  «Systems theory, computer simulation and archaeology» paru dans \emph{World Archeology} en inscrivant de façon provocatrice sa rupture avec les méthodes de ces derniers années \footnote{Until comparatively recently, say fifteen years ago, interpretive archeaology was a discipline which made no use of mathematical or scientific techniques and concepts. Few suggested that it should.}, s'inscrit dans comme un des premiers à voir dans la simulation comme un "laboratory-like experiments". En utilisant la simulation non pas comme un solveur d'équation mais en utilisant la puissance des opérateurs symboliques à sa disposition pour la mise en temporalités de systèmes d'interaction dans des sociétés passés, Doran décrit une vision de la simulation qui n'est pas sans rapeller le multi-agent d'aujourd'hui. Une conception de la simulation reprise et concrétisé par DH Thomas en 1972. Pour anecdote, ce mouvement pionnier de l'archéologie et de l'anthropologie vers la simulation fait étonnant écho avec le retour de Doran sur le devant de la scène, en collaboration avec Nigel Gilbert, déclaré chef de file du mouvement inter-disciplinaire qui va reaffirmer l'importance de la simulation dans des disciplines ou elle avait été mis de coté jusque là après la crise de confiance des années 1970. Pour plus de référence sur ce sujet, le lecteur pourra se référer à une discussion sur ce sujet sur la mailling liste de SIMSOC \footnote {www.jiscmail.ac.uk/cgi-bin/webadmin?A2=ind04\&L=simsoc\&F=\&S=\&P=39083}

% En géographie 

%Paragraphe à retravailler.
Pour Olivier Orain \autocite[26]{Orain2001}, l’adoption dès les années 1960 de cette théorie systémiste par la \textit{locational analysis} de l'école anglo-saxonne joue un grand rôle dans la diffusion de ces concepts auprès de l'école française de géographie. Ainsi Peter Haggett affirmait dès la première édition de \textit{L’analyse spatiale en géographie humaine} :

"Au cours de la dernière décennie, la biologie et les sciences du comportement ont manifesté un intérêt croissant pour la théorie générale des systèmes (Bertalanffy, 1951). Quelques tentatives ont été faites (notamment par Chorley, 1962) pour introduire les concepts de cette théorie dans la géomorphologie et la géographie physique, et on ne voit pas pourquoi le concept de système ne pourrait pas être étendu à la géographie humaine." \autocite{Haggett1965}

Quand on connait l'accueil qu'à reçu la traduction de ce livre en France, il est alors possible d'imaginer l'écho des paroles d'autres auteurs phare comme Berry, Haggett, Gould, Pred, etc. lorsqu'il développe ces concepts auprès des oreilles alors grande ouvertes des jeunes géographes français des années 1960 1970. \autocite[26]{Orain2001}

Olivier Orain pour qui les géographes français se sont très rapidement emparés de la notion de système émet alors comme hypothèse explicative principale "que les Nouveaux Géographes des années 1970-1980 ont trouvé dans l’idée de système un appareil conceptuel permettant à la fois de penser l’intégration de l’hétérogène et d’apporter une légitimité scientifique à l’étude de la région" \autocite[23]{Orain2001} 

% % transition vers Accompagnent d'une  nouvelle démarche ?

\autocite{Batty1976} Paul Claval pour qui la nouvelle géographie devient beaucoup plus soucieuse d'explication logique que de reconstitution historique, décrit très bien ce glissement méthodologique qui touche la discipline. Ainsi pour lui la nouvelle géographie " [...] essaie de proposer une interprétation théorique des phénomènes spatiaux : elle veut dégager des principes à partie desquels il est possible de comprendre leur articulation, de saisir leur fonctionnement et de reconstituer leur logique interne. Elle procède selon le modèle hypothético - déductif commun à toutes les disciplines scientifiques, et renonce au privilège jusque là attribué à l'induction" \autocite[p22]{Claval1977}

La démarche scientifique hypothético-déductive est appelé dans le cadre d'un nouveau rapport, cette fois ci plus nomothétique qu'idiographique construction théorique de la discipline géographique.

Cela dit il serait délicat de classer cette "nouvelle géographie" comme étant seulement inspiré du courant néo-positiviste ( ou positivisme logique) dicté par les seul épistémologues du cercle de Vienne, il est vrai très populaire à cette période. [A COMPLETER PAR UNE DEFINITION ? > ian hacking par exemple, cf cours d'Olivier Orain ?] L'inscription néo-positivistes qui touche principalement l'école américaine et britannique est plutôt du fait de certains auteurs ayant marqué la discipline par leur écrit théorique comme Bunge ou Harvey qui dans ces écrits se réclame ouvertement de cette école épistémologique. Harvey rompt très rapidement avec sa propre lecture néo-positiviste de la géographie établit dans "Explanation in géography" en 1969, et remis en cause en 1970-71 par de nouvelles inspirations plus marxiste. Ce livre, ainsi que celui de Bunge inspirera toutefois une longue série d'ouvrage du coté anglo-saxon, sans que Claval note une réelle diffusion de cette épistémologie à l'école Française, certe intéressé par ce débat épistémologique mais bien trop occupé alors à intégrer les fascinantes et toutes dernières techniques quantitatives pour qu'une synthèse voit le jour sur le sujet.\autocite[27-29]{Claval2003}

Pour \autocite[162-165]{Claval2001}, restreindre le tournant vécu par la géographie à ce seul objectif visant la recherche de "loi spatiale" comme seul dérivé de cette nouvelle démarche scientifique serait réducteur. La déception qui résulte de l'application des modèles de l'économie spatiale à de plus grandes échelles, confronté à des données observés en 1960-1965, rend d'autant plus pertinent la remarque précédente de Claval, pour qui finalement ce tournant doit avant tout "[...] sa fécondité à une idée simple : celle que les relations économiques, sociales ou politiques à l'oeuvre dans une société sont affectés par la distance qui sépare les partenaires.[...]", une idée introduite par Ullman en 1954, et qui marque bien la diversité des questionnement à l'oeuvre dans la construction de ce tournant.

% changement de causalité ?

Le passage à une causalité systémique va de paire avec le développement des outils à même de traiter cette dernière. 

"La conduite de la recherche moderne implique [...] l'utilisation d'un arsenal de démarche beaucoup plus sophistiquées et diverses que celles connues jusque-là. La nouvelle géographie ne serait pas possible sans une profonde révolution méthodologique"\autocite[p27]{Claval1977}. Il n'y aurait pas de méthodes si il n'y avait pas les outils qui les supportent, cf \autocite{Gould1970} \autocite{Gould2004} : "Mais il faut bien se rendre compte que la formulation de questions et les avancées méthodologiques sont les deux faces d'une même piece".

%Berthelot2001 [p100] voit deux composante qui doivent mener à  une tradition de base empiriste, et une tradition plus théorique.
Le processus d'absorption de l'approche systémique est communément abordé de façon conjointe avec une révolution quantitative, et fait référence à deux dimensions développés par Berthelot pour décrire ? :   %2 dimension de berthelot ?
 
\autocite{Haggett1969} nous indique alors qu'à cette période l'ordinateur intervient comme le support indispensable dans au moins quatres usages qui font écho aux méthodes modernes décrites par \autocite{Claval1977} : (i) statistiques multivariés, (ii) surface de tendances, (iii) graphismes, (iv) simulation. 

Afin d'illustrer l'importance de l'outil "simulation" dans la construction théorique, deux axes qui se juxtaposent dans l'espace et dans le temps peuvent être développés. D'une part il y a l'apparition et la rencontre début des années 1960 de deux pôles académiques innovants avec d'un coté les universitaires américains de la cote ouest dirigés par Garrison et de l'autre les universitaires Suèdois avec Hagerstrand, et d'autre part il y a cette montée en puissance simultanée (1959-68) des instituts de planning encadrés par le think-thank de la RAND corportation, qui va piloter la construction d'un ensemble de modèles de simulations urbains sous l'impulsion des récents succès de modèles d'économie urbaine remis au goût du jour par Isard, fondateur des récentes "sciences urbaines" avec ses disciples Alonso et xx de l'université de Pennsylvanie. \autocite{Batty1976}

La synthèse qui résulte de l'étude de ces deux axes permettra d'une part d'affirmer l'importance du niveau micro individuel comme facteur explicatif dans l'explication de l'organisation des systèmes sociaux, et d'autre part va permettre de lever un certain nombre de limitations à la fois institutionnelle, méthodologique et techniques qui vont avoir pour effet d'entamer rapidement cet optimisme un peu naïf envers les possibilités de la simulation,  caractéristique de cette période des années 1960-70. Ce mouvement en géographie est loin d'être isolé, les autres sciences sociales touchés par ce problème de la scientificité qui attrait à la construction et a la validation de modèle + limitation lié à l'abstraction mathématique (dont je n'ai pas parlé mais dont il faudra parler) %Il sera intéressant de faire un comparatif de cette situation avec les difficultés que rencontre aujourd'hui la discipline, et de voir comment il est possible de s'appuyer sur l'accès à de nouveaux outils pour tenter de dépasser ces limitations, de la même façon que des nouveaux outils ont permis de dépasser les problématiques de l'époque.

\subsubsection{Partie Garrison / Hagerstrand}

% Prévision => Rapport avec la démarche scientifique 
% Sur la causalité la conclusion p91 de "causalité et géographie" est très bien ! 
% p92-p102  + p109 => histoire de la geographie Claval 910.3 cla
% p168 -179 => p172 Sur la ville Claval2001 + systémisme dans le meme bouquin ! 

%economie spatiale : Isard Ponsard 

Il est difficile de faire un récit linéaire de ce que l'on apelle aujourd'hui "la révolution quantitative", notamment du fait du caractère multi-site de cette contestation qui s'enracine dans la défiance progressive d'une jeune génération envers une ancienne école faite d'universitaires pour la plupart attachés à leurs façons de faire. \autocite{Gould2004}  propose de s'attarder en particulier sur deux foyer important de cette révolte. Le premier se situe dans quelques universités de la cote ouest des Etats-Unis \autocite{Gould2004} parmis lesquels Washington, Iowa et NorthWestern; le deuxième en Suède via l'université de Lund.

C'est à Washington, sous la direction de Ed Ullman et William Garrisson, considéré comme l'un des premiers à voir l'intérêt général de l'usage de l'ordinateur pour la géographie, qu'à la fin des années 1950 se forme un groupe d'étudiants qui va marquer le renouveau de la géographie.  L'innovation des thèmatiques abordés dans les publications ( catégories à lister), mais aussi des formations proposés va de pair avec l'entrainement mutuel qui anime cette équipe de jeunes doctorants que l'on appelera plus tard le groupe des "Space Cadets". Brian Berry, William Bunge, Richard Morril, Duane Marble, bientôt rejoint par Hägerstrand sont les premiers à mettre en pratique les techniques computationelles les plus récentes. 

% Déjà au courant des approches systémiques a cette période (voir olivier orrain)
Chorley qui dès le début des années 1960 les concepts de système  \autocite{Johnston2004} , et dans la même période enseigne les concepts de programmation linéaire et autres techniques \autocite{Haggett1969}

% Apport important de l'école suedoise, déjà largement au courant (voir olivier orrain + )
Inspiré principalement par l'économie spatiale, % a completer

Une deuxième percée va être impulser à la discipline suite au déplacement de Torsten Hägerstrand aux états Unis.  Deux années après sa première publication en anglais en 1957, Hägerstrand est aussitôt repéré et invité par Garrisson en 1959 à présenter ses travaux novateurs, dont la première publication date de 1952, une période ou la géographie est encore uniquement idiographique en Angleterre et aux Etats-Unis. La rencontre a lieu à Washington dans un séminaire intitulé "simulation modelling of the diffusion of innovation". Encore réalisé à la main lors de sa venu à Washington, les premières simulations Monte-Carlo impressionne les disciples de Garrisson, notamment Morril, qui à la suite de cette expérience va partir plusieurs mois en Suède\autocite{Morril2005}. Pour la petite histoire, c'est via un voyage aux Etats Unis que le physicien Karl Erik Frödberg, un ami d'enfance de Torsten Hägerstrand, recupère un texte polycopié présenté par John Von Neuman et Stanislas Ulam sur les méthodes de Monte-Carlo. Alors appliquées au calcul de l'épaisseur des chapes de béton pour les centrales nucléaires, la technique est utilisé pour pallier à une résolution impossible de ce problème via les approches mathématiques classiques. Hägerstrand ayant déjà travaillé à l'étude de l'émigration en 1949, trouvera dans cette technique un écho innovant à sa problématique d'alors, la propagation des idées et des innovations dans l'agriculture suédoise. \autocite[26-28]{Gould2004}]

Quatre innovations fondamentales sont relevés ainsi par Peter Gould : "First was the introduction (at least at the geography) of the idea of spatial and time-processes, that geographic development over time could be understood and modeled; second was the particular processes of spatial diffusion; third was the technique of Monte-Carlo simulation; and fourth was the idea that individual behavior, not just that of large groups, could be modelled"

% + sur école suedoise ?
En 1960, le symposium de Lund, alors en pointe dans ce domaine, attire de nombreux géographe urbains de par le monde, et aura toujours selon Morril un large impact sur la diffusion des méthodes et des théories dans la géographie urbaines. A son retour Morril contribuera lui aussi largement à diffuser cette méthode au travers d'un papier applicatif sur le ghetto \autocite{Morril1965}. 

Même si il faudra attendre 1967 \autocite{Hagerstrand1967} pour que les simulations soient effective sur ordinateur, pour Gould, cette première demonstration de simulation probabiliste au niveau micro consacre la "simulation" comme un outil désormais indispensable à la discipline, et cela au travers de deux constats : "First, that there are elements of chance in human spatial behavior which cannot be modelled in traditional deterministic form ; and, secondly, that processes acting over space by definition act overtime. Handling space and time simultaneously is a difficult business, and simulation, for all its current detractors, often appears to offer the only feasible way out" \autocite{Gould1970}

% En france : 910.3 CLA : Histoire de la géographie Francaise Paul Claval
% Choley en France 1970 / Pinchemel qui introduit en France traduction Berry + Hagget 
% Après 1968, et l'amorce du tournant dans la discipline, la plupart des géographes qui se lance dans l'aventure de la nouvelle géographie puisent leur inspiration dans des ouvrages anglo saxon de la fin des années 1960, le plus lu étant surement Models In Geography de Chorley et Hagget 1967. \autocite[342]{Claval1998}
% \autocite[354]{Claval1998} => Structure système causalité

D'un point de vue plus technique \autocite{Haggett1969} cite comme véritable point de départ dans la discipline la  démocratisation de l'accès à la ressource informatique issue de fait de la diffusion d'une deuxième générations d'ordinateurs (IBM7094 series) beaucoup plus accessible et performants. Dans une période ou les compétences informatiques nécessaires à la programmation se font encore très rare, des listes de programmes disponible dans les universités sont peu à peu publiés, et des pionniers comme Marble ou Tobler mettent à disposition dès la fin des années 1960 différente routines informatique en libre accès.

% Difficulté à trouver des réponses dans la communauté, elle aussi perplexe vis à vis des problématiques de validation qui se pose à l'interdisciplinaire (annexe sur autres disciplines)

Mais l'utilisation des programmes par des non initiés amène à des erreurs d'interprétation. 

En 1972 Duane Marble \autocite{Marble1972} publie un article qui nous renseigne bien sur l'avancement des simulations de modèles spatiaux au milieu des années 1970. Il apparait de façon assez claire que peu d'avancée ont pu être réalisé du fait des limitations techniques, le traitement de données spatialisés étant largement au dessus des moyens techniques de l'époque. Ainsi la quasi totalité de la recherche fondamentale réalisé durant cette période 1959 - 1968 sera lié aux efforts des organismes plannificateurs dans la mise en place de programme nationaux, les seules à l'époques à se pourvoir de moyen humains et techniques suffisament importants pour développer des simulations de ce type. Sur ce point, nous pouvons aussi citer Hagget1969, relayé par Batty1976 qui montrent à quel points les attentes vis à vis de ces programmes sont fortes\autocite{Haggett1969}, et à quel point les retombés seront importantes, cela malgré les constats d'échec d'un certains nombre d'auteur. 

+

%Ces réflexions théoriques se propagent très vite, Chorley et Hagget (55-65) en angleterre  

Transition => Attente très fortes vis à vis des modèles dirigés par les instituts de planning, qui dispose des moyens nécessaires.. 

% Explicativité se greffe à l'individu, travaux d'orcutt à la meme période vont dans le meme sens.

(B) Partie ThinkThank + construction Orcutt

La RAND corporation, un ThinkThank américain fondé juste après guerre, va avoir une large influence sur un large panel de discipline, dont la géographie fait partie.

L'application des modèles statiques à la réalité urbaine dans une perspective de prédiction, un axe qu'a précédemment choisi de développer l'école américaine en remettant au goût du jour des modèles économiques, va très rapidement s'avérer décevante.

Les programmes de plannification, exclusivement orienté vers la prédiction, 

- Limite prédiction vs explication => incapacité à produire des prédictions (un débat encore actif aujourd'hui)

Transition => Retour à des modèles plus simples, construction de modèle dynamique, 

- Introduction par Orcutt du niveau micro avec dynamique en opposition au large scale model, 
	=> Limitation technique, attente de nouveaux ordinateurs lui aussi ...

=> Forrester, Dynamo, succès formalisme par les géographes avec diffusion des modèles via club rome ?
 
(C) Forrester polarise le débat, changement dans la construction des modèles

Apport pour les géographes est important, il permet de se désolidariser encore un peu plus de la trajectoire historique classique ...  cf article de Denise sur la complexité.

Changement dans la structuration / construction des modèles, prise en compte de la complexité des systèmes urbains : auto organisation, bifurcation, etc. 

\section {Validation, Évaluation de modèles agents} 

Tout au long de ce chapitre, il serait fait régulièrement référence aux différents travaux et publications de Frédéric Amblard, car ils constituent à bien des égards des points d'entrées importants dans notre réflexion sur la validation dans la simulation de modèle en géographie.

\subsection{Un transfert de l'ingénierie à la simulation en science humaine et sociale}

Les références à Sargent, Balci,sont de par leur nature générale couramment reprise dans différents ouvrages ou publications.

Sur la question du transfert des épistémologues se sont déjà penchés sur la question ... voir Numo

Partir de Naylor pour évoquer la nécessité d'une prise de recul dès lors que l'on cherche à caractériser le retour sur investissement

Toutefois plusieurs autres auteurs viennent nous rapeller que le jeu joué par Naylor est dangereux, car sans tomber dans un relativisme qui se voudrait naïf et dangereux dans une perspective inter-disciplinaire, meme si la validation est avant tout problème qui doit être abordé à l'aube de la discipline qui la mobilise. Il ne s'agit donc pas de réduire la validation à un question sociologique ou psychologique lié aux individus ou groupes d'individu ici, mais simplement d'exposer la validation à une forme d'expertise plus large, à meme de comprendre les enjeux et les moyens mobilisés pour tenter de mener à bien une évaluation qui se veut un tant soit peu objective.

Pour prendre un exemple plus concret, si on considère la mobilisation dans nos modèle de simulation du  modèle gravitaire, qui est une forme stylisé et éprouvé par l'expérience de processus réel en jeu dans la réalité, alors il est admis que la connaissance produite par ce modèle puisse être jugé de façon objective compte tenu des théories ainsi injectés.


\section{Objectiver les processus à l'oeuvre dans le cadre d'une évaluation par les pairs}

\subsection{au regard de la littérature en géographie}

\subsection{au regard de la notion d'équifinalité}

=> Osullivan nous propose (dit de facon implicite) de se tourner  vers l'évaluation collective, la seule pour lui à même de dégager une connaissance. KISS en lui même {Axelrod1997} porte cet idéal de simplicité (qui ne renie pas en tant que telle la complexité et la richesse descriptive des mécanismes) pour favoriser l'échange, la diffusion des modèles. Hors de ce coté, le constat est maigre. Le mouvement M2M {Amblard, Rouchier},{Rouchier}, {M.Batty/P.Allen}

Sur ces points Hedstrom rejoint OSulivan et les autres, la Discussion de la scientificité des modèles se fait avant tout sur la qualité des mécanismes et des causalités à l'oeuvre (Entities / Activities de Machamer).  

Présentation du plan lié à ce paradoxe, il faut mobiliser des moyens techniques et humains important pour que puisse mettre  en place des outils standardisé à même d'externalisé le débat non plus sur la question de l'évaluation, mais sur l'objet de cette évaluation. Autrement dit pour pouvoir discuter de la connaissance apporté par le modèle, il faut être capable de produire des coupes fiables des multiples dynamiques à l'oeuvre dans nos modèles .: corrélation entre paramètres, 

Ces outils existent, non seulement car ils font l'objet de recherche en eux même, mais aussi parcqu'ils sont appliqués de façon systématique dans certaines disciplines. Pourtant en modélisation en géographie et dans d'autre disciplines des sciences humaines, il semble peu mobilisé, et cela de façon historique.

L'évaluation de par sa nature contextuelle doit être faite au préalable par les pairs, tout en restant ouverte à la critique interdisciplinaire. Toute la difficulté de tels modèles résidant donc dans le placement du curseur entre ces deux pôle que l'on pourrait qualifier d'attracteurs, de par les moyens qu'il faut mobiliser pour s'en rapprocher dans un cas ou dans un autre.

\subsection{Le processus de création intègre la validation}

Présentation concept de validation interne / externe.

De très/trop nombreux guides méthodologiques pointent trop souvent la construction de modèle comme la recherche d'un état fini, donné, qui n'est pas compatible, ni avec la notion d'équifinalié, ni avec l'idée qu'un modèle est souvent révisable continuellement.

\subsubsection{État des lieux, des moyens inadaptés à la création des modèles}

Complexification, Généralisation, et l'infernal aller retour entre les deux, voir concept théorie précédant à l'observation en épistémologie ?

=> Pour être à même de mesurer le retour de connaissance dans un modèle qui suit une courbe de complexification, deux concepts au moins doivent pouvoir être mobilisé : 

\begin{itemize}
\item La ou les briques ou incrément unitaire qui encapsule l'ajout de connaissance qui concrétise un différentiel avec le modèle précédent, que l'on peut résumer ainsi "En quoi ce nouveau modèle est un modèle différent vis à vis de la question posé par le modèle"
\item Le processus de mobilisation de cette brique unitaire de reflexion dans l'exploration de la dynamique, que l'on peut evoquer de la facon suivante "quel est l'impact d'une insertion, de la modification ou d'un retrait d'une brique, et qu'est ce que je peux en dire vis à vis de la question posé par le modèle"
\end{itemize}

\subsubsection{Le choix d'une unité de raisonnement aproprié}

Une prise de recul reflexive nécessaire pour comprendre ou se situe la création de connaissance vis à vis de la question posé.
Ce qui nous amène à nous intéresser autant aux processus de création de cette connaissance qu'à cette connaissance elle même.

Grille de lecture existante : 
> Guide de bonne pratiques
> Fer à cheval d'Arnaud, etc.

=> ok mais ca suffit pas, on a vu dans le cadre des questionnements originaux en géographie, la notion de mécanisme

\subsubsection{La spécificité du questionnement géographique}

Discussion / Remise en cause de la notion de mecanismes avec la géographie ? 
A regarder la littérature actuelle sur la relation micro/macro dans les modèles agents, il n'y a peu ou pas de prise en compte du spatial dans la construction des relations micro macro observé dans les modèles.

Le modélisateur doit toutefois être attentif sur au moins deux points : 
 a) La biologie n'est pas la géographie, et les analogies ont le sais peuvent rapidement être dangereuse (ex ville comme organisme vivant, etc.)
 b) Ne pas s'enfermer dans les modèles edictés par de tels raisonnements, cf exemple de René pumain sur le passage de l'influx nerveux, et le fait que la mitocondrie se met tout à coup à pariticper au passage des ions, alors qu'en fait on pensais ce truc complétement passif dans l'échange d'ions ... 

Equivalent pauvre en géographie du "schemata" evoqué par Machamer serait les chorème de Roger Brunet, toutefois ceux ci sont limités dans le sens ou ils sont statiques. En ce sens, en terme d'outil pour penser, la simulation accède au rang de "schemata", support dynamique à la réflexion.

\section{Une double contrainte sur la réalisation des modèles, explicativité et parcimonie}

Si on considère l'ajout de mécanismes au modèle, celui ci est contrainte au moins de deux façon : 
> La recherche d'une parcimonie minimale explicative du point de vue de la question
> La recherche de la meilleur estimation rapport à une série de données, ou à un ou plusieurs fait stylisés

Quels sont les cas possibles :
=> Il y a correspondance parfaite entre réel et simulé, le modèle est surdéterminé
=> il y a non correspondance entre réel et simulé, mais l'ajout de mécanisme apporte un gain
=> il y a non correspondance entre réel et simulé, mais l'ajout de mécanisme n'apporte pas de gain, du moins pas dans la dynamique actuelle

Schéma évoquant la double contrainte, permet d'évoquer les problématiques associés et de dérouler par la suite sur l'absence technique de support pour une telle exploration : algorithme génétique, description de la contrainte, objectif à réaliser, ajout mécanisme, etc.

\subsection{Le processus de diffusion}
\subsubsection{Des moyens inadaptés à la diffusion des modèles}
Absence de plateforme de publication
Pas de protocole standard ou de pratiques etablies dans la discipline

\section {Isoler les pratiques et des outils existant} 

Ce travail a été largement initié suivant des problématiques déjà très bien décrite par Thomas Louail dans sa thèse...
Ajouter tout le travail réalisé pour la présentation ecqtg2013

\subsection {La modélisation agent en géographie} 

Cette section est volontairement détaché du reste, car si la modélisation agent amène certe de nouvelles problématique à la notion d'évaluation, il n'empêche que cette problématique peut être traité séparément de par son ancienneté, et sa relative indépendance vis à vis des techniques employés.

Les technique de construction que nous présentons sont évidemment à rattacher aux pratiques actuelle de modélisation utilisant les agents, il n'en reste pas moins.

% Faire remonter l'analyse des pratiques existantes ?

\subsection{Proposition de formalisation des limites} 

En s'inspirant du travail de formalisation déjà exposé dans \autocite[120]{Louail2010}], voici une relecture des limites pour la construction de modèle agents telles qu'elles ont étés évoqués dans le chapitre 1.

%\textbf{< Numero de la limite : Catégorie, Support, Description courte > }

%\begin{itemize}
%
%Sur la construction / validation de modèle :
%
%\item \textbf{ < 1a : Methodologie, Publications, Critique sur la scientificité >  }
%
%La remise en cause de la scientificité des méthodes et des modèles soit un phénomène récurrent dans l'histoire de la simulation de modèles, cela bien avant les critiques plus récentes sur les ABM, a tel point que ces dernières années ce sont développés des publications oeuvrant pour la dotation d'un argumentaire commun \autocite{Waldherr2013} \autocite{Miller2007} justifiant cet approche pour la construction théorique. Cette limite s'inscrit bien évidemment dans un cercle vicieux qui impacte tout autant qu'elle découle de l'ensemble des autres limites exposés par la suite. Ainsi par exemple le peu de publication relevés exposant des méthodes pour construire et évaluer des modèles est tout autant un effet de ces critiques qu'une cause justifiant dès lors son abandon.
%
%\item \textbf{ < 1b : Methodologie, Publications, Absence de cas d'utilisation concret >  }
%
%C'est un point soulevés régulièrement dans la littérature des ABM {ref}
%
%\item \textbf{ < 1c : Methodologie, Formations, Absence de formation adaptés >  }
%
%De ce coté là les choses ont beaucoup évolués ces 5 dernières années, et en France des formations se mettent en place peu à peu, via des réseau comme MAPS, dans les écoles doctorales, des écoles d'étés, ou dans des Master spécialisé (Créteil, Erasmus Mondus.). C'est une pratique encore restreinte de la modélisation, plus sous la forme d'initiation, limité à un nombre restreint et motivé de personnes, et qui à l'heure actuelle ne permet pas de cumuler l'ensemble des compétences nécessaire à la construction et l'exploration de modèle dans un seul individu, surtout lorsque celui ci provient d'une formation en sciences humaines et sociales.
%
%\item \textbf{ < 1d : Methodologie, <Publications, Formations, Application>, Absences de methodes unifiés >  }
%
%Cette limite est particulière car impossible à résoudre. En effet l'appui inter-disciplinaire qui supporte la modélisation agent en science sociale associé a la nature contextuelle de la validation est une double contrainte qui limite la construction d'une méthodologie unifié. Tiraillé d'une part par l'ouverture du champ propre à l'exercice des système complexes, et d'autre part par la nécessité d'une validation par les pairs, le modélisateur est facilement pris en défaut lorsqu'il s'agit d'imaginer une méthodologie couvrant les besoins de l'ensemble des disciplines. Le protocole ODD par exemple, bien que proposant une grille commune pour la description de modèles, ne parvient pas à fourni un cadre suffisament précis pour contenter les géographes \autocite{Schmitt2013}. L'absence de méthode unifiés impacte ou du moins limite la création ou la diffusion de publications, de formations, et d'applications.
%
%\item \textbf{ < 1e: Methodologie, < Publications, Formations, Application>,  Absences des ressources informatiques nécessaires > }
%\item \textbf{ < 1f: Methodologie, < Publications, Application>,  Coûts temporels > }
%\item \textbf{ < 1g: Methodologie, < Publications, Application>,  Coûts financier > }
%\item \textbf{ < 1h: Methodologie, < Publications,  Application>,  Modèle est un objet de recherche non borné > }
%
%\end{itemize}
%
%\textbf{ < 2a : Outils, Formations, Absence de formations adaptés >  }
%\textbf{ < 2b : Outils, Application, Absence des ressources informatiques nécessaires >  }
%\textbf{ < 2c : Outils, Application, Absence des ressources humaines nécessaires >  }
%\textbf{ < 2d : Outils,  Applications, Couts temporels de développement }
%\textbf{ < 2e : Outils,  Applications, Couts financiers de développement }

\stopcontents[chapters]

%\begin{table}
%\centering
%\subfloat[Source: élaboré à partir de Courel et al. (2005)]{
%    \includegraphics[width=160mm]{ClassificationMotifs.pdf}
%    }
%    \captionstyle{\centerlastline}
%    \caption{Typologie classique des motifs de déplacements}
%    \label{tab:classificationmotifs}
%\end{table}



