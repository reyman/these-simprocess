% -*- root: These.tex -*-

\newcommand\litem[1]{\item{\bfseries #1,\enspace}}

\section{Accéder à des ressources informatiques adaptés, le logiciel OpenMOLE}
\label{sec:openMOLE}

\subsection{Engager un retour vers le \textit{High Performance Computing} (HPC) en géographie}

Si le terme de HPC semble d'origine récente lorsqu'il est entrepris par les institutions \autocite{HPCHorizon2020}, il suffit de jeter un regard en arrière pour trouver l'emploi du terme chez les géographes à la fin des années 1990. A la lecture de ces ouvrages \autocites{Turton1998, Openshaw2000, Openshaw2000b}, on s'apercoit rapidement que cet acronyme n'a vraiment de sens que si on le contextualise. Ainsi, pour \textcite{Openshaw2000} \foreignquote{english}{The term \enquote{HPC} is very easy: it stands for high-performance computing (or \enquote{computer} depending on context), but the definition of what is \enquote{high-performance} is vague, relative and almost constantly changing as hardware continues to improve. It is a characteristic feature that today's workstations now offer levels of performance (or better) than only three-five years ago required extremely expensive HPC hardware in the form of vector supercomputers.}

En ce sens, la source brute de calcul impliqué par le terme HPC sous sa forme actuelle correspondra probablement d'ici 20 ans à la puissance d'une micro-ordinateur standard, comme on déjà pu le voir dans le passé sous le coup de la loi de Moore, dont on a fété il y a peu les 50 ans (1965 - 2015); une loi quasiment devenu une prophétie auto-réalisatrice pour certains observateurs, de part la pression économique et la course à l'innovation qu'elle maintient dans l'industrie et la recherche.

Qui plus est, créer et utiliser de super-ordinateurs (\textit{supercomputers}) pour réaliser des prouesses en calculs impossible autrement, et cela avec ou sans paralléllisme des applications, toutes ces idées trouvent comme on le verra plus loin des racines évidentes dans les touts débuts de l'informatique.

Le HPC ne peut donc que difficilement être vanté comme provenant d'une quelconque révolution matérielle ou logicielle récente, et ne se substitue qu'à l'expression logique d'un mouvement entamé il y a bien longtemps, celui-ci s'étant largement diversifié par la suite. Se sont ainsi succédés depuis les années 1930 (Z1 mécanique) / 1950 (ENIAC electronique), une succession d'innovations dans le matériel, le logiciel, et plus généralement dans les paradigmes informatiques qui peuvent les motiver. Le HPC est donc un terme utilisé pour pointer tout autant les anciens (le mythique et monstrueux Cray-1 de 1976) que les plus récents super-ordinateurs (ADA ou TURING du laboratoire CNRS IDRIS), mais également depuis les années 1990 la mise en réseau de machine standard à moindre coût (Avec la grappe de machine Beowulf, une idée du HPC régulièrement renouvellé par les bidouilleurs, avec l'utilisation aujourd'hui de micro-pc mis en parallèle : Arduino/Raspberry/Edison), ou encore une puissance répartie sur la base d'une mise en réseau de milliers d'ordinateurs de particuliers bénévoles (le 13 mai 2015 l'initiative BOINC rassemble 720000 ordinateurs totalise 6,7 PFlops); sans compter enfin cette infrastructure scientifique nommé grille de calcul supportant à une maille locale, nationale, ou internationale la mise en réseau de tout ce qu'il convient d'apeler aujourd'hui de façon générique des noeuds de calculs tant leur nature est hétérogène.

Il y a évidemment des différences dans l'organisation et la gestion de ces différentes ressources, ou les motivations guidant leur construction et leur bonne utilisation; mais en soit la finalité du projet semble assez similaire, avec la mise à disposition au plus grand nombre d'une ressource de calcul permettant la résolution la plus rapide possible, et/ou à moindre cout, de problèmes scientifique ou industriel de nature plus ou moins complexe.

Le terme HPC ne semble être au final qu'une réminiscence d'une course à la performance d'origine bien plus ancienne; un terme parapluie qui accueille dans son ombre une diversité toujours plus grande de matériels, et de logiciels.

L'\textit{European Technology Platform for HPC} (ETP4HPC) créé en 2011, reconnu comme la plateforme de référence pour l'établissement de partenariat public privé et la réalisation des objectifs du projet HPC de l'Horizon2020 \autocite{ETP4HPC2013}, définit le HPC ainsi :

\foreignquote{english}{
High-Performance Computing (HPC) refers to any form of computing where the density of processing or the size of the problems addressed require more than a standard or commodity computing system in order to achieve the expected result under the given constraints, and the application of advanced techniques such as the use of multiple processors (tens, hundreds, thousands or even more) connected together by some kind of network to achieve a performance well above that of a single processor. Both projects began in 1993, with significant systems deployed in 1994, and both had strong
impact on the community, essentially defining the range
of capabilities and techniques to be incorporated to this
day.

Two traditional HPC categories are distinguished:

\begin{itemize}
\item Capability computing refers to the use of a large and high-performing computing infrastructure to solve a single, highly complex problem in the shortest possible time. Systems used for this purpose are called supercomputers and consist of many tightly coupled compute nodes with distributed memory, all controlled by a single unit.

\item Capacity computing refers to optimising the efficiency of using a compute system to solve as many mid-sized or smaller problems as possible at the same time at the lowest possible cost.
\end{itemize}
}

Même si il y'a du vrai dans cette définition synthétique, l'approche contextualisé du terme tel que choisi par Openshaw est non seulement plus éclairante sur la diversité des approches aujourd'hui masqués par l'acronyme, mais aussi probablement plus robuste dans le temps. Les années 1993 et 1994 cité par l'ETP4HPC n'étant qu'une des étapes importante ayant vu l'accélération et la généralisation des moyens pour effectuer du calcul scientifique, qu'il faut elle même inclure dans une histoire des usages du calcul \enquote{intensif} par les scientifiques, d'origine beaucoup plus ancienne. En résumé, ce que l'on apelle \textit{supercomputers} ne tient pas seulement d'une histoire qui a démarré en 1993. Par contre là ou cette définition rejoint celle d'Openshaw, c'est sur cette impressionante montée en intensité permises par l'apparition d'architecture massivement parallèle au tournant 1980-1990, décrite un peu plus loin dans la section \ref{sssec:Tournant1980}.

Toutefois, il faut bien reconnaitre de façon positive l'intérét récent sur ces ressources porté par les institutions européennes, et la considération de celles-ci à juste titre, comme un levier aujourd'hui indispensable pour définir l'innovation. Comme en témoigne les intitulés des projets européens de l'Horizon2020, il s'agit aujourd'hui de se positionner dans une course qui n'est pas sans rapeller celle que l'on a connu pour l'espace, où la mutualisation est devenu nécessaire pour \enquote{tenir la distance} face à certaines puissances (la chine possède le numéro 1 du top 500 depuis 2013, avec Tianhe-2 à 33,86 PFlops \Anote{comparaison_tianhe_idris}). C'est du moins ce que laisse entendre les intitulés des projets pour \autocite{HPCHorizon2020} \foreignquote{english}{High-Performance Computing: Europe's place in a global race}, encore repris sous une autre forme dans le programme de l'ETP4HC \autocites{ETP4HPC2012, ETP4HPC2013} \foreignquote{english}{Today, \enquote{to Out-Compute is to Out-Compete} best describes the role of HPC.}

Qu'en est il aujourd'hui de la place des géographes et de la géographie dans ces grand challenges scientifique imposés par l'union européenne, dont le HPC et ses usages scientifiques fait partie intégrante ?

Comme l'indique Arnaud Banos dans le texte de présentation d'une conférence sur le calcul intensif en SHS, les sciences humaines et sociales sont les grandes absente du nouveau livre blanc du CNRS sur le calcul intensif. Si on regarde également du coté des derniers rapports d'activités de GENCI, un des organismes chargé de fédérer les ressources pour le HPC en France, les sciences humaines sont encore une fois totalement absentes du coté des usages.

Il serait hasardeux de tenter une réponse ici sans tenter de défricher d'abord \textit{Quel est l'intérét du HPC en géographie ?}

Cet état de fait n'a pas toujours été vrai, et on a bien vu dans le chapitre 1 que les géographes, et certaines modélisateurs pionniers dans les sciences humaines, se sont déjà tournés vers ce qui était alors les rares et unique ressources informatiques permettant d'être en adéquation avec les enjeux scientifiques de son temps. Nous verrons dans la section \ref{sssec:histo_centrecalcul} que ces traces existent, même si elles sont encore assez peu documentés.

On pourrait arguer qu'ils s'agissait d'un autre temps, et que le supercomputer était alors le seul moyen de faire de l'informatique. Oui, c'est vrai. Cela prouve aussi que les géographes ont été capable d'apprendre l'informatique et de surmonter des obstacles bien plus important que ceux pouvant se dresser devant nous aujourd'hui pour accéder au HPC (ce qui explique aussi le manuel d'Openshaw sur le HPC, qui croyait vraiment à cette possibilité de dépassement dans la discipline), preuve que lorsque les enjeux scientifiques sont à la hauteur, rien n'est impossible. Toutefois, si on revient à l'usage du supercomputers, il ne faudrait se contenter de dérouler trop facilement un argument commode, qui consisterait à penser que dès lors que la micro-informatique apparait, tout les géographes disparaissent des centres de calculs.

Là encore ce n'est pas si simple, car si la majorité des équipes vont effectivement se tourner vers la micro-informatique dès lors qu'elle se démocratise, on retrouve aussi dans plusieurs équipes des accointances durables avec certains centres de calculs qui se prolonge bien au delà de l'apparition de la micro-informatique, et dont on trouve encore trace aujourd'hui dans des équipements, mais aussi des structures d'accueil particulières. Preuve aussi qu'il y avait dans la pratique de ces centres inter-disciplinaire d'autres projets plus stimulants que celui du seul accès à une ressource informatique pour faire des statistiques. %On a tendance à l'oublier mais il y a aussi eu de l'innovation logicielle fruit de la rencontre de compétences informatiques, géographique spécifique au calcul intensif.

De façon tout à fait générale et pragmatique, on pourrait après tout se dire que c'est une ressource informatique comme une autre, alors pourquoi ne pas l'utiliser lorsque le besoin s'en fait sentir ? Celle-ci ayant prouvé son utilité dans bien d'autres disciplines scientifiques faisant partie de la famille des systèmes complexes, alors pourquoi ne pas l'utiliser également dans nos pratiques de simulation, dont on martelle depuis longtemps leur appartenance à cette famille ? Une nouvelle façon de de réaffirmer que la simulation des systèmes sociaux vaut bien la complexité des systèmes physiques ou biologiques, dans une conjoncture plutot favorable où de grands challenges et des financements européens fleurissent justement sur ces thèmes, comme FuturICT et ses 1 milliard d'euros de financement sur 10 ans ?

Openshaw, Turton et les géographes de l'école de Leeds, qui ont vu certaines de leurs prévisions technologiques largement confirmés ces dernières années \Anote{prevision_osaw}, ont aussi largement balisé dans leurs travaux pionniers, mais également dans des manifestes paradigmatique (la GeoComputation \autocite{Openshaw2000b} et son support HPC \autocite{Turton1998}), quels pouvaient être les avantages génériques évident, certain à gain immédiat d'autre atteignable à la condition d'être un peu plus aventureux, dont pouvait tirer parti la géographie en usant d'une telle technologie :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item To speed up existing computer-bound activities so that more extensive theory-related experimentation can be performed or to enable real-time analysis of geoinformation;
\item To improve the quality of the results by using computing-intesive methods to reduce the number fo assumptions and remove short-cuts and simplfications forced by computational constraints that are no longer relevant;
\item To permit larger databases to be analysed and/or to obtain better results by being able to proces finer-resolution data and make good use of very large computer memory sizes, and finally;
\item To develop new approaches and new methods based on computational technologies to provide new analytical tools and models, both of which are going to be highly important in the geoinformation-rich world of the future.
\end{enumerate}

Si je ne devais retenir qu'une phrase dans la prose si stimulante d'Openshaw pour résumer tout l'espoir que nous avons placé dans ces nouveaux usages du HPC pour la simulation, alors cela serait cette phrase au accent universel qui donne au simple modélisateur de son époque les moyens de devenir un explorateur toujours plus aguerri de la complexité : \foreignquote{english}{Computation permits the investigator to test theory by simulation, to create new theory by experimentation, to obtain a view of the previously invisible, to explore the previously unexplorable, and to model the previously unmodellable.}

Une autre façon de prendre conscience de l'impact possible de cette ressource dans notre quotidien est de prendre le problème à l'envers, comme le propose \textcite{Openshaw2000}  \foreignquote{english}{ [...] how would do you research if that PC on your desk was suddenly 10000 times faster and more powerful. It is likely that some researchers would not know what to do with it, some would not want it, but some would spot major new possibilities for using the computer power to do geography (and geo-related science) differently.} Toutefois, comme Openshaw le fait remarquer ensuite, quel intéret y a t'il de passer d'une opération dont la durée est de 30 minutes à une opération durant à peine 5 secondes ?

Oui c'est tout à fait vrai, mais il faut aussi garder en tête ce témoignage d'Alexandre  Kych, qui conforte largement cette analyse de Pierre Mounier-Kuhn, lorsqu'il fait état d'une école mathématique francaise ayant vu dès le départ d'un très mauvais oeil ce qui est devenu par la suite la discipline informatique : \enquote{Pour travailler sur des fichiers numériques, nous n'avions pas le choix. C'était un centre de calcul ou bien la règle à calcul ou les tables de Barlow ou de Bouvart et Ratinet. Au début des années 70, les calculettes commençaient tout juste à apparaître. Je me souviens avoir assisté à la réunion annuelle des professeurs de mathématiques du secondaire à Rennes dans la 1ère moitié des années 70. Il y avait plusieurs centaines de personnes. Les organisateurs avaient convié la société Commodore à présenter sa dernière calculatrice. Cette calculatrice aurait fait pale figure 20 ans après. Il n'empêche qu'une partie des mathématiciens présents était hostiles à ces premières calculatrices et ils ont même fait appel à un calculateur prodige qui a montré qu'il calculait plus rapidement.}

Il nous faut donc selon Openshaw un challenge scientifique, une motivation, une première façon de montrer que cet effort pour engager les géographes sur la voie du HPC est aussi justifié par des apports scientifiques.

Cette motivation, il me semble que les géographes l'ont trouvé déjà une première fois en françe dans les années 1970, dans l'application notamment de toute nouvelles techniques pour la statistiques ou la simulation, les deux marchant de paire avec la découverte de l'informatique. Repris de façon hétérogène par les sciences humaines, l’usage de \textit{supercomputers} est resté pendant quelques décennies le premier contact \enquote{contraint et forcé} que ces pionniers ont eu avec l'informatique. A ce titre, le chapitre 1 a déjà donné un aperçu des nombreux travaux réalisés par ces chercheurs en sciences humaines, et il me semble important de re-souligner le courage qu'il a fallu pour braver à plusieurs, ou parfois seul, toutes les contraintes à la fois intellectuelles, financières et physiques permettant approcher de telle machines, avant tout construite pour les besoins des sciences physico-mathématiques. Le \textit{supercomputers} a continuer à exister, certains géographes ont continué à l'utiliser, d'autres non, et se sont reposé sur la seule évolution du matériel micro-informatique. Pourquoi cet abandon ? Car le supercomputers, et c'est bien la définition qu'il convient d'employer, reste un moyen de calcul qui, quelque soit l'époque, propose une ressource de calcul plus importante qu'un ordinateur.

Dans son manuel co-écrits avec Ian Turton pour former les géographes au HPC paru en 1998, Openshaw relève huits points important pouvant expliquer le retard, voire le desintérét des SHS et des géographes pour le HPC \Anote{pincette_oshaw} :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item Jusqu’à récement (1998 du point de vue de l’auteur), cette ressource n’était pas satisfaisante pour envisager une utilisation plus généralisé en géographie. Il est a noté selon lui que la plupart des usages du HPC en géographie ne permettent pas un usage incrémental, et il existe un véritable seuil à partir duquel les ressources à disposition deviennent intéressante. Il est vrai que la mémoire mise à disposition des géographes dans ce type de machine est par exemple un facteur important d’utilisabilité, car que cela soit pour l’analyse spatiale, la simulation, il est important de pouvoir utiliser et stocker un maillage spatial suffisament représentatif de la situation étudié. Une matrice de 9 par 9, ou de 73 par 73 (p40 openshaw 1988), telle qu’elle est par exemple utilisé par openshaw dans son générateur de modèle AMS en 1988, est certe intéressante d’un point de vue théorique, mais n’apporte pas vraiment de comparaison possible avec l’empirie.
\item  La critique de fond adressé aux quantitativistes en géographie depuis les désillusions des années 1970, et la montée en puissance des géographie(s) plus qualitatives.
\item L’absence de tradition dans l’utilisation de ressources de calculs importantes, et le peu d’applications est en partie la conséquence des deux points suivants.
\item  Les objections philosophiques hérités des critiques du néo positivisme ou positivisme logique, la peur qui n’arrive pas à concevoir l’outil technologique comme un support neutre : “Computation is just a tool, and how the tool is used and what it is used for and the context in which it is used depend on the interests, skills, and value systems of the user, which are themselves grounded in contemporary society.”
\item La complexité “apparente” de la programmation et la nécessité d’obtention de nouvelles capacités qualifié de difficile par beaucoup de géographes ne programmant plus depuis longtemps.
\item Le fait qu’on puisse puisse imaginer cette activité computationelle comme un renoncement automatique de toutes pensées critiques.
\item Il n’y a aucun agenda de recherche et aucune ressources informatiques mise à disposition des géographes pour s’initier où encourager l’usage des HPC.
\end{enumerate}

En soit donc, cette progression les géographes modélisateurs l'ont déjà expérimenté au travers des différents paradigmes informatique qu'ils ont mobilisés dans leur modèles de simulation. Par contre, la mise à disposition du HPC pour l'exploration des modèles de simulation, même si elle a été tenté à plusieurs reprises par certains modélisateurs géographes francais ou anglo-saxon, n'a jusqu'à présent pas débouché sur une systématisation des pratiques, et semble être une pratique qui tient du domaine du cas particulier.

un chall camouflé dans un problème de validation dont il n'est, comme on a vu précédemment, qu'une facette,  existe déjà depuis longtemps, latent, touchant des tout premiers modèles informatiques en géographie jusqu'aux plus récents.

De fait je crois que notre initiative se positionne clairement dans le mouvement de GéoComputation inité par Openshaw. % Avoir pour le placer à la fin plutot ?

Produire une meilleur exploration des modèles, dont on a vu qu'elle était une autre façon plus modeste, mais aussi plus honnete, de parler de la validation (évaluation) \autocite{Amblard2006}

Si il ne peut donner réponse à lui seul au problème de la validation, celui-ci peut au moins nous donner les bases d'une discussion honnete, comme support préalable à cette part d'expression collective de la validation.


Personne ne s'étonne de pouvoir lancer des simulation d'automates cellulaires sur des matrices 2D disposant de millions d'éléments, et pourtant, on a vu dans le chapitre 1 que les pionniers comme Marble et Pitts, programmant sur des superordinateurs de l'époque, ont du malgré tout se resigner à redimensionner leur problèmes dans des proportions informatiquement acceptables (données et complexité), au détriment du raisonnement scientifique entrepris au départ ?

N'est on pas dans une situation similaire ou la construction et l'exploration de nos modèles de simulation est encore largement contrainte par les ordinateurs équipants nos postes de travails, et cela probablement pour encore quelques temps ?

\textit{Qu'en est il aujourd'hui de ces pratiques ? Qu'en est il de cet appui structurel ?  }

%Peut on encore ignorer les avancées du HPC ces vingt dernier années dans nos pratiques ?

Dans son manifeste pour le HPC \autocite[2]{Openshaw2000}, paru la même année que son ouvrage sur la GéoComputation \autocite{Openshaw2000b}, Openshaw et Turton n’hésitent pas à engager les géographes dans une métaphore qui les mets en proie à une forme de maladie des plus virulentes. S'en prenant spécifiquement aux géographes \enquote{SIG-istes} (et accessoirement aux sciences sociales en général), c’est une forme particulièrement menaçante de \foreignquote{english}{let others do the programming for us} qui condamnerait peu à peu ces scientifiques à delaisser la programmation plutôt qu’à s’en emparer pour devenir acteur de leurs propre outils \Anote{openshaw_virus}. \textit{Vrai ?} ou \textit{Faux ?}, cet état des lieux que l'on imagine volontairement provocateur, est le reflet d'une réalité loin d'être aussi simple à trancher qu'il n'y parait, l'outil informatique étant définitivement à double tranchant; un argument que n'hésiterons pas d'ailleurs à mobiliser et remobiliser sous son plus mauvais jour les critiques récurrents d'une science géographique quantitativiste soit disant aliéné, stérilisé par ces choix technologiques. Comme le résume bien Maryvonne le Berre, mais également d'autres géographes modélisateurs probablement avant ou après elle ( Sanders2000 déjà cité section \hl{XX}), \enquote{Il n’y a donc pas d’adaptation de la géographie à la technique mais recherche d’une technique adaptée à chaque objet d’étude géographique} L'idéologie ne vient qu'aprés coup, dans la construction et la manipulation des hypothèses constitutives des modèles, ou encore dans l'interprétation qu'on veut bien en faire. Si il y'a inévitablement eu des abus et des erreurs de jeunesse (dont quelqu'une sont exposé dans le chapitre 1), l'examen de certains projets scientifiques inscrit dans le long terme est là pour montrer qu'un tel constat est heureusement très loin d'être généralisable à l'ensemble des géographes modélisateurs. Si on prend l'exemple de la théorie évolutive des villes de Denise Pumain, force est constater que pratiquement toutes les nouveautés, technologiques et/ou paradigmatiques susceptibles d'un apports théorique ou méthodologique servant cette théorie (et par extension les objets géographiques concernés), ont étés mis en perspective de façon critiques dans chaque publications : les outils statistiques uni et multi-variés, les analyses factorielles, les systèmes dynamiques non linéaires, les réseaux de neurones, les automates cellulaires, la modélisation agent, tout a été testé avant d'être intégré dans la boite des outils potentiellement re-mobilisable. Car il n'y a pas une technique meilleure qu'une autre, mais bien une combinatoire de techniques uniques, constitutives d'un raisonnement scientifique, dans lequel chacune peut exposer tour à tour de multiples facettes. Pour raccrocher cette remarque avec les dernieres analyses portant sur l'usages des techniques de simulation en géographie, les observateurs (Varenne) tout autant que les acteurs (\hl{ref}) de ces pratiques tendent à mettre en avant une tendance croissante à la pluri-formalisation croissante des modèles agents, preuve encore de cette diversité des approches. \hl{retrouver la ref}

Si on se concentre en france sur l'exemple d'une autre technologie ayant impacté la géographie, le SIG. D’adoption plus ancienne et donc logiquement largement plus diffusé que le HPC, on retrouve pourtant dans l'HDR de Thierry Joliveau une forme d'écho à l'argumentaire d'Openshaw quant celui-ci pointe le danger qu'il y a délaisser ainsi les apports technologiques. Joliveau pointe de façon assez effrayante les dégats terrible que cette pensée au final auto-réalisatrice d’une \enquote{dérive technologique} de la géomatique, et du SIG, est en train de causer à la discipline géographique en France; les chercheurs géographes seraient en train de délaisser l'outil au profit de ce qui ressemble à une industrie, laissant ainsi le champs libre à d'autres classes de métiers, faisant évidemment une tout autre utilisation de cette donnée spatiale. Que faut-il en déduire ? Alors même que la géomatique peine déjà à intéresser certains géographes en France, devrait on en plus se préparer à être totalement exclus de la majorité des futurs projets necessitant l’usage du HPC en géographie ? Ne prend-t-on pas le risque de passer à coté de sujets comme le Big Data, les Smart Cities, qui engage derrière une enveloppe marketing des enveloppes d'argent mirobolantes amenant la possibilité  de traiter, et d’analyser des données géographiques récoltés à des échelles et avec des moyens inégalés jusque là ? (p481-482 Joliveau2004) Alors même que l’Europe vient de débourser un milliard d’euros sur le projet FuturICT composant une très forte composante simulation, la programmation pour la modélisation est loin d’être couramment enseigné chez les géographes et les chercheurs géographes, quant à l'introduction aux moyens et aux techniques du HPC, celle-ci est quasi-inexistante. Doit on s’attendre à voir ces précieuses données géographique analysés comme de simple données xy, à la barbe des géographes francais et au profit de seules sociétés privées avec qui il faudra bien ensuite négocier ?

Ce détachement progressif des géographes avec \enquote{la programmation} autre que seulement statistique n’est-il pas en train de devenir un contresens dans une société occidentale ou au contraire, la courbe va en s'accélérant ? A tel point qu'il devient de plus en plus difficile d'imaginer, même à court terme, quel forme sociétale viendra s'appuyer sur un paysage technologique aussi variable ? Après tout, ce qui révélait encore de chimère il y'a à peine 20 ans, comme l'ordinateur quantique, les drones personnels, sont touchés par des percées technologiques répétés qui font de ces objets autrefois de science fiction des objets sur le point d'intégrer à tout moment notre réalité sociétale. Une intrusion d'autant plus violente que certaines se font presque en silence, laissant au final très peu de temps aux acteurs publics pour intégrer ces changements; on pensera notamment à la banalisation et la diffusion de l'usages des drones comme outils personnels à tout faire, une révolution silencieuse dont les états peine encore à mesurer l'ampleur. Pour Turton et Openshaw \Anote{openshaw_revolution}, le HPC rentre aussi dans cette catégorie des technologies ayant infiltré, presque sans bruit, la réalité d'un certain nombres de disciplines scientifique de façon continue depuis 1980, jusqu'à apparaitre tout à coup comme d'un intérét scientifique évident pour les quelque géographes encore ouverts à l'innovation informatique \Anote{note_equipe}.

Comme le dit pourtant l'écrivain Cyberpunk William Gibson\Anote{gibson}, la pratique de l'anticipation reste un exercice en définitive beaucoup plus facile que d'imaginer le fonctionnement de sociétés vidés de ces technologies aujourd'hui largement répandu. Il faut donc s'imaginer quelques secondes quelle rupture a pu être l'arrivée physique de la micro-informatique dans certaines pratiques bien installés des sciences humaines. Tout d'un coup un objet un plus gros qu'une calculette apparait sur votre bureau, et permet de réaliser des calculs similaires à ce que vous faisiez en à peine quelques heures de plus sur des superordinateurs. Considérée comme un événement bénéfique par quelques scientifiques déjà converti à cette pratique de l'informatique depuis 10 ans, ou objet considéré comme inutile, décoratif, voire dangereux pour beaucoup; force est de constater que l'implantation progressive des \textit{micro-computer} dans les facultés au cours des années 1980 à finalement réussi à quand même fini par rendre en partie obsolète une pratique de l’informatique jusqu'alors contrainte à l'utilisation des centre de calculs, certe pour le pire, mais aussi, comme nous allons le voir avec quelques témoignages, pour le meilleur. Car l'autonomie et l'indépendance permises par l'utilisation de cette nouvelle ressource, sont des qualités qui peuvent à l'inverse être aussi approchés comme une invitation au repli sur soi, à la dispersion des efforts impliquant l'apparition de \enquote{re-création informatique} stérile \Anote{remarque_informaticien_roue}, et l'effacement possible de ce riche horizon inter-disciplinaire et de l'activité de co-construction qu'il supporte.

Si il n'est pas possible de statuer sur les sciences humaines en général, la voie nouvelle offerte par la simulation chez les géographe francais ne fera pourtant que révèler aux yeux des premiers modélisateurs l'existence de certains challenges qu'il faudra bien résoudre en tenant compte de \enquote{tous} les progrès de l'informatique, logiciels, matériels, paradigmatique, ou algorithmiques.

On ne saurait toutefois ignorer les efforts toujours investi par les géographes dans une forme de programmation orienté vers les statistiques (SPSS, SAS, R) ou la cartographie (R, Flash, et Webmapping en général), ainsi que le combat toujours en cours de certains pour engager les modélisateur dans \enquote{des pratiques de modélisation et de simulation libérées} \autocite{Banos2013}. Les plateformes de modélisation agents plus \textit{user-friendly} apparu dans les années 1990 ayant permis une approche à la fois plus ludique, et plus pragmatique de la programmation : Netlogo, Gama, Repast, etc. C'est bien dans le prolongement de ce qui constitue encore un petit monde que se place notre tentative pour démocratiser le HPC dans la simulation.

Ce qui nous amène à la question suivante, pourquoi les géographes, et surtout les modélisateurs, devraient il à nouveau s’engager dans cette voie \enquote{difficile} que semble être celle du HPC ? Quels sont donc ces anciens ou nouveaux challenges que l'on peut imaginer résoudre avec l'usage du HPC en simulation ? Et surtout, par quels nouveaux moyens attractifs pourrait on inviter des géographes modélisateurs ayant déjà largement enjambé dans les années 1990 cette barrière disciplinaire les séparant de l'informatique, à intégrer le HPC à leurs pratiques existantes ?

Avant d'engager cette discussion dans le cadre d'une évolution des pratiques au laboratoire géographie-cités, il était important il me semble de donner quelques élements d'histoire sur les pratiques des premiers géographes quantitatifs au contact des centres de calculs. En effet, ces structures qui ont pour la plupart disparu ou sont devenus inacccessible aux géographes d'aujourd'hui, semble pourtant représenter auprès des témoins interviewé un esprit qui n'est pas sans évoquer un acteur majeur nous ayant permis de concrétiser, et de prolonger ce projet d'une plateforme pour la construction et l'évaluation de modèle de simulation en géographie.

\subsubsection{Un bref historique des usages du HPC en géographie et en science humaine et sociale}
\label{sssec:histo_centrecalcul}

%Avant de revenir sur cette question, il est important de noter que ces centres de calculs, à la différence peut être de ce qu’ils sont devenus pour certains ces dernières années, sont avant tout des lieux propices à l’inter-disciplinarité dans les années 1970-1980.

\paragraph{Centres de calculs générique nationaux ou dédiés aux SHS dans les années 1970-1980}

% SMA encore existant ?
% PACTE : Sonia Chardonnel, Elise Beck

Tableau 1 : Une idée des ressources informatiques disponible au CNRS, de l'IBP à l'IDRIS.

\begin{table}[!h]
	\centering
	\begin{sidecaption}[fortoc]{Les sources pour construire ces deux tableaux proviennent de \autocites{CNRS1972, Boucher2000, Mounier2010}}[tab:machines]
		\begin{minipage}{1\textwidth}
			\centering
			\subbottom[]{
				\begin{tabular}{@{}cll@{}}
				\toprule
				\multicolumn{3}{l}{A l’Institut Blaise Pascal IBP ( 1946 - 1969)} \\ \midrule
				Entrée      & Identifiant machine      & Remplacement         \\ \midrule
				1955        & Elliot 402               & (1962)               \\
				1957        & Gamma 3, AET             & ( ? ,  ?)            \\
				1958        & IBM 650                  & 1966                 \\
				1961        & IBM 1401                 & ?                    \\
				1962        & Elliott 803, IBM 704     & ( ? , 1964/1966)     \\
				1963        & CAB 500, IBM 1401        & (? , ?)              \\
				1966        & CDC 3600                 & ( 1972 )             \\ \bottomrule
				\end{tabular}
		 	\label{machines_a}}
		 \end{minipage}\qquad
		 \begin{minipage}{1\textwidth}
		 	\centering
			\subbottom[]{
				\begin{tabular}{@{}cll@{}}
				\toprule
				\multicolumn{3}{c}{Au CIRCE ( 1969 - 1993 ) à Orsay} \\ \midrule
				Entrée   & Identifiant machine      & Remplacement   \\ \midrule
				1965     & Univac 1107 (Faculté)    & 1968           \\
				1966     & CDC 3600, IBM 360 / 40   & (1972, 1967)   \\
				1967     & IBM 360 / 50 et / 75     & (1972, 1972)   \\
				1968     & Univac 1108 (Faculté)    & 1970           \\
				1969     & IBM 360/75               & (1972, 1972)   \\
				1970     & Univac 1106 (Faculté)    & ?              \\
				1972     & IBM 360/20, 370/165      & ( ?, ?)        \\
				1985     & AMDAHL 470/V7, NAS9080   & (?, ?)         \\
				?        & IBM 3090/200             & ?              \\ \bottomrule
				\end{tabular}
			\label{machines_b}}
		\end{minipage}
  \end{sidecaption}
\end{table}

%IBM 704 : 20 KIPS/12 kFLOPS
%CDC 3600 : 1.3 MFLOPs
%CRAY1 : 160 MFLOPS

%http://www.tablesgenerator.com/latex_tables

% A l’institut blaise pascal ( 1946 - 1969) :
% 1957 :  un  Gamma  3  puis  un  AET.  De  Possel,  chef  du  labo  à  l' IHP,  passe  à  l' IBP.
%  1958 :  un  IBM  650 ;  de  Possel  devient  directeur  de  l' IBP.
% 1961 :  un  IBM  1401
%  1962 :  un  Elliott  803  (acheté  1  MF)  puis  une  IBM  704
%  1963 :  une  CAB  500  et  une  seconde  1401
%  1964 :  une  CDC  3600,  remplaçant  la  704
%  1966 :  une  IBM  360 / 40  à  disques  et  bandes,  dans  le  nouveau  local  du  CIRCE  à Orsay
%  1967 :  une  IBM  360 / 50  remplaçant  la  prècédente

% Au Circe (1969 - 1993) :
% Univac 1107 -> 1108 (1968)
% Univac 1106 (1970)
% CDC 3600 (1968-1972) et IBM 360/75 ( ?- 1972), IBM 360/20, 370/165 (1972 - ?)
% 1985 : AMDHALV7 et NAS9080
% IBM 3090/200

% A l’IDRIS (1993 - maintenant ):
% Vectoriel Cray C98 (en 1993) et Cray C94 (en 1994) jusqu’à 2000
% Cray T3D fut installé en 1995
% Cray T3E en 1996
% 2001 IBM Power4
% etc.

L’accès aux packages logiciels ou à la programmation directe en cartographie et en statistiques se fait soit en interne lorsque les équipes ou les universités possèdent du matériel, soit par un accès direct aux centres de calcul, soit par un accès indirecte par des terminaux en contact avec ces même centres. Si les la présence de centres à proximité des équipes coincident parfois, la qualité de l'activité ne saurait se rapporter à cette seule présence, ou à la taille d'une université. \autocites{Wieber1980}[448]{Joliveau2004}

A Besançon par exemple, \textcite[131]{Cuyala2014} souligne l’importance des collaborations entre Jean-Claude Wieber, géographe alors issue de l’institut de géographie alpine sous le patronage éclairé de Paul Claval, avec le mathématicien Jean-Philippe Massonie.

%IBM 1630 ?
\textcite{Massonie1986} a créé le laboratoire Mathématique Informatique et Statistiques (MIS) en 1964. Implanté dans une UER de Lettres et Sciences Humaines à l’université de Franche-Comté de Besançon, ce laboratoire atypique accueille une inter-disciplinarité qui n'est pas seulement dirigé vers la géographie \Anote{massonie_texte}. Disposant d’un petit centre de calcul rattaché à l'UER, la présence initiale d'un matériel modeste (IBM 1130 jusqu'en 1975 \autocite[22]{Wieber1980}, puis IRIS 80 plus puissant) n'empeche en rien l'inovation, avec l'apparition très rapide publications abordant les toutes nouvelles méthodes statistiques \autocite{Massonie1971}, ou encore la mise en place de l'important colloque de Besançon dès 1972. Intitulé \enquote{L'application des méthodes mathématiques à la géographie}, ce lieu d'échange joue selon \autocite[331]{Cuyala2014} un poids indégnable dans l'histoire de la géographie quantitative francaise; on trouvera pour plus d'information un compte-rendu des dix ans d'activités dans les annales de géographie en 1983, deux ans avant sa disparition \autocite{Massonie1983}

Avant même le début des années 1980, ce groupe investira très vite dans du matériel micro-informatique, en partie du fait de nouvelle contraintes liés à l'utilisation du matériel, comme le laisse deviner le commentaire de Massonie sur sa page Internet personnelle à propos de l'IRIS 50 \enquote{Cela devient un vrais ordinateur parce qu'on y a plus accès. La bête est enfermé dans un cage de verre, des techniciens le font marcher. Le matin on dépose ses cartes dans un bac, à midi on récupère les cartes et le listing des résultats. On a perdu le coté ludique. Les informaticiens ont pris le pouvoir. Certes il est plus rapide, mais que m'importe que mon calcul qui durait 1 heure, ne dure plus que 10 minutes, si je ne récupère les résultats que 3 heures après.} Un micro-ordinateur est acquis par Massonie dès 1978 pour développer \enquote{un logiciel d'analyse des données dont l'utilisation soit bon marché et accessible à des non-informaticiens.} Devant les performances satisfaisantes de ces micro, et l'autonomie qu'ils permettent, de nombreux logiciels micro sont adaptés ou créés à partir de là (ANACONDA, SADE, PATATE, etc.)\Anote{massonie_1978} \autocite{Massonie1986}. Cette volonté d'une \enquote{informatique conviviale} \autocite{TSH1984}, on la retrouve par la suite tout au long des transformation successives englobant ou accompagnant cette structure initiale originale \Anote{lab_mis}.

Le pôle Strasbourgeois impulsé par Sylvie Rimbert regroupe à partir de 1968-1969 Etienne Dalmasso, Monique Schaub, Colette Cauvin \Anote{informations_colette_cauvin}. En 1970, le statisticien et débutant en informatique Michel Pruvot intègre l'équipe à son retour du Canada, également complété en 1971-72 par Gérard Schaub, puis Henry Reymond en 1973 \autocite[135-153]{Cuyala2014}. De retour du canada, ce dernier remplace Etienne Dalmasso alors nommé à Paris. L'équipe active en statistique informatique est principalement composé de Michel Pruvot, Colette Cauvin, Sylvie Rimbert et Henri Reymond.

Fort d'un formation à l'étranger pour certains, apprentissage initié ou complété par les formations du CNRS (1971 à Strasbourg, 1972 à Paris) pour d'autres, ce petit groupe est en tout cas très rapidement opérationel en informatique, statistique et cartographie au début des années 1970. C'est cette triple compétence qui permet à l'équipe d'utiliser très rapidement les ressources informatiques à leur disposition sur le campus universitaire de Strasbourg. En 1971, deux projets sont finalisés, une des toutes premières applications d’ACP sur un milieu géographique intitulé \textit{Essai d’application de quelques méthodes statistiques à la région milanaise} \autocite{Dalmasso1971}, mais également une commande d'atlas informatisé pour l'agence d'urbanisme de la ville de Strasbourg. Un véritable tour de force chez les géographes de cet époque, car ce travail s'appuie sur l'utilisation dès le dernier trimestre 1970, des ressources informatiques mis à disposition par le Centre de Calcul de Strasbourg-Cronenbourg (CCSC) créé sur le campus en 1968-69 pour les besoins de la physique.

C'est grâce aussi à la collaboration régulière avec Anne Engelmann, une physicienne et informaticienne du CEREG disposant d'un bureau au CCSC, que les géographes de cette équipe vont accéder à la fois à des facilités matérielles \Anote{collette_ccsc_centre} et logicielles; une collaboration qui s'inscrit physiquement dans les murs d'un lieu où on ne s'attendait surement pas à voir y résider de façon durable des géographes. Ainsi outre les bibliothèque logicielle déjà accessible (BDMP permettant de faire des analyses factorielle et des classifications), Anne Engelmann a également assuré plusieurs installations de logiciels sur les machines du centre : le logiciel SYMAP (1965) d'Harvard dont une version en Fortran a été apporté par des étudiants canadien, divers logiciels de cartographies acheté ou récupéré par la suite par Sylvie Rimbert entre 1973 et 1979, etc.

Alors que l'arrivée de la micro-informatique dans le laboratoire se fait de façon progressive à partir de 1982-1983, les activités de programmation peuvent se diversifier semble-t-il sans mettre en danger cette liaison avec le centre de calcul. Une fréquentation du centre qui restera continue jusqu'à la fermeture du centre en décembre 1993, avec l'organisation d'enseignements, et/ou la mise en oeuvre d'activité de recherche impliquant l'instalation, l'utilisation ou le développement de logiciels. En 1985-90, l'activité des géographes de Strasbourg se developpe même sur un double lien, avec la mise en oeuvre d'une liaison avec le CNUSC (Centre Universitaire de Calcul du Sud), essentiellement motivés par des collaborations avec les géographes de Montpellier (on on parle un peu plus loin).

Michel Pruvot développera des logiciels sur les différents équipements micro successifs du laboratoire, alors que Colette Cauvin et Sylvie Rimbert, bien qu'écrivant également de petit programmes, préfére mettre l'accent sur le dialogue et la co-construction avec les développeurs spécialistes de l'équipe. C'est le cas avec Anne Engelmann, mais également avec Jacky Hirsch (qui dispose lui aussi d'un bureau au centre) et Charles Schneider, deux scientifiques rodés à l'informatique ayant rejoint l'équipe, qui développe ensemble en 1983 le logiciel de télédetection Cartel sur l'Univac 1100 du centre.

C'est Colette Cauvin qui gère les relations et les finances du centre pour les étudiants jusqu'à sa fermeture et son remplacement par le centre universitaire de calcul CURRI \Anote{calcul_curri}, un CRI ou celle-ci occupe les même fonctions. Il faut en effet rapeller que l'institut n'a disposé de véritable salle pour la micro-informatique que vers les années 1988-90, des apareils utilisés en premier lieu pour les cours de statistiques.

Les autres cours, mais également une partie des logiciels de l'équipe implémentés initialement sur les \enquote{grosses machines} du centre de calcul, doivent donc s'adapter très régulièrement au changement de matériels informatiques qui touche le centre : en 1968-69 un IBM aurait apparemment existé, remplacé dès 1970-71 par un Univac 1108 \textcite{Dalmasso1971}, un Univac 1100 en 1983, puis au plus tard en 1985 un IBM3060/IBM3090. Ce dernier système marque la fin des perforatrices et l'arrivée de terminaux interactifs IBM 3179 \autocites{Rimbert1984,Cauvin1986}. Un peu plus tard encore, l'arrivée progressive de terminaux à l'université va permettre à l'équipe de se connecter au centre distant de quelques kilomètres. Un évolution qui rend enfin possible l'execution d'un certain nombres d'opérations à distance, les listings résultats étant ensuite renvoyé par le centre deux à trois fois par jour.

En dehors d'une partie des formations en statistiques qui vont basculer sur les micro-ordinateurs lorsque ceux-ci seront disponible, la plupart des formations, comme les stages, puis les cours de cartographie démarré au centre en 1973-1974 se font toujours au centre en 1988-90. Avec comme on a vu des adaptations majeures, les perforeuses ont été remplacé par des terminaux plus récents, les programmes ont souvent été réécrit plusieurs fois, et  l'arrivée de développeurs ou l'amélioration des compétences de certains chercheurs en informatique a permis le développement de programmes propre aux besoin d'enseignements et de recherches.

A partir de 1978 Jacques Hirsh développe (parfois avec l'aide d'étudiants) des logiciels spécifique à partir de librairie acquise par le CCSC (comme par exemple la librairie d'application graphique Danoise Uniras acquise en 79-80) ou de tout nouveaux logiciels comme Cartel. Ce dernier logiciel servant notamment d'appui aux stages de télédetection ayant lieu en 82-83 au CCSC. Michel Pruvot, responsable des enseignements de statistiques au CCSC, se met rapidement au développement de logiciels sur micro.

% le seul on imagine à disposer de l'équipement adéquat pour les sorties de logiciels de cartographie.

Comme nous allons le constater dans la plupart des autres centres du CNRS, toutes ces prestations étaient payantes dès le début des années 1970 : temps de calculs, et accès aux facilités matérielles et humaines du centre de calcul.

A Grenoble (H. Chamussy , P. Dumolard) c’est l’Institut de Géographie Alpine (IGA) qui se coordonne avec le Centre Inter-universitaire de Calcul de Grenoble (CICG, formalisé en 1972). Le modele AMORAL développé par les membres de cette équipe a été écrit dans le langage DYNAMO, après l'impulsion \Anote{note_amoral_difficulté} donné par François Rechenman de l'IRIA, et Patrice Uvietta du laboratoire Atelier de Recherches sur les Techniques Mathematiques et Informatiques de Systemes (ARTEMIS) de l’Institut d'Informatique et de Mathématiques Appliquées de Grenoble (IMAG). Ce dernier intervenant également de façon plus générale chez les géographes par le biais de son inscription au groupe DUPONT, et dans la participation aux différentes formations associés à cette période à l'analyse de systèmes en géographie, comme celle par exemple organisé par Guermond en 1982 \autocite{Guermond1984}.

A Paris c’est principalement avec le Centre de Calcul de Paris 1 relié au Centre Inter-Régional de Calcul Electronique Paris (CIRCE) que les premiers calculs statistiques des géographes sont réalisés, par exemple lors des cours de 1969-1970 de Bernard Marchand ou les étudiants sont amenés à envoyer leur carte perforé au centre pour execution (Cuyala2014 p? ); Ce centre fera partie du panel de ressources que les membres de la future équipe P.A.R.I.S n'hésite pas à mobiliser, que cela soit pour des analyses statistiques ou un peu plus tard pour des modèles de simulations, comme on va le voir dans la section suivante.

L’ORSTOM (ex-IRD) dispose également d’un certain matériel informatique et de connexion avec la plupart des centres de calculs parisiens : la société STAD, le CIRCE, etc. \autocite{Dejardin1992}

A Montpellier, la maison de la géographie ouverte en octobre 1964 accueille le GIP Reclus (1984-1997) piloté par Brunet \autocite{Brunet1988}. Cette structure collabore sur plusieurs aspects (stockage des données, calcul scientifique, cartographie) avec le centre Universitaire de Calcul du Sud (CNUSC) \autocite{Waniez2010}. Ouvert en 1981, celui-ci est conçu comme un équivalent du CIRCE en tout points : formations, services d’aide, logiciels et matériels étant à disposition des sciences humaines et sociales pour leur travaux \Anote{presentation_cnusc} \Anote{centre_formation}.

L’occasion aussi de dire que le personnel de ces centres \Anote{note_documentation_ccalcul}, et plus spécifiquement ceux des pôles plus spécifique pour les SHS, ont probablement joué un premier rôle important dans l’accompagnement de la plupart des géographes, mais aussi plus généralement des chercheurs en sciences humaines.

Disposant la plupart du temps d'équipements assez léger, mais équipé de connection avec les grands centres de calcul, le CNRS va également mettre à disposition des SHS des pôles dédiés qui semble plus accessibles que les grand centres. C’est le cas par exemple du Laboratoire Informatique pour les Sciences Humaines (LISH) \autocite{MSH1975} créé en 1975 par Mario Borillo dont deux unités sont à Marseille (une \enquote{Unité de Recherches Méthodologiques} reprenant l'équipe de mathématiciens, informaticiens et linguistes de l'URADCA, une \enquote{Unité d’Applications Informatiques} au centre de calcul du Pharo), et un \enquote{Service de Calcul} à Paris (Mathieu2014 p154, Bulletin novembre MSH 1975, 1982); suivi en septembre 1980 par la création d’une unité supplémentaire au Mirail à Toulouse (Bulletin octobre MSH 1980). Déjà installé depuis 1970 à la Maison des Sciences de l'Homme du 54 boulevard Raspaill, le CNRS décide en 1975 de réorganiser le Centre de Mathématique Appliquées et de Calcul de la Maison des Sciences de l’Homme (CMAC) pour en faire le laboratoire de service CNRS-LISH, logé au sous-sol du batiment. Il est à noter que ce service dispose avant même sa restructuration d’ordinateurs et de terminaux d’accès à CNUSC et CIRCE (rapport d’activité \autocite{CNRS1972} et numéro 0 du bulletin de la MSH \autocite{MSH1973})

Dans une conjoncture de mutation très forte, autant dans les sciences humaines qu'en informatique, Le LISH tente d'assurer cette mission de plus en plus complexe de réunion des deux mondes autour de 5 activités principales : Accueil et assistance, Formation des nouveaux utilisateurs, Animation et organisation de réunions et rencontres, recherches et collaborations scientifiques. \autocite{LISH1980}

%Genet1981

% Voir information ici http://cams.ehess.fr/document.php?id=947
Parmis ces autres laboratoires de la VIème section - science économiques et sociales - de l'EPHE venant s'installer dans le batiment de la MSH (qui deviendra l'EHESS après son déclaration d'autonomie en 1975), il en est un des plus important dans l'histoire de la formation des SHS aux mathématiques et à la statistique. Dirigé par Guilbaud en 1955 dans le cadre de la VIème section, le Groupe de Mathématique Sociale (GMS) devient ensuite associé en 1967 au CNRS et prend successivement le nom de Centre Mathématique Sociale (CMS) puis de Centre d’Analyse et de Mathématiques Sociales (CAMS) en 1981 sous la direction de M. Barbut.

La partie recherche de l'unité LISH de Paris est décrite ainsi par son responsable Gian Perro Zarri : \foreignquote{english}{On 27th November 1980, the Control board of the \enquote{Laboratoire d'Informatique pour les Sciences de L'Homme} ratified the reorganization of L.I.S.H. in the form of a \enquote{network}, composed of several Units,each with a certain amount of autonomy [...] the principal objective of this Unit is to lead a series of pilot actions, designed to demonstrate the utility of conceptually advanced Computer Science instruments in the domain of the Humanities and the Social Sciences. Particular importance is given to Artificial Intelligence and Knowledge Representation techniques. [...]  The permanent C.N.R.S. staff which makes up this new Unit, comprises, apart from myself, Pierrette Andres(secretary), Wenceslas Fernandez de la Vega, Ruddy Lelouche, Jacqueline Leon,and Vincent Meissonnier} \autocite{Zarri1981}

Pour donner un aperçu des activités et de l'implications des membres du LISH dans l'outillage et la formation des SHS; Ruddy Lelouche est par exemple à l'origine de la première installation du package \textit{Statistical Package for Social Sciences} (SPSS) en février 1976 (sous la demande d'un étudiant américain), premier représentant de la France au IV congrès international annuel des utilisateurs du package, et pour lequel il va ensuite organiser de nombreux stages de formations, et rédiger des manuels.

De façon plus générale, outre leur différentes spécialisations sur le plan recherche, ces différentes unités proposent en général : une forte activité éditoriale dans différents bulletins, des publications scientifique, l'organisation de conférences, des stages de formations dans les centres de calcul nationaux, des formations à différents packages, différents langages (Fortran, Logo en 1984), des ateliers thématiques et techniques, la participation à des écoles d'été (Grenoble), des échanges internationaux, etc. Le lecteur curieux d'en savoir plus pourra se référer aux différentes descriptions des activités de ce laboratoire dans les bulletins de la MSH, et dans différents journaux, en particulier dans \enquote{le médiéviste et l'ordinateur}, un des rare journal issue de cette \enquote{littérature légère} qui a été entièrement numérisé \Anote{litterature_legere}.

On peut également citer l'exemple de Philippe Cibois, qui a eu des responsabilités à l'unité de calcul Parisienne du LISH de 1975 à 1989. En formation sur l’Analyse de données, il réalise son stage d’informatique (1971) appliqué au science humaines sur les premiers ordinateurs de la MSH et des ATP, et il collabore fréquemment avec Marc Barbut (1971-1975). C’est ainsi rompu à la programmation et à l’analyse de données qu’il passe son mémoire (puis plus tard sa thèse 80) avec Boudon au GEMAS ou il deviendra coordinateur technique amené à travailler sur la partie informatique/statistique de divers contrats et projets (\textit{l’inégalité des chances}). C’est suite à un article dans \textit{Informatique et Sciences Humaines} en 1975 inspiré par son travail d’enquete l’ayant mené au colloque de organisé par Gardin en 1972 que celui-ci est remarqué par Mario Borillo, un des proches collaborateur de Gardin, qui l’invite aussitot à intégrer comme \enquote{secrétaire scientifique} la toute nouvelle unité du LISH de Paris en 1975 ou il restera jusqu’à 1989. A partir de là, il arrête de travailler pour Boudon (il soutiendra toutefois sa thèse en 1980 sous la direction de Boudon), et il organise pendant 10 ans (1979-1989) de très nombreuses formations sur la programmation, les statistiques au LISH mais également lors de stages dans divers organismes. En plus de son activité dans \enquote{la feuille d’avis du LISH}, il co-éditera également un journal dédié aux statistiques en sociologie à partir de 1983 (Bulletin de méthodologie sociologique). Il est également l’auteur d’un logiciel TRI-DEUX démaré en 1970 et toujours actualisé. Ces travaux et ces publications ont probablement joué un grand rôle dans l’évolution de l’analyse de données en sociologie, et l’introduction des analyses factorielles au plus grand nombre.

Il raconte dans son HDR passé en 1993 (p21) comment l’usage de l’informatique était organisé au sein du LISH lorsqu’il était animateur  :

\enquote{Pour ce qui est de l'utilisation de l'informatique, je soutins, en collaboration étroite avec la responsable du service, Monique Renaud, que le centre de calcul ne pouvait correctement fonctionner qu'en \enquote{libre-service}, c'est à dire avec un mode de fonctionnement où les utilisateurs créaient eux-mêmes leurs cartes perforées et sortaient eux-mêmes les listings qui en résultaient. Cela pouvait sembler peu de chose mais cette organisation supprimait l'ancien système du travail \enquote{à  façon} où un ingénieur  prend (mal) en charge des travaux  qui sont ensuite examinés (sans investissement suffisant) par un chercheur. La nouvelle organisation permettait au chercheur de faire lui-même les travaux informatiques à la condition qu'il y soit formé : des stages nombreux permirent à des utilisateurs de s'initier aux joies du système d'exploitation IBM 360 du Circe et le LISH grâce à cette nouvelle organisation devint rapidement un centre de calcul performant et très fréquenté.}

La géographe Nicole Mathieu ferait partie des quelques géographes à avoir fréquenté ce laboratoire au début des années 1980, comme en témoigne sa réponse à Denise Pumain lors d'une rencontre sur l'inter-disciplinarité : \enquote{L'époque de la carte perforée a commencé bien avant celle que tu mentionnes et a duré jusqu'à la fin des années 1970; elle a été suivie par une phase où nous élaborions des fichiers éléctroniques qui étaient traités au Lish, boulevard Raspail au début des années 1980 [...]} \autocite[154]{Mathieu2014}

En recoupant les informations donné Denise Pumain, et suite à une correspondance privé avec Alexandre Kych, et Philippe Cibois, les principaux géographes ayant travaillés au LISH sont :

- ceux qui ont travaillé pour Françoise Cribier (Géographie Sociale et Gérontologie) et d'autres chercheurs apparentés (Catherine Omnès, Elise Feller) : Alexandre Kych, Catherine Rhein et des doctorants de Benzécri comme Jean Marc Blosseville.

- Fla, Marie-Odile et Irène: Jeanine Cohen (Laboratoire de Géographie Humaine)

- Gérard Joly alimentant la base de l'INIST

- Françoise Pirot qui s'est intéressé très vite aux systèmes d'information géographiques, dans le cadre d'un labo à l’institut de géographie, puis à la MSH.

- Ceux qui ont suivi des formations à FORTRAN-4 ou aux stats de base, formations organisées par Claude Deniau et quelques autres au début des années 70: les futurs géographes quantitatifs de région parisienne comme Denise Pumain, Thérèse Saint-Jullien, François Durand-Dastès

On sait également que certains liens se noue entre des acteurs du LISH de Marseille et les géographes du sud-est, comme en témoigne la personne de Jean-Paul Cheylan, architecte et géographe affilié au LISH-Marseille entre 1978 à 1984. Celui-ci va ensuite participer aux travaux de conception, de mise en place et de fonctionnement du GIP RECLUS, cela à partir de 1982 (\hl{ref cv de JP Cheylan}).

Enfin, sur la question des usages des ressources de calculs dans les grands centres, il est également intéressant de constater que les sciences humaines sont en 1972, avant même la création du LISH, déjà consommatrice de ressources de calcul, le rapport d’activité du CNRS de 1972 faisant état pour l’année 1972 sur le 3600 CDC de CIRCE de 2 517 heures de calcul dont 3.25\% proviennent des sciences humaines, ce qui n’est pas du tout négligeable quant on connait la difficulté de ce contexte technique pour ces chercheurs ! Autre statistique étonnante, les utilisateurs se répartissent sur cette années là entre toutes les disciplines  de la façon suivante : mathématique et informatique 10\%, environ, physique 40\% environ, chimie 25\% environ, sciences humaines 15\% environ, sciences de la terre 3\% environ, sciences de la vie 3\% environ, gestion-documentation 4\% environ.

%CRISS Centre  de  Recherche  en   Informatique  appliquée  aux  Sciences  Sociales
%Il existe également à Grenoble une UER pour l'Informatique et mathématiques en sciences sociales (IMSS) de l'UFR SHS de l'université Pierre Mendès France.

%IHRT Grenoble
%Jacques Rouault

%1984 LOGO

\paragraph{Usages des ressources de calculs pour la simulation}

Prenons comme exemple deux foyers marquant par leur travaux sur la simulation au début des années 1980, comme l’équipe auteur du modèle Analyse et MOdélisation Régionale des ALpes (AMORAL), et les projets plus diversifiés de l’équipe P.A.R.I.S. Ces deux équipes sont formés de géographes qui ont été formés à l’informatique, soit en autodidacte, soit à l'étranger, soit au cours des différentes formations entre 1970 et 1980.

Si on en croit les travaux de fouille bibliographique réalisé par \textcite{Rey1983} pour un numéro  spécial des annales de géographie (numéro 511 spécial \textit{Informatique et Géographie}), le modèle AMORAL (1981) se positionne comme la seule tentative de modélisation jusqu'à la simulation d'une décennie de travaux dans la géographie rurale \enquote{Quantitative et Théorique}. Si on recoupe les informations donnés dans les différentes publications sur ce modèle de système dynamique, et le témoignage de \textcite{LeBerre1987}, on apprend que ce sont les mathématiciens/informaticiens du laboratoire ARTEMIS de l’Institut de Mathématique Appliquées de Grenoble (IMAG)\Anote{kuntzmann} qui contacte en premier lieu les géographes à la recherche d’application concrête de leur travail, cela dès 1976. Patrice Uvietta, chercheur à l'IMAG et membre du groupe Dupont, joue probablement un grand rôle dans cette médiation entre informaticiens et géographes. Un autre modèle sur le vignoble, nommé BIBINE, à l'état d'ébauche encore en 1989, est développé sur Macintosh avec STELLA, un logiciel dédié à la formalisation de système dynamique. \autocite{Chamussy1989}

Cette collaboration avec les informaticiens du centre de calcul de Grenoble qui rend l’émergence de ce projet possible, du fait entre autres de certaines manipulations complexes (quotation). Du coté de la toute jeune équipe de géographe de P.A.R.I.S, si on en croit le témoignage de Denise Pumain, il y a donc bien eu interventions sur les programmes informatiques récupérés, bien que ceux-ci est été plus motivés par la tentative d’un ancrage empirique de ceux-ci (ce qui constitue déjà une innovation à part entière ! \autocite{Pumain1982} ), que par une réelle volonté de modification structurelle du programme informatique, un exercice qui aurait pu justifier une republication.

\enquote{J’ai en partie réécrit un programme, en \enquote{mettant les mains dans le cambouis }, parce que les sorties à l’époque étaient vraiment très élémentaires. Je vous parle d’expérimentations conduites au début des années 1980 ; Patrice Langlois a connu, lui aussi, ces époques héroïques du calcul. On avait en sortie des listings... il fallait les envoyer au Circe... attendre assez longtemps des retours, pour s’apercevoir que l’on avait oublié dans la carte perforée une parenthèse ou un point-virgule... et recommencer. C’était un processus lent qui permettait de bien réfléchir entre deux envois de simulations. Et nous n’avions pas de sortie cartographique : on avait un format d’impression qui nous donnait une visualisation approximative de la carte de la ville telle qu’elle ressortait à l’issue des simulations. C’était il y a vingt ans, ce n’est pas si loin.} \autocite[154]{Mathieu2014}

Une telle timidité dans les réalisations de modèles de simulations à la charnières des années 1970-1980 est tout à fait compréhensible, les géographes modélisateurs ayant déjà fort à faire avec l'émergence, ou plutot la réémergence de l'idée de système en géographie (avec un pic entre 1979 et 1984 selon \textcite{Orain2001}), un processus réflexif qui s'accompagne d'une formalisation (voir le contenu du Géopoint1984 par exemple) pour parer d'éventuelles dérives dont on sait qu'elles ne manqueront pas d'apparaitre, la systémique étant utilisé plus majoritairement sous sa forme heuristique (diagramme sagitaux), peut être plus facile à pervertir, qu'une systémique à destination algorithmique. Une remarque qu'il faut tout de suite donc relativiser, car la discipline connait malgré tout la parution de très bonne études systémiques heuristiques existe (Durand-Dastès, Auriac, etc.), alors que la rigeur mathématique n'empêche en rien l'apparition de modèles au faisceau d'hypothèses a posteriori très contestables (Forrester?). \textcite{Orain2006}. Enfin, c'est encore sans compter la diffusion d'un nouveau type de modèles de simulations, dont certains  (Wilson, Allen) ne demandent qu'à être appliqués, répliqués sur des jeux de données empiriques dont il faut pour certain encore les construire \Anote{consommateur_data}. Par nouveaux il faut d'abord comprendre \enquote{nouveau support}, car malgré le peu de matière algorithmique ou informatique donné par Peter Allen dans ses publications, il est évident que ce modèle de simulation parcequ'il intègre du spatial, est présuposé à l'expression d'une forme de stochasticité à même de modifier sa dynamique, car on ne connait pas le mécanisme d'ordonnencement dirigeant l'ordre d'intégration des fonctions attribués à chaque zone. \autocite[231-233]{Varenne2014}

Ce sont également des modèles dont la compréhension appelle la maitrise d'un tout nouvel outillage conceptuel et mathématique, découvert par d'autres réseaux (AFCET, club de rome, publications anglo-saxonne, etc.) que ceux proposé directement par le CNRS pour l'informatique et les sciences humaines - on voit bien par exemple que les aspects modélisations pour la simulation sont absent des formations du LISH - . Il semblerait donc que cela soit guidé par l'intuition d'une définition de la complexité plus adapté aux systèmes géographiques qu'une progression s'organise autour de multiples collaborations durant les années 1980, notamment avec des physiciens (Ecole de Prigogine, de Haken) (Pumain2002, Pumain2003).

Cette observation qui semble également valable pour les modèles de simulations anglo-saxons sur cette période. \textcite{Batty2014} ou Wilson, dont on sait qu'ils programment\Anote{batty_code} depuis leurs débuts, ne donne en effet que très rarement à voir les codes sources accompagnant leurs publications, sans que l’on sache si c’est parce qu’il existe un réseau de distribution de codes sources parallèles chez les universitaires anglo-saxons, ou si c’est parce que c’est une information considéré sans intérét pour la publication.

Toujours est-il que les \enquote{pratique scientifique} détaillant l’execution et la modification de codes sources, l'utilisation de logiciels ou de matériels informatiques reste sur cette période 1980-1990 très peu documenté, et ne donne à voir dans la littérature que les projets les plus aboutis. Sur ce point, une interrogation orales des principaux acteurs, similaire à celle réalisé par \textcite{Cuyala2014}, pourrait apporter un nouvel éclairage sur ces aspect plus techniques.

Des publications des géographes modélisateurs de l’équipe PARIS, c’est surtout les problématiques liés au calibrage et plus généralement l’exploration des modèles qui viennent témoigner de la difficulté d’un tel exercice, dans un contexte technique encore très limité pour ce type d’application (\hl{ ref a ajouter 1989 ville et auto-organisation, thèse Léna Sanders, système ville synergetique}). Cela n'empeche pas cette équipe d'appliquer ces modèles sur plusieurs villes françaises, comme en témoigne cette note de bas de pages \autocite[134]{Pumain1984} : \enquote{Des applications de modèles de P. Allen sont en cours dans le cadre d'un contrat PIREN du CNRS sur les agglomérations de Rouen, Nantes, Bordeaux et Strasbourg (par l'équipe P.A.R.I.S) et à l'université Libre de Bruxelles sur les régions hollandaises et l'agglomération de Bruxelles (par P. Allen, G. Engelen et M. Sanglier)}

Si on regarde par exemple l'execution sur la ville de Bordeaux réalisés par Olivier Milan \hl{ajout ref}, on apprend page 14 qu'une fois le modèle de Peter Allen appliqué à des données réelles, cela représente l'évaluation consécutives de 138 équations. Ce qui fait selon Milan, une fois les paramètres estimables à priori enlevés, un total restant de 68 valeurs à ajuster.L'utilisation d'un algorithme de descente de gradient (mono-objectif en fonction de l'erreur quadratique entre variable réelles et simulées) n'ayant pas abouti du fait d'une part du trop grand nombre de paramètres à ajuster, et du fait d'autre part de la sensibilité trop grande de certains paramètres, \enquote{Il a donc fallu ce résoudre à calibrer les paramètres \enquote{manuellement}, par tâtonnements successifs, au prix fastidieux de centaines d'essais, en ajustant à chaque fois certains d'entres eux.}  On est très loin donc du modèle simple comportant 5 paramètres ayant déjà nécessité d'accumuler entre 2012 et 2014 des millions de simulations pour être calibré, ce qui donne un aperçu de la difficulté de cette tâche \autocite{Schmitt2015}.

On s'expose donc à une forme de paradoxe, l'attrait même des géographes pour cette double capacité des systèmes dynamiques à cumuler tout à la fois des comportement éloignés de l'équilibre, tout en restant très sensibles aux conditions initiales devient aussi ce qui empêche toute possibilité d'un calibrage manuel exhaustif.

Les intrications de dynamiques deviennent trop complexe pour être prévisible entièrement, et ce n'est pas parce qu'un paramètre a été choisi dans une équation pour dénoter un phénomène du réel, qu'il se comporte forcément comme celui-ci. Il ne faudrait donc pas confondre ce que l'on attend d'un paramètre seul dans l'expression d'une dynamique, avec les possibilités beaucoup plus complexe d'expression qu'il endosse une fois placé dans un réseau complexe d'interaction (ce qui n'empeche pas les études comparatives, comme celle réalisé par Pumain et son équipe). Ne pas l'étudier, c'est se couper d'une partie de la connaissance que peut nous apporter le modèle, et prendre aussi le risque de donner de fausses interprétations. Quant à la valeur des paramètres, même lorsqu'ils sont fixés à priori par expertise, ceux-ci ne représentent jamais qu'une des expertises possibles, il donc tout aussi intéressant de les faire varier dans une fourchette défini comme acceptable, ne serait ce que pour mieux mesurer la robustesse du système à de telles variations.

Il devient également possible d'obtenir des jeux de valeurs de paramètres exposant des comportements finaux similaires (attracteur), qu'il faut découvrir pour pouvoir donner un poids aux paramètres. Le modèle permet il d'exposer n'importe quel comportement attendu en compensant certaines valeurs de paramètres par d'autres ? Peut on donner la liste de ces compensations ?

Enfin, il faut encore intégrer l'impact que peut avoir cet incertitude contenue dans les données une fois reporté sur l'évolution d'un système. Ce dernier pouvant tout à la fois être robuste (jusqu'à un certain seuil de bifurcation), ou à l'inverse chaotique, ou les deux.

Dernier point, calibrer en se basant sur un seul point de comparaison entre données réelles et données simulés ne peut pas être une fin en soit, car elle ne garantit pas que de la qualité même de la dynamique au cours de la simulation. Des critères restent donc à inventer pour mesurer la qualité de cette dynamique au cours de la simulation, et non pas seulement en sortie.

Pour toutes ces raisons, dont les géographes sont conscients depuis le début, il devient urgent d'investir dans de nouvelles technique de cartographie plus automatique et complète des comportements des modèles : soit on leur posant des questions (auquel ils peuvent ou ne peuvent pas répondre), soit en essayant de les pousser dans l'expression d'une gamme la plus variée possible de leur comportements, stochasticité comprise. Une aide qui permettra au final de situer plus exactement les conditions d'apparitions des attracteurs/répulseurs, et de trouver qu'est ce qui dans les conditions initiales (paramètres, données) induisent (ou n'induisent pas !) de possibles bifurcations.

Si on revient à l'étude des supports incitant les géographes à la programmation Fortran/Dynamo pour la modélisation, quelqu'uns sont bien publiés, contenant pour certains des codes sources ou algorithmes détaillés, comme celui de \textcite{Guermond1984}, \textcite{Dauphine1987}. Toutefois on est en droit de se questionner sur la mise en pratique réelle de ces enseignements informatiques pour la création de nouvelle simulation par les géographes. En effet en dehors de l'application de modèle existants, peu de \textbf{nouveaux} modèles de simulation, c'est à dire conçu et programmé pour aborder de façon spécifique une nouvelle question géographique, semble émerger dans cette période 80-90 en dehors des équipes et des modèles déjà cités.

En comparaison, alors qu'en France sort le livre d'André Dauphiné introduisant Dynamo/Stella aux géographes \autocite{Dauphine1987}, si on regarde ce qui se fait de plus innovant pour la simulation en grande bretagne, par exemple sous la plume d'\textcites{Openshaw1983, Openshaw1988, Openshaw2000}, c'est un fossé immense que l'on apercoit entre ces deux pratiques de l'informatique pour la simulation. A peine quelques années après les articles de Couclelis rapellant l'importance pour les géographes d'utiliser certains formalismes pour la simulation : Automates Cellulaires \autocite{1985}, Intelligence Artificielle \autocite{1986}, voilà déjà qu'Openshaw met en oeuvre dans \enquote{Building an automated modelling system to explore a universe of spatial interaction models.} un système hautement sophistiqué de construction de modèle de simulation appuyé par les dernières innovations en intelligence artificielle (\textit{simulated annealing},\textit{genetic algorithm}, \textit{genetic programming}) pour construire et évaluer des modèles de simulation sur les équipements informatiques les plus puissants de l'époque. Même si ce modèle est critiquable en plusieurs points, il nous aura fallu presque trente ans, une équipe inter-disciplinaire rodée, et l'appui de divers partenaire institutionel pour produire une expertise technique similaire au prototype réalisé par Openshaw en 1988. Il va s'en dire que même au royaume-uni Openshaw et son équipe ont du rester quelque temps pionniers, voire peut être même incompris chez leur contemporains géographes anglais.

Comme déjà dit auparavant, entre le fait que cette communauté de géographe ayant \enquote{mis les mains dans le cambouis} soit en définitive relativement restreinte, l'absence de diffusion de code source dans les publications, et le coût informatique et humain qu'un tel développement de zéro présupose sur le plan de l'expertise conceptuelle (simulation événement discret) et technique pour des langage tel que Fortran, la subtilité d'utilisation de certaines nouvelles techniques mathématiques, il est tout à fait logique de voir ces équipes se diriger vers des collaborations. On comprend mieux aussi dans ce contexte l'appel par Denise Pumain lancé aux géographes quantitativiste lecteur de l'espace géographique lorsqu'elle titre  \enquote{Après l'analyse factorielle, quoi de neuf en géographie ?} \autocite{Pumain1984}

Dans ce contexte, une équipe comme le MTG de Rouen alors dirigé par Guermond choisit une toute autre voie courant des années 1980. A cette période, comme dans la majorité des autres laboratoires, seuls des modèles de simulations relativement simple ont été expérimenté, ici le cas de Rouen \autocite{Guermond1983}. Patrice Langlois témoigne \enquote{Au début des années 1980, on avait déjà organisé à Rouen une École thématique sur la dynamique des systèmes– Peter Haggett et Allen étaient déjà dans l’air du temps – mais, jusque-là, on ne s’était pas lancés dans la programmation de modèles dynamiques.} \autocite{Mathieu2014} \Anote{description_laurini_algo}

Guermond va décider d'embaucher directement dans leur équipes des mathématiciens/informaticiens comme Patrice Langlois. Une décision qui permet de maintenir un certain niveau technique dans le laboratoire, d’introduire de nouvelles compétences comme par exemple la manipulation d'Automates Cellulaires courant des années 1980, tout en assurant une certaine continuité dans l’enseignement de la programmation et des statistiques aux Géographes dans ces universités.

Pour \autocite[320-321]{Cuyala2014} cet événement marque une deuxième phase dans les orientations du petit groupe de géographes quantitativistes francais. Des mathématiques à l'analyse spatiale, ce changement de projet ira va de paire avec l'arrivée d'une génération de géographes. Celle-ci cumule les apprentissages de la génération précédente, et engage le mouvement vers l'auto-suffisance et l'auto-reproduction; ce qui se traduit aussi par une plus grand prise en charge des formations et de leurs thématiques, par exemple dans le cadre des modèle de simulation : Banos, Daude, Thannier, etc. pour les SMA, Josselin pour les réseau de neurones \autocite{Dumolard1994}, etc.

%Il semble bien donc que cela soit par la construction d’une coopération entre géographes et disciplines plus rompu à l’informatique, comme les statistiques, mathématiques ou la physique que se construisent majoritairement en France les nouveaux modèles de simulations.

La découverte en 1990 de tout nouveaux paradigme technologique comme la programmation orienté objet, ou les langages acteurs, supports principaux de la modélisation agents se fait donc soit par le biais d’informaticiens devenus géographes, ou par la mise en oeuvre de collaborations. Celle-ci, riche d’enseignement, ont permis de façon conjointe avec la démocratisation des outils de modélisations de faire évoluer progressivement ce schéma initial finalement assez rigide de collaboration avec les informaticiens, jusqu’à la constitution d’une équipe inter-disciplinaire qualifié non seulement pour raccrocher les tout derniers dévelopement informatiques, mais également capable de proposer ces outils dans un référentiel technique susceptible je pense d'intéresser une partie des géographes modélisateurs.

%Un autre marqueur intéressant peut être soulevé, révélateur d'une époque ou le programme informatique n'a pas encore acquis de valeur patrimoniale, est celui d'une absence totale de stratégie pour la sauvegarde des modèles de simulations ainsi réalisés. Pour un modèle tel que Simpop 1 réalisé au début des années 1990, il est étonnant de retrouver les compte rendu de réunion semaine par semaine parfaitement conservé sous leur formes papiers et éléctroniques, mais aucune version éléctronique archivé, la gestion de cet aspect étant délégué de façon implicite au travail des informaticiens. Comme si la seule vrai valeur du modèle résidait plus dans sa fonction explicative, formalisatrice, permise par sa construction, plutôt que dans sa capacité à produire et reproduire un résultat.

\subsubsection{Le paradoxe du l'usage du HPC dans les années 1980-1990}

Ce que ne dit pas Openshaw dans sa métaphore, c’est que ce transfert de compétence, s’accompagne aussi d’un mouvement de démocratisation des techniques chez les géographes. Si certains géographes anglo-saxons des années 1970 ont pour certains accès à cette double compétence informatique dans leur formations (Batty par exemple programme depuis ses débuts, les différents pionniers américains Marble, Pitts, etc.), ce n’est par exemple pas le cas en France en Géographie, ou une phase d’acquisition des techniques informatiques, mathématiques et statistiques constitue un préalable à une plus large diffusion de la toute nouvelle géographie quantitative. L’apprentissage de l’informatique parait effectivement nécessaire pour dérouler les programmes informatiques révolutionnaires venu des Etat-Unis \autocite[150,127]{Cuyala2014}, mais également pour executer les nouvelles statistiques uni et multivarié, quand ce n’est pas tout simplement pour manipuler les entrées / sorties d’un ordinateur. Information difficile à trouver sans réaliser un travail historiographique important, la thèse de \textcite{Cuyala2014} nous donne quand même quelques informations sur le bagage techniques de certaines des premiers géographes quantitativistes francais des années 1960-70 : Sylvie Rimbert (formation au Fortran à Ottawa p137) ;  Cosinschi, Racine, Lemay, Cavalier (formation Fortran à Ottawa p150), Pumain ( Ottawa p153), B.Marchand(p154), Durand-Dastès (p313) Dumolard (p ), Sanders (), et surement d’autres du fait de leur profils hybride, ou de leur attirances pour une certaine rigueurs mathématique (p157-158).

L’émergence dans les années 70-80 de nombreux packages logiciels \autocite[444]{Joliveau2004}, puis la diffusion de la micro-informatique et de logiciels plus évolués sur le plan graphique et de l’interaction homme-machine courant des années 1980 a rendu complétement obsolète l’usage des centres de calculs pour l’usage statistique et cartographique \autocites{Joliveau2004, Waniez2010}.

\textcite{Lecarpentier1983} expose dans les \textit{Annales de Géographie} une première description technique du \textit{micro-ordinateur} et de ses avantages, une argumentation justifier par les résultats d'une enquete réalisé en 1981 \enquote{ Une enquête récente menée auprès de géographes universitaires français a montré que le recours au centre de calcul reste le
moyen le plus répandu mais que utilisation du mini- ou du micro-ordinateur en est souvent complémentaire. Les grandes universités (Paris, Montpellier,  Grenoble, Strasbourg) utilisent surtout ou exclusivement leur centre de calcul et apparaissent sous-équipées en matériels spécifiques. Les micro-ordinateurs se diffusent rapidement un peu partout mais leur généralisation est gênée par absence de logiciels adaptés aux besoins des géographes Quelques équipes Besan on Lille Rouen ont entrepris la réalisation de logiciels de ce genre [...] }

En 1984, à l'occasion du Congrès International de Géographier de tenant pour la première fois tenu  à Paris, \textcite{Faugieres1984} cite dans l'ouvrage de synthèse \textit{La recherche géographique Française} une enquête du CNRS daté de 1982. Il en tire les éléments suivants : sur 162 formations en géographie, ou dirigé par des géographes, 31 seulement ont pu founir des indications sur l'utilisation de moyens informatiques, 8 départements de géographie n'ayant aucune activité informatique. Sur ces 31, l'utilisation se répartit de la façon suivante, 9 utilisent des centres de calculs, 12 ont à leur disposition des terminaux, 2 ont accès à des \enquote{mini}, et 13 ont acquis un ou plusieurs micro-ordinateurs (23 au total).

L'\enquote{esprit micro}, c'est l'occasion pour les utilisateurs de réinventer les règles du jeu \Anote{esprit_micro_jeu}, de se regrouper en club d'utilisateur \hl{ref lish micro}, d'entamer la création ou la migration de logiciels des centres de calculs vers les postes micro, etc ...

Comme témoigne Philippe Cibois (Interview avril 2015), \enquote{La micro-informatique a fait perdre au Lish sa raison d'être de centre de calcul : nous avons formé des chercheurs à la micro-informatique et ils sont devenus autonomes}. Un discours que l’on retrouve aussi dans les mots de Michaël Hainsworth, directeur du LISH en 1983; avec l’expression d’une nouvelle orientation de structure pour le service, qui bien qu’elle continue à offrir un double service sur les petits et gros équipements, semble assez différente de celle dicté par Cibois en 1975 : \enquote{Depuis janvier 1983, le Centre de Calcul du LISH a changé de direction. Ce changement de direction ne signifie pas tant le remplacement d'un homme par un autre, mais la mise en place d'une structure différente. Il s'agit désormais pour nous d'être à la fois une vitrine de l'informatique et une station service pour les chercheurs en Sciences Humaines et Sociales. [...] Point d'intersection de cette structure modulaire, lieu de rencontre, de débats, d'enseignement, le LISH devrait aussi offrir, ce qu'il offre déjà partiellement, un service de programmation. Il est en effet hors de question de former tous les chercheurs qui ont besoin de l'informatique, surtout si ceux-ci ont des besoins très réduits. [...] On pourrait croire à la lecture de ces lignes que nous souhaitons réaliser un service très centralisé et la nature exceptionnelle du Centre de Calcul du LISH, unique dans ses prestations destinées à un public aussi particulier que celui des sciences humaines, pourrait sembler aller dans ce sens. Nous sommes, au contraire pour une informatique répartie dans les laboratoires, sur les lieux où se trouve l'information, dans un grand réseau où chacun se retrouve lié avec tous les autres. Nous ne souhaitons constituer qu'un lieu de passage dans lequel les chercheurs du CNRS et leurs homologues universitaires ou étrangers puissent trouver l'information dont ils ont besoin ou celle qui leur fait défaut.}

Parmis les nombreux facteurs intervenant en faveur de cette démocratisation de la micro-informatique, on remarquera que si le calcul devient certe moins rapide sur micro-ordinateur (les centres de calcul possèdent toujours des machines beaucoup plus puissante !), ces contraintes est largement compensé par la disparition des quota d'utilisation, et surtout de \textbf{la facturation des temps de calculs jusqu’à alors imputé par le CNRS à ses propres chercheurs ...}

Il n’y a pas je crois de travail de synthèse existant permettant d'acter en géographie la “possible” expression de ce désengagement des géographes dans la formation en “programmation”, et la façon dont elle pourrait se traduire à la fois dans les enseignements et son impact sur les projets dans les laboratoires de recherche plus spécialisé dans la modélisation. Car cette démocratisation de l’outil, si elle permet le passage  plutot \enquote{heureux} de certaine de ces techniques dans le vocabulaire courant du géographe (AFC; ACP; SIG; etc.) \autocite{Pumain2002} elle dessert également une autre vision de l'informatique, celle de l'informatique \enquote{boite-noire}. En effet qu’advient t il de la programmation comme activité “créative” dès lors que son autre fonction principale définissant l’apprentissage de celle-ci comme un \enquote{passage obligé vers d’autres applications} disparait au profit de logiciel plus simple à utiliser ? \textcite[4]{LeBerre1987} cite ainsi à propos de sa formation à l’informatique opéré dans un contrat entre la DGRST et le groupe Dupont, \enquote{il m’a fait refuser l’utilisation de l’informatique presse bouton, dangereuse pour le travail scientifique, et qui malheureusement se répand avec la diffusion des micro-ordinateurs}

Ce mouvement est peut être en train de s'inverser avec les efforts de la génération de modélisateurs précédentes, et l'avénement de support plus accessible pour la modélisation. Si ce combat n'est pas encore gagné, un autre nous attend déjà, il s'agit maintenant de ré-engager les géographes modélisateurs à utiliser la puissance mise à disposition par le HPC.

\begin{table}[!htbp]
\begin{sidecaption}[fortoc]{Classement basé sur la brochure pour les 20 ans du centre, ainsi que les dernières informations disponible sur le site de l'IDRIS}
	[tab:prankingIDRIS]
	\centering
	\begin{tabular}{@{}llllll@{}}
\toprule
\multicolumn{6}{c}{A l'IDRIS (1993 - maintenant)} \\ \midrule
Durée & Identifiant machine & Nom & Type & Processeurs & Puissance \\ \midrule
1993-1999 & Vectoriel Cray C98 & Atlas & vectoriel & 8 & 7,6 GFlops \\
1994-1999 & Vectoriel Cray C94 & Axis & vectoriel & 4 & 3,8 GFlops \\
1995-1997 & Cray T3D & Kaos & processeurs & 128 & 19,2 GFlops \\
1996-2002 & Cray T3E & Aleph & processeurs & 256 & 153,6 GFlops \\
1999-2006 & NEC & Uqbar & vectoriel & 38 & 304 GFlops \\
2002-2008 & IBM power 4 & Zahir & processeurs & 1024 & 6,5 TFlops \\
2008-2012 & IBM Blue Gene & Babel & processeurs & 40960 & 139 TFlops \\
2013 & IBM x3750 & Ada & coeurs & 10624 & 230 TFlops \\
2013 & IBM Blue Gene & Turing & coeurs & 98304 & 1258 TFlops \\ \bottomrule
\end{tabular}
\end{sidecaption}
\end{table}

Dans ce contexte d'abandon progressif des usages des centre de calculs dans la fin des années 1980, l'équipe de modélisateurs de géographie cités, bien consciente des problématiques de calibration décrites chez les anglo-saxon \autocite{Batty1976} s'est penché sur les techniques et surtout l'expertise disponible en France. Il y a donc eu des tentatives d'utilisation des centres de calculs pour la calibration de modèles, par exemple celui de Peter Allen, tentatives qui n'ont malheureusement pas abouti à l'époque, probablement du fait d'un argument qu'à également énoncé Openshaw en 88 : La puissance de calcul et les méthodes associés (les techniques multi-objectif progressent par exemple surtout à partir des années 1990 ! voir section \ref{sssec:historique_EA}) alors disponible n'était vraiment pas suffisante pour ce type d'application.

Cela dit, trente ans plus tard, pourquoi le modèle de Peter Allen n'a toujours pas été réévalué à la lumière de nouvelles techniques d'analyse dont l'évolution s'est calé sur une puissance informatique disponible presque mille fois plus importante qu'au début des années 1990 ?

Pour donner un ordre d'idée sur l'évolution des moyens de calcul disponible au CNRS de 1990 à 2015 on peut regarder l'évolution des supercalculateurs de l'IDRIS (ex-CIRCE fondé en 1993) dans le tableau \ref{tab:prankingIDRIS} : entre les deux calculateurs vectoriels Cray disponible en 93/94, le premier bond réalisé en 1996 par le passage à un autre type d'architecture parallèle, et la puissance aujourd'hui disponible sur les calculateurs Ada et Turing, on est passé du GFlops au PFlops (1 PFlops = 1000000 GFlops). On verra que ce n'est pas la seule révolution des années 1990, et que la présence de cette puissance à disposition, même si elle devrait être un motif suffisant pour amener les géographes à imaginer de possibles applications, doit encore être complété pour sa démocratisation par un mode d'accès plus adapté aux usages réels des modélisateurs.

Cette remarque est également valable pour des modèles de simulation plus récents, comme le modèle Luti MobiSim développé par l'équipe Théma. Déjà famillière du mésocentre de calcul de Franche-Comté, il est pourtant indiqué dans la thèse de \textcite{Hirtzel2015} soutenue en février 2015 une limite dans l'utilisation possible des infrastructures disponibles, les simulations s'executant seulement sur des machines de 12 coeurs (5h30 par simulation) ou 16 coeurs (6h30 par simulation). 8 machines ($ x 12$ ou $ x 16$ donc) permettait de faire des simulations en parallèles pour un total de 25 jours de calcul, 790 simulations executés équivalent à 60000 heures de calculs cumulés pour l'analyse de sensibilité présenté. Hirtzel évoque dans sa thèse l'impact important qu'une telle contrainte a eu dans la préparation, la plannification, et la simplification de cette analyse de sensibilité assez complexe devant évaluer l'interaction entre 79 paramètres. L'effort est déjà impressionant, mais pourquoi se contenter de cette infrastructure et ne pas directement taper à la porte des physiciens ou des biologistes disposant d'équipements nationalisés coutant plusieurs dizaine de millions d'euros chacun ? La question semble évidemment naive tant la présence des sciences humaines dans les projets d'avenir concernant le calcul intensif a été évacué.



T : Toutes les conditions sont réunies pour expérimenter ce retour au HPC dans le cadre des modélisations à Geographie Cités, et les challenges ont étés ciblés dans le cadre des systèmes complexes depuis bien longtemps. \hl{(Co construction des savoirs, faut il en parler avant ?)}

\subsubsection{Tournant des années 1990 dans le HPC}
\label{sssec:Tournant1980}

La plupart des informations historiques et techniques proposés ici sont tirés des ouvrages suivants \autocites{Fox1994, Fox1988, Seitz1985, CM2-1990, Lerman1993, Padua2011, Dietrich1984}[81-84]{Culler1998, Steele2011} ainsi que des \textit{Working Report} suivant \autocites{Athas1987, Su1987, Seitz1983, Seitz1984a, Seitz1984b} dont la liste plus complète est disponible sur le \href{http://authors.library.caltech.edu/view/person-az/Seitz-C-L.html}{@site} des archives de Caltech.

Entre les années 1960-1980 on peut encore parler d'une phase de recherches et d'expérimentation, avec la création et l'amélioration des premiers \textit{supercomputers} à processeurs vectoriels, particulièrement adapté au calcul scientifique rattaché généralement à cette époque à l'architecture SIMD \Anote{simd_def} qui va ensuite dominer le marché du HPC jusqu'en 1990. C'est le cas plus particulièrement de ceux qui vont germer dans l'esprit de Seymour Cray : Le CDC 6600 premier processeur multi-coeur scalaire, le CDC Star-100 premier processeur vectoriel (1974), l'Illiac III puis IV (1965, abandonné en 1975) considéré comme le premier d'architecture SIMD \autocite{Muraoka2012}, et enfin le mythique Cray-1 de 1976 à  80 MHz. Mais il y'a bien eu d'autre projets similaires durant ces années d'expérimentations comme cite \textcite[387-388]{Steele2011} : Solomon computer (1960's), Texas ASC (1966), Goodyear Staran ( SIMD array processor, 1972) , ICL Dap (SIMD array processor, 1972 papier, 1974 prototype, 1979 commercialisation) , % Goodyear MPP plus tard, basé sur staran.

%Sur une autre branche, les MIMD, de nombreuses expérimentations concernant les architectures multi-processeurs, et la gestion du parallélisme ont également lieu : D825 (1962), CDC 6500 (1966), Multics system (1969)

Sur la base de ces premiers super ordinateurs, très couteux, des années pre-1980, le domaine naissant du HPC va connaitre un certain nombre de révolutions entre 1980 et 1990.

Entre autres innovation technologiques, l'introduction progressive des microprocesseur depuis les années 1970 va permettre d'aller beaucoup plus loin dans l'invention fin 1970, début des années 80 de nouveaux paradigmes de construction. %Le parallèlisme peut être pensé autrement, et s'appuyé sur la baisse des couts qui frappe l'industrie informatique dans les années 1980.

Danny Hillis publie dès 1981 un mémo au MIT donnant les concepts d'une nouvelle machine massivement parallèle nommé “Connection Machine (CM), qui va amener un certain renouveau dans la façon de penser ce type d'architecture SIMD.

Ce dernier travaille d'abord avec Minsky et Papert au laboratoire MIT sur le langage LOGO, avant de créer une société en 1983 \textit{Thinking Machine} commercialisant une série de CM (Connection Machine) d’architecture SIMD (puis MIMD), dont le premier prototype est présenté à la DARPA en 1985, et la première livraison commerciale en 1986. Ces machines constituent une référence historique initiale importante dans la construction de machines composé de plusieurs milliers de processeurs. En misant sur des processeurs plus simple, mais plus nombreux, réparti en noeud (4096 noeud chacun comportant 16 processeurs, ce qui fait au total jusqu'à 65536 processeurs, non vectoriel, très simple de 1 bit / 4kbit mémoire pour la CM-1, ce qui en fait un SIMD à mémoire distribué). Pour cela il repense la façon dont les processeurs sont ammenés à communiquer entre eux via des routeurs distribuant les messages de façon très efficiente sur un réseau spécialisé. Appuyé sur une topologie de connexion en forme hypercube (12-dimensions, $2^12 = 4096$ noeud) intégrant naturellement une réprésentation sous forme de grille de n-dimensions (pratique pour la simulation 2D et 3D), alors classique de cette époque, de multiple moyens de communications entre processeurs sont mis à disposition des utilisateurs. Le premier donne accès à une communication aux processeurs voisins (16 sur chaque noeud) de façon directe sans passer par le système de routing (système North/East/West/South NEWS), et le deuxième permet d'accéder à tout les autres processeurs via les routeurs implémenté sur chaque noeud (un pour 16 processeurs). Un système de processeurs virtuel permet de s'abstraire du nombre de processeurs physique, ce qui rend les programmes opérant sur cette machine fonctionnel quelque soit la configuration existante (en plus de temps si le nombre de processeurs est moindre que le nombre virtuel choisi par l'utilisateur). Ces techniques permettent de se focaliser non plus sur la puissance des processeurs (cf l'unique processeur vectoriel du Cray-1 par exemple) mais sur la flexibilité et l'efficience des communications entre ceux-ci.

Sachant que David Hillis s'est largement inspiré des travaux de Minsky et de son livre \textit{Society of Mind}, cette architecture n'est pas sans évoqué l'image idéalisé du fonctionnement d'un cerveau tel qu'on se la représente à l'époque en IA symbolique, les CM's ciblant d'ailleurs résolument ce type d'application avec l'implémentation machine d'une version spécialisé de lisp, le *lisp (star lisp). Ainsi même si ce type d’architecture SIMD (dominante sur le marché des HPC jusqu’au années 1990) est généralement plus complexe à appréhender, la CM a par exemple été utilisé très tot pour développer des applications rattachés aux systèmes complexes dans le cadre de l’Artificial Life et des CAS, car elle s’appuie dans sa construction (voir la participation de Feynamn, et de Wolfram sur l'architecture de la CM) sur la propriété de parallélisme propres aux automates cellulaires (1 processeur = 1 cellule/patch !) \Anote{CA_physical}. Les CM proposent certe une alternative intéressante par cette possibilité augmenté de parallélisme, mais elle brille aussi par leur performance, la CM-1 (prototype) de 1985-1986 , la CM-2 (commercialisation) de 1986-1987 (2.5 à 6 GFlops), la CM-5 (qui passe d'une architecture SIMD à MIMD, avec 1024 processeurs) est numéro 1 du premier \href{http://www.top500.org/featured/systems/cm-5-los-alamos-national-lab/}{@Top500} de 1993 avec 59,7 GFlops sur le Linpack test. En comparaison, à l'IDRIS en 93, le Cray C98 Axis a 8 processeurs dépasse à peine les 7 GFlops.

Des travaux similaires ont également lieu depuis les années 1970 dans l’équipe de physiciens de Toffoli (en contact aussi avec les travaux du francais Yves Pommeau) au MIT, qui finissent par concevoir courant des années 1980 un hardware spécifique (CAM-6, CAM-8) pour paralléliser les CA en s’appuyant sur leurs propriétés fondamentales \Anote{ca_simd_avantage}.

En France, il existe également des développement similaires en physique, avec la famille de machine R.A.P (Réseau d’Automates Cellulaires) démarré en 1986 à Paris, OUPPY à Marseille\Anote{ouppy_marseille}. Ces deux architectures inspirés par les CA; issue de travaux démarré aux MIT (Hillis1983, Toffoli1987), sont également utilisés pour des applications scientifiques similaire (Epez1993, Toffoli2005 ). L’équipe de l’UCLA de David Jefferson et ses modélisations de fourmillières (Tracker, Genesys, AntFarm) utilisent une CM contenant pour la premières fois plusieurs milliers de fourmis, un préalable pour observer des comportements auto-organisationel. Ou encore la première version parallélisé du langage LOGO qui sera développé sur ce type de machine CM avant de passer plus tard sur macintosh (Starlogo 1991). Au Los Alamos National Lab (LANL) Langton recrute Hiebeler\Anote{hiebeler_parcours} en 1989 pour travailler sur le logiciel \textit{CellSim} initié en 1988 et mis à jour jusqu’en 1990; Hiebeler possède une expertise sur les CAM-6, mais également sur les CM car il a travaillé à Thinking Machine entre 1991 et 1992, ce qui lui permet de développer une interface sur ce logiciel vers des machines de type CM. Ces deux types de hardwares seront voués à un échec commercial, pour des raisons diverses qui ne touche pas forcément au seul aspect scientifiques, car ces outils ont constitué de réelles avancées d’un point de vue de la computation scientifique aux moments ou elles étaient disponible.

Dans l'avalanche d'innovations ayant lieu dans le courant des années 1970-80 au niveau du software et du hardware (topologie hypercube, vlsi et microprocesseur, baisse des couts de production, langage acteur, unix, etc. ), le paradigme de construction basé sur l'architecture MIMD connait lui aussi un certain rafraichissement au tout début des années 1980, bien avant la livraison de la première Connection Machine CM-1.

C'est le cas par exemple du prototype \textit{Caltech Cosmic Cube} dont la première version fonctionnelle est finalisé en 1983. Ce dernier est une architecture MIMD à mémoire distribué \Anote{mimd_def}, réparti sur 64 noeud un système contenant : un micro-processeur intel 8086, un coprocesseur 8087 pour les flottants, une mémoire propre de 128K, et 6 canaux 2Mbits/s pour l'envoi de message selon une pile FIFO (\textit{First In First Out} ) aux autre noeuds dans une topologie hypercube. Chaque noeud ne possède pas encore de véritable système d'exploitation propre (128k étant trop limite pour envisager la charge d'un OS complet de type Unix), mais un ou plusieurs logiciels de routages qui prennent en charge la gestion des messages entre les noeuds, regroupé sous le nom de \textit{CrOS}. L'avantage d'une telle architecture est évidente, car elle pousse le constructeur à s'abstraire toujours un peu plus de la partie matérielle; en effet en passant d'une gestion de message entre processeurs purement éléctronique à une gestion purement logicielle, on peut envisager de changer plus facilement certains composants du système tout en conservant un software d'échange de message \enquote{assez} similaire. Ainsi caché derrière cet acronyme \textit{CrOS}, la librairie de fonctions de transfert de message va très vite s'étoffer et s'améliorer pour se diriger vers un système \textit{loosely synchronous} \Anote{loosely_sync} de plus en plus indépendant de la machine.  %La programmation de ce type de parallélisme reste toutefois beaucoup plus difficile à gérer que les systèmes SIMD des années 1980 disposant à leur sortie de langages aux primitives adaptés pour l'application aisé de ce type de parallélisme.

En un sens ce choix d'une gestion logicielle donne lieu à plusieurs logiciels, soit repris et amélioré à partir du premier CrOS du Cosmic Cube ( NX-$n$ de Intel , PSE de NCube , CrOS-$n$ de Caltech) ou de conception propre à d'autres formes spécifiques d'architecture MIMD à mémoire distribué (Ibm EUI, Meiko CS, TMC CMMD, etc.). Evidemment incompatible entre eux, cette variété préfigure l'expression chez les utilisateurs d'un besoin rapide pour la constitution rapide d'un standard/norme d'échange de message implanté de façon transparente sur chaque machine : MPI. Cette machine universitaire plusieurs fois amélioré de façon interne (Mark II (1984)/ Mark III (1986)) aggrège autour d'elle plusieurs dizaines de programmes de recherches en tout genres, pour son utilisation ou son amélioration. Cette machine va également être décliné dans des version commerciales plus ou moins différentes (meilleur processeurs, mémoire augmenté, ajout de composants pour traitement spécifique, utilisation d'Ethernet pour les échanges de messages, nombres de noeuds, logiciels pour la gestion des messages différents, etc.) : nCUBE, Ametek, Intel iPSC, FPS T-Series, Paralex Gemini.

Autre avantage de cette machine, pour 1\% prix du Cray-1, elle produit 10\% performance, qui prouve pour la première fois la faisabilité et les bons résultats d'une architecture parallèle utilisant des composants standard à moindre couts. L'intel 8086 étant le premier micro-processeur de la famille x86 que l'on va retrouver dans une variante sur les IBM PC vendu dès 1981 à plusieurs millions d'exemplaires. %Sur la base d'une architecture MIMD à mémoire distribué la communication entre processeurs se fait sur la base d'une topologie en hypercube, avec un protocole d'échange de messages entre processeurs qui n'est plus piloté de façon physique par des routeurs, largement inspiré par la logique acteur de Hewitt.

On pourrait également citer par exemple les travaux sur le NYU Ultracomputer, de David E. Shaw sur la machine “Non-Von” (pour “Non Von Neumann”) de la Colombia University, le Goodyear Massively Parallel Processor (MPP), et probablement d'autres moins connus. Toujours est il que ces différentes initiatives sont toutes reconnu au début des années 1980 comme étant des pionnieres dans la construction d’architecture \textbf{massivement parallèle}, c'est à dire mettant en oeuvre au moins plusieurs centaines de processeurs.

Touché avec l'apparition ou la maturation d'une véritable grappe de technologies durant les années 1980-1992 qu'émerge la notion moderne de cluster : les protocoles d'échanges de messages PVM puis MPI décrit ci dessous, mais également Ethernet, le développement d'Internet, la miniaturisation constante et l'augmentation des fréquences d'horloges des processeurs, l'apparition de logiciel et de matériels libre, etc. que des projets émerge pour définir la notion moderne de \textit{cluster}.  Ainsi cet esprit de réutilisation de \enquote{matériel standard} déjà expérimenté par l'équipe de Berkeley sur le \textit{Cosmic Cube} trouve son apogée dans l'invention plus ou moins simultanée de projet de mise en grappe de micro-ordinateurs dans les années 1990, en s'appuyant non plus cette fois ci sur une architecture (hardware) spécialisé reliant les différents composants (processeurs), mais sur leur simple mise en réseau appuyé par des applications (software) de partage et d'échange de données efficient entre les machines.

%Cluster p292 / le projet NOW de Berkeley (NOW emphasized high-end workstations, high performance networks, and proprietary software) / et le projet Beowulf de la Nasa (Beowulf emphasized low cost personal computers, mass market networks (Ethernet), and the new trend in open source software including the Gnu editors and compilers and the Linux operating system)

Mais en dehors de ces applications et de ces architectures pionnières, d’un point de vue utilisateur la révolution opère tient surtout de l’émergence et de la mise à disposition d’un plus large public de protocole de communication permettant de s’abstraire complétement de l’infrastructure qui supporte physiquement le parallèlisme, que celui-ci opère sur un \textit{cluster}, ou sur des ordinateurs distants reliés par internet, ou un mélange des deux. Ce type de norme a ainsi permis le développement plus aisé de programme distribué de façon massivement parallèle, sans avoir à se soucier du langage à adopter, et de l’ensemble des contraintes techniques propre à chacune des technologies supportant ce parallèlisme. Par exemple, la norme MPI (\textit{Message Passing Interface}) est devenu depuis sa création en 1992-1994 le modèle quasi-dominant implémenté encore aujourd’hui sur l’ensemble du matériels dédiés à une utilisation partagé.

Ainsi, ces différentes visions d'une infrastructures support du parallélisme peuvent cohabiter et soulager l'utilisateur de la contrainte lié au hardware. Il est possible aujourd'hui de programmer un logiciel qui marche tout à la fois sur son propre \textit{cluster} personnel construit à base d'Arduino/Raspberry/Edison, que sur des architectures techniques plus complexes mélangeant différentes techniques ou paradigme choisi pour une utilisation adéquate. Ainsi, les supercalculateurs d'aujourd'hui sont souvent eux même des formes de clusters, à la fois héritier des premier prototypes expérimentant des combinaisons de hardwares et softwares innovants, mais également héritier de toutes les avancées liés à la démocratisation du matériel/software informatique.

Le calculateur Turing de l'IDRIS \Anote{idris} est une architecture massivement parallèle (MPP de type \textit{MIMD distributed memory} \autocite{Snir2011}) cumulant de très nombreux noeud de calcul (6.144 noeuds de 16 coeurs à 1 Go de mémoire par coeur) pouvant être connecté selon de multiples topologies, et peut être accédé comme une seule et même machine (jusqu'à 65.536 coeurs simultanés autorisés en exécution). En comparaison, le \textit{cluster} ADA dispose d'une mise en réseau locale très rapide de plusieurs centaines de noeuds de calculs séparés : 332 noeud d'architecture type \textit{Symmetric MultiProcessor} SMP) qui mélange deux type d'architectures : \textit{MIMD shared memory} sur le SMP et \textit{MIMD distributed memory} pour assurer la communication entre les SMP. Chaque noeud dispose d'un ensemble plus de processeurs multi-coeurs (32 coeurs) avec une mémoire partagé plus importante (4 à 8 go par coeur), constituant ainsi au final une architecture multi-processeur destiné à des calculs plus communs (jusqu'à 2048 coeurs autorisés en exploitation) sur Turing, tout en restant très facilement extensible via l'ajout de nouveau noeud.

Comme on peut le voir dans les quelques paragraphes ci-dessus, le jargon technique (SIMD, MIMD-SH, MIMD-DM, DMM, SMP, etc.) \Anote{exemple_simd_mimd} qui accompagne l'environnement du HPC est un préalable à l'utilisation \textbf{probablement rédhibitoire pour qui s'intéresse au HPC sans être du domaine}. Si l'émergence, l'amélioration successive, et le maintien ces 20 dernières années de la norme MPI comme un standard indétronable dans l'industrie du HPC, a certe beaucoup simplifié les prérequis nécessaire pour user de ces ressources (voir les raisons déjà cités), il ne faut pas oublier que la compréhension des différences opérant entre ces ressources informatiques reste un plus pour mieux les choisir et mieux programmer.

\hl{reintegrer avec le reste de HPC}
Mais, a t on besoin de tout ces détails ? Comme le dit si bien Openshaw (p6-7) “We really do not need a whole lot of unnecessary details from computer science and computer engineering. Why not simply take it for granted and concentrate on using the technology? [...] Geographers should be viewing parallel computing as a tool that can be used on their problems; there is absolutely no need to know more than about 0.01 percent of the technical details of how the hardware actually works”



Malgré des efforts conséquents des géographes de l'école de Leeds \autocite{Openshaw2000} pour introduire le HPC et la norme MPI aux géographes dans les années 1990-2000, on ne peut pas vraiment dire que cette pratique se soit diffusé outre-manche ces 15 dernières années (et peut-être même en dehors d'un cercle très restreint de géographe informaticien de Leeds). Faut il persévérer dans cette voie, ou existe il aujourd'hui des solutions alternatives pour amener le HPC jusqu'au bout de doigts des géographes majoritairement non programmeur ?

--

Il existe effectivement différents niveaux de granularité envisageable quant on parle de paralléliser des applications :

- le parallélisme peut être envisagé de façon interne au niveau des primitives
- le parallélisme peut être envisagé de façon interne au niveau des objets
- le parallélisme peut être externe au programme

Les niveaux ne sont pas exclusifs, et une application peut très bien cumulé ces différents niveaux.



Enfin, toujours dans la diversité des types d'architectures disponible dans le domaine du HPC, nous nous intéresserons principalement par la suite aux \textbf{grilles de calculs}, pour plusieurs raisons :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
\item La finesse de parallélisme engagé dans cette architecture est largement suffisante pour engager une première étape dans la voie du HPC immédiatement bénéfique pour la simulation en géographie,
\item On a accès à une ressource globale masquant l'hétérogénéité du parc de machines qu'elle représente,
\item Des logiciels permettent de s'abstraire de la couche de programmation MPI normalement nécessaire pour accéder à ce type de ressources HPC,
\item Les modalités d'accès à cette ressource sont plus compatible avec les pratiques réelles des modélisateurs
\end{enumerate}

le plus haut niveau de parallélisme envisageable, permettent de partager des executions de calculs sur des ressources complétement hétérogènes distribuées un peu partout sur la planète en communiquant avec elle via Internet. Cette ressource ne fournis pas de mémoire partagé et/ou de mécanisme de partage de données entre les différents noeuds, considéré comme autonome, ce qui limite leur utilisations dans le cadre de certaines applications nécessitant à la fois beaucoup de mémoire, et une interaction fortes entre les différents noeuds de calculs : c'est le cas par exemple des astrophysiciens travaillant sur des modèles de simulations de l'univers composé de plusieurs milliard d'étoiles en interaction.

%MPP p574

%Distributed Memory = Chaque processeur a sa mémoire propre, et peux accéder à la mémoire des autres via une forme de connection (réseau local, réseau spécialisé, bus spécifique, etc.)

%Blue Gene L = Distributed Memory MIMD computer

\subsection{Replacer le HPC dans l'évolution des pratiques à Géographie-Cités}

%Note : Montre le fait que HPC est un probleme qui n'est pas nouveau chez les géographes, et que certains avait déjà préssenti son utilité, pas que pour la modélisation : openshaw HPC

Du coté de l'utilisation des ressources HPC existantes habituellement à disposition des sciences physique et mathématique, un dossier administratif indiquant la nature du projet scientifique et le temps de calcul moyen doit être déposé plusieurs mois à l'avance, avant d'être approuvé. Là encore, nous sommes très éloignés des pratiques, mais également des besoins en SHS.
Si on regarde à l'inverse du coté des sciences physique, ce type d'accès est en train de se normaliser, et devient accessible dans le cadre de formations. La \enquote{Maison de la Simulation} dispose par exemple pour les travaux pratiques de ces formations d'un accès dédiés au ressources HPC disponibles sur le campus du plateau de Saclay.

Le Grid Computing, même si il offre des limites évidentes pour certains usages (bande passante limité, mémoire sur chaque noeud limité, communication entre les noeuds impossible, etc.) s'avère en revanche très intéressante par la flexibilité d'emploi qu'elle permet.

Ces modalités d'accès sont donc beaucoup plus intéressante du point de vue d'un modélisateur dont l'activité de construction nécessite d'évaluer très régulièrement ses modèles de simulation par l'usage de techniques diverses, toutes relativement exigente sur les aspects computationels : analyse de sensibilité, plan d'expérience divers, usage d'algorithmes d'optimisation pour la calibration ou l'exploration des comportements du modèles, etc.

L'accès administratif est géré par des organisations virtuelles délivrant des certificats d'accès à la ressource d'un an renouvelable. La ressource est donc disponible de façon illimité et relativement immédiate (moyennant la rapidité de transferts des programmes sur la ressource distance, et la charge existante de la grille auquelle on accède) dès lors qu'on possède un de ces certificat.

De multiples équipements informatiques implenté et accessible à des mailles géographiques différentes, d'utilisation pour la production ou la recherche, sont réunis et accessible aux chercheurs par le biais de divers portails administratifs. Apellé Organisation Virtuelle (\textit{Virtual Organization}), ces entités administratives virtuelles régissent l'attribution des ressources d'une grille en fonction des différents accords passés entre acteurs intervenant dans la constitution de la grille, et cela selon divers critères qui peuvent être d'ordre thématique, technique, de zones géographiques, etc.

En France c'est depuis 2010 le GIS France-Grilles (vo.france-grille.fr) qui mène cette mission de mutualisation entre les différents acteurs public (Laboratoires, Méso-Centres, Centres nationaux) et privé au niveau national (Initiative Nationale de Grille ou NGI), mais aussi européen en étant le partenaire et représentant francais de l'EGI (European Grid Infrastructure). Pilotés par des institutions scientifiques majeures (MESR, CNRS, CEA, INRA, INRIA, INSERM, CPU, RENATER), ce GIS coordone la mise en place d'une grille de calcul nationale a priori accessible à tout les disciplines scientifiques par le biais de différentes VO. D'après les statistiques 2012, sur 700 utilisateurs regroupés en 89 VO rattachés à la VO france grille, seulement 12 utilisateurs sont enregistrés dans la catégorie systèmes complexes, et 0 pour les sciences sociales. Sur 1450 référencements entre 2010 et 2013, la collection HAL maintenue par France-Grilles pointant les publications utilisateurs de cette grille nationale, il y'a 1 seule publication référencé en science de l'homme et de la société. Aujourd'hui il y'en environ une dizaine de référencé, provenant en majorité des géographes.

Dans le cadre des SHS en France, il y a également la mise à disposition de ressources informatique par la collaboration au CNRS entre le \href{http://cc.in2p3.fr/}{@CC-IN2P3} et l'ancien TGE Adonis devenu TGIR \href{http://www.huma-num.fr/}{@Huma-Num}. Human-Num offre une liste de service à disposition des laboratoires, dans lequel figure un accès à des ressources informatiques de type grille de calcul par le biais de l'IN2P3, accessible via la technologie Grid-Engine.

https://hal.archives-ouvertes.fr/FRANCE-GRILLES
https://hal.archives-ouvertes.fr/FRANCE-GRILLES/search/index/q/%2A/domain_t/shs/
GRID2-FR

Des formations sont évidemment ouvertes dans ces différents instituts, mais non seulement ces organisations ne sont pas connus de la plupart des géographes, et de plus le niveau technique nécessaire à la compréhension de ces formations est trop complexe et trop éloigné du domaine de compétence des géographes pour que ceux ci s'y présentent spontanément.

Autrement dit, et sans cette formation, une fois l'utilisateur connecté à une grille de calcul, que le curseur du terminal informatique clignote dans le vide, quels sont ensuite les manipulations, les commandes à connaitre et à réaliser pour exécuter nos calculs en parallèle sur cette infrastructure ?

Voici ce pour quoi openMOLE est un outils indispensable pour faire le liens entre les usages en SHS, et ces ressources distribués.

Les géographes francais utilisant actuellement la grille de calcul pour l'execution de simulation sont rattachés à la VO des système complexe européenne (\textit{vo.complex-systems.eu}), et sont principalement issue du laboratoire Géographie-Cités à Paris, et du laboratoire GEOLAB de Limoges.

Sans cette collaboration à la fois humaine et technique avec l'Institut des systèmes complexes de Paris (ISC-PIF), ce premier contact avec le \textit{Grid Computing} n'aurait probablement pas pu se faire.

 mais reste encore très éloigné de la sphère des pratiques et des formations actuelles, peu de monde ayant le niveau technique suffisant pour mettre en oeuvre l'amorce technique nécessaire à la bonne utilisation de cette ressource informatique, fourni ici sous sa forme la plus brute \Anote{human_num_note}.

 GRID2-FR est l'autorité de certification du CNRS, le laboratoire doit être enregistré auprès de cet organisme pour pouvoir ensuite récupérer des certificats.

% Cette plateforme peut elle faire autre chose ? Vais je pouvoir partager mon modele ou mon experience avec d'autres scientifiques ?

Dans un article daté de 2015, johnatan dursi
\href{http://www.dursi.ca/hpc-is-dying-and-mpi-is-killing-it/}{hpc is dying}

\subsection{Historique}

Evidemment il faut citer le travail important réalisé sur le prototype par Amblard, à la fois dans le démocratisation de la validation, mais également dans les outils.

\subsection{Principes et mise en oeuvre}

\section{Un nouveau framework pour systématiser l'évaluation des modèles de simulations : MGO}
\label{sec:MGO}

%%%%%%%%%%%%%%%%%%%%%%%%%%
%% NOTE CLEMENTINE
%%%%%%%%%%%%%%%%%%%%%%%%%%
% Je t'ai mis surtout des détails de forme dans le documents en pj parce que je suis incapable de juger le fond.
% Dans l'ensemble, tu avais l'air d'être inquiet de la lisibilité pour le néophyte, donc :
% - effectivement, c'est pas fastoche fastoche !
% - en fait je pense que là ou tu pourrais gagner en accessibilité (on va pas envisager le géographe des migrations en Afrique mais disons le quantitativiste moyen :), c'est sur le tout début.
% Au fur et à mesure de la lecture, on a tout les éléments on s'y retrouve et c'est intéressant et ça se lit bien.
% Par contre le début c'est chaud, et à mon avis pour deux raisons :
% - c'est la partie la plus théorique et on sent que même toi tu doutes un peu de l'intérêt de classifier les algo alors on est pas convaincu non plus et on sait pas ou ça va nous mener.
% - je pense qu'il faut que tu annonces beaucoup plus tôt, plus fort et plus souvent à quoi ça sert qu'on s'intéresse aux métaheuristiques, aux espaces de résultats et aux fronts de Pareto. Pour ne pas avoir à tout réorganiser, tu peux surement tester ce que ça donne de présenter dès le début le besoin d'algo evolutionaire en simulation géo. et comme ça on apprend plein de trucs par la suite, mais on voit ou tu nous emmènes et comment on fait notre choix parmi toutes les solutions que tu présentes...

%%et pourquoi ne pas utiliser les modèles au début pour annoncer les problèmes de modélisation et les enjeux de calibration?

%%%%%%%%%%%%%%%%%%%%%%

% Présentation de l'interet de ces techniques
% A priori déjà présenté ailleurs ?
%\subsubsection{Quelle utilité pour la construction et l'évaluation de modèle de simulation ?}

%Le chapitre 1 se terminait déjà sur la difficulté pour calibrer les modèles. Le chapitre 2 a prouvé que la construction et l'évaluation d'un modèle était deux processus indissociables,


% Fil plus chronologique, guidé par les besoins !

% - Accéder au HPC et Grid Computing
%

% - Présentation modèles exemples
% - OpenMOLE
% - MGO
%
% Plan temporaire MGO :
% - Présentation besoins / objectifs.
% - Insufisance EC existant (2.2.3.1 actuel)
% - Présentation plus large de la discipline + encadré resituant SLocal
% - Mise en oeuvre MGO
%	- Historique
%   - Principe conception innovant
% - Mise en oeuvre couplage MGO - openMOLE
% - Premier prototype, bilan autour de l'expérience SLocal

% Prise de recul sur la méthodologie, accointance et critique de la méthode POM avancé par Grimm ?
\subsection{Le modèle SimpopLOCAL}

L'idée d'une plateforme pour la construction et l'évaluation de modèle n'est

Ce modèle est le coeur d'une collaboration s'étalant sur plus de trois années.



(R2) : Des le premier comité de thèse (mai 2010), et suite déjà à de nombreux re-développement touchant le modèle SimpopLocal, la question d'un versionnement possible des hypothèses et d'une architecture à base de composant réutilisable est discuté de manière informelle, les solutions techniques ne seront trouvés que bien plus tard.

(R3) : Certaines valeurs de paramètres produisent des comportements qui ralentissent énormément l'execution du modèle, le modèle présente une palette de temps d'execution très variable.

(R4) : Les données produite par une campagne d'exploration des modèles, même simple, est très rapidement problématique : 20 Mo fichier csv par execution. La fouille de données des campagnes d'expérimentation à posteriori est une solution de moins en moins viable. L'expertise doit être exporté sur grille de calcul, et appliqué à la fin de chaque série de réplication, afin de récolter moins d'information.

(R5): Ajout des catastrophes au modele par Robin Cura en mai 2011

Voici quelques jalons du développement pour ce prototype

- De mai à septembre 2010, travail sur le modèle Simpop2, construction avec Thomas Louail d'un projet d'évaluation automatique des modèles de simulation développés à Géographie Cités.

- En janvier 2010, début du développement de SimpopLocal, avec Sébastien Rey Coyrehourcq, Clara Schmitt, Arnaud Banos, Denise Pumain. Le calendrier de travail se cale sur la base de réunion tout les 15 jours environ.

- En janvier 2010, rencontre avec l'équipe de développeur d'OpenMOLE, Romain Reuillon et Mathieu Leclaire. Devant l'intérét des deux parties (recherche d'application pour OpenMOLE, usage d'une plateforme pour le HPC pour l'autre partie) une convention de collaboration est établi en mars 2010.

- En mars 2010, Suite aux expériences déjà réalisé de leur coté avec les algorithmes génétiques, Romain et Mathieu nous demande de réflechir les entrées et surtout les sorties de notre modèle de façon beaucoup plus formalisé qu'elle ne le sont actuellement. Autrement dit, la question \enquote{Qu'est ce que vous voulez démontrer avec votre modèle ? } s'impose aussi comme une question préalable à la compréhension entre les deux équipes.

- Mars 2010 Arnaud Banos envoie une demande pour que le laboratoire Géographie-Cités intègre les organismes CNRS pouvant délivrer des certificats pour une utilisation de la grille de calcul système complexe.

- Mai 2010 Romain créé une tâche Netlogo4 pour OpenMOLE

- Juin 2010 Après plusieurs rencontres, et quelques essai de construction de workflow dès lors que le plugin Netlogo est opérationel, la toute première utilisation de SimpopLocal sur grille a lieu lors d'un stage en juin 2010.

- Juillet 2010 Création d'un rapport avec Hélène Mathian et Saber Maarouchi pour l'équipement du laboratoire en nouveaux matériels, contenant un serveur de calcul propre, devenant ainsi une antenne des systèmes complexes.

Les premiers travaux pour tenter de caractériser de façon automatique les sorties du modèles SimpopLocal commence dès juin 2010, le test Kolmogorv Smirnov pour vérifier la lognormalité des distributions est approché pour la première fois à cette période, mais ne sera implémenté que plus tard entre la du deuxième semestre 2010 et le premier semestre 2011.

Deuxième semestre 2010

- En octobre 2010, Première présentation du projet d'utilisation de la grille pour SimpopLocal au journée Grid Day en 2010.

Les données générées en sortie par une exploration systématique de modèles de simulation sont très volumineuses, il est décidé de les stocker en base de données. L'exploration des résultats se fait sur la base de fouilles de données a posteriori, automatisé : importation des résultats en base de données, requétage, création automatiques de graphiques pour expertise par les géographes. Les indicateurs ne sont pas assez synthétiques, les graphiques peu lisibles du fait de la prise en compte nécessaire de la stochasticité. Il est proposé au comité de thèse (novembre 2010) de réaliser une évaluation semi automatique établissant un score basé sur des indicateurs en sortie du modèle. L'utilisation de ce score pour des algorithmes génétique est préssenti, mais pas encore mis en oeuvre.

- En Novembre 2010 rencontre avec jean marc favaro pour discuter d'un objectifs important : caractériser la forme des hierarchies produites par le modèle de façon automatique en utilisant un test Kolmogorov Smirnov.

- Avril 2011, le projet de couplage entre MGO et SimpopLocal est opérationnel, et les trois objectifs sont opérationnels; la librairie peut être appelé par les différentes tâches du workflow

Les premier essai d'algorithmes génétiques montre pour la première fois que certaines zones de valeurs de paramètres extreme vont poser problèmes sur les temps d'execution.

- Mai 2011, présentation des critères objectifs retenue pour évaluer la simulation au comité de thèse; la question des statistiques à appliquer pour prendre en compte la forme de la distribution pour chaque objectifs est discuté. La déviation absolue moyenne (moyenne arithmétique des valeurs absolues des écarts à la moyenne) est rapidement écarté pour une solution plus robuste aux valeurs extremes, l'écart absolue à la médianne (Median Absolute Deviation).

- Aout 2011, Romain étend la tâche Netlogo à la version 5, récupération des tout premier résultats sur les trois objectifs

- Présentation de la méthodologie et du workflow à l'ECTQG de septembre 2011 pour les géographes, puis premier papier présentant la calibration par optimisation multi-objectif soumis à la conférence V2CS, plus orienté public  informaticien.

- En 2011, la bibliothèqe MGO permettant l'utilisation de méta-heuristique dans OpenMOLE est complétement reprise pour devenir un framework à base de composant réutilisable. L'objectif et les résultats de cette opération de transformation sont décrite plus en détail dans la section \ref{ssec:historique_mgo}

- En 2011, openMOLE devient la plateforme de référence



- En mars 2012, création d'un plugin Netlogo pour tenter d'accélérer les échanges d'innovation, goulot d'étranglement concernant les performances du modèles.

- En Mars 2013, une nouvelle problématique se pose à l'équipe. Le nombre de réplications défini à 30 n'est probablement pas idéal. On se rend compte que certains des candidats retenu par l'algorithme génétique peuvent avoir été selectionné selon un coup de chance. Une campagne de réplications démarre pour déterminer quel est le nombre idéal de réplications pouvant limiter cette erreur, 100 s'avère être un minimum, 1000 une valeur plus sure. Ce nombre étant beaucoup trop important pour être applicable sur grille de calcul, un mécanime original est ajouté par Romain à la librairie d'algorithme génétique pour compenser cette erreur. Désormais, les individu selectionné seront régulièrement réévalué au hasard.

- En avril 2013, le projet SimPuzzle démarre.

A partir de l'architecture à base de composant éprouvé lors du développement du framework MGO, il est proposé de construire une bibliothèque de composant permettant de capitaliser les mécanismes inclue dans les anciens et les nouveaux modèles de systèmes de villes, en Scala.

- Fin 2013, peu avant une présentation à l'ECTQG de Dourdan, l'idée germe au sein de l'équipe d'une utilisation plus automatisé des blocs composants les modèles.

Simpuzzle permet de capitaliser différentes stratégies d'implémentation pour une même hypothèse, il s'agit donc



\subsection{Objectifs}

Plutot que de parler d'objectifs initiaux, je crois que le déroulé de notre expérience ces 5 dernières années prend une forme plus modeste de jalons, qui releve parfois plus d'une intuition motrice

L'idée d'une évaluation plus encadré des modèles de simulation développé au laboratoire s'inscrit d'une part dans les travaux  suite au travaux de thèse de Thomas Louail, et  préliminaire réalisés et de Clara Schmitt,

En ce sens, la frustration a été de façon étonnante je crois, un élément moteur dans l'émergence d'une écosystème ouvert à l'intégration  .

 2009 et 2010, l'objectif partait d'un certains nombre de frustration prenant racine  dans l'observation des limites des pratiques existantes dans le laboratoire et dans la littérature, pour évaluer les modèles existants, mais également les modèles en cours de construction.

  - celle de ne pas comprendre la dynamique de nos modèles, pourtant encore très simple
  - la constatation dans la littérature d'une certaine d'outils support des méthodes

  - très peu de réutilisabilité des comportements implémentés : vous vous trouver devant un choix d'implémentation, lequel sera le bon ?


Ces frustrations ont données naissance à plusieurs pistes de développement d'outils : SimPuzzle, MGO.


- Construire une méthodologie autour d'un prototype






Le chapitre 3 a fait le point sur les enjeux d'une solution globale capable de prendre en compte toute les dimensions rattaché au bon déroulement de cette construction.

Le \textit{framework} MGO (Multi Goal Optimization)

Pour mieux comprendre par la suite quelle est la spécificitée des algorithmes evolutionnaires (\textit{Evolutionary Algorithms} ou EA), il est nécessaire de donner quelques éléments de contexte et de définitions plus généraux concernant la branche d'étude dans lequel ceux-ci se situent. Il faut par ailleurs mettre en garde le lecteur que la plupart des définitions et des analyses présentés ici sont inspirés ou extraits d'ouvrages de synthèses à destination d'un public très large \autocites{Weise2011, Luke2013, Brownlee2012}. Par conséquent il faut garder à l'esprit que plusieurs de ces termes peuvent être discutés, enrichis, critiqués ou prendre des sens différents dans chacune des sous branche (voir figure \ref{fig:S_OverviewOptimisation}) que compte ce domaine très général qu'est l'optimisation.

\begin{figure}[h]
\begin{sidecaption}[fortoc]{ Vue d'ensemble des algorithmes d'optimisation repris de l'état de l'art très complet de \textcite[32]{Weise2011}}[fig:S_OverviewOptimisation]
  \centering
 \includegraphics[width=.9\linewidth]{overview_optimisation_algorithm.png}
  \end{sidecaption}
\end{figure}

\subsection{Le domaine des algorithmes métaheuristiques, une sous-discipline de l'Optimisation}

\subsubsection{Q'est ce que l'optimisation ?}
\label{sssec:Optimisation}

Pour \textcite[22]{Weise2011}, l'optimisation \foreignquote{english}{ [...] is the process of solving an optimization problem, i. e., finding suitable solutions for it}, un problème d'optimisation nécessitant de trouver \foreignquote{english}{ [...] an input value $x^*$ for which a mathematical function $f$ takes on the smallest possible value (while usually obeying to some restrictions on the possible values of $x^*$ )}, la notation mathématique astérisque $^*$ désignant ici une valeur optimale.

Sortie de cette définition mathématique, l'optimisation peut également se définir par la mise en oeuvre d'algorithmes spécifiques. La littérature informatique met à disposition des programmeurs un ensemble d'algorithmes capables de fournir des solutions exactes dans un temps fini à un certain nombre de problèmes bien définis. C'est le cas par exemple des nombreux algorithmes de tri. Une autre classe d'algorithmes (\textit{optimization algorithms}) peut être employée lorsqu'il n'existe pas d'algorithme dédié (\textit{dedicated algorithms}), soit parce que le problème est trop spécifique, soit parce que personne n'a trouvé de solution efficace pour résoudre ce problème.

Dans ce cadre, le terme d'optimisation globale \foreignquote{english}{ [...] is optimization with the goal of finding solutions $x^*$ for a given optimization problem which have the property that no other, better solutions exist.} Le terme \enquote{global} nécessite à la différence d'une recherche qui serait \enquote{locale}, de se concentrer sur l'obtention souvent plus couteuse et plus complexe d'un optimum global, minimum ou maximum, dominant par sa qualité l'ensemble des valeurs recherchées en entrée de la fonction à optimiser.

Bien que souvent beaucoup plus lent, moins précis, et plus consommateurs de ressources que les algorithmes dédiés, ces algorithmes d'optimisations nécessitent aussi beaucoup moins d'informations pour pouvoir être executés : \foreignquote{english}{Most often, these algorithms only need a definition of the structure of possible solutions and a function $f$ which tells measures the quality of a candidate solution. Based on this information, they try to find solutions for which $f$ takes on the best values.} \autocite[24]{Weise2011}

Ces algorithmes s'appuient donc sur différents types de stratégies pour tirer parti du peu d'information obtenue via cette fonction $f$. De nature très diverse, on retient pour séparer une première fois ces stratégies une typologie en deux classes.

\begin{itemize}[label=\textbullet]
\litem{\textit{Probabilistic Approaches}} Les approches stochastiques désignées dans la fig. \ref{fig:S_OverviewOptimisation} sont capables de trouver un optimum assez rapidement, mais ne peuvent pas en garantir la propriété \enquote{globale}
\litem{\textit{Deterministic Approaches}} Les approches déterministes également désignées dans la fig. \ref{fig:S_OverviewOptimisation} peuvent certes garantir au moins théoriquement l'obtention d'un optimum global, mais s'éxecutent souvent au détriment d'un coût computationnel elevé.
\end{itemize}

Ces deux approches partagent également des difficultés communes, et découvrent leurs limites à des degrés divers en fonction des stratégies mise en oeuvre, dès lors que l'espace de recherche à parcourir devient trop important.

C'est le cas par exemple de l'espace de recherche de toute une sous-catégorie de problèmes \textit{NP-Complet} \Anote{np_complet_def} d'optimisation combinatoire \textit{Combinatorial Optimization Problems} (COP). Ce domaine contient par exemple les problèmes bien connus du voyageur de commerce \textit{Travelling salesman problem} (TSP), ou encore le problème du sac à dos \textit{Knapsack Problem} (KP) \Anote{note_knapsack}. Avec l'augmentation du nombre d'éléments entrant dans la définition de ces problèmes, il devient impossible de passer en revue l'ensemble des combinaisons (solutions possibles). Ce qui a pour conséquence de rendre difficile tout autant la découverte d'un optimum global, que la mesure de qualité de celui-ci, car pour établir cette dernière il nous faudrait logiquement connaitre la solution optimale, or c'est cela même que nous cherchons.

Cette première typologie recoupe une autre propriété des algorithmes. La littérature informatique qualifie ainsi d'\textit{exacts} les algorithmes dont l'exécution garantit un résultat optimum à coup sûr, d'\textit{approximate algorithms} les algorithmes capables de donner une mesure proche d'un optimum sans pouvoir en garantir la qualité, et d'\textit{approximation algorithms} les algorithmes capables de donner une mesure proche d'un optimum assortie d'une preuve de qualité. Cette dernière classe n'est pas à confondre avec une classe d'algorithmes cherchant à conserver l'optimalité en limitant par diverses stratégies le coût temporel de résolution, mais bien l'inverse, relâcher la contrainte d'optimalité, mais aussi peu que possible. Les \textit{approximations algorithms} sont une donc une sous classe d'\textit{approximate algorithms}, et constituent une branche d'étude à eux seuls, car même dans le cas de problèmes \textit{NP-Complet}, ils offrent dans des dimensions raisonnables et propres à chacun des problèmes une solution sub-optimale d'erreur mesurable et donc potentiellement améliorable, voire comparable, notamment avec les résultats donnés de façon non analytique par d'autre stratégies.
%http://en.wikipedia.org/wiki/Approximation_algorithm#cite_ref-kann92onthe_3-4

On retrouve parfois rangé \enquote{en vrac} dans la classe des \textit{approximate algorithms} la classe des heuristiques et métaheuristiques, deux termes définis plus en détail dans la section suivante.

%On nomme métaheuristique (\textit{metaheuristic}) ce type d'algorithmes s'appuyant sur des heuristiques (\textit{heuristic}).

\subsubsection{Quelle définition peut on donner pour une heuristique (\textit{heuristic}) ? }
\label{sssec:heuristique}

Le terme heuristique \textit{heuristic} vient du Grec \textit{heuriskein} que l'on peut traduire par \foreignquote{english}{to find}, ou \foreignquote{english}{to discover}. D'usage plus large que dans la simple discipline informatique, nous retiendrons ici ce terme seulement sous son sens spécifique contextuel à l'optimisation. Rattaché à la définition d'un problème (\textit{problem dependent}), on définit une heuristique comme une mesure approximative pour définir la qualité d'une solution candidate \autocite[34]{Weise2011}.

%http://stackoverflow.com/questions/9140860/heuristic-function-for-finding-the-path-using-a-star
%http://stackoverflow.com/questions/9140860/heuristic-function-for-finding-the-path-using-a-star
%http://stackoverflow.com/questions/11779589/connection-between-a-star-search-and-integer-programming-extending-a-star
Si on se penche sur la classe d'algorithmes dédiés au problème de recherche du plus court chemin, les heuristiques sont souvent utilisées en appui des algorithmes de parcours de graphe, soit pour converger plus rapidement vers une solution optimale, soit pour justement se libérer de cette contrainte d'optimalité en visant un gain de temps au détriment de la précision. Si on prend par exemple l'algorithme de Djikstra, celui-ci n'utilise pas d'heuristique et garantit que le plus court chemin résultant sera optimal, car tous les chemins possibles entre le point de départ $A$ et le point final $B$ auront été analysés par celui-ci. Il est néanmoins connu comme étant très coûteux d'utilisation dès que le graphe dépasse un certain nombre de noeuds. L'algorithme déterministe $A^*$ s'appuie par contre sur une fonction heuristique $h(n)$ (une estimation du coût minimal reliant le noeud $n$ au noeud final) pour guider l'algorithme dans le processus incrémental de sélection d'un prochain noeud constitutif d'un chemin. En jouant sur cette heuristique, on est ainsi capable de déterminer si l'algorithme doit mettre la priorité sur la vitesse ou la précision, $h(0)$ étant équivalent ici à l'algorithme de Djikstra. Si l'heuristique est bien choisie (on dit ici que l'heuristique est admissible), alors $A^*$ garanti aussi l'optimalité du chemin trouvé, avec à la clef un coût computationnel moindre, car seule une partie des noeuds de l'ensemble du graphe auront été explorés par l'algorithme. Une autre heuristique misant plus sur la vitesse d'exécution pourra définir un chemin cette fois-ci sub-optimal avec un coût computationnel encore plus réduit. Il est à noter ici que l'utilisation d'une heuristique dans un programme n'est pas forcément motivée par la recherche d'un optimum global, mais par le gain de temps. Ainsi, un utilisateur peut très bien avoir les moyens d'obtenir un chemin optimal (Djikstra) sur une petite combinatoire de noeuds, mais peut vouloir prendre un raccourci en utilisant une méthode moins couteuse ($A^*$). Un scénario très souvent mis en avant dans la programmation de jeux sur ordinateur, où l'on cherche régulièrement à gagner du temps, tout en se rapprochant d'un comportement faillible imitant plus un adversaire de type humain.

La forme prise par une heuristique est variable, et peut aller comme vu ci-dessus avec l'exemple $A^*$ d'une simple fonction mathématique de coût intégrée à un algorithme classique de parcours de graphes, à un algorithme beaucoup plus complexe intégrant de multiples prises de décisions pour estimer ce même coût. Dans le livre \textit{Code Complete} de \textcite[12]{McConnell2004}, celui-ci donne un exemple assez parlant pour illustrer la subtile différence qui sépare la description d'un algorithme employé au sens courant pour désigner un algorithme déterministe exact fournissant à coup sûr une solution, et la description d'un algorithme déterministe ou stochastique heuristique (ou appuyé par une heuristique) fournissant seulement un guide pour trouver, éventuellement, une solution.

\foreignquote{english}{Here's an algorithm for driving to someone's house: Take Highway's 167 south to Puyallup. Take the South Hill Mall exit and drive 4.5 miles up the hill. Turn right at the light by the grocery store, and then take the first left. Turn into the driveaway of the large tan house on the left, at 714 North Cedar}

\foreignquote{english}{Here's an heuristic for getting to someone's house: Find the last letter we mailed you. Drive to the town in the return adress. When you get to town, ask someone where our house is. Everyone knows us - someone will be glad to help you. If you can't find anyone, call us from a public phone, and we'll come get you.}

Il faut toutefois éviter de considérer les heuristiques comme appartenant à la seule classe des \textit{approximate algorithms}, car le terme ne se laisse pas facilement enfermer dans une typologie trop simple. En effet de multiples problèmes trouvent une solution exacte jusqu'à un certain niveau de complexification, à partir duquel on fait généralement appel aux heuristiques, soit par un appel à d'autres méthodes intégrant des heuristiques, soit par une intégration d'heuristiques aux méthodes existantes. Ainsi de nombreuses classes d'heuristiques sont utilisées de façon transversale, et apparaissent donc aussi comme composantes manipulées dans la classes des \textit{approximation algorithms}. L'heuristique gloutonne \textit{greedy algorithm} \Anote{greedy_description} apparaît de façon transversale à la fois comme une solution d'approximation pour le \textit{Knapsack Problem} (KP) mais également comme moteur dans le cadre d'algorithmes déterministes exacts comme la recherche du plus court chemin de Djikstra. Un autre algorithme nommé \textit{A*} (\textit{A-Star}) qui englobe Djikstra comme cas particulier, est quant à lui capable de fournir tout à la fois une mesure exacte ou approximée en fonction de l'heuristique injectée et du niveau de complexité du problème abordé.

\subsubsection{Quelle définition peut on donner pour une métaheuristique (\textit{metaheuristic}) ?}
\label{sssec:metaheuristique}

Le terme métaheuristique est d'origine plus moderne \autocite{Glover1986}, et a permis d'englober a posteriori des algorithmes jusque là qualifiés d'heuristiques. C'est le cas par exemple d'une bonne partie des algorithmes évolutionnaires, qui émergent principalement au cours des années 1960-1970. Cette remarque d'ordre historique est à l'origine d'une première ambiguité entre les termes auquelle il faut encore ajouter les inquiétudes exprimées par \textcite{Luke2013}. Pour ce dernier, le terme métaheuristique est en réalité plutôt malheureux pour définir cette catégorie d'algorithmes, car contrairement à ce que laisse entendre ce terme, \textit{une heuristique pour ou à propos d'une heuristique}, ce n'est pas de cela dont il s'agit ici.

Voici comment \textcite[8]{Brownlee2012} perçoit la différence entre les deux termes : \foreignquote{english}{Like heuristics, metaheuristics may be considered a general algorithmic framework that can be applied to different optimization problems with relative few modifications to adapt them to a specific problem. The difference is that metaheuristics are intended to extend the capabilities of heuristics by combining one or more heuristic methods (referred to as procedures) using a higher-level strategy (hence ‘meta’). A procedure in a metaheuristic is considered black-box in that little (if any) prior knowledge is known about it by the metaheuristic, and as such it may be replaced with a different procedure. Procedures may be as simple as the manipulation of a representation, or as complex as another complete metaheuristic. Some examples of metaheuristics include iterated local search, tabu search, the genetic algorithm, ant colony optimization, and simulated annealing.}

Le terme \enquote{méta-} renvoie plus en définitive au concept générique de \enquote{stratégie de recherche} prenant la forme d'un algorithme d'optimisation capable de mélanger, manipuler des heuristiques ou d'autres métaheuristiques (cf. points \ref{enum_meta_a} et \ref{enum_meta_h}) \Anote{def_meta_weise}. Contrairement aux heuristiques, les métaheuristiques se définissent plus comme un système fait de composants, dont la plasticité permet le support et l'interaction nécessaire au développement d'heuristiques plus ciblées (\textit{problem dependent}) \Anote{def_meta_sorensen}. La structure offre un patron d'usage initial (\textit{pattern}) qui reste indépendant du problème abordé (\textit{problem independent}) (cf. \ref{enum_meta_g}), tout en restant évolutif, comme le montre le fort développement de cette discipline depuis les années 1980. Ce principe de flexibilité, on le retrouve par exemple dans la classe des EC, comme le mettent bien en valeur Bach, Hammel et Schwefel en 1997, dans une publication introduisant l'EC dans la série renommée des \textit{IEEE Transactions} :

\foreignquote{english}{We argue that the most significant advantage of using evolutionary search lies in the gain of exibility and adaptability to the task at hand, in combination with robust performance (although this depends on the problem class) and global search characteristics. In fact, evolutionary computation should be understood as a general adaptable concept for problem solving, especially well suited for solving difficult optimization problems, rather than a collection of related and ready-to-use algorithms. The majority of current implementations of evolutionary algorithms descend from three strongly related but independently developed approaches: genetic algorithms,evolutionary programming , and evolution strategies. [...] The fundamental difference in the evolutionary computation approach is to adapt the method to the problem at hand. In our opinion, evolutionary algorithms should not be considered as off-the-peg, ready-to-use algorithms but rather as a general concept which can be tailored to most of the real-world applications that often are beyond solution by means of traditional methods. Once a successful EC-framework has been developed it can be incrementally adapted to the problem under consideration, to changes of the requirements of the project, to modifications of the model and to the change of hardware resources.} \autocite{Back1997a}

Enfin, toujours dans une tentative de positionner ce terme dans une typologie, il faut savoir qu'une classification trop rapide de ces méthodes dans les seuls \textit{approximate algorithms} peut également être critiqué. Si les méthodes métaheuristiques sont effectivement souvent connues pour ne pas avancer de preuve, des travaux récents montrent toutefois qu'il existe de nouveaux algorithmes permettant de garantir dans certaines conditions un optimum global (CP-Algorithm de \autocite{Reuillon2015}). Tout dépend donc du degré et de la nature que l'on veut bien associer à la notion d'\textit{approximation} lorsqu'il s'agit de fournir une mesure d'éloignement de l'optimum. Les \textit{approximation algorithms} semblent toutefois plus intéressés par l'établissement d'une preuve au sens mathématique, et se concentrent avant tout sur un ensemble relativement limité de problèmes d'optimisation discret, ce qui ne semble pas être le but des métaheuristiques dans les deux cas. \autocites[1-6]{Kann1992}[13-15]{Williamson2011} %Metaheuristics: From Design to Implementation Par El-Ghazali Talbi

%You could think of a heuristic like an approximate (not approximation) solution to a problem. The difference between approximate and approximation is that the first is about getting a good guess of the solution of a problem, but that you don't really know how good it is. The second is about getting a solution for which you can prove how close it is to the optimal solution.

Enfin bien d'autres sous classifications sont possibles prenant plus ou moins en compte les spécificités propres aux différents algorithmes, comme celle opposant par exemple les stratégies utilisées en interne pour parcourir l'espace de recherche (generationel contre \textit{steady-state}, ou individuel contre populationel), la dimensionnalité possible pour la résolution des problèmes (mono-objectif contre multi-objectif), l'inspiration d'origine (naturelle biologique contre inspirations autres), etc.

Toute classification monocritère est donc rendue très difficile, une voie s'étant même ouverte pour tenter de classer ces algorithmes suivant la nature et le niveau d'opération de ces hybridations. L'origine de cette difficulté tient dans une pratique courante et assumée d'hybridation entre les différentes techniques afin de réunir le meilleur de chacune d'elles au sein de nouvelle proposition de recherche. De fait, il est important pour la suite de cerner au mieux la classe d'algorithme d'optimisation que nous allons aborder, et de définir pourquoi nous l'avons abordé. Nous nous intéresserons principalement dans la suite de cette présentation aux approches stochastiques métaheuristiques inspirées par la métaphore biologique, nommée \textit{Evolutionary Computation} (EC) ( voir figure \ref{fig:S_OverviewOptimisation}). La section \ref{xx} permettra de dégager les spécificités de cette subdivision, mais en attendant il nous faut d'abord présenter les principaux termes et concepts communs à cette classe d'algorithmes d'optimisation.

Devant la difficulté d'établissement d'une définition englobante, plusieurs auteurs semblent s'accorder pour faire du rattachement d'un algorithme à cette catégorie, une correspondance plus ou moins lâche avec un ensemble de propriétés généralement observées. En évitant une définition trop vague ou trop restrictive, on espère ainsi récupérer dans cette classe certains hybrides intéressants.

Voici un exemple de propriétés issues de \textcite{Blum2003} et traduites ci dessous :

%label=$\blacktriangleright$
\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
	\item Les métaheuristiques sont des stratégies qui \enquote{guident} le processus de recherche. \label{enum_meta_a}
	\item Leur objectif est d'explorer l'espace de recherche efficacement pour trouver les solutions quasi-optimales. \label{enum_meta_b}
	\item L'étendue des techniques que constitue la classe des algorithmes métaheuristiques va de la simple recherche locale à un processus d'apprentissage complexe. \label{enum_meta_c}
	\item Les algorithmes métaheuristiques sont approximatifs et la plupart du temps non déterministes. \label{enum_meta_d}
	\item Les métaheuristiques peuvent incorporer des mécanismes pour éviter d'être piégé dans une portion confinée de l'espace de recherche. \label{enum_meta_e}
	\item Les concepts de bases des métaheuristiques permettent d'adopter un certain degré d'abstraction dans la description. \label{enum_meta_f}
	\item Les métaheuristiques ne sont pas \textit{problem-specific}. \label{enum_meta_g}
	\item Les métaheuristiques peuvent faire usage d'une expertise du domaine au travers des heuristiques controlées par une stratégie de plus haut niveau. \label{enum_meta_h}
	\item La plupart des métaheuristiques actuelles font appel à une mémoire pour améliorer le processus qui guide la recherche. \label{enum_meta_i}
\end{enumerate}

Afin de mieux comprendre cette table de propriétés un peu abstraite, il est proposé de reprendre ces différents points au travers d'une lecture commentée, en commencant par une question ciblé sur la mécanique interne régissant ce type de technique.

\textit{Comment se matérialise la recherche de solutions optimisées dans une métaheuristique ?}

Si on considère les problèmes de combinatoires discrets comme \textit{TSP} ou \textit{Knapsack}, on a déjà vu que le nombre de combinaisons à évaluer lors d'une augmentation du nombre d'éléments participant à la définition du problème devient très vite problématique si on cherche à trouver une solution optimale exacte. Si on prend le cas d'un exemple plus ludique d'optimisations discrètes dans la branche des jeux (\textit{Combinatorial game theory}), le nombre de combinaisons légales possibles pour un plateau de 19 par 19 dans le jeu de GO chinois est estimé par \textcite{Tromp2007} à $2.08168199382×10^{170}$ . Même si certains auteurs comme Tromp estime qu'un tel calcul sera possible d'ici quelques mois \Anote{tromp_appel_calcul}, les problématiques posés par ce jeu mettent au défi les meilleurs programmes en intelligence artificielle \autocite{Bouzi2001}, et cela malgré des progrès spectaculaires ces dernières années, via notamment l'utilisation d'heuristique plus efficace que les approches classiques \Anote{mcts_go}.  Dans le cas d'un problème continu discrétisé, comme la recherche des meilleures valeurs de paramètres pour une simulation, la mise en oeuvre d'un plan factoriel complet (d'autres types de stratégies beaucoup plus fines existent) pose un problème double.

\hl{Correction orthographe à faire}
D'une part ce choix ne résout en rien la problématique combinatoire. Donnons un exemple plus concret, si une simulation possède 5 paramètres, chacun de ces paramètres étant discrétisé en 10 pas, cela nous donne déjà $10^5$ combinaisons possibles à évaluer. Si on considère que le modèle de simulation ainsi exécuté est stochastique (10 réplications), dans un délai relativement rapide (1 minute), la durée totale d'exécution de ce plan, pourtant relativement \enquote{grossier} d'un point de vue de la couverture de l'espace des paramètres, est environ égale à 2 années de calcul... La parallélisation d'un tel calcul, c'est à dire son execution sur plusieurs processeurs ou ordinateurs en parallèle, pourrait évidemment réduire ce temps de calcul à des dimensions plus raisonnables, mais c'est sans compter sur un deuxième problème, plus contraignant.

Avec le choix d'une telle maille pour la discrétisation des paramètres, c'est prendre le risque de passer à côté de solutions potentielles, une problématique contraignante d'autant plus qu'elle se complexifie avec l'augmentation du nombre de paramètres, comme le dicte le phénomène de \textit{Curse Dimensionality} établit par Richard Bellman. Ce problème est principalement d'ordre statistique, là ou 100 points peuvent suffire dans un espace entre $(0..1)$ de dimension $1$ pour commencer à inférer ($0.01$ de distance entre chaque point), 100 points dans un même espace $(0..1)$ de dimension $10$ ne couvre plus qu'une toute petite partie du volume disponible. Chaque point est alors entouré d'une large portion de vide qui rend délicates toutes inférences à partir d'une si faible couverture d'un tel espace. Pour obtenir une couverture équivalente avec une distance de $0.01$ entre chaque points, il faudrait disposer de $10^{20}$ points, ce qui semble considérable \autocite{Bellman1961}.

% ENCADRER ?
\paragraph{Simpop Local}

% Exemple pour illustrer ca au travers des problèmes SLocal qu'on a eu ?

% Jonction des trois fonctions objectifs

% Plutot une argumentation pour justifier de la nécessité d'explorer rapidement les modèles. Mais dans un deuxième temps, sur les analyses de sensibilités, car analyse de sensibilité c'est pas tip top

Lors de l'implémentation une deuxième vague de paramètres est à prendre en compte lors des expérimentations. En effet lorsque vient la nécessité d'implémenter les hypothèses dans un programme informatique, le modélisateur doit faire face à la fois aux ambiguités du langage naturel, mais doit également prendre en compte des externalités qui interfere avec l'image du modèle conceptuel.

Les informaticiens sont habitués à cette exercice de décomposition, recomposition qui permet de passer d'une description très abstraite, à une implémentation fonctionelle, par exemple dans la manipulation de base de données. Seulement ici, ce n'est pas tant la qualité fonctionelle qui guide l'implémentation du modèle mais bien la qualité de similarité avec le modèle conceptuel. L'implémentation . C'est cette fameuse étape de vérification qui est censé nous garantir que le modèle dit bien ce que l'on veut qu'il dise.

Toute nouvelle variable exogène à ce modèle conceptuel rentre donc soit dans la catégorie des omissions involontaire, jalon naturel d'une bonne progression dans un raisonnement scientifique, soit dans la catégorie des paramètre a-thématique informatique, dont on ne peut s'empecher de penser qu'ils interferent et dégradent forcément ce bel édifice scientifique en cours de construction.




Contrairement à d'autres méthodes d'optimisations, les métaheuristiques font généralement appel à un processus d'échantillonnage (voir point \ref{enum_meta_b}) pour explorer de façon stochastique un espace de recherche de toute façon beaucoup trop vaste pour être parcouru de façon exhaustive. Cela permet de repousser en partie ce problème de couverture de l'espace de paramètres lié à l'augmentation de la dimensionnalité du problème, cas nous verrons que les méta-heuristique opérant dans des espace de paramètres discret mais aussi continu, sans qu'une discrétisation préalable soit nécessaire en amont.

C'est pour cela que \textcite[7]{Luke2013} nous propose de voir ce type de problème autrement, partant du postulat assez logique qu'une solution \enquote{même non optimale} est un point de départ pour l'amélioration de toute façon bien meilleure que \enquote{pas de solution du tout}.

\foreignquote{english}{ Metaheuristics are applied to \enquote{I know it when I see it} problems. They're algorithms used to find answers to problems when you have very little to help you: you don't know what the optimal solution looks like, you don't know how to go about finding it in a principled way, you have very little heuristic information to go on, and brute-force search is out of the question because the space is too large. But if you're given a candidate solution to your problem, you can test it and assess how good it is. That is, you know a good one when you see it.}

\hl{schéma}

Suivant ce raisonnement, la connaissance d'un problème se construit au travers d'une confrontation répétée de nos représentations, de nos interrogations avec la forme réelle et encore inconnue prise par celui-ci. La carte de ce nouveau territoire se révélant peu à peu dans la projection sur l'espace des solutions des choix effectués lors de la sélection des nouveaux candidats à évaluer (solutions candidates).

Les métaheuristiques sont donc là pour faciliter l'exécution de cette tâche complexe et répétitive qui consisterait à améliorer notre connaissance du problème en proposant de façon pertinente de nouvelles solutions candidates à évaluer, ces dernières étant choisies si possible en fonction des résultats obtenus par les précédentes (voir point \ref{enum_meta_i}). La perspective d'une telle automatisation pose évidemment un certain nombre de questions.

Quels sont les choix mis à disposition de l'optimiseur pour améliorer la réponse attendue des solutions candidates entre chaque incrément ? \autocite[19]{Weise2011}

Une comparaison automatisée nécessite pour être mise en oeuvre de définir \begin{enumerate*}[label=(\alph*)]
\item sur quelle base se fonde l'évaluation d'une solution,
\item la comparaison entre les solutions évaluées,
\item et la sélection de nouvelles solutions candidates.\end{enumerate*} Car l'optimiseur, tout comme nous, ne connait pas directement la forme prise par l'espace des solutions, et doit bien concevoir en interne les choix permettant, par la selection de nouvelles solutions candidates à évaluer, de progresser si possible vers une solution optimum.

De fait dans un tel scénario, et pour éviter une recherche aléatoire, l'évaluation de solution candidate renvoie à l'existence d'une expertise externe à l'optimiseur, le seul capable de formaliser ce qui différencie une bonne solution d'une mauvaise solution. On revient à parler ici d'heuristique, et de leurs diversités, car si celles-ci interviennent dans l'évaluation des solutions candidates (a), elles interviennent aussi dans les autres cas (b) et (c). Elles se présentent sous la forme de différents types de connaissances, interrogent différents espaces, et s'intègrent souvent sous la forme de composants dans la structure plastique des métaheuristiques.

L'injection de connaissance (voir point \ref{enum_meta_h} )dans ce type d'algorithme métaheuristique est donc double, et opère à la fois de façon précise dans la formalisation d'un ou de plusieurs critères qui vont servir pour l'algorithme optimiseur à déterminer la qualité, bonne ou mauvaise, d'une solution candidate; et de l'autre elle intervient cette fois ci de façon moins contrôlable dans la façon dont l'expérimentateur va construire et paramétrer une métaheuristique pour l'adapter au mieux à son problème. La qualité interne (paramètre, structure) de la métaheuristique définit aussi en quelque sorte le processus d'exploration, ce qui explique aussi la dépendance de ce type d'algorithmes à l'environnement qu'ils doivent explorer.

\begin{figure}[ht]
	\begin{sidecaption}[fortoc]{Projection du vecteur de points $\{a \dotsc n\}$ dans l'espace des objectifs. Les couleurs représentent les différents axes de projection ordonnés de 1 à 3 sur $(x,v)$ et de 1 à 4 sur $(y,v)$}[fig:spacePspaceOmultimodal]
	 \centering
	 	\includegraphics[width=.9\linewidth]{espaceP_espaceO_multimodal.pdf}
	\end{sidecaption}
\end{figure}

L'objectif est rendu complexe car la relation entretenue entre ces deux espaces, celui des solutions candidates disponibles, et celui des évaluations est bien souvent dissymétrique. Pour mieux comprendre cette relation, la figure \ref{fig:spacePspaceOmultimodal} illustre cette correspondance des solutions candidates $\{a \dotsc n\}$ décrites par leurs coordonnées $(x,y)$ lorsqu'elles sont projetées dans l'espace des objectifs $\mathbb{Y}$ en suivant la transformation attendue par la fonction boite noire de dynamique non linéaire $f(x,y)$. Les valeurs $v = f(x,y)$ des différentes solutions candidates sont également projetées sur le plan 2D $(x,v)$ et $(y,v)$ pour mieux visualiser la forme prise par cette surface en 2D.

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Les couleurs indiquent la  des valeurs $v = f(x,y)$ mesurée dans la figure \ref{fig:spacePspaceOmultimodal}, sachant qu'on cherche à minimiser la valeur de v :
\parbox{\marginparwidth}{
\begin{enumerate}[label={},labelindent=0pt,leftmargin=*]
        \item \sqbox{tangoBlue1} indique une fitness minimale, cf. qui maximise $v$
        \item \sqbox{tangoOrange1} indique une fitness intermédiaire et,
        \item \sqbox{tangoRed1} indique une fitness maximale, cf. qui minimise $v$
\end{enumerate}}}[fig:xyspacePspaceOmultimodal]
	 \centering
	  \subbottom[]{
	 	\includegraphics[width=0.4\linewidth]{xyespaceSolutionCandidate_a.pdf}
	 	\label{subfig_xyespaceSolutionCandidate_a}}
	 \subbottom[]{
	 	\includegraphics[width=.4\linewidth]{xyespaceSolutionCandidate_b.pdf}
	 	\label{subfig_xyespaceSolutionCandidate_b}}
	 \subbottom[]{
		\includegraphics[width=.4\linewidth]{xyespaceSolutionCandidate_c.pdf}
		\label{subfig_xyespaceSolutionCandidate_c}}
	\end{sidecaption}
\end{figure}

Pour visualiser la valeur $v$ prise par chacune des solutions candidates, on projette celle-ci dans l'espace $(x,y)$, ce qui nous permet de mieux constater l'éclatement des valeurs de $v$ sur la figure \ref{fig:xyspacePspaceOmultimodal}.

Deux solutions proches dans l'espace des solutions candidates peuvent amener à des résultats très différents, et inversement, pour deux évaluations proches peuvent correspondre des solutions candidates très éloignées, comme le détaille la figure \ref{fig:xytrajectoire}. Il s'agit d'une propriété bien connue des fonctions non linéaires, qu'elles soient décrites de façon explicite via le formalisme mathématique, ou de façon implicite dans l'expression des dynamiques complexes de modèles de simulation.

Il est clair que l'information récoltée par un tel déplacement basé sur une distance euclidienne dans le plan $(x,y)$ n'est pas vraiment pourvoyeur d'intuitions sur l'emplacement possible des meilleures solutions (voir figure \ref{fig:xytrajectoire}). Il semble par exemple plus intéressant pour l'optimiseur d'accéder aux solutions par le prisme d'ensembles construits sur la base d'une valeur $v$ commune (voir figure \ref{fig:xyspacePspaceOmultimodal}). Une information qui peut être exploitée de multiples façons, toujours en permettant à l'optimiseur de déterminer un nouvel ensemble de solutions candidates à évaluer.

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Représentation de deux déplacements dans l'espace des solutions candidates et son équivalent dans l'espace des objectifs
	\parbox{\marginparwidth}{
	\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
	        \item Partant de $b$, on se déplace d'une unité vers $c$ ou $a$, ce qui dans l'espace des objectifs équivaut également à un déplacement vers $h$; $v=2$ pour $v_h, v_a, v_c$
	        \item Partant de $b$, on se déplace toujours d'une unité vers $f$, ce qui dans l'espace des objectifs équivaut également à un déplacement vers $d$ et $i$; $v=1$ pour $v_d,v_i,v_f$
	\end{enumerate}}}[fig:xytrajectoire]
	 \centering
	  \subbottom[]{
	 	\includegraphics[width=0.6\linewidth]{xytrajectoire_a.pdf}
	 	\label{subfig_xytrajectoire_a}}\qquad
	 \subbottom[]{
	 	\includegraphics[width=.6\linewidth]{xytrajectoire_b.pdf}
	 	\label{subfig_xytrajectoire_b}}
	\end{sidecaption}
\end{figure}

Si l'obtention d'une cartographie complète d'un tel espace de solutions peut être l'objectif de ce type de raisonnement, la recherche d'un optimum en est un autre. Dans un cas on aura tendance à maximiser la diversité dans le choix de solutions candidates à évaluer, afin d'essayer de couvrir au mieux le territoire à explorer. Cette idée on la retrouve dans l'établissement d'une \textit{fitness landscape}, ou dans sa version multi-objectif, d'un \textit{problem landscape} \autocite[93-94]{Weise2011}, un paysage cumulé de l'espace des objectifs indiquant toutes les valeurs prises par ceux-ci au cours de l'exploration \Anote{paysage_cumule}. Un espace mis à profit par l'optimiseur pour améliorer la proposition de solution candidate, par exemple en se basant sur la construction de cluster de valeurs intéressantes comme indiqué précédemment, ou encore en cherchant à favoriser les zones de cet espace encore peu explorées, etc. Alors que dans le cas d'une optimisation pour la calibration ou la prédiction, trouver le plus rapidement possible un minimum local ou global peut constituer un objectif suffisant.

En réalité, ces deux objectifs sont souvent liés, et c'est souvent l'expertise humaine intervenant de façon externe à l'optimiseur qui va déterminer l'importance de l'un ou de l'autre dans la stratégie à suivre. Dans le cas par exemple d'une optimisation de paramètres nécessaire à la marche efficiente d'une centrale nucléaire, la découverte d'un minimum local robuste peut s'avérer beaucoup plus intéressante qu'un minimum global instable. La topologie proche de l'espace des solutions déjà exploré peut constituer un facteur de connaissance d'intervention plus ou moins importante dans l'expertise d'une bonne ou d'une mauvaise solution.

Cette mécanique on la retrouve également à un autre niveau, dans le fonctionnement interne des métaheuristiques. En effet, celles-ci s'appuient le plus souvent sur la métaphore biologique évolutive pour mettre en tension une recherche de solutions guidée toute à la fois par l'\textit{exploration} (trouver des solutions originales), et l'\textit{exploitation} (améliorer les solutions existantes).

\begin{figure}[!htbp]
\begin{sidecaption}[fortoc]{Recherche d'un minimum global.}[fig:hmap2ab]
 \centering
 \subbottom[Une fonction $f(x)$ présentant un unique minimum global]{
 	\includegraphics[width=.4\linewidth]{heightmap2a.pdf}
 	\label{subfig_hmap2ab_a}}\qquad
 \subbottom[Une fonction $f(x)$ présentant un minimum local et global]{
	\includegraphics[width=.4\linewidth]{heightmap2b.pdf}
	\label{subfig_hmap2ab_b}}
\end{sidecaption}
\end{figure}

Les opérateurs intervenant comme stratégies dans la médiation de ces deux concepts sont conçus pour éviter à l'optimiseur un certain nombre d'écueils. Trop longue pour être abordée ici de façon exhaustive, cette liste évoquant les problèmes et solutions qui résultent du rapport entre les formes de problèmes abordés et les faiblesses génériques ou dépendantes des métaheuristiques utilisées, \textcite{Weise2011} en donne une description experte sur une centaine de pages. On peut également se référer à une autre synthèse, abordant ces problèmes avec un angle un peu plus spécifique aux algorithmes évolutionnaires, réalisée en 2001 par \textcite[316-445]{Deb2001}.

En se limitant aux pièges dépendant de la topologie de l'espace des solutions (voir point \ref{enum_meta_e}), \textcite[140]{Weise2011} a proposé un tableau synthétique dont on extrait ici quelques exemples légèrement modifiés pour éclairer notre argumentaire. Les exemples des figures \ref{fig:hmap2ab} et \ref{fig:hmap2cd} mettent en oeuvre un optimiseur générant de façon incrémentale de nouvelles solutions, chacune représentée par un point. Il faut donc lire ces exemples en tenant compte du fait qu'ils présentent une représentation cumulative des différents points parcourus dans le temps par l'optimiseur.

La figure \ref{fig:hmap2ab} démontre un fonctionnement normal de l'optimiseur, capable quelque soit son placement initial (rouge ou bleu), de trouver le minimum global d'une fonction relativement simple \ref{subfig_hmap2ab_a}. Un comportement équivalent est observable dans la figure \ref{subfig_hmap2ab_b}, le compromis \enquote{exploitation - exploration} étant suffisant pour que l'optimiseur bleu surmonte l'obstacle posé par la présence d'un minimum local dans cette fonction.

\begin{figure}[!htbp]
  \begin{sidecaption}[fortoc]{Deux types de fonctions sont rendues difficiles à optimiser du fait d'une topologie marquée.}[fig:hmap2cd]
  \centering
  \subbottom[Une fonction $f(x)$ multimodale acceptant plusieurs minimum locaux, et un seul minimum global]{
  	\includegraphics[width=.4\linewidth]{heightmap2e.pdf}
  	\label{subfig_hmap2cd_c}}\qquad
  \subbottom[Une fonction $f(x)$ contenant très peu d'information de gradient pour guider l'optimiseur]{
	\includegraphics[width=.4\linewidth]{heightmap2c.pdf}
  	\label{subfig_hmap2cd_d}}
 \end{sidecaption}
\end{figure}

A l'inverse, on perçoit bien sur ce schéma \ref{subfig_hmap2cd_c} quel effet peut avoir un déséquilibre entre les deux stratégies, une exploitation trop appuyée au détriment de l'exploration amenant souvent à une convergence \Anote{def_convergence} prématurée, c'est-à-dire à un piège dans un optimum local.

La figure \ref{subfig_hmap2cd_d} montre également que face à une topologie de fonction présentant un plateau relativement uniforme, l'optimiseur sera en peine pour trouver un minimum, même local. Un paramétrage différent de l'exploration pourra peut être résoudre ce problème, sans pour autant que l'on en soit sur.

Ce qui nous permet d'évoquer une faiblesse connue des métaheuristiques, héritée des remarques déjà faites sur les algorithmes d'optimisations stochastique \Anote{stochastic_note} dans laquelle on les place habituellement. La découverte garantie d'une solution globale optimale est en général difficile avec ce type d'algorithmes (voir point \ref{enum_meta_d}) \Anote{equipe_mixite}, au moins pour deux raisons :

\begin{enumerate}
\item la variabilité qui opère lors de la selection des solutions candidates à un instant $t$ ne permet pas de garantir qu'il n'existe pas quelque part une solution candidate sélectionnée à $t+1$ dont l'évaluation révélera un meilleur optimum. La définition d'un critère d'arrêt est donc rendue délicate.
\item La variabilité dans l'établissement d'une trajectoire de recherche implique qu'un algorithme de même qualité puisse passer une première fois à coté d'un optimum, et une deuxième fois trouver celui-ci.
\end{enumerate}

\begin{figure}[!htbp]
\begin{sidecaption}[fortoc]{Représentation d'une navigation indirecte de l'optimiseur dans un espace de solution $z = f(x,y)$.}[fig:hmap1]
  \centering
  \subbottom[]{
  	\includegraphics[width=.4\linewidth]{heightmap1a.png}
  	\label{subfig_hmap_a}}\qquad
  \subbottom[]{
	\includegraphics[width=.4\linewidth]{heightmap1b.png}
  	\label{subfig_hmap_b}}
\end{sidecaption}
\end{figure}

Pour mieux comprendre les problèmes posés par des espaces de solutions multi-modaux, déjà figurés en deux dimensions dans \ref{subfig_hmap2cd_c}, on représente cette fois ci dans la figure \ref{fig:hmap1} l'optimiseur dans un espace en trois dimensions similaire à celui vu dans la figure \ref{fig:spacePspaceOmultimodal}, à la recherche d'un optimum global. La fonction ainsi représentée comporte deux entrées $(x,y)$, et une sortie $z = f(x,y)$ représentant la valeur numérique résultat de l'optimisation.

Attention à la lecture de ces schémas, il ne faut pas oublier que l'optimiseur \textbf{ne se déplace pas directement} sur le terrain visible dans la figure \ref{subfig_hmap_a}, et pour laquelle celui-ci n'a justement aucune visibilité. C'est un peu comme visualiser un labyrinthe de l'extérieur sur une feuille, puis de l'intérieur quand on s'y projette, la difficulté pour résoudre celui-ci n'est plus la même. La visibilité dont dispose l'optimiseur est celle des résultats de solutions candidates déjà évaluées (voir point \ref{enum_meta_i}). Il s'agit donc de proposer de nouvelles solutions candidates soit en les composant à partir d'une manipulation des solutions candidates déjà évaluées, soit en introduisant de toutes nouvelles solutions candidates prises de façon aléatoire. Au cours de l'itération mesurant la progression de l'algorithme, c'est bien l'évaluation de cette nouvelle population de solutions candidates qui détermine si il y'a effectivement eu un déplacement qualitatif dans l'espace des solutions evaluées. Le déplacement du point rouge dans cet espace n'est donc effectif que si on trouve à un instant $t + 1$ une solution plus intéressante qu'à l'instant $t$.

A partir des résultats de la première solution candidate évaluée figurée ici en rouge dans \ref{fig:hmap1}, les opérateurs de recherches soumis à l'aléa d'une recomposition ou d'un tirage aléatoire peuvent tout à fait proposer un candidat à $(x,y)_{t+1}$ qui débouche sur un résultat $z = f(x,y)$ plaçant l'optimiseur dans le sillon d'un gradient de pente parmi plusieurs. Ce qui mènera probablement l'optimiseur à découvrir des optimums de qualités très différentes : $A$ (local), $B$ (global), $C$ (local), $D$ (local).

Autrement dit, en plus de la stochasticité inhérente de ces algorithmes, non seulement un algorithme de type $A$ n'aura pas les mêmes résultats qu'un algorithme de type $B$, mais celui-ci sera également différent d'un algorithme $A'$ du fait d'un paramétrage différent.

Comme déjà évoqué dans les différentes définitions, on retrouve ici la qualité de flexibilité des métaheuristiques, permettant de transformer ce qui pourrait de prime abord paraitre pour un défaut, en qualité. L'utilisation de celle-ci permettant d'étendre toujours un peu plus leurs champs d'utilisation, en facilitant la réponse aux questions suivantes \Anote{q_ppr} :
\begin{enumerate}
\item  \foreignquote{english}{What parameter settings do I use to get good results when applying heuristic method X to problem Y?}
\item  \foreignquote{english}{How do I adjust the parameters of heuristic X so I get better results on problem Y?}
\item \foreignquote{english}{Which is \enquote{better}, heuristic X or heuristic Y?}
\end{enumerate}

On pourrait ainsi ne retenir que cette citation de source inconnue, lorsqu'elle définit une métaheuristique comme \foreignquote{english}{ a pretty good rule for finding pretty good rules.}

Cette flexibilité vient compléter et compenser efficacement cet horizon de connaissance assez limité, nécessaire à une généricité d'emploi. Les métaheuristiques fournissent ainsi le support générique initial pour en faire un outil d'usage indépendant du problème, tout en fournissant les outils pour favoriser également leur propre modification en vue d'une amélioration de résultat pour un problème donné. Elle cumule donc en quelques sortes les deux propriétés de dépendance et d'indépendance face à un problème donné.

De plus, la recherche dans cette discipline ne se contente pas d'organiser une forme de compétition qui mènerait à elle seule, par l'apprentissage répété de fonctions aussi standardisées que celles utilisées dans les figures précédentes, à une surestimation de certains algorithmes \Anote{test_fonction_surutilisation}, et se nourrit également d'une recherche plus appliquée à des problématiques réelles. Ce qui permet par effet retour, d'espérer voir appliquer à des formes de problèmes génériques, des opérateurs dédiés à l'origine à des problématiques spécifiques. La construction et l'évaluation d'heuristique plus performante servant toujours indirectement une cause plus générale.

Enfin, une des propriétés qui n'a pas encore été introduite dans ce résumé est la capacité de notation et de description abstraite des métaheuristiques (voir point \ref{enum_meta_f} ). Des concepts de plus haut niveau sont introduits pour désigner l'expression et la manipulation d'heuristiques et de classes d'heuristiques dans un système composant la métaheuristique. Mais avant de pouvoir introduire ces subtilités de typologie propre à chaque classe de métaheuristique, il faut également rappeler l'existence d'une base commune de formalisation mathématique permettant la description des problèmes. Autrement dit, cela revient à introduire ou à poser sur une partie des mots déjà utilisés dans cette section, un certain nombre de notations mathématiques d'utilisation relativement standard dans cette communauté informatique utilisant les métaheuristiques.

Il nous restera également à aborder dans la section suivante, la question des \textbf{moyens} mis à disposition de l'optimiseur pour opérer la selection de nouveaux candidats à évaluer. Jusqu'ici seule une représentation de ces solutions candidates dans l'espace des solutions candidates possibles a été abordée, ainsi que l'espace contenant les résultats des solutions candidates évaluées. Mais ces deux espaces ne constituent pas les véritables espaces sur lesquels l'optimiseur est amené à travailler, et cela bien qu'il puisse les intégrer à son expertise pour la selection de nouveaux candidats à l'évaluation \Anote{remarque_section_metaheuristique}.

\hl{les 4 paragraphe ci dessous sont à faire descendre avec notation mathématique, pour compléter la section suivante, et sans briser le suspens ?}

L'introduction d'un nouvel \enquote{espace de recherche} est nécessaire, et  correspond à la somme des entrées, des paramètres, sur lequel l'optimiseur va pouvoir jouer directement, afin de modifier cette fois-ci indirectement l'expression de la solution candidate ensuite évaluée.

Autrement dit, il faut retenir qu'une solution candidate fait partie d'un espace de solutions candidates possibles, et que l'exploration de ce dernier est dépendant des bornes fixées par l'expert pour délimiter l'espace de recherche de chacun des entrant, notamment pour limiter le champ de recherche de l'optimiseur à celui des valeurs empiriquement et théoriquement possibles. Ce qui introduit aussi la possibilité d'une nouveau \textit{mapping} entre les valeurs de ces deux espaces, de recherche, et du phénomène à évaluer, qui ne sont pas nécessairement de même nature.

On peut s'appuyer sur l'exemple de bras robotisé donné par \autocite{Weise2011} pour illustrer ce cas. On a d'un côté les paramètres de positionnement des éléments de bras d'un robot, contraint par la structure théorique de celui-ci, et de l'autre l'expression spatiale finale du bras représentatif de cette combinaison de paramètres dans l'espace des solutions possibles, potentiellement infini, et dont on n'a pas la maitrise directe. L'optimiseur s'appuie ensuite sur l'évaluation de cette configuration spatiale à l'aide des critères qu'on lui a donné pour induire des opérations non pas dans l'espace d'expression spatialisé du bras, mais dans l'espace de recherche des vecteurs de paramètres permettant l'amélioration de ce résultat.

\subsubsection{Une formulation mathématique standardisée pour encadrer les problèmes d'optimisations et les métaheuristiques}
\label{sssec:math_opti}

%search space p 82
%structure p 101
% pareto ranking p 275

Pour comprendre comment se déroule de façon générale la résolution d'un problème d'optimisation, il faut poser un certain nombre de notions qui nous seront utiles par la suite. Cet exercice de description plus mathématique et générique s'appuie là encore principalement sur les écrits de \textcite{Weise2011}

La première étape selon Weise dans la construction d'un problème d'optimisation est de définir le type de structure qui peut être associée à l'expression des solutions possibles et spécifiques à notre problème.

Autrement dit, il s'agit de déterminer quel est l'espace dans lequel évolue la donnée figurant la solution attendue pour cette optimisation. L'expression de cette solution peut appartenir à l'espace des réels $\mathbb{R}$, comme par exemple une valeur numérique se rapportant à l'optimisation d'une fonction mathématique. Mais celle-ci peut également s'exprimer dans un repère beaucoup plus complexe, en faisant référence par exemple à un repère géométrique définissant le cadre  d'une forme à optimiser comme une pièce de moteur, une pièce d'avion, etc. \autocite[43]{Weise2011}

Cet espace du problème (\textit{problem space}) $\mathbb{X}$ est défini comme \foreignquote{english}{ [...] the set containing all elements $x$ which could be its solution.}

Une solution candidate $x$ est quant à elle définie comme \foreignquote{english}{ [...] an element of the problem space $ \mathbb{X}$ of a certain optimization problem.}

L'objectif de l'optimisation est donc de trouver par le biais d'un algorithme adapté l'ensemble des solutions candidates $x^*$ appartenant à l'espace du problème répondant le mieux aux critères définis par l'utilisateur. Ce qui suppose de pouvoir qualifier une solution candidate $x_1$ tiré de $\mathbb{X}$ par rapport à une autre solution candidate $x_2$ elle aussi tiré de $\mathbb{X}$.

\textit{Une deuxième étape logique serait donc d'établir comment se fait la mesure établissant la qualité d'une solution ?}

Comme défini précédemment, ce qui va guider l'algorithme optimiseur dans sa prise de décision, c'est l'évaluation d'une fonction heuristique, ou d'une fonction objectif (\textit{objective function}) \Anote{difference_objective_heuristique}

\foreignquote{english}{An objective function $f: \mathbb{X} \mapsto \mathbb{R}$ is a mathematical function which is subject to optimization.}

Cette fonction objectif lorsqu'elle prend pour paramètre un élément candidat $x$ pris dans l'espace du problème $ \mathbb{X}$ renvoie une valeur définissant sa qualité par rapport au problème posé. \autocite[44]{Weise2011}

\sloppy La plupart des problèmes nécessitent toutefois d'optimiser plusieurs critères simultanément. La relation entre ces critères peut d'ailleurs être elle aussi multiple : dépendante (conflictuelle, en harmonie), indépendante. Nous allons donc nous intéresser directement à la définition de ce type de problème, résumable ainsi :  $min(f_1(x), \dotsc, f_k(x)$ avec $k > 2$

La littérature fait également plus souvent référence à ce type de problème en faisant appel à une notation sous forme de fonction vecteurs. Un ensemble $\vec{f} : \mathbb{X} \mapsto \mathbb{R}^n$ fait de $n$ fonction objectif $f_i : \mathbb{X} \mapsto \mathbb{R}$ avec $\forall i \in 1 \dotsc n$. Appliqué à une solution candidate $x \in \mathbb{X}$ cette fonction renvoie un vecteur de réel de dimension $n$ qui peuvent être projeté dans un espace $\mathbb{R}^n$, aussi appelé espace des objectifs (\textit{objective space}) $\mathbb{Y}$.

En résumé, à chaque association d'un vecteur de fonction objectif $\vec{f}$ et d'une solution candidate $x$ correspond après évaluation un vecteur de réel de dimension $n$ permettant le positionnement de la solution candidate dans l'espace $\mathbb{R}^n$ des objectifs aussi nommé $\mathbb{Y}$.

C'est à partir du positionnement des solutions candidates dans cet espace $\mathbb{Y}$ que l'optimiseur va décider de la prochaine solution candidate à évaluer.

\textit{Dès lors, comment ce choix se fait-il dans une perspective multi-objectifs a priori contradictoires ?}

\begin{figure}[!hbtp]
	\begin{sidecaption}[fortoc]{ Pour la valeur $x = 0$, $f1(x) = 0 $ et $f2(x) = 4 $, pour $x = 2$,  $f1(x) = 4 $ et $f2(x) = 0 $ , donc la configuration inverse. La solution pour minimiser les deux fonctions $f1$ et $f2$ tient donc forcément dans un compromis dans la valeur prise par $x$.}[fig:S_Schaffer]
	\centering
	\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=1.0cm,y=1.0cm]
	\draw [color=cqcqcq,dash pattern=on 1pt off 1pt, xstep=1.0cm,ystep=1.0cm] (-5,-1) grid (5,5);
	\draw[->,color=black] (-5,0) -- (5,0);
	\foreach \x in {-5,-4,-3,-2,-1,1,2,3,4}
	\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt) node[below] {\footnotesize $\x$};
	\draw[->,color=black] (0,-1) -- (0,5);
	\foreach \y in {-1,1,2,3,4}
	\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt) node[left] {\footnotesize $\y$};
	\draw[color=black] (0pt,-10pt) node[right] {\footnotesize $0$};
	\clip(-5,-1) rectangle (5,5);
	\draw[color=ttttff] plot[raw gnuplot, id=func2] function{set samples 100; set xrange [-4.9:4.9]; plot x**2};
	\draw[color=fftttt] plot[raw gnuplot, id=func3] function{set samples 100; set xrange [-4.9:4.9]; plot (x-2)**2};
	\begin{scriptsize}
	\draw[color=ttttff] (-2.26,6.14) node {$f$};
	\draw[color=fftttt] (-0.24,6.14) node {$g$};
	\end{scriptsize}
	\end{tikzpicture}
 \end{sidecaption}
\end{figure}

\sloppy Si on prend pour exemple la fonction multi-objectifs de Schaeffer décrite dans l'équation \ref{eq:schaffer}, $f1(x)$ and $f2(x)$ deux fonctions objectifs à minimiser $\vec{f} = (f1(x),f2(x))^T$  avec $\vec{f}: \mathbb{X} \mapsto \mathbb{R}^2$

\begin{equation} \label{eq:schaffer}
Minimize =
	\begin{cases}
	 f1(x) = x^2 \\
	 f2(x) = (x-2)^2
	\end{cases}
\end{equation}

Si on superpose les deux fonctions comme dans la figure \ref{fig:S_Schaffer}, on voit bien qu'elles sont contradictoires, il s'agit donc de trouver un compromis.

Cette opération que nous pratiquons tous les jours sans forcément le savoir peut être plus facilement expliquée en faisant appel à cet exemple concret. Dans le cas d'un acheteur à la recherche d'une voiture à la fois économe de par sa faible consommation et si possible disponible à un moindre coût, celui-ci devra bien se plier à l'exercice de positionnement des voitures résumé dans le graphique \ref{fig:voiture}.

Dans le graphique \ref{subfig_voiture:b} on constate rapidement que le modèle de voiture que l'acheteur va acheter à de fortes chances de se trouver dans la liste de voitures $\{ A,B,C,D,E \}$ colorées en rouge, aussi appelée front de Pareto, ou optimum de Pareto. Ce terme apparait en économie en 1950, en référence directe des travaux de l'économiste italien Vilfredo Pareto. Le lecteur plus curieux de ces questions pourra trouver de multiples points d'entrées sur ces questions dans les publications suivantes \autocites{Ehrgott2012,Koksalan2011,Koksalan2013}.

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Exemple simplifié d'une catégorisation de voitures selon deux axes comprenant d'une part le coût d'achat et d'autre part la consommation de chaque voiture.}[fig:voiture]
	\centering
	  \subtop[]{
  	\includegraphics[width=.3\linewidth]{opti1.png}
  	\label{subfig_voiture:a}}\qquad
    \subtop[]{
	\includegraphics[width=.3\linewidth]{opti2.png}
  	\label{subfig_voiture:b}}
    \subtop[]{
  	\includegraphics[width=.3\linewidth]{opti3.png}
  	\label{subfig_voiture:c}}
 \end{sidecaption}
\end{figure}

\sloppy En regardant en détail les capacités des voitures figurant dans ce front \ref{subfig_voiture:c}, on voit bien que chacune d'entre elle domine une partie des autres voitures sur au moins un des deux objectifs. Si on prend le cas de la voiture F, celle-ci n'est pas prise en compte dans les solutions optimums, car le point est dominé sur ses deux objectifs par d'autres voitures : $ prix(B) < prix(F) < prix(A) $ et $consommation(A) < consommation (B) < consommation(F)$.

Le front de Pareto renvoie un ensemble de solutions compromis optimal, l'expertise finale sur la ou les solutions à adopter est donc le résultat d'un choix expert externe introduit comme support à l'algorithme optimiseur. Dans notre exemple, l'acheteur devra pour finaliser son achat mettre en avant au moins un des deux critères afin de départager les voitures.

Si certains algorithmes ont introduit cette expertise par le biais d'une sélection interactive guidant l'optimiseur à chaque étape de sa recherche, ce n'est pas cette usage qui nous intéresse ici. L'expertise n'intervient qu'une fois les solutions convergées, dans l'observation des résultats finaux.

Dans notre cas, on considère que l'optimiseur doit sélectionner avec les moyens qu'on lui a fournis les solutions candidates sur lesquels il doit miser pour converger. Il doit donc être capable de séparer les solutions en appliquant un ou plusieurs critères de séparation. Il existe plusieurs types de stratégies (\textit{Agreggation based, Criterion based, Dominance based, Indicator based}), et chacune d'elles peut être croisée ou dérivée en de multiples variantes \autocites[28]{Zitzler1999a, Deb2001}[7]{Liefooghe2009}. Nous limiterons ici notre analyse à un seul de ces cas, en nous focalisant d'ores et déjà sur les méthodes les plus utilisées en EC, basées sur la dominance celles-ci sont directement inspirées des travaux de Pareto.

% TODO : Finir ce paragraphe historique rapide, qui permettra de faire la différence ensuite avec les algorithmes inspirés par Pareto.
% TODO : Ajouter ref de Goldberg sur les sciences humaines + GA dans le chapitre 1

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Graphique en deux dimensions des fonctions objectifs $f_1$ et $f_2$ du tableau \ref{tab:pranking}}[fig:pranking_a]
		\centering
		\includegraphics[width=.4\linewidth]{pareto_ranking_a.pdf}
	 \end{sidecaption}
\end{figure}

\begin{table}[!htbp]
\begin{sidecaption}[fortoc]{Tableau de résultats des fonction objectifs $f_1$ et $f_2$ pour un vecteur de solutions candidates $\{a \dotsc p\}$ dans l'espace des objectifs $\mathbb{Y}$, et résultat du \textit{Pareto Ranking}}
	[tab:pranking]
	\centering
	\begin{tabular}{>{$}l<{$} >{$}l<{$} >{$}l<{$} >{$}l<{$} >{$}l<{$} >{$}l<{$}}
			\toprule
			\text{solutions candidates} & f_1 & f_2 & \text{dominé par} & v_1 & v_2 \\
			\midrule
			a      & 3.5    & 1    &  \varnothing & 0 & 0 \\
			b      & 3      & 1,5  &  \varnothing & 0 & 0 \\
			c      & 2      & 2    &  \varnothing & 0 & 0 \\
			d      & 1      & 3    &  \varnothing & 0 & 0 \\
			e      & 0.5    & 4    &  \varnothing & 0 & 0 \\
			f      & 0.5    & 4.5  &  \{e \}  & 1 & 0 \\
			g      & 1.5    & 4.5  &  \{d,e,f,h \} & 4 & 0 \\
			h      & 1.5    & 3.5  &  \{d \} & 1 & 0 \\
			i      & 2      & 3.5  &  \{c,d,h \} & 3 & 0 \\
			j      & 2.5    & 3    &  \{c,d \} & 2 & 0 \\
			k      & 3.5    & 2    &  \{a,b,c \} & 3 & 0 \\
			l      & 4.5    & 1    &  \{a \} & 1 & 0 \\
			m      & 4.5    & 2.5  &  \{a,b,c,k,l \} & 5 & 0 \\
			n      & 4      & 4    &  \{a,b,c,d,e,h,i,j,k,o \} & 10 & 0 \\
			o      & 3      & 4    &  \{b,c,d,e,h,i,j \} & 7 & 0 \\
			p      & 5     & 4.5   &  \{a,b,c,d,e,f,g,h,i,j,k,l,m,n,o \} & 15 & 0 \\
			\bottomrule
	\end{tabular}
  \end{sidecaption}
\end{table}

Pour comprendre comment cette stratégie et définir mathématiquement la notion d'optimum de Pareto, il faut introduire la notion de \textit{dominance} sur lequel elle s'appuie. Pour cette tâche on s'appuie sur les définitions données par \textcite[65]{Weise2011} :

\foreignquote{english}{An element $x_1$ dominates (is preferred to) an element $x_2 (x_1 \dashv x_2)$ if $x_1$ is better than $x_2$ in at least one objective function and not worse with respect to all other objectives.}

Ce qui dans le cas d'une minimisation se traduit mathématiquement par les conditions suivantes :

\begin{align*}
	(x_1 \dashv x_2) \Leftrightarrow &\forall i \in 1 \dotsc n \Rightarrow - i f_i (x_1) \leq i f_i (x_2) \land \\
	&\exists j \in 1 \dotsc n : j f_j (x_1) < - j f_j (x_2)
\end{align*}

Cette notion de \textit{domination} ($\succ$) \Anote{notation_dominance} permet de dégager ces trois possibilités

\begin{itemize}
\item $x_1$ domine $x_2$ , qui peut également s'écrire $x_1 \succ x_2$
\item $x_1$ est dominé par $x_2$
\item $x_1$ n'est pas comparable avec $x_2$
\end{itemize}

Celle-ci possède les propriétés suivantes, qui définissent dans l'espace des objectifs $\mathbb{Y}$ un \textit{strict partial order} :

\begin{enumerate}
\item{\textbf{non reflexive}}  $x_1$ ne peux pas se dominer lui même
\item{\textbf{non symétrique}} $ x_1 \succ x_2$ n'implique pas $x_2 \succ x_1$, alors que l'opposé est vrai, $x_1 \succ x_2$ implique $x_2$ ne domine pas $x_1$
\item{\textbf{transitive} }
\end{enumerate}

Différents degrés de dominance ont été développés, comme par exemple la notion de \textit{strong dominance} : $x_1$ domine fortement $x_2$ ($x_1 \succ \succ x_2$) si $x_1$ est strictement meilleur que $x_2$ sur tout ses objectifs.

Pour bien comprendre comment se construit l'ensemble $X^*$ de solutions non dominées $x^* \in \mathbb{X}$ ,on peut étudier en détail comment la dominance se calcule entre les points $e$ et $f$ présentés sur la figure \ref{fig:pranking_a}.

\begin{table}[!h]
	\centering
	\begin{sidecaption}[fortoc]{Application des règles de dominance aux points $e$ et $f$. \\ \\
		   \begin{tabular}{>{$}l<{$}>{$}l<{$} >{$}l<{$}}
					\toprule
					 & f1 & f2 \\
					\midrule
					e      & 0.5    &  4   \\
					f      & 0.5    & 5,5  \\
					\bottomrule
			\end{tabular}\\ \\
			(a) $e \succ f$ car e est bien le meilleur sur au moins un des deux objectifs, et n'est pas pire sur aucuns des autres objectifs ($e \succeq f$ ) \\
			(b) f ne domine pas e car f n'est pas meilleur sur aucun des deux objectifs et il est pire sur au moins un des deux objectif}[tab:pranking]

		\begin{minipage}{0.5\textwidth}
			\centering
			\subbottom[e est dominé par f ?]{
				\begin{tabular}{>{$}l<{$}>{$}l<{$} >{$}l<{$}}
					\toprule
					    & f1 & f2 \\
					\midrule
					e \leq f & \text{true} & \text{true} \\
					e < f   & \text{false}  & \text{true} \\
					\bottomrule
				\end{tabular}
		 	\label{pranking_a}}
		 \end{minipage}\hspace{1em}
		 \begin{minipage}{0.5\textwidth}
		 	\centering
			\subbottom[f est dominé par e ?]{
				\begin{tabular}{>{$}l<{$}>{$}l<{$} >{$}l<{$}}
					\toprule
					  & f1 & f2 \\
					\midrule
					f \leq  e & \text{true} & \text{false} \\
					f < e  & \text{false}  & \text{false} \\
					\bottomrule
				\end{tabular}
			\label{pranking_b}}
		\end{minipage}
  \end{sidecaption}
\end{table}

Les solutions admises parmi le front de Pareto (voir figure \ref{fig:frontoptimal}) sont donc ici toutes celles qui ne sont pas dominées faiblement ($\succeq$), ce qui revient à exclure les points $f$ et $l$ du front optimum $\{a,b,c,d,e\}$ car ils sont dominés faiblement ($e \succeq f$); alors que dans le cadre d'une dominance forte ($\succ \succ$), ceux-ci auraient fait partie du front $\{a,b,c,d,e,f,l\}$. En effet si on prend toujours le cas de $e$ et $f$, la condition testant que $e$ est strictement meilleur que $f$ sur tous les objectifs n'est pas remplie. %Cet ensemble de cardinalité forcément inférieure ou égale est qualifié \enquote{d'ensemble fort non dominé} (\textit{Strongly non dominated set}).

\begin{figure}[!htb]
	\begin{sidecaption}[fortoc]{Tracé du front optimum à partir du calcul des individus non dominés, cf. l'ensemble vide $\varnothing$ dans le tableau \ref{tab:pranking}}[fig:frontoptimal]
		\centering
		\includegraphics[width=.4\linewidth]{pareto_front.pdf}{
		}
  \end{sidecaption}
\end{figure}

Le front de Pareto n'est en général jamais entièrement couvert, cela pour diverses raisons :

\begin{itemize}
\item L'évaluation de la fonction à optimiser est souvent coûteuse, comme dans le cas de modèle de simulation dont l'exécution peut prendre jusqu'à plusieurs dizaines de minutes,
\item La zone d'exploration est volontairement bornée du fait des objectifs des expérimentateurs,
\item La stochasticité oblige l'exécution de nombreuses réplications d'une même évaluation,
\item On dispose de ressources finies, or l'espace du front est souvent continu et non borné en dehors des contraintes que l'on aura nous-mêmes fixées.
\end{itemize}

Selon \textcite[70]{Weise2011} et \autocite[19]{Zitzler1999a}, on peut s'aider dans cette tâche d'établissement d'un front de Pareto correct en étant attentif aux points suivants:

\begin{enumerate}
\item{\textbf{Proximité}} Les solutions découvertes doivent être les plus proches possibles du front de Pareto optimal.
\item{\textbf{Diversité}} Si le front optimal possible est trop large, la répartition des solutions \textit{spread} doit être maximisé sur toute la surface de celui-ci, si possible suivant une distribution uniforme.
\item{\textbf{Pertinence}} Les solutions découvertes doivent correspondre aux intérêts définis par le problème, et n'ont aucun intérêt si l'opérateur humain ne peut, ou ne sait les utiliser.
\end{enumerate}

On retrouve dans ces objectifs la tension entre exploration et exploitation, le front de Pareto devant être exploité de façon homogène, tout en garantissant à terme (et si possible le plus vite possible) la convergence vers une zone d'intérêt pour l'expérimentateur (voir figure \ref{fig:convergence_diversite}). Les métaheuristiques n'ayant pas d'apriori sur la forme de problème abordée, c'est dans l'originalité, la diversité des constructions proposées qu'une solution optimale et dédiée peut être trouvée. Il est donc très difficile de faire un listing des meilleures stratégies, et des meilleures combinaisons de stratégies permettant une sélection garantie des meilleurs candidats en fonction de ces objectifs, l'établissement de cette liste ne pouvant être que contextuelle d'un problème d'optimisation donné.

\hl{ref no free lunch theorem }? Wikipedia : Le théorème du « no free lunch » explique qu’aucune instance de métaheuristique ne peut prétendre être la meilleure sur tous les problèmes. Une métaheuristique (M) n’est performante que pour une classe de problème (P) donnée.

Heureusement, un certain nombre de combinaisons, souvent éprouvées par de multiple tests sur des fonctions aux caractéristiques et difficultés soigneusement étudiées (ZDT, etc.), se démarquent par des capacités de résolution acceptable. C'est d'ailleurs souvent sur cette première base que se construisent ensuite les améliorations nécessaires à une réponse optimale, cela en partie grâce à la flexibilité des composantes caractéristique des métaheuristiques.

\begin{figure}[!htbp]
  \begin{sidecaption}[fortoc]{Convergence et maintien de la diversité au sein du front de Pareto}[fig:convergence_diversite]
  \centering
  \subbottom[Un front de pareto sans maintien suffisant de la diversité]{
  	\includegraphics[width=.4\linewidth]{pareto_convergence_a.pdf}
  	\label{subfig_convergence_diversite:a}}\qquad
  \subbottom[Un front de pareto avec maintien de la diversité]{
	\includegraphics[width=.4\linewidth]{pareto_convergence_b.pdf}
  	\label{subfig_convergence_diversite:b}}
 \end{sidecaption}
\end{figure}

\textit{Une fois défini cet ordre partiel entre les solutions candidates évaluées, sur quelle base l'optimiseur prend sa décision pour sélectionner les individus les plus prometteurs ? et comment celui-ci garantit l'évolution des solutions candidates selectionnées au regard des trois objectifs fixés ?}

Comme le dit de façon très claire,\textcite[94]{Weise2011} \foreignquote{english}{Such comparisons, however, only state whether one candidate solution is better than another one or not, but give no information on \textbf{how much} it is better or worse and \textbf{how interesting} it is for further investigation. Often, such a scalar, relative measure is needed.}

L'optimiseur n'ayant pas les capacités pour comparer des fonctions entre elles, c'est par l'attribution d'un scalaire caractérisant chaque vecteur $z^* \in \mathbb{Y}$ résultat de l'évaluation d'une solution candidate, que celles-ci vont pouvoir être départagées. On parle de \enquote{fonction d'utilité}, ou de \enquote{fonction \textit{fitness}} pour désigner cette opération de transformation dont le résultat $z$ est utile uniquement en se plaçant dans le référentiel de l'optimiseur. Il s'agit d'un classement relatif des solutions les unes par rapport aux autres, calculés indépendamment des valeurs prises par les fonctions objectives, et intégrant un certain nombre d'autres critères, définis en réponse aux exigences des trois objectifs déjà évoqués (respect de la diversité, qualité de convergence, pertinence vis-à-vis du problème).

Ainsi, un des tout premiers paramètres à intégrer dans le calcul de cette fonction \textit{fitness} tient évidemment dans le choix d'une stratégie pour tirer un meilleur parti des informations récoltées dans l'application de cet ordre partiel sur l'espace $\mathbb{Y}$. Là ou des algorithmes vont appuyer la sélection des solutions à partir d'un calcul de rang (je ne garde que les $n$ premiers rangs), d'autres vont le faire à partir d'un décompte des non dominés (je ne garde que les individus non dominés $< n$), à partir d'une profondeur (je ne garde que les $n$ premiers fronts), ou encore en mélangeant ces trois informations (voir le résultat du calcul de ces trois informations dans \ref{tab:pranking} \hl{a finir}) A cela il faut également ajouter la diversité de choix à disposition dans la selection d'une dominance, par le changement de l'opérateur utilisé dans le calcul (\textit{weak dominance}, \textit{strong dominance}, etc.), ou même la relaxe de celui-ci (\textit{epsilon-dominance}). Des choix de première importance, car ils interviennent directement dans la construction de l'ensemble de solution retenue.

C'est donc dans l'espace des objectifs $\mathbb{Y}$ que se révèle la première information pertinente pour l'optimiseur, nous indiquant, peu importe la forme de l'une ou de l'autre des fonctions et le positionnement des points sur celles-ci, une première sélection de solutions parmi les solutions candidates évaluées sur laquelle l'effort de l'optimiseur doit porter en priorité.

Mais lorsque l'on reprojette les résultats du front de Pareto dans l'espace figurant la dynamique supposée de chacune des deux fonctions objectifs, on observe que la prise de décision basée sur le seul ordonnancement des solutions n'est pas suffisante pour garantir une selection optimale des meilleurs candidats à l'évolution (voir figure \ref{fig:mo_landscape}).

La forme des fonctions dans cette figure est représentée en pointillé car elle n'est qu'une description temporaire d'un paysage en partie inconnu, en attente d'être révisée par l'évaluation de nouveaux points. Le tracé d'un paysage ne se confond plus comme cela pouvait être le cas dans une optimisation mono-objectif avec la valeur prise par la fonction objectif, et doit maintenant intégrer un intermédiaire supplémentaire plus complexe qui est le calcul d'une fonction \textit{fitness}, et dont la formulation, dépendante de nombreuses stratégies, va modifier les solutions choisies dans le futur, et donc modifier la façon dont on va découvrir l'approximation de ce paysage, cela de façon indépendante aux objectifs choisis. \autocite{Weise2011}

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Projection du front de Pareto optimal (point \sqbox{tangoBlack1}), et des autres solutions candidates dominées (point \sqbox{tangoGrey1}) sur l'espace de variation du paramètre $x \in \mathbb{R}$, un schéma inspiré par \textcite[67]{Weise2011}
	\parbox{\marginparwidth}{
	\begin{enumerate}[label={},labelindent=0pt,leftmargin=*]
	      \item \sqbox{tangoBlue1} $f_{1}(x)$
	      \item \sqbox{tangoRed1} $f_{2}(x)$
	\end{enumerate}}\\
	Les fonctions $f_{1}(x)$ et $f_{2}(x)$ sont représentées en pointillé car elles sont inconnues de l'optimiseur, et ne servent que de repère au lecteur pour mieux comprendre comment un paysage caractérisant l'intersection des deux fonctions peut émerger durant l'optimisation, et pourquoi cela peut être intéressant d'intégrer son analyse à l'optimiseur.}[fig:mo_landscape]
	 \centering
	 	\includegraphics[width=0.8\linewidth]{multi_objective_landscape.pdf}
	\end{sidecaption}
\end{figure}

%on se rend également compte qu'un surplus d'information tiré de l'exploitation d'autres espaces pourrait être utile au choix de l'optimiseur

%\Anote{weise_multi2D}
Déjà beaucoup plus difficile à imaginer que dans l'exemple précédent de l'équation de Schaeffer, la re-projection des solutions candidates évaluées se fait sur un nouvel espace $\mathbb{G}$ (voir figure \ref{fig:relation_espaces}), qui inclu l'ensemble de tout les éléments $g \in \mathbb{G}$ qui peuvent être manipulés par les opérateurs de recherche à disposition de l'optimiseur \autocite[82]{Weise2011}. Un processus détaillé un peu plus tard dans cette section.

Dans notre cas $x \in \mathbb{R}$, on a donc un paramètre qui est manipulable et peut prendre une infinité de valeurs dans le cadre des contraintes définies pour $x$ (par exemple une valeur de 0 à 10 pour $x$) \Anote{remarque_resolution}. L'espace $\mathbb{G}$ contient la codification du problème, ce qui par exemple dans le cadre de simulation, se traduit pour chaque élément $g$ par l'attribution d'un vecteur de paramètres définissant les entrées de la simulation sur lequel l'optimiseur va pouvoir \enquote{jouer} pour optimiser les différentes fonctions objectifs.

La fonction $gpm : \mathbb{G} \mapsto \mathbb{X}$ est une translation opérée lorsque les deux espaces sont de nature différente, par exemple pour passer d'un espace Binaire à un espace de Réels $\mathbb{B} \mapsto \mathbb{R}$. Dans le cadre de simulations, les deux espaces sont souvent de nature similaire $\mathbb{G} = \mathbb{R}$. On pourra se référer à \textcite[86-88]{Weise2011} pour plus de détails.

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Résumé des relations entre les différents espaces dans une optimisation}[fig:relation_espaces]
		\centering
		\includegraphics[width=.7\linewidth]{objectifsToSearchSpace.pdf}
  \end{sidecaption}
\end{figure}

%D'une part, l'observation de dynamiques en partie contraires sur ces deux fonctions $f_1$ et $f_2$ nous permet de constater encore une fois pourquoi un déplacement de l'optimiseur sur l'une ou l'autre des fonctions dirigé par la recherche d'un optimum n'a aucun sens.

L'opération de sélection des solutions candidates est souvent rattachée au processus de convergence. L'objectif de l'optimiseur est d'évaluer au mieux le potentiel de chacune des solutions durant cette phase de sélection pour intégrer et conserver les meilleurs éléments à son référentiel entre deux itérations. On imagine pourtant très bien bien l'effet que peut avoir une sélection trop restrictive sur le maintien de la diversité. C'est le cas par exemple si l'optimiseur ne décide de garder que le front de Pareto, on voit bien sur la figure \ref{fig:mo_landscape} à quel point la couverture de la dynamique des deux fonctions ressortirait considérablement appauvrie à la suite d'un tel choix. On en déduit que la frontière entre stratégies de convergence, et stratégie de maintien de la diversité doit être assez perméable pour garantir le choix de solutions candidates pertinentes en dehors du seul front de Pareto. Zitler \hl{ref autre que ppt à trouver} retient par exemple parmi ces classes de stratégies celle s'appuyant sur le couple associant espace des objectifs et au choix la dominance, la densité, le temps, ou encore la chance. Ce sont des heuristiques qui vont intervenir en amont sur la qualité et la diversité des solutions candidates (par exemple les stratégies de \textit{sharing}, \textit{crowding}, etc.) qui peuvent ensuite être manipulées par les opérateurs de recherches de l'optimiseur.

Viennent ensuite les stratégies de recombinaison des solutions selectionnées, créatrices de nouvelles solutions candidates à évaluer. Un processus qui peut être là aussi rattaché tout autant au maintien de la diversité qu'à une volonté de convergence accrue. Il n'y a là encore aucune règle d'applications spécifique, et tout dépend de l'objectif fixé de façon initiale ou au cours de l'expérimentation. Ainsi, certaines stratégies intégrés aux opérateurs peuvent être mis en place pour limiter une convergence trop rapide des solutions (\textit{premature convergence}) liée à une perte de diversité, alors que d'autres vont tenter d'accélérer cette convergence par la mise en oeuvre d'opérateur de recherche plus agressif, soit pour trouver le plus rapidement possible un minimum (ou maximum) local, soit car la topologie de l'espace des objectifs est de topologie difficile.

La sélection de candidats à la manipulation dans l'espace des objectifs $\mathbb{X}$ se réfère, une fois projetée dans cet espace $\mathbb{G}$, aux éléments $g$ accessibles à la manipulation par les opérateurs de recherche de la fonction $searchOp$. Chacun de ces opérateurs, dont le nombre et la nature est un paramètre de l'optimiseur, s'appuie sur la transformation d'une ou plusieurs solutions candidates dirigée par la création d'un nouvel élément $g$, dont on attend si possible un meilleur résultat à l'itération suivante. Un postulat très fort est posé par ce type de méthodes d'optimisation, l'introduction de petites variations sur les valeurs de l'espace de recherche est également censée apporter de petites variations dans l'espace des objectifs, que cela résulte en une amélioration ou en une dégradation de la qualité. Appelée \textit{strong causality} \Anote{note_strong}, cette propriété est évidemment dépendante de la forme prise par le paysage du problème (\textit{problem landscape}), et plus celui-ci est accidenté, rugueux, plus sa résolution est considérée comme complexe \Anote{note_weak}.

En relation avec cette observation, l'éclatement de cette population de solutions candidates évaluées sur le paysage nous permet de constater (voir la figure \ref{fig:mo_landscape}) à quel point la notion de distance entre les points parait différente entre ces deux espaces. $f$ et $c$ apparaissent ici beaucoup plus proches de trouver un optimum global que $a$ et $b$, pourtant plus proche de $c$ dans l'espace des objectifs. On voit bien ici que la sélection de solutions candidates intéressantes peut intégrer d'autres informations utiles, en supplément de celle fournit par l'analyse de $\mathbb{Y}$, au travers de l'analyse de cet espace $\mathbb{G}$; et cela toujours afin de guider au mieux l'optimiseur dans la selection des candidats à l'évolution. Un croisement du positionnement des individus $f$ et $c$ donnerait ainsi une bien meilleure valeur de $x$ à évaluer, probablement meilleure que celle d'un individu $a$ et $c$. Si la solution $f$ avait été éliminée sur le fait d'une sélection aux critères plus drastiques, c'est aussi la possibilité d'un croisement fructueux avec $c$ qui disparait.

%A ces stratégies principales s'ajoute un autre ensembles de stratégies, dont certaines sont plus spécifiques, ou constitutives des types d'algorithmes utilisés. %Le maintien d'une diversité de solutions entre les itérations fait partie de ces stratégies qui font partie d'un set plus large de stratégies permettant de contrer l'émergence des différentes difficultés (stochasticité, topologie, etc.) caractéristique d'un problème de résolution unique.

%Généralement nommé \foreignquote{english}{Pareto Ranking} \Anote{utilisation_pareto_ranking} aussi nommé par Weise \foreignquote{english}{Prevalence Ranking}.

% Ou introduire la notion d'individu ?

\begin{figure}[!ht]
	\begin{sidecaption}[fortoc]{Résumé simplifié du déroulement d'une optimisation selon \textcite[109]{Weise2011}}[fig:resume_opti]
		\centering
		\includegraphics[width=\linewidth]{espace_resume.pdf}{
		}
  \end{sidecaption}
\end{figure}

La description des étapes de la figure résumé \ref{fig:resume_opti} sont les suivantes :

\begin{itemize}[label=\textbullet]
	\litem{1} Une première population $P \in {1 \dotsc n}$ de vecteurs paramètres ${p}$ est générée par l'optimiseur ou introduite par l'expérimentateur, puis soumis à évaluation.
	\litem{2.a} La fonction à optimiser est évaluée autant de fois qu'il y a de vecteurs ${p}$
	\litem{2.b} Les fonctions objectifs $\vec{f}$ sont calculés, ce qui permet de créer autant de vecteur ${v}$ correspond au résultats des fonctions qu'il y a de $P$ évalué. Ces vecteurs $P(v)$ peuvent être positionné dans un espace des objectif $\mathbb{Y}$
	\litem{3.a} Le calcul de fitness $f$ est effectué pour chaque élément de $P$ en utilisant les informations rapportés par un ensemble d'heuristiques sur $\mathbb{Y}$ et, ou $\mathbb{G}$
	\litem{3.b} A partir du calcul de cette fitness $f$ pour chacun des éléments de $P$, on selectionne les $P^*$ meilleurs éléments.
	\litem{4} A partir d'un ensemble d'opérateur ${op}$ on va générer de nouveaux vecteurs de paramètres $P(p)$, qui va constituer le nouveau jeu de solution candidates à évaluer à l'étape (1), et dont on espère qu'elles seront si possible meilleures que les précédentes.
\end{itemize}

\hl{Manque la notion d'invididu = fitness + genotype + phenotype}

% Injection de connaissance se fait un peu partout pour la construction d'une fitness.

% Penser à dire qu'il y a plusieurs stratégies de comparaison autre que Pareto ?

%Première fois utilisé en 1989

%Si on transfère ce langage neutre au vocabulaire que l'on peut trouver courrament dans l'EC, alors l'espace des solution devient le \textit{phenome}, et le point de cet espace qui correspond à la solution candidate devient un \textit{phenotype}.

%\begin{figure}
%\begin{sidecaption}[fortoc]{ POM cycle for developping theory for an agent behavior \autocite[245]{Railsback2012}}[fig:S_syntheseGrim]
%  \centering
% \includegraphics[width=.9\linewidth]{cyclePOMcomportement.png}
%  \end{sidecaption}
%\end{figure}


%Dans notre étude, l'objet à optimiser ne se réfère pas à une expression mathématique, mais à un modèle de simulation, sur lequel on va déterminer un ensemble de critères qui vont faire figure d'équivalent de ces fonction objectives. Dans ce cas d'utilisation, l'optimisation est plus souvent employé comme une forme de calibration inversé \autocite{Grimm2011}, dans laquelle on cherche à déterminer si il existe un ou plusieurs jeu de valeur de paramètres du modèles de simulation respectant la plage de valeur viable empiriquement qui permettent de maximiser l'obtention d'un ou de plusieurs critères experts. Il est plus parlant dans notre cas de désigner l'espace de recherche comme l'espace des paramètres.

La branche des métaheuristiques EC que nous allons étudier plus spécifiquement s'appuie sur l'observation de phénomènes naturels, comme l'évolution, ou l'organisation, pour la construction et la mise en oeuvre d'algorithmes mimant certaines propriétés intéressantes de ces processus, cela sans être rattaché à une contrainte de réalisme biologique.

\subsection{Les métaheuristiques bio-inspirées, la branche des Algorithmes Evolutionnaires}
\label{ssec:EA}

\subsubsection{Un rapide historique de la discipline}
\label{sssec:historique_EA}

On a déjà rapidement décrit dans la section à propos de l'Artificial Life \ref{p:heritage_complexe} les deux voies qu'il était possible d'emprunter dans l'intéret porté sur la définition du processus naturel d'évolution.

Il existe en effet au moins deux façons aujourd'hui d'introduire des développements informatiques se rapportant à ce processus évolutif. D'un côté, les tentatives de reproduction plus ou moins fidèles des différents mécanismes à l'oeuvre dans le processus d'évolution mettent en avant un objectif de compréhension, alors que la focalisation sur ces mêmes mécanismes pour leur seule capacité d'apprentissage tend à s'éloigner de la réalité biologique pour s'orienter plus vers le développement d'algorithmes désignés comme métaheuristiques. Autrement dit, là ou des chercheurs vont tenter de reproduire au mieux le processus d'évolution dans ce qu'il a de créatif, de non optimisé, de coévolutif car construit par \Anote{note_pattee_semantic_closure} et avec l'environnement, d'autres vont reprendre ce même processus en vue d'une évolution si possible bornée et dirigée par la résolution efficace d'un ou de plusieurs objectifs définis de façon fixe et extrinsèque \autocites{Taylor2001, Taylor2012}.

Lorsqu'on s'intéresse de plus près à la littérature scientifique de ces algorithmes regroupés depuis Fogel sous le terme d'\foreignquote{english}{Evolutionary Computation} (EC), on constate pour toute une partie des publications une de-contextualisation complète de leur utilisation. La question d'une similitude initiale avec le vivant n'étant le plus souvent évoquée que pour illustrer des racines historiques éloignées, alors qu'une meilleure connaissance de cette histoire pourrait au mieux participer d'une amélioration des algorithmes, et au pire au moins éviter la persistance de certains malentendus \autocite{DeJong1993a}. Ce qui peut apparaitre comme une forme de surspécialisation est en quelque sorte le prix à payer d'une évolution de la discipline avant tout dirigée par une communauté de chercheurs informatiques motivés par la recherche d'algorithmes performants et d'applications génériques.

Si aujourd'hui on peut observer un tel cloisonnement, un regard sur l'histoire de la discipline tend à montrer tout l'inverse, car nombreux sont les pionniers ayant développé des intérêts simultanés pour ces deux approches : les expériences très longtemps restées inconnue du mathématicien Barricelli dès 1954 \Anote{barricelli_multi_utilisation}, l'approche du généticien \textcite{Fraser1957} qui décrit et simule l'évolution de population génétique dès 1957 \Anote{fraser_comment}, les travaux de Pattee et Conrad avec EVOLVE à la fin des années 1960 \autocite{Conrad1970}, les algorithmes génétiques \Anote{holland_multi_utilisation} de Holland, un élève de Burks, un scientifique dont on a déjà vu dans le paragraphe \ref{p:va_automate_cellulaire} qu'il était proche de Von Neumman.

Il existe toutefois une littérature scientifique parallèle qui continue de motiver la rencontre autour de disciplines scientifiques ayant un intérêt pour la recherche en \textit{Artificial Life}. C'est le cas par exemple de la biologie, ou de l'écologie \autocite{Hamblin2013} qui organisent autour de publications transverses la réflexion sur la reintroduction des outils tel qu'ils sont développés en informatique, entrainant de fait aussi la création et l'évolution de ces derniers \autocite{Hogeweg2011}. C'est également le cas en biologie, ou on imagine l'importance que peuvent avoir les travaux de \textcites{Taylor2001}[221]{Taylor1999} pour la mise en oeuvre de modèles de simulation dirigés vers l'émergence \enquote{créative} de nouveaux phénotypes dans un environnement ouvert \autocite[33]{Taylor1999}. Une critique récurrente adressée aux modèles d'auto-organisation actuels \autocite{Pumain2003}, encore incapables de simuler l'émergence de nouvelles structures, de nouvelles entités de façon crédible. Une autre forme de relation entre les deux approches est également envisageable dans certaines disciplines, comme en écologie, où celles-ci peuvent parfaitement se côtoyer : \foreignquote{english}{The first of these requires the application of the evolutionary process in much the same way as it has been traditionally applied within A-Life: as a means to dynamically adjust agent parameter values to support their viability and reproduction within the virtual environment. [...] The second approach we suggest employs artificial evolution to match simulation patterns against data gathered from the level of specific species up to data concerning specific ecosystems. Once the parameters of the system have been optimised so as to reproduce the patterns observed in field data, the evolution algorithm is turned off. The model may then be employed to answer questions relating to the specific ecosystem and species that it represents. Unfortunately it may not then be used to study the evolution of these specific species in specific environments. This is a shortcoming of the artificial evolution algorithm (it does not model real evolution in detail) that would be worth overcoming.} \autocite{Dorin2008}

Le lecteur souhaitant obtenir une vue plus globale des différents concepts et ramifications disciplinaires réunit sous le terme parapluie d'\textit{Artificial Life} peuvent se référer à l'article d'\textcite{Aguilar2014} qui concentre un grand nombre d'entrées bibliographiques essentielles pour aborder les entrées de cette thématique dans chacune des disciplines. On trouvera également une description plus précise sur l'histoire commune de ces deux voies de recherches, telles quelle est perçues par les acteurs historiques de l'EC, dans les ouvrages de \autocites{DeJong2006a, Fogel1998, Fogel2006a, Fogel2006b, Back1996, Back1997}.

Dans cette section, c'est bien la deuxième branche de recherche qui est suivie, celle visant l'\enquote{optimisation}. Les développements tels qu'ils sont abordés ne se mesurent donc plus en fonction d'un critère de réalisme biologique, mais en fonction de critères informatiques et mathématiques se rapportant plus à la capacité de résolution des algorithmes, et aux supports de mise en oeuvre et de mesure de ces derniers : rapidité, diversité, robustesse, qualité, etc.

\textcite{DeJong2006a} retient trois foyers importants pour le développement de cette deuxième branche dans les années 1960, la \textit{Technical University} de Berlin avec Rechenberg, Biernet et Schwefel \autocite{Beyer2002}, UCLA à la même période avec Lawrence J. Fogel, et l'université du Michigan avec John Holland.

De ces trois branches vont émerger au cours des années 1970 ce que \textcite{DeJong2006a} qualifie comme des \foreignquote{english}{Evolutionary Algorithms (EA)} canoniques. Autrement dit, ce sont des algorithmes matures, qui ont prouvé leur capacité à produire des solutions dans un contexte précis : \foreignquote{english}{Evolutionary Programming (EP)}, \foreignquote{english}{Evolution Strategy (ES)}, \foreignquote{english}{Genetic Algorithm (GA)}

Ils vont représenter chacun le foyer d'un développement qui va s'accélérer dans les années 1980, avec l'amorce d'une popularisation de ces techniques permises entre autres par l'avènement de capacités de calcul plus conséquentes et la reconnaissance de l'efficacité de ces algorithmes pour la résolution de problématiques industrielles plus concrètes. L'ouvrage de synthèse écrit par \textcite{Goldberg1989} contribue de façon très importante à cette diffusion, et constitue également un apport théorique important dans la naissance de la branche multi-objectif de cette discipline.

Les années 1990 vont quant à elles consacrer la rencontre et l'unification de ces différentes approches restée jusqu'alors assez indépendantes si on en croit \textcite{DeJong2006a}. De cette confrontation nait la reconnaissance d'un seul terme fédérateur, l'\textit{Evolutionary Computation (EC)} motivant alors la création de nouvelles conférences et de nouveaux journaux structurant cette nouvelle discipline. C'est aussi à partir de cette période que l'on observe la mise en place d'une hybridation accélérée entre les différentes approches qui s'accompagne d'une forme de remise à plat théorique et l'émergence d'un cadre de réflexion unifié. \autocites[23-31]{DeJong2006a}{Back1997}

Si on se concentre plus précisément sur la branche multi-objectif de la discipline, la première introduction théorique d'une stratégie s'appuyant sur le calcul de l'optimum de Pareto pour définir un classement original des solutions évalués est présenté à la page 197 de \textcite[197]{Goldberg1989}. Cette technique nommé \textit{Non Dominated Sorting} (NDS) \autocite[40-43]{Deb2001}, probablement la plus efficace et plus célèbre, sera reprise et implémenté presque dix ans plus tard en 1994, dans l'algorithme célèbre NSGA (Non dominated Sorting Algorithm) de Deb et Srinivas.

Les travaux de \textcite{Goldberg1989} ont influencé tout une génération de chercheurs à partir de cette simple ébauche théorique de tri basé sur la dominance de Pareto, et nombreux sont ceux qui se sont par la suite appuyés \autocite[175, 235]{Deb2001} sur les informations du calcul de dominance pour développer diverses stratégies d'attribution de \textit{fitness}, comme MOGA (Fonseca et Flemings 1993), NPGA (Horn et Nafpliotis 1994), NSGA (Deb et Srinivas 1994), et bien d'autres \autocite[14]{Zitzler1999a}. Ce que l'on peut considérer comme la génération suivante d'algorithmes, que \textcite{Coello2006} \Anote{coello_note} fait démarrer avec l'apparition de l'élitisme \Anote{note_elitisme}, est plus axée encore sur l'efficacité de ces derniers avec notamment l'ouverture d'une branche de recherche développant des métriques de performance, et de nouveaux standards de mesures \autocites{Coello2006, Zitzler2003,Huband2006}, dont on s'aperçoit qu'elles sont devenues nécessaires pour comparer correctement les algorithmes entre eux \autocite[14-15]{Zitzler1999a}. Parmi ces nouveaux algorithmes, devenus depuis canonique, on trouve PAES (Knwoles and Corne 2000), SPEA (\autocite{Zitzler1999}), ou encore NSGA 2 (Deb 2000) etc. On trouve à ce sujet un état de l'art et des exemples de calculs à la main pour ces différents algorithmes dans un des premiers et très bon ouvrage de synthèse de \textcite{Deb2001}, aux chapitres 5 et 6. Il est toutefois à noter, comme le fait déjà Golberg en 1989, que cette problématique de recherche d'une solution à un problème multi-critères, puis multi-objectifs est d'origine bien plus ancienne, et a pu servir de support à la mise en oeuvre de techniques plus ou moins similaire à celle de Pareto. Ainsi, les premières traces en EA d'un intérêt théorique et parfois pratique de ces problèmes semblent remonter à Box et Draper (1957), Fogel (1966), Rosenberg (1967) \autocite[174-175]{Deb2001}. Mais la première implémentation informatique est en général attribuée à David Schaffer, avec son travail de thèse (1984) et l'implémentation de l'algorithme d'optimisation VEGA (\textit{Vector Evaluated Genetic Algorithm}) \autocite{Schaffer1985}. Un autre état de l'art sur l'optimisation multi-objectifs s'appuyant sur Pareto en dehors des techniques purement évolutionnaire est également possible. C'est grâce à l'existence de ce cadre formel mathématique permettant la description d'un problème d'optimisation, tel que nous l'avons un peu abordé dans la section précédente en s'appuyant sur les écrits de \autocite{Weise2011}, que \textcite[50-79]{Deb2001} indique par exemple comment certaines de ces stratégies hors EC ont été pour certaine également transférées avec plus ou moins de succès aux EA \textcite[171-237]{Deb2001}.

Enfin on notera qu'il existe également une autre classe proche d'algorithmes d'optimisation basée sur une observation des mécanismes naturels, celle-ci n'étant plus basée sur la métaphore évolutive par reproduction (même si l'hybridation est envisageable), mais sur les capacités d'organisation et d'auto-organisation observées chez certains animaux comme les fourmis, les abeilles. Ces comportements ont d'abord inspiré les développements de plateformes informatiques adaptées à l'émergence de ce type de comportements, avant d'être repris et utilisé de façon beaucoup plus abstraites par la suite pour résoudre des problèmes d'optimisation. Aujourd'hui regroupées sous le terme de \foreignquote{english}{Swarm Intelligence}, ce sont par exemple les algorithmes PSO (Particle Swarm Optimization), ACO (Ant Colony Algorithms), ABC (Artificial Bee Colony), etc.

\subsubsection{Les principes sous-jacent aux EA}
\label{ssec:principesEA}

Afin de pouvoir mettre en oeuvre la possibilité d'une telle souplesse dans l'adaptation de l'algorithme à une problématique d'optimisation donnée, \textcite[49]{DeJong2006a} a considéré la construction d'un modèle conceptuel plus abstrait capable d'englober dans sa description les mécanismes d'au moins ces trois version canoniques GA, ES et EP. Les concepts clef qui se dégagent d'une telle prise de distance peuvent ainsi être repris non seulement pour décrire les version canoniques mais également pour développer de nouvelles variantes ou extensions d'algorithmes.

Les éléments communs retenues sont les suivants :

\begin{itemize}
\item Une population de taille constante $m$ évolue au cours du temps
\item La population courante est utilisée comme une source de parents pour produire une progéniture (\textit{offsprings}) de taille $n$
\item La population étendue ainsi constitué est réduites de $m + n$ à $m$ individus.
\end{itemize}

\hl{a completer avec schéma expliquant les différentes etapes de l'algorithme générique en EA}

% En général puis recentrage sur les détails ?
\paragraph{Les avantages et les inconvénients d'une terminologie spécifique}

En 2014 une publication sur le blog du spécialiste de la discipline Thomas Weise's \Anote{billet_weise} revient longuemment sur les problématiques de ce vocabulaire inspiré par la biologie et ancré dans les différentes branches composantes l'EC. Il retient quatre problématiques dans l'usage de cette terminologie, parmi lesquels l'incompatibilité des terminologies entre les différentes branches, la dissonance entre la terminologie et la réalité d'application des algorithmes, le fait que l'optimisation au sens naturel n'est pas forcément une bonne optimisation, le fait également que cette terminologie sonne comme anti--professionnelle \Anote{note_pengouin}. Le plus grand problème étant dans ce cadre l'invention de néologisme ne faisant référence ni au domaine biologique, ni au domaine informatique. Ce n'est pas la seule voix à se faire entendre sur ce sujet, comme l'article de \textcite{Sorensen2013b} qui pointe clairement les dérives néfastes associées à l'usage systématique et surtout non réellement argumenté de nouvelles analogies ou métaphores, que celle-ci soit biologique ou pas ...

Si la perspective d'un changement d'annotation et de vocabulaire est probablement conçue comme une étape majeure dans la progression et l'unification d'une discipline depuis quelques années déjà sur la voie de la maturité, Weise tout en prônant au maximum la bonne parole continue comme beaucoup d'autres à utiliser cette terminologie \autocite{Weise2011}, très ancrée dans un folklore qui tient à l'historique de la discipline. La librairie logicielle MGO décrite par la suite s'appuie elle aussi sur cette terminologie, aussi nous n'utiliserons donc les terminologies alternatives proposées par Weise que sous forme de complément, afin de ne pas introduire trop de distance entre les termes décrivant les algorithmes dans ce manuscrit et la réalité du programme tel qu'il est conçu pour le moment.

\begin{table}[!htbp]
\begin{sidecaption}[fortoc]{Tableau de correspondance entre les notations à consonances biologiques et les notations plus génériques liées à l'optimisation, lorsque celles-ci existent. Une traduction appuyée sur la section précédente, et les travaux de \textcite{Weise2011}}
	[tab:ptraduction]
	\centering
	\begin{tabular}{ll}
		\toprule
		générique & biologique (FR)\\
		\midrule
		Espace du problème ($\mathbb{X}$) & Phénome \\
		Espace de recherche ($\mathbb{G}$)   &  Génome \\
		Point dans un espace de recherche ($g \in \mathbb{G}$) & Génotype \\
		Solution candidate, Point dans un espace de solutions ($x \in \mathbb{X}$) & phénotype \\
		Opérateurs de recherches ($searchOp$) & Reproduction \\
		Opérateur de recherche Unaire & Mutation \\
		Opérateur de recherche Binaire & Crossover \\
		Itération & Génération \\
		?  & Progéniture \\
		?  & Mating pool \\
		\bottomrule
	\end{tabular}
  \end{sidecaption}
\end{table}

\paragraph{Les avantages et les inconvénients des EA}

Ces deux listes comparant les avantages et les inconvénients des algorithmes évolutionnaires sont tirés des ouvrages de Deb2001, Fogel2000, Back1997, \autocite[104,105]{DeJong2006a} mais également Openshaw qui a exploré ces techniques en géographie. \hl{ref a completer}

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]

	\item Evaluation d'une population entière par génération, ce qui n'est pas le cas de nombre de techniques fournissant une seule solution par exécution d'algorithme.
	\item Facile à paralléliser, une propriété étudiée très tôt \autocite[444]{Alba2002}
	\item Ne demande a priori aucune connaissance de la forme du paysage, même si en réalité c'est un plus pour bien choisir et paramétrer les métaheuristiques
	\item Applicable à des problèmes continus, discrets, ou les deux.
	\item Ne demande pas forcément de repartir de zéro entre chaque analyse en comparaison à d'autres algorithmes
	\item Le nombre de degrés de liberté accessible pour modifier une métaheuristique est très important, ce qui augmente les chances de trouver une combinaison adaptée à un problème complexe donné
	\item Les principes de mise en oeuvre sont relativement faciles à comprendre
    \item Efficace, même sous une forme canonique
    \item Capacité à explorer de très large espace de recherche
\end{enumerate}

Les désavantages, limitations :

\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
	\item le fonctionnement reste opaque, et le résultat n'est souvent pas tractable mathématiquement
	\item la stochasticité demande la mise en oeuvre de réplication
	\item pas de garantie de trouver un optimum global
	\item le nombre de degrés de liberté demande une certaine forme expertise pour en tirer le meilleur parti, la construction d'un EA optimal pour un problème donné étant progressif, incrémental
    \item trop facile à comprendre, il en résulte une certaine illusion quant aux capacités \enquote{magiques} de ce type d'algorithme.
    \item necessite une source conséquente de puissance pour réaliser de grands nombres de calculs en parallèle
    \item les performances se dégrade avec l'augmentation de l'espace de recherche (dimensionnalité) ou le nombre d'objectifs (on touche les limites de l'approche Pareto) \hl{ref Zitzler1999a page 24 cite Fonseca Flemming 1995}
\end{enumerate}

\hl{2 paragraphe qui suivent à replacer au bon endroit}

L'optimisation de paramètres est un domaine d'application reconnu des EA du fait aussi de la facilité de mapping entre vecteur de paramètres et génome \autocite[83]{DeJong2006a}

Dans la lignée des objectifs définis dans le chapitre 3, cette branche spécifique des EA est celle qui est à la fois la facile d'accès en terme de compréhension pour les débutants tout en restant également suffisament flexible et efficace pour convenir à notre utilisation.

De plus certains désavantages sont de conséquences plus limitées dans le cadre de nos objectifs. En effet, pour le profil d'utilisateur modélisateur que nous visons la garantie d'un optimum global n'est pas la priorité immédiate en comparaison de l'importance d'accéder rapidement à des premiers résultats via un premier EA générique exécutable, qu'il pourra de toute façon ensuite améliorer du fait de la nature métaheuristique de ces algorithmes. De façon similaire, le fait que le résultat ne soit pas tractable mathématiquement n'est pas vraiment problématique dans le cadre des systèmes complexes, ou ce type d'observation est justement une propriété récurrente des systèmes que l'on cherche à simuler pour mieux les comprendre.

Enfin, il est important de noter que cette classe d'algorithmes accueille les approches les plus efficaces pour la résolution de problèmes multi-objectifs, une propriété courante des problèmes abordés dans notre discipline, car les modèles de simulation construisent souvent leur crédibilité aux croisements de multiples critères. (\hl{POM, cf problème inverse calibration déjà expliqué en chapitre 3?})

\hl{Introduction à un algorithme simplifié, soit sous forme d'algorithme, soit sous forme de schéma}

% \begin{algorithm}[H]
%  \KwData{this text}
%  \KwResult{how to write algorithm with \LaTeX2e }
%  initialization\;
%  \While{not at end of this document}{
%   read current\;
%   \eIf{understand}{
%    go to next section\;
%    current section becomes this one\;
%    }{
%    go back to the beginning of current section\;
%   }
%  }
%  \caption{How to write algorithms}
% \end{algorithm}

%Chacun de ces items ouvre quasiment la voie à des sous-domaines d'expertises spécifiques.
Voici un aperçu cumulatif des élements susceptibles de varier d'un algorithme à un autre, et d'une application à une autre, dont certain sont hérités de la nature métaheuristique des EC, puis de la nature spécifique des EA, et enfin de la nature multi-objectifs (*) : \autocite[69,72,115]{DeJong2006a}[264-269]{Weise2011}[91]{Liefooghe2010} :

\begin{itemize}
\item la stratégie de représentation interne d'une solution
\item la stratégie d'initialisation et de maintien d'une ou de plusieurs populations
\item les stratégies de sélection des parents pour la reproduction
\item les stratégies de réintroduction des enfants dans la ou les population(s)
\item le groupe d'opérateurs choisi et la stratégie d'utilisation de ces opérateurs dans le processus de reproduction
\item le choix d'une fonction de translation \textit{gpm} entre $\mathbb{G}$ et $\mathbb{X}$
\item (*) les stratégies de préservation de la diversité
\item (*) les stratégies élitistes de sélection et de maintien des survivants
\item (*) la méthode d'attribution d'une \textit{fitness} $v$
\item le critère d'arrêt
\end{itemize}

Si cette liste de classe de choix permet de cerner de façon plus globale les questions à se poser lorsqu'on construit ce type d'algorithmes, cette représentation est encore trop vague, trop linéaire, et ne rend pas compte de la plasticité et des contraintes voulues ou imposées par la construction dynamique d'un algorithme véritablement adapté au problème. Les dépendances entre éléments de la liste n'apparaissent pas dans cette représentation, or pour chacun des choix réalisés par l'expérimentateur a lieu un recalcul des degrés de liberté, ce qui entraine l'apparition ou la disparition de nouveaux choix, en fonction des dépendances existantes entre chaque élément. \Anote{reflexion_DeJong}

Par exemple, le choix d'une représentation interne d'une solution sous forme de vecteur de binaire, réel ou encore mixte, de taille dynamique ou fixe, doublé ou non de paramètres spécifiques de convergence, joue de façon assez logique sur les choix disponibles dans chacune de ces classes. Ainsi le groupe d'opérateurs choisis pour manipuler ces vecteurs lors de la reproduction des individus ne seront pas les mêmes selon qu'on manipule des éléments Binaires ou Réels.

De plus, il faut imaginer que chaque stratégie est accompagnée de son lot de paramètres associés, et ce n'est qu'à terme d'une construction, lorsque le choix d'une combinaison d'éléments est actée, que la liste de paramètres définitive de la métaheuristique apparait de façon claire à l'expérimentateur.

Enfin, dans notre cas, où il est question d'utiliser ces algorithmes évolutionnaires en s'appuyant sur toute la puissance informatique disponible, de nombreux nouveaux choix \autocite[221-224]{DeJong2006a} émergent à la lumière des modèles plus poussés de parallélisation des EA. Par exemple la mise en place d'une stratégie de parallélisation en ilôts de population, dont on verra un peu plus loin qu'elle est optimale pour une utilisation des métaheuristiques sur une grille de calcul, pose les nouvelles questions suivantes :

\begin{itemize}
	\item quelles sont les stratégies de migration des individus entre les différentes populations ?
	\item combien d'ilots sont nécessaires ?
	\item les populations initiales de chaque ilot sont elles identiques ou différentes ?
	\item quelle topologie d'ilot est la plus adaptée ?
	\item etc.
\end{itemize}

C'est un aperçu des problèmes que nous tenterons de résoudre avec la construction d'une librairie logicielle à l'architecture originale, couplé avec openMOLE pour gérer la partie parallélisation, et qui sera exposé dans les sections suivantes.

Une partie de la modularité inhérente aux métaheuristiques a déjà pu par chance être saisie dans le développement de nombreuses librairies logicielles, il est alors légitime de se poser la question suivante, \textit{pourquoi développer et surtout maintenir une nouvelle librairie ? }

Les raisons de ce choix sont guidées par une observation critique des librairies existantes, réalisée un peu plus loin dans cette section, et la volonté de satisfaire au mieux les critères évoqués dans le chapitre 3 \hl{ref section}.

%\paragraph{Expression au niveau de l'expérimentation : }

%\begin{enumerate}

%\item{Besoin de plus de flexibilité ?}

%\end{enumerate}

% Faire un rapel vers la grille.
% MODYSS !

\paragraph{Au niveau utilisateur, cas d'utilisation orienté vers les métaheuristiques}

MGO devra être supporté par une architecture qui répond aux exigences différentes d'un public novice et expert, ce qui suppose une analyse de la possible évolution des besoins.

\hl{a developper} Qu'est ce qui fait la facilité de prise en main ? Respecter les pratiques existantes, tout en offrant des solutions alternatives dès que le modélisateur en ressent le besoin, si possible à moindre coût.

\begin{enumerate}
	\item{\textbf{Besoin de plus de flexibilité ?}} L'algorithme évolutionnaire proposé en l'état (canonique) ne donne pas de bons résultats, le programme doit permettre d'accéder \textbf{facilement} à toute la combinatoire offerte par la variation des différents composants intégrant cette branche des métaheuristiques.

	\item{\textbf{Besoin de plus de puissance ?}} Fonctionnel sur une machine standard, les algorithmes évolutionnaires doivent pouvoir tirer parti de ressource informatique plus importante de façon locale (multi-coeur) ou distribuée (cluster, grille de calcul), et cela en utilisant les méthodes adaptées. Ce passage d'une exécution locale à une exécution distribué doit être possible le plus \textbf{facilement} possible.

	\item{\textbf{Besoin de plus d'extensibilité ?}} Je ne trouve pas le composant nécessaire à la construction d'une métaheuristique adaptée à mon problème, quels sont les outils mis à ma disposition pour que je puisse ajouter le ou les composants facilement, à moindre cout, sans que l'ensemble du programme ne soit affecté par mes modifications ?
\end{enumerate}

En résumé, Mgo doit assurer \hl{repetition ici, a revoir} :

\begin{itemize}
	\item La mise à disposition d'une collection d'algorithmes évolutionnaire mono et multi-objectifs, directement utilisable
	\item mais également extensible et modifiable car décrit dans une syntaxe lisible exposant la structure interne en partie modulaire de ces métaheuristique.
	\item Une prise en charge automatique et transparente des architecture multi-coeur locale ou distribué
\end{itemize}

Mais si on se contente d'évoquer seulement ces objectifs-là, on reste dans une construction isolée dont il faut encore l'interfacer, la relier, à l'exécution de nos modèles de simulation. Ce qui suppose d'un point de vue technique la prise en compte d'un certain nombre de problématiques techniques qui tiennent déjà en réalité d'expertises très différentes.

Un processus au coeur de l'optimisation, car c'est bien sur la base des résultats fournis par l'évaluation d'une population de modèles, devant encore être passés au crible des critères objectifs définis par les modélisateurs, que l'optimiseur va se baser pour cheminer vers des solutions quasi optimales.

\paragraph{Au niveau utilisateur, cas d'utilisation orienté vers la modélisation }

\begin{enumerate}

	\item{\textbf{Changement de simulateur ?}} Mon modèle évolue pour changer de plateforme de simulation, en passant par exemple de Netlogo à Gamma. Est-il possible de conserver mon expérience définissant l'optimisation et les paramètres de l'optimisation au cours de ce changement ? Autrement dit, changer le moteur implique-t-il le changement automatique de toute la voiture ?

	\item{\textbf{Bibliothèque d'exemples disponible ?}} Existe-t-il une bibliothèque d'expérimentations comportant un ou plusieurs exemples ou patron(s) pour une utilisation du modèle de simulation avec des métaheuristiques ?

	\item{\textbf{Liens entre logiciels de modélisation et ressources informatiques ?}} L'utilisateur doit-il réaliser lui-même la brique logicielle permettant le couplage entre le simulateur et les différents logiciels spécialisés pour utiliser des ressources informatiques distribuées ?

	\item{\textbf{Comment se définissent les expériences ?}} Existe-t-il des facilités pour décrire les plans d'expériences ? Comment le modèle est-il intégré dans une chaîne de traitement comportant une optimisation par métaheuristique ? Comment réalise-t-on le mapping entre les paramètres du modèle et la représentation interne d'une solution ? Comment décrire et transmettre les fonctions objectifs aux algorithmes pour évaluer les sorties du modèle une fois exécuté ?

	\item{\textbf{Quels supports d'interactions ?}} Est-ce que les fonctionnalités sont accessibles par une manipulation interactive dans un logiciel ou par le biais de commandes dans des scripts ?

	\item{\textbf{Quelle reproductibilité ?}} Au-delà de la simple réplication des simulations, quel niveau de reproductibilité, quelle confiance peut-on attendre d'un couplage de logiciels aussi complexe ?

\end{enumerate}

\paragraph{Au niveau institutionel, cas d'utilisation orienté vers le déploiement }

Question de la distribution des algorithmes évolutionnaires. Quel cas d'utilisation est le plus aisé à mettre en place dans des laboratoires de science humaines ?

\hl{en cours}

% Les questions à ce niveau là sont encore plus nombreuses.

% - Modulation de l'accès à la puissance informatique indépendante du modèle : Intégré à OpenMOLE, le couplage doit apparaitre comme transparent, tout en restant hautement flexible ce qui suppose l'existence de primitive de plus haut niveau qui assure la partie parallélisation nécessaire à l'usage confortable de tels algorithmes.

% - Découpler les plateformes de simulation

% - Supporter différents niveau de parallélisme au niveau des métaheuristiques

% - La facilité d'ajout de nouveaux composants

% - Une bibliothèque d'algorithmes canonique à disposition

% - La documentation

% Dans l'association entre modèle de simulation et métaheuristique : Modalités de jointure entre le modèle de simulation \enquote{tel qu'il est développé} et la librairie d'algorithme évolutionnaire.

% - Séparation entre modèle de simulation
% -

% Deux phases ?
% - Usage indépendant
% - Usage associé à openMOLE

% fait apparaitre l'optimisation comme une étape supplémentaire dans l'expérimentation,

% %BehaviorSearch follows in the tradition of NetLogo [Wilensky, 1999, 2001; Tisue & Wilensky, 2004], and Logo [Papert, 1980] before it, in embracing the twin design goals of “low threshold” and “high ceiling”. By this we mean that the BehaviorSearch tool should be both easy for beginners to learn and use (“low threshold”), while also providing advanced features that will allow expert modelers to engage in cutting-edge research and analysis (“high ceiling”). To be clear, the “low threshold” goal for NetLogo, which aims to support use by elementary school students, is lower than that of BehaviorSearch, which primarily targets NetLogo’s research audience. However, increasingly NetLogo is being used by undergraduates or even high school or middle school students who are developing agent-based models for research projects, and we would like BehaviorSearch to be accessible to these audiences, as well as researchers from various disciplines who are non-expert programmers but have adopted ABM methodologies for their research. Just as NetLogo strives to make the creation of agent-based models accessible to children and novices, BehaviorSearch aims to facilitate model analysis by making search and optimization techniques accessible to all modelers.

% State of the Art
% Flexibilité

% Facilité de parallélisation
% Facilité de construction

\subsection{Un point rapide sur les solutions EC existantes}
\label{ssec:EC_existantes}

Les librairies logicielles permettant la mise en oeuvre d'algorithmes évolutionnaires existent dans de très nombreux langages informatiques. Les \textit{Survey} ou \textit{state of the art} sont régulièrement mis à jour dans cette discipline, et il est inutile de se substituer ici à ce type de travaux en évoquant les avantages et les inconvénients comparés de toutes ces librairies. Le lecteur pourra se référer à l'étude très complète de \textcite{Parejo2012} comparant selon 271 critères 11 des plus importantes plateformes sur les 33 qu'ils ont identifiés. On se contentera ici d'illustrer ce que l'on considère comme les principaux défauts du point de vue de notre grille de lecture en sélectionnant pour cette critique une ou plusieurs librairies parmis les plus usitées.

Un premier filtre permet d'éliminer toute celle qui ne s'adresse qu'à une seule branche des EA, ou qui n'implémente aucun des algorithmes multi-objectifs.

Un deuxième filtre permet d'éliminer également toutes les librairies qui sont intégrées à un logiciel, donc impossibles à utiliser en dehors de celui-ci. Le cas particulier des logiciels de modélisations (simulateurs) intégrant des algorithmes EC sera tout de même abordé dans la section suivante afin de situer les limites de ces approches. Comme la librairie logicielle réalisée doit être compatible a minima avec plusieurs logiciels de modélisations existants, il ne parait pas intéressant de partir vers une solution se basant sur l'extension de ces derniers.

Afin de satisfaire les objectifs que nous avons fixés, la librairie doit pouvoir fonctionner avec OpenMOLE, car une des tâches de ce dernier va être d'orchestrer de façon transparente la parallélisation de ces algorithmes évolutionnaires, ce qui suppose une interaction assez fine entre les deux outils, et un langage informatique compatible avec Java ou Scala, les deux langages sur lesquels est construit OpenMOLE.

\subsubsection{Les approches intégrées}
\label{sssec:approche_integree}

\paragraph{Les tentatives isolées}

Openshaw Diplock,
Hepenstall et al,
etc.

\hl{en cours}

\paragraph{Le {BehaviorSearch} de Stonedahl}

La librairie \textit{BehaviorSearch} développée par Railsback pour Netlogo intègre une librairie d'algorithme génétique.

Les solutions existantes de couplage, comme le \textit{behavior search} déjà évoqué dans \hl{la section XX} ne sont pas entièrement satisfaisantes, cela sur plusieurs points déjà évoqués et résumés ci-dessous, auxquels on rajoute de nouveaux inconvénients propres à la manipulation avancé des métaheuristiques :

a) Le cycle de vie d'un modèle ne se limite pas forcément à l'établissement d'un seul modèle Netlogo, mais plusieurs, et de complexité différente. Si Netlogo est un outil indispensable de par la force et la rapidité de concrétisation d'une idée scientifique qu'il permet, les scientifiques auto-didacte peuvent rapidement être piégés par des problématiques tenant plus de la \enquote{science informatique} que de leur domaine initial.

b) Le niveau de prise en charge de l'expérimentation est insuffisant pour assurer une recherche reproductible au-delà du seul modèle. Par là il faut comprendre que le protocole scientifique supportant l'évaluation du modèle de simulation n'est pas accessible, or tout comme le modèle, celui-ci possède sa propre voie de construction, et porte au contact du premier une responsabilité dans l'évolution des choix de sa structure interne. Autrement dit, sans la présence de ces deux supports de connaissances, c'est toute une discussion collective qui est rendue plus complexe, alors même que celle-ci se révèle comme un support important, voire même constitutive de ce processus de validation.

c) Le support du parallélisme implicite à ce type de métaheuristique EC en local, c'est-à-dire sur un ordinateur personnel, même lorsqu'il est associé à des techniques pour réduire le nombre d'exécutions des modèles de simulation (en utilisant par exemple du \textit{fitness caching} \autocite[245]{Stonedahl2011a}), ne semble pas suffisants pour une utilisation confortable et répétée d'algorithmes métaheuristiques multi-objectifs. Ces derniers pouvant demander pour des modèles relativement simples et optimisés jusqu'à plusieurs millions d'executions, résultat d'une accumulation d'expérimentation nécessaire pour la construction et l'évaluation du modèle de simulation \autocites{Schmitt2014, Cottineau2014b}. Le framework théorique QBME (\textit{Query-Based Model Exploration}) de Stonedhal est intéressant, et ressemble par certains aspects à la méthodologie POM de Grimm, et permet comme dans cette dernière de questionner la progression du modèle sous la forme d'une question inversée par rapport au questionnement plus traditionnel. On ne se demande plus \enquote{Quels comportements vais je obtenir avec ce jeu de paramètres} mais plutôt \enquote{Quels types de paramètres m'a permis d'obtenir (ou de ne pas obtenir) un certain comportement ?}. Ce type d'analyse se base sur la construction de multiples critères d'évaluation, potentiellement contradictoire, questionnant la dynamique du modèle, et qui il me semble, se rattache plus à l'expression d'une analyse multi-objectifs. Or, les algorithmes proposés par le \textit{BehaviorSearch} se limitent pour le moment à des algorithmes mono-objectif que les cas d'utilisation réels risquent de très rapidement mettre en défaut.

d) Le support ne permet pas à l'heure actuelle de jouer \enquote{facilement} avec les briques mises à disposition par les métaheuristiques, ce qui on l'a vu précédemment va à l'encontre de l'esprit de tels algorithmes. Même si une certaine extensibilité du logiciel a été prévu par le concepteur, sa mise en oeuvre demande des connaissances supplémentaires dans un autre langage de programmation que Netlogo (Scala ou Java), ce qui vient encore surcharger un peu plus les prérequis d'un utilisateur débutant qui doit déjà explorer le domaine des algorithmes évolutionnaires.

Même si l'auteur est effectivement d'accord avec le concepteur du \textit{BehaviorSearch} pour dire que cet outil constitue en lui même déjà un grand pas vers la démocratisation de techniques d'expérimentation plus évoluées jugées nécessaire pour améliorer les pratiques des débutants, nous pensons qu'il est possible d'aller encore beaucoup plus loin. A ce titre, le couplage que nous visons dans ce projet se rapproche des visions d'avenir évoqués par le concepteur, dont la dernière version du logiciel est daté de 2013 \textcite[295]{Stonedahl2011a} \foreignquote{english}{In the not-so-distant future I envision in a “begin parallel search” button appearing in toolkits like NetLogo that would seamlessly launch dozens or hundreds of simultaneous genetic algorithms searches on a remote grid/cluster, reporting back the most promising results to the user as they are discovered in real-time.} La différence c'est que ce bouton n'est pas envisagé dans Netlogo, car ce n'est pas le propre de cet outil, mais dans un logiciel tel qu'openMOLE, dont la fonction est entièrement dédié à l'exécution de tâche en parallèle sur des environnements locaux (un ou plusieurs processus d'une machine), ou distribués (sur une grille de calcul, ou un cluster d'université)

Si on envisage en effet la construction de modèles sur la base d'une alternance régulière entre construction de modèle et évaluation, il est tout à fait possible et même certain que l'expérimentateur soit un jour ou l'autre confronté à une ou plusieurs limitations provenant de la chaine de progression naturelle prévue par les concepteurs de cet outil \foreignquote{english}{Netlogo(for building the model) => Behavior Space (for simple model analysis) => BehaviorSearch (for more advanced analysis and exploration)} \autocite[340]{Stonedahl2011a} ? Quels sont les choix à disposition des modélisateurs débutants ayant expérimenté l'une ou l'autre de ces difficultés ? (\hl{A voir si cela ne remonte pas dans le chapitre 3 pour clôturer la partie netlogo})

En résumé, les librairies déjà intégrées au logiciel de modélisation, comme le \textit{BehaviorSearch} de Netlogo, sont limitées en termes d'algorithmes, d'accès la puissance informatique nécessaire, et reste liées à un seul et unique support alors même que le modèle peut être amené à migrer de plateforme. On en déduit que cette solution, bien qu'utile pour du prototypage et de l'apprentissage, ne permet pas d'envisager sereinement la construction d'un modèle ou d'une expérimentation sur une durée plus longue.

\paragraph{Les algorithmes EC dans Mason}

Le framework de développement de simulation multi-agent MASON (\textit{Multi-Agent Simulation Of ...}) présenté pour la première fois en 2003 tient d'un effort conjoint entre la section d'\textit{Evolutionary Computation} du laboratoire de science informatique et le \textit{Center for Social Complexity} tout deux de la George Mason University. Ecrit en Java, solidement documenté, de développement ancien, associé depuis les débuts à un laboratoire spécialisé auteur d'une librairie datant de 1998 dédié aux EC nommée ECJ (1998), les deux logiciels étant développés par les mêmes personnes et régulièrement mis à jour ... ce framework apparait au premier abord comme un challenger sérieux pouvant se substituer à Netlogo sur des projets plus complexes.

MASON se situe sur la même ligne de développement que les librairies de développement multi-agents plus anciennes comme Swarm, Ascape, ou Repast. Tout comme ces dernières, la généricité, la performance et la modularité sont des composantes de l'application au coeur des préoccupations de Sean Luke, un des développeurs à l'origine du projet.

Une des spécificités très fortes de MASON, et qui rend cette librairie vraiment différente des trois autres, tient dans son histoire particulière. Si on en croit le développeur Sean Luke \Anote{sean_luke_mason}, c'est à la suite de sa thèse en 1998 et du développement de la librairie ECJ \Anote{sean_luke_ecj} dédié aux algorithmes d'EC, que celui-ci a ressenti le besoin d'une nouvelle librairie en accord avec ses problématiques de recherches en robotiques. A cette époque déjà, il utilise ECJ pour optimiser le comportement de robots opérant dans un environnement partagé, un domaine de recherche dont on a vu dans la section \hl{ref} que la simulation multi-agents était très liée historiquement. Cette pratique nécessite l'exécution de centaines de milliers de simulations, à la recherche de combinaisons de paramètres satisfaisants les critères dirigeant l'optimisation. La rapidité d'exécution d'une simulation devient un élément clef dès lors que ce sont des milliers, voire des millions de simulations qui doivent être executées en parallèle. C'est donc tout naturellement que celui-ci a initié sa propre librairie de simulation multi-agent, orienté vers l'utilisation efficiente des architectures multi-coeurs et depuis peu multi-ordinateurs (extension D-Mason). MASON étant conçu en parallèle de la librairie \href{http://cs.gmu.edu/~eclab/projects/ecj/}{@ECJ}, les deux outils fonctionnent très bien ensemble, et permettent l'exploitation de ressources informatiques dans des environnements distribués, initialement des clusters \autocite[211]{Luke2014}, bien qu'une \href{http://www.parabon.com/dev-center/origin}{@extension} apparemment payante permette de faire du \textit{Grid Computing}.

ECJ ou Mason sont des outils à destination de développeurs spécialistes lorsqu'il s'agit de coupler les deux outils. Ce point de vue est délibérement assumé par l'auteur dans le manuel de MASON \Anote{sean_luke_masondifficile}, et ECJ \Anote{sean_luke_ecjdifficile}.

Il est toutefois intéressant de voir que malgré des optiques de développements différentes et une réalisation inverse à la nôtre, l'objectif motivant la construction est le même, mettre à disposition des ressources informatiques nécessaires à l'utilisation de méta-heuristiques pour l'évaluation de modèles de simulation. En effet, là ou Luke justifie d'un nouveau framework agent pour rendre efficiente l'utilisation de sa librairie d'EC, c'est pour nous la démarche inverse qui prime, et c'est la nécessité d'intégrer l'optimisation comme pratique standard dans la construction d'un modèle qui justifie d'un framework EC adapté. Les deux approches sont complémentaires, et cette question de l'efficience légitime tout à fait un changement de support de modélisation dès lors qu'on essaye de complexifier les modèles. Car comment évaluer un modèle de simulation dont une seule des exécutions dure plusieurs heures ? Notre approche se veut toutefois plus respectueuse des pratiques existantes, et la réalisation support de l'exploration du modèle, une fois mise en oeuvre, doit concéder aux utilisateurs la même facilité d'accès aux EC, qu'ils utilisent Netlogo, Mason, ou une plateforme de leur initiative.

Notre approche semble se situer d'un point de vue de l'expérience utilisateur entre ces deux voies présentées précédemment.

\subsubsection{Les librairies standards}

Apache Commons

\href{http://www.moeaframework.org/}{@MOEAFramework}

\href{http://dev.heuristiclab.com/}{@HeuristicLab 2002 .Net CSharp Microsoft dependent}

\href{http://jmetal.sourceforge.net/}{JMetal (2010)}

\href{http://image.diku.dk/shark/sphinx_pages/build/html/index.html}{Shark machine learning library (c++)}

\href{http://www.tik.ee.ethz.ch/sop/pisa/?page=documentation.php}{PISA (C / C++) 2003}

Paradiseo-MOEO et Paradiseo-PEO (C ++)
Logiciels de l'INRA

\href{http://cs.gmu.edu/~eclab/projects/ecj/}{ECJ (1998) (orienté GP)}

\href{http://opt4j.sourceforge.net/}{Opt4J (Java) 2011}

\href{http://esa.github.io/pygmo/}{Pygmo (Python) PaGMO (C++) (ESA)}

\href{http://jgap.sourceforge.net/}{JGAP}


\subsubsection{Le choix d'un nouvelle librairie compatible avec openMOLE}
\label{sssec:choix_lib_EA}

%La généricité d'application de cette librairie à différentes classes de problèmes tient dans la sémantique associé à chacun des éléments de la terminologie.

%Dans notre étude, les \textit{individus} représentent une instance de simulation,

\subsection{Bref historique du \textit{framework} MGO}
\label{ssec:historique_mgo}

\hl{a verifier avec Romain pour la date de début}

Le projet démarre un peu avant 2010, à l'Institut des Systèmes Complexes, principalement sous l'impulsion de Romain Reuillon, Salma Mesmoudi et Evelyne Luthon. Un premier prototype est utilisé pour isoler un ensemble viable de trajectoire dans un processus maitrisé d'affinage de camembert. La définition de ce problème constitue un problème multi-objectifs nécessitant l'usage de ressources informatiques importantes, si possible parallélisé, pour être résolu de façon exhaustive. C'est à la convergence d'une triple expertise, entre génie des procédé agro-alimentaire, expertise dans le domaine des EC et du calcul distribué que nait le premier prototype de ce programme \autocite{Mesmoudi2010}

Bien que conçue dès le départ pour être adossée à OpenMole, cette librairie pour le moment de conception très classique va subir un redéveloppement majeur en 2011-2012 sous l'impulsion de Gabriel Cardoso, Romain Reuillon et Sébastien Rey-Coyrehourcq, avec la migration progressive d'une architecture classique vers une architecture construite sur la base d'un système inter-connecté de composants, décrits par la suite, et toujours en cours de développement.

Ce redéveloppement poursuit un triple objectif initial, a) il s'agit d'affirmer MGO comme un framework totalement indépendant implémentant les tout derniers algorithmes issus de la littérature méta-heuristique, b) s'appuyant sur les dernières innovations en génie logiciel permettant de dévoiler la structure interne des méta-heuristiques, c) pour faciliter leur modification, leur extension d) dirigé par un objectif de parallélisation grâce à un meilleur couplage de ce framework avec openMOLE.

%tout en servant d'architecture support pour l'exploitation et la création de toutes nouvelles méta-heuristiques, c) dont certaines adaptés pour exploiter la puissance informatique à disposition d'environnements distribués.

L'enjeu est donc double, les algorithmes doivent pouvoir s'exécuter indépendamment d'openMOLE, comme une librairie standard, mais les composants constitutifs doivent également pouvoir être adapté sous forme de tâches intégrées dans un workflow openMOLE permettant la mise en oeuvre de version parallélisés de ces derniers.

En résumé, MGO fournit les briques et les algorithmes méta-heuristique utilisant ces briques, alors que OpenMOLE réutilise les briques dans des workflow décrivant de nouvelles versions parallélisés de ces algorithmes.

\subsection{Des principes de conception innovants}

Écrite dans le langage informatique Scala, cette librairie s'appuie sur la possibilité d'application d'une technique informatique particulière permettant une plus grande souplesse dans la manipulation des différentes briques composant les algorithmes evolutionnaires, sans sacrifier l'extensibilité et en garantissant une plus grande lisibilité sur la structure interne de ces algorithmes à destination d'un public moins initié.

Cette technique est plus connue sous le nom d'\textit{injection de dépendance} (\textit{dependency injection}). Le meilleur moyen de comprendre cette technique est encore de donner un exemple pour illustrer son fonctionnement sans utiliser de jargon informatique.

\foreignquote{english}{When you go and get things out of the refrigerator for yourself, you can cause problems. You might leave the door open, you might get something Mommy or Daddy doesn't want you to have. You might even be looking for something we don't even have or which has expired.

What you should be doing is stating a need, \enquote{I need something to drink with lunch,} and then we will make sure you have something when you sit down to eat.}

Ce qui signifie que le programme informatique, tout comme le petit garçon ou la petite fille de notre exemple, fait état de ses besoins minimums à l'utilisateur pour être fonctionnel. Ce qui est intéressant avec cette technique, c'est qu'elle intègre spontanément le principe dit d'inversion de contrôle (\textit{Inversion Control}) et d'inversion de dépendance (\textit{Dependency Inversion}).

Le premier principe d'inversion de contrôle renvoie à la possibilité d'externalisation du programme ou des composants du programme. Les appels ne sont plus limités à un contrôle interne, statique, et peuvent être commandés par des appels extérieurs, par un utilisateur, ou un autre programme, souvent de façon dynamique. C'est typiquement ce qu'on observe lorsqu'un utilisateur manipule une interface graphique. Chaque action réalisée, par exemple l'appui par l'utilisateur d'un bouton sur cette interface, renvoie à l'execution d'un ou de plusieurs bouts de code dans le programme, de façon dynamique, sans que cette invocation en particulier soit décrite physiquement dans le programme. De nombreux langages intègrent ou étendent ce principe sous divers noms et techniques : \textit{Events} et \textit{Callbacks}, \textit{Reactive programming}, \textit{Observer Pattern}, etc.

L'inversion de dépendance est un principe un peu plus complexe à comprendre, et celui-ci pourra surement être mieux compris avec l'appel à un schéma simplifié, comme celui de la figure \ref{fig:decouplage_principe}. Celui-ci questionne la façon dont les dépendances sont fixées entre les composants de notre programme informatique. Le fait que les dépendances soient fixées par le composant lui-même est caractéristique d'une inversion de contrôle, et le fait que celui-ci les obtienne de l'extérieur, et non pas par une création interne est une inversion de dépendance (voir \ref{subfig_decouplage:b}).

L'avantage de voir le déroulement d'un programme de cette façon, c'est que chaque composant est défini en fonction d'une tâche en particulier, si possible la plus atomique possible, afin de maximiser sa réutilisation, et de limiter les effets de bords - c'est à dire les comportements imprévus - en cas de remplacement de celui-ci. C'est un jeu de brique, vous définissez les briques de façon à pouvoir les réutiliser si possible dans toutes vos constructions, et si possible sans avoir à les remodifier. Ce faisant vous n'avez pas à vous soucier de ce que font ou permettent les autres briques autour de vous, en dehors de celle dont vous dépendez pour fonctionner correctement.

Le flot d'exécution du programme ne s'appuie plus sur une description statique des dépendances entre objets composants le système, qui rendent au départ sa modification plus difficile. Ainsi, dans notre cadre, l'inversion de contrôle renvoie à l'expression des dépendances entre composants d'un programme, celles-ci étant fournies au cas par cas pour chaque composant de façon automatique par le reste du programme, car nécessaire à son fonctionnement. On lui préfère une description sous forme de graphe de composants. Chaque composant du graphe fait état de ses besoins pour fonctioner, ce qui le rend en partie indépendant du reste du fonctionnement du programme. Lors de l'exécution, le programme se déroule en interprétant de façon dynamique le graphe de composants tel qu'il a été défini par l'utilisateur.

\begin{figure}[!htbp]
  \begin{sidecaption}[fortoc]{Illustrations simplifiées des stratégies possibles pour découpler des composants logiciels entres eux. \parbox{\marginparwidth}{
	\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
	        \item Le composant \sqbox{tangoBlue1} défini ses dépendances à \sqbox{tangoOrange1} et \sqbox{tangoRed1} de façon interne, le couplage entre les composants est très fort. Si ces deux dépendances sont présentes dans de très nombreux autres composants, alors un changement même mineur dans la forme d'un de ces deux composants entraine de nombreuses modifications du programme.
	        \item Une première solution pour rendre les composants plus indépendants est de déclarer les dépendances au niveau de \sqbox{tangoBlue1}, et de fournir ces composants de façon externe à celui-ci. Cette solution ne résout toutefois pas le problème d'un changement de forme.
	        \item Une solution s'appuyant sur (b) est d'utiliser un composant abstrait intermédiaire, qui reste toujours valable du point de vue de la forme attendue par \sqbox{tangoBlue1}. A charge des composants \sqbox{tangoOrange1} et \sqbox{tangoRed1} d'implémenter le minimum requis par ce composant abstrait.
	\end{enumerate}}}[fig:decouplage_principe]
  \centering
  \subbottom[]{
  	\includegraphics[width=.7\linewidth]{composants_principes_a.pdf}
  	\label{subfig_decouplage:a}}\qquad
  \subbottom[]{
	\includegraphics[width=.8\linewidth]{composants_principes_c.pdf}
  	\label{subfig_decouplage:b}}
  \subbottom[]{
	\includegraphics[width=.8\linewidth]{composants_principes_d.pdf}
  	\label{subfig_decouplage:c}}
 \end{sidecaption}
\end{figure}

Si on recontextualise ces principes à notre problématique, les classes de composants nécessaires à l'exécution minimale d'un algorithme évolutionnaire dans MGO sont décrites dans la figure \ref{fig:cake_classe}, et s'appuie sur le travail déjà décrit de la communauté des EA pour unifier la description de ces algorithmes.

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Classe de composants nécessaires pour l'exécution d'un EA.}[fig:cake_classe]
		\centering
		\includegraphics[width=0.7\linewidth]{cake_example.pdf}{
		}
  \end{sidecaption}
\end{figure}

Chacune de ces classes de composants est définie comme nécessaire pour le fonctionnement d'un algorithme d'évolution, dont la mise en dynamique du comportement générique est implémentée dans le composant \keyword{Evolution}. Tant que l'utilisateur n'aura pas fourni un composant compatible avec chacun de ces types de composants, alors le composant \keyword{Evolution} ne pourra pas s'exécuter. Le programme est défini comme un texte à trou, les boites sont bien placées, et le programme prêt à être exécuté suivant cet ordre, seulement ce n'est qu'un patron, une image, et cet ensemble de boites dont seule une partie de la logique a été intégrée, doit encore être complété par l'utilisateur. C'est un peu comme un puzzle dans lequel il manque des pièces, vous devez soit créer de nouvelles pièces, soit retrouver les pièces qui respectent la forme de chaque emplacement pour que le puzzle soit de nouveau complet.

L'inversion de contrôle détaillée dans les paragraphes précédents se matérialise de nouveau ici au travers du fait que c'est l'utilisateur qui définit sa composition, en s'assurant avec l'aide des instructions du programme que les dépendances propres au fonctionnement de chacun des composants sont bien fournies. Tout programme ne remplissant pas les conditions renverra un message d'erreur à son execution. En ce sens MGO est probablement plus un \textit{framework} qu'une librairie logicielle \Anote{martin_fowler}. L'autre avantage d'une telle approche c'est que l'application peut être livrée avec un vaste catalogue de pièces compatible avec chaque type d'emplacement, laissant à l'utilisateur la possibilité de choisir sa propre combinaison, voire même de créer ses propres pièces s'il estime quelles sont manquantes.

Ils existent différentes techniques pour qu'une telle architecture puisse être mise en oeuvre. Par exemple l'utilisation mixte de \textit{classe abstraite} et d'\textit{interfaces} permettent dans de nombreux langage informatique, comme Java ou C\#, de reproduire le découplage entre composants tel qu'on la vu dans la figure \ref{fig:decouplage_principe}.

En réalité, pour certains programmeurs \textcite{Odersky2005} \Anote{odersky_note_cake} ce type de techniques, dont l'existence, les avantages et les contraintes ont largement été discutés, ne constitue pas selon lui la meilleure réponse d'un point de vue technique pour garantir une meilleure réutilisabilité des composants informatiques. Il faut savoir à ce sujet qu'il existe en informatique une branche de recherche tourné vers la construction de langage de programmation aux caractéristiques innovantes. Ainsi les propriétés du langage Scala utilisé pour implémenter MGO expose la solution fournie par Martin Odersky à cette question de recherche que représente la recherche d'une meilleure modularité des programmes informatiques. Tout comme dans les années 1980, le langage Smalltalk d'Alan Kay a introduit une autre façon de structurer les programmes avec le paradigme Orienté Objet, Scala permet ici de penser de façon originale la modularité des composants en usant d'un tout nouveau couplage de différentes abstractions informatiques ( \textit{abstract type members}, \textit{explicit self-types}, \textit{modular mixin composition}) \Anote{note_informatique_mixin}. Autrement dit, ce que Scala va nous permettre d'exprimer comme degré de modularité lors de la description informatique de nos composants constitue en soit une innovation qui n'est pas présente, ou présente sous des formes trop complexe vis à vis de notre cahier des charges, dans bien d'autres langages informatiques.

%Si l'utilisation de classe abstraite est donc intéressante, elle ne résout pas tout les problèmes à elle seule. Ainsi par exemple, lorsqu'on utilise des classes abstraites, il n'est pas possible d'utiliser des dépendances cycliques entre composants, et la nécessité de respecter un ordre d'initialisation entre composants devient également rapidement une contrainte.

%Scala permet par exemple de définir des \keyword{traits} qui possède des caractéristiques plus intéressant que des classes abstraites ou des interfaces, tout en assurant à minima un comportement similaire. Il est par exemple possible d'assembler, de mixer de façon dynamique plusieurs traits, et l'addition des comportements se fait automatiquement.

%Il est également possible de définir des traits qui contiennent totalité ou seulement partie d'une implémentation. Ce type de programmation n'est pas permises par d'autres langage informatique, comme Java par exemple.

Souvent apellé de façon jargonnesque \foreignquote{english}{Cake Pattern} pour la possibilité de composition qu'elles offrent (comme dans un cake, un gateau dont la recette peut accueillir une très grande variété d'étapes, de composantes), ces outils aussi connus sous le nom \foreignquote{english}{Scalable Component Abstractions} ou encore \foreignquote{english}{Component Based Dependency Injection} regroupent  l'ensemble des techniques permettant d'accéder à cette modularité est donc présente de façon native dans le langage, car pensé et implémenté par son créateur \autocite{Oderskyxx}.


La figure \ref{fig:principe_mixin} expose les différentes

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Un exemple plus proche de l'implémentation, appuyé par une version UML orienté Scala, pour comprendre comment les élements manquants (attributs, méthodes) définis en \sqbox{tangoRed1} dans ce diagramme de classe sont indiqués au programme lors de la création de nouveau objets. Les boites rouges représentent les erreurs retournés à l'utilisateur lors des tentatives successives de déclaration d'un nouvel élément \keywordmin{D1}, puis \keywordmin{D2}. \parbox{\marginparwidth}{
\begin{enumerate}[label=(\alph*),labelindent=\parindent,leftmargin=*]
       \item Un diagramme UML pour illustrer un scénario de dépendance entre trois composants exemples \keywordmin{A}, \keywordmin{B}, et \keywordmin{C}. Le composant A utilise une fonction run qui nécessite la définition d'une \keywordmin{methode\_b} et d'une \keywordmin{methode\_c} provenant d'un composant de type \keywordmin{B} et \keywordmin{C}.
       \item Un premier scenario illustre de façon simplifié la logique d'héritage permettant la création d'un nouveau composant D1 à partir des composants A, B1 et C1.
       \item Dans ce deuxième scénario, on illustre la nécessité de rédéfinir dans D2 les attributs, mais également les méthodes, qui n'ont pas été apporté par des composants extérieurs.
\end{enumerate}}}[fig:principe_mixin]
	\centering
	\subbottom[]{
		\includegraphics[width=0.8\linewidth]{exemple_mixin.pdf}
		\label{subfig_principe_mixin_a}}
	\subbottom[]{
		\includegraphics[width=0.8\linewidth]{exemple_mixin2.pdf}
		\label{subfig_principe_mixin_b}}
	\subbottom[]{
		\includegraphics[width=0.8\linewidth]{exemple_mixin3.pdf}
		\label{subfig_principe_mixin_c}}
	\end{sidecaption}
\end{figure}

%Utilisé de façon complémentaire ces deux techniques permettent une grande flexibilité, une méthode ou un attribut est définit comme abstrait, et son implémentation doit être apportée par un composant externe.

%utilisant le cake pattern ( abstract type members, explicit self-types, modular mixin composition )

\subsubsection{Mobiliser les bons composants}

Un des grand défaut de cette technique, c'est qu'elle rend souvent la lecture du code source plus difficile du point de vue d'un observateur extérieur. Le programme est en effet morcelé dans un ensemble de composants contenant chacun une petite partie de la logique du programme total, assemblé par le compilateur avant l'execution du programme. De plus lorsque l'utilisateur choisit un composant, celui-ci ne fait pas qu'offrir de nouvelles alternatives à l'utilisateur, il en retire également, comme on pourrait s'attendre lorsqu'on manipule un arbre de dépendance complexe.

\hl{legende a faire, probleme entre les deux exemples.}

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Illustration d'un premier scénario dans la selection de composant parmis les types $X$, et $Z$}[fig:composant_expli1]
		\centering
		\includegraphics[width=0.7\linewidth]{dependanceExemple.pdf}{
		}
  \end{sidecaption}
\end{figure}

Si on considère un programme simplifié ayant besoin de deux types de composants pour fonctionner : $X$ et $Z$. Dans le premier scenario \ref{fig:composant_expli1}, l'utilisateur à choisi le composant $Z(a1)$ autonome, ainsi que le composant $X(a3)$. Ce dernier, contrairement au composant $X(a1)$ et $X(a2)$, dépend de nouveaux composant définis sur la ligne $b$ du schéma. On voit que dans les nombreux composants disponibles dans cette catégorie, seuls quelque uns s'avérent compatible avec le reste de la configuration de composant choisi par l'utilisateur. Que se passe t il lorsque l'utilisateur choisi pour $Z$ le deuxième composant $Z(2a)$ ?

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Illustration d'un deuxième scénario dans la selection de composant parmis les types $X$, et $Z$.}[fig:composant_expli2]
		\centering
		\includegraphics[width=0.7\linewidth]{dependanceExemple2.pdf}{
		}
  \end{sidecaption}
\end{figure}

C'est ce scénario qui est développé dans l'exemple \ref{fig:composant_expli2}. On voit que le choix de $Z(2a)$ implique de nouvelles restriction de choix dans la branche $X$, mais aussi la possibilité de nouveaux choix. Ainsi $X(b2)$ qui n'était pas compatible avec le composant $Z(a1)$ devient compatible avec au moins un des composants de $Z$, à savoir ici le composant $Z(b1)$ dont dépend $Z(a2)$ selectionné par l'utilisateur.

%Prenons un exemple simple, si l'utilisateur décide de ne pas utiliser de composant pour l'\keyword{Archive} des meilleurs individus, alors c'est tout un ensemble de stratégies dépendant de l'existence de ce composant dans le programme qui ne peux plus être mobilisé.

\subsubsection{Définition d'une méta-heuristique dans MGO}

L'implémentation d'un algorithme méta-heuristique prend cette forme dans MGO :

\begin{listing}[H]

\begin{minted}[linenos=true,frame=single,fontsize=\footnotesize]{scala}

trait NSGAII <: Evolution
  with BinaryTournamentSelection
  with TournamentOnRankAndDiversity
  with NonDominatedSorting
  with SBXCrossover
  with PolynomialGAMutation
  with GAGenome
  with Crowding
  with Pareto
  with NonStrictDominance
  with NoArchive
  with GeneticBreeding
  with MGFitness

\end{minted}
\caption{Exemple de définition d'une méta-heuristique dans MGO}
\label{alg:nsga2}
\end{listing}

La structure interne des composants constitutif de l'algorithme NSGA2 est lisible dès lors qu'on a déjà étudié le fonctionnement de métaheuristique. Chacune des briques (ici une par ligne) peut être remplacé ou modifié à partir du moment ou les conditions de dépendance entre composants sont respectés.

\begin{figure}[!htbp]
	\begin{sidecaption}[fortoc]{Schéma de dépendance existant entre les différents types de composants pour la définition de l'algorithme NSGA2 sous MGO}[fig:cake_classe]
		\centering
		\includegraphics[width=0.9\linewidth]{diagrammeclassemgo.pdf}{
		}
  \end{sidecaption}
\end{figure}

Avoir accès de façon lisible permet de mieux comprendre les différences entres les différents algorithmes. Prenons un exemple simple de construction d'algorithme génétique multi-objectif utilisant l'algorithme de classement de solution candidates dites du \textit{Non Dominated Sorting} (NDS), tiré de \autocite{Goldberg1989} et implémenté par Deb dans NSGA \autocite{Deb1994} puis NSGA2 \autocite{Deb2001}. Pour fonctionner dans ces deux variantes, élitistes ou non élitistes, ce composant à besoin que l'utilisateur choisisse deux autres composants de type \keyword{Ranking}, et de type \keyword{Diversity}. Dans sa version non elitiste, le composant \keyword{Diversity} n'est pas utilisé.

Dans cet algorithme, les individus $x$ de la population $p_1$ sont partitioné selon un premier critère $c_1$, et de l'ensemble de partition ainsi formé, on ne retient que là premiere pour former un front $f$. Les individus de ce front sont supprimé de la population $p$, puis on recommence cette opération $n$ fois, cela jusqu'à ce que la somme des individus contenu dans l'ensemble des fronts $f_n$ soit égale à la population attendue pour $p_2$. Dans une utilisation non élitiste de cet algorithme, comme dans NSGA, la population $p_1 = p_2$. Dans une version élitiste, comme celle de NSGA2, la population $p_1$ contient également la progéniture (offspring), donc $p_2 = p_1 / 2$. Comme tout les individus de $p_1$ ne pourront pas participer à $p_2$, l'inclusion du dernier front dans $p_2$ doit être géré comme un cas particulier, et nécessite potentiellement d'être tronqué de façon intelligente, pour éviter de perdre en diversité. On fait donc appel à une méthode supplémentaire de calcul de diversité qui constitue en soit un deuxième critère de selection $c_2$ définissant les individus les uns par rapport aux autres.

Le choix d'un composant \keyword{Ranking} va déterminer quel va être le critère $c_1$ utilisé, et le choix d'un composant \keyword{Diversity} va déterminer selon quel critère $c_2$ les individus participant au dernier front vont être selectionnés.

On voit bien que les stratégies permettant de fixer ces deux critères peuvent être de nature très différentes. Pour $c_1$ le critère permettant de créer les ensembles le plus classique est le critère de dominance de Pareto définit par le composant du même nom \keyword{ParetoRanking}, mais d'autres critères sont tout à fait envisageable.

\hl{ A corriger avec les evolutions proposés}
Inversement, la même brique atomique définissant le calcul de dominance de Pareto est ainsi utilisé pour différents algorithmes canoniques, comme MOGA ou bien NSGA2. Ainsi là ou NSGA va utiliser cette brique plusieurs fois dans le composant NDS, MOGA va utiliser ce composant une seule fois en l'encapsulant dans un autre composant qui implémente la façon dont MOGA partitionne les individus.

Pour $c_2$, le critère utilisé par NSGA2 est une distance de crowding, mais là aussi d'autres variantes existe, comme par exemple le calcul d'hypervolume de Ziztler, Baume.

Le défaut majeur d'une telle approche, c'est qu'elle ne renseigne immédiatemment l'expérimentateur sur les choix valides ou invalides qui sont à sa disposition, et celui-ci peut alors se perdre dans une logique d'execution du code très éclaté. Pour résoudre ce problème, deux stratégies sont mise en oeuvre, la première c'est celle que l'on observe dans le listing \ref{alg:nsga2}, à savoir l'implémentation pré-existante dans le framework de plusieurs algorithmes évolutionnaires canonique prêt à l'emploi, et donc également prêt à être modifié, comme CMAES \autocite{Hansen}, NSGA2 \autocite{Deb2001}, ou SMS-EMOA \autocite{Beume2007}.

\hl{Capture d'écran à faire avec Mathieu L.}

%%JANET est en cours.

Un outil est également en cours de développement par Mathieu Leclaire pour explorer ce type de graphe. Nommé \textit{JANET}, cette application est capable de lire et de reconstruire le graphe de dépendance entre les différents composants d'un programme qu'on lui donne à explorer. A partir de là, il autorise l'experimentateur à composer son propre graphe de façon totalement interactive, en cliquant sur les composants du graphe.

A chaque selection de composant dans le graphe, l'ensemble des dépendances est recalculé afin de montrer à l'utilisateur quelle dépendance sont accessible de façon valide à partir de ses choix. Enfin \textit{JANET} est capable de générer un squelette de code similaire à celui exprimé dans le listing \ref{alg:nsga2}, utilisable directement comme nouvel algorithme.

\subsubsection{Définition d'un problème d'optimisation}

\hl{a finir peut etre avec un exemple concret de définition d'algorithme génétique usant d'une fonction test.}

\begin{listing}[!htbp]

\begin{minted}[linenos=true,frame=single,fontsize=\footnotesize]{scala}

trait Schaffer extends GAProblem with MGFitness {

  def genomeSize = 1
  def min = Seq.fill(genomeSize)(-100000.0)
  def max = Seq.fill(genomeSize)(100000.0)

  type P = Seq[Double]

  override def express(g: Seq[Double], rng: Random) = Seq(f1(g(0)), f2(g(0)))
  override def evaluate(p: P, rng: Random) = p

  def f1(x: Double) = pow(x, 2)
  def f2(x: Double) = pow(x - 2, 2)

}

\end{minted}
\caption{Exemple de définition d'un problème multi-objectifs dans MGO}
\label{alg:Schaffer}
\end{listing}

La librairie est conçu de façon à être totalement indépendante, car elle intègre à la fois les concepts nécessaire à la définition du problème, et la résolution d'un problème. En ce sens, elle peut donc être utilisé ou intégré à n'importe quel autre logiciel qui respecte le formalisme mis en place.

Le listing \ref{alg:Schaffer} est un exemple de définition d'un problème d'optimisation dans MGO. Dans celui-ci, il s'agit de résoudre la fonction test F2 multi-objectif de Schaffer \textcite[94]{Schaffer1985}, déjà définit dans l'équation \ref{eq:schaffer}.

La ligne 5 contient la signature du composant, et indique à MGO qu'il s'agit d'un problème de type algorithmes génétique \keyword{GAProblem} de type multi-objectif \keyword{MGFitness}.

Pour que le composant \keyword{GAProblem} soit opérationel, celui-ci a besoin que l'utilisateur lui fournisse un certain nombre d'élements en entrée : une taille de génome \keyword{genomeSize}, une type de Phenotype \keyword{P}, la définition des fonctions \keyword{express(...)} et \keyword{evaluate(...)}.

\begin{figure}[ht]
	\begin{sidecaption}[fortoc]{Hierarchie entre les composants pour la définition d'un problème, un exemple avec la fonction de test F2 de Schaffer}[fig:hierarchieComposants]
	 \centering
	 	\includegraphics[width=.7\linewidth]{HierarchieComposants.pdf}
	\end{sidecaption}
\end{figure}

Si on regarde les dépendances de \keyword{GAProblem}, alors on voit qu'il dépend de \keyword{Problem}, lui même étant défini comme étant une extension d'\keyword{Evolution}. Concrétement cela veut dire que le problème Schaffer ne pourra être résolu que si l'ensemble des composantes cumulées de \keyword{GAProblem} , \keyword{Problem} et \keyword{Evolution} sont satisfaites au moment de la résolution, défini dans la section ci dessous.

Les fonctions $f1$ et $f2$ prennent un seul et même paramètre de variation $x$ non contraint. La taille \keyword{genomeSize} du génome est donc fixé à 1, et la définition \keyword{min} et \keyword{max} de sa variation est fixé ici à $-10^{5}$ et $10^{5}$.

Le type \emph{P} indique la nature du phénotype attendu pour ce problème. Dans notre cas la représentation d'une solution candidate dans l'espace de solution $\mathbb{X}$ correspond à un vecteur de réels \emph{Seq[Double]} qui accueille le résultat des fonctions objectifs $f1$ et $f2$.

Les deux fonctions \keyword{express(...)} et \keyword{evaluate(...)} doivent également être définies car elles sont utilisés par le composant \keyword{Evolution}, et fixe les règles de constitution d'un \keyword{Individu}, une structure de données permettant de regrouper le génome, le résultat des fonctions objectifs pour ce genome, ainsi que le résultat de la future fitness.

\begin{itemize}[label=\textbullet, noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt]
\litem{\keyword{express}} prend en paramètre un vecteur de double qui correspond à la taille du génome. Dans le cas de Schaffer, il n'y a qu'un seul paramètre $x$, donc le génome $g$ est de taille 1, et donc $g(0)$ correspond forcément à la seule valeur $x$ contenu dans le génome. Cette fonction retourne l'expression de la fonction objective à évaluer, donc un vecteur contenant l'expression des deux fonctions objectives $f1$ et $f2$.

\litem{\keyword{evaluate}} est une fonction qui prend un phénotype et renvoie un vecteur de valeur correspondant à l'évaluation des fonctions objectifs. Ici les valeurs de $f1$ et $f2$

\end{itemize}

Dans le cas d'une simulation, \keyword{express} est la fonction qui va executer le modèle de simulation et récupèrer les valeurs en sortie, qui constitue le Phénotype. On voit bien ici que le phénotype ne permet pas forcément de discriminer les résultats de deux simulations, autant il peut s'agir d'indicateur réutilisable directement comme fonction objectifs, autant il peut également s'agir de données brutes qui peuvent être analysé en de multiples façons.  La fonction \keyword{evaluate} applique le vecteur de fonctions objectifs sur ces mesures en sortie de simulation. Un \keyword{Individu} dans la \keyword{Population} est une structure de données composée d'un Genome (paramètres du modèles), du résultat de la fonction \keyword{express} qui transforme un Génome en Phénotype, et du résultat de la fonction \keyword{evaluate} qui transforme un Phenotype en vecteur de valeurs d'objectifs.

%difference par rapport aux autres librairies ?

\paragraph{Execution d'un problème d'optimisation}

\begin{listing}[H]

\begin{minted}[linenos=true,frame=single,fontsize=\footnotesize]{scala}

object TestNSGAII extends App {

  val resolve =
    new Schaffer with NSGAII with CounterTermination {
      def steps = 1000
      def mu = 200
      def lambda = 200
      def genomeSize = 1
    }

  implicit val rng = newRNG(42)

  val res =
    resolve.evolve.untilConverged {
      s => println(s.generation)
    }
}

\end{minted}
\caption{Evaluation d'un problème multi-objectif à l'aide de l'algorithme NSGA2}
\label{alg:Evaluation_Schaffer}
\end{listing}

Dans l'implémentation \ref{alg:Evaluation_Schaffer}, la variable \keyword{resolve}  contient la définition de la marche à suivre : il s'agit ici de résoudre le problème \keyword{Schaffer} (voir \ref{alg:Schaffer}) en utilisant la métaheuristique \keyword{NSGA2} (voir \ref{alg:nsga2}) en utilisant un indicateur de fin d'optimisation de type compteur de génération \keyword{CounterTermination}.

\hl{verifier mu et lambda dans le code MGO}
Les variables \keyword{steps} (nombre d'itération avant arrêt de la métaheuristique), \keyword{mu} (), \keyword{lambda} (), \keyword{genomeSize} (taille du génome) sont des variables qui n'ont pas trouvé de valeur dans la composition du graphe de dépendance reliant les différents composants, le compilateur fait donc remonter (sous forme d'erreur à la compilation) la nécessité de définir ces variables aux derniers niveau de définition, qui constitue la tête de l'arbre, afin que ceux ci soient renseignés par l'utilisateur.


\subsubsection{La mise en oeuvre du couplage MGO - openMOLE }

Dans la section justifiant la création d'une nouveau framework dédié aux méta-heuristiques, on a abordé des points d'objectifs à la fois centré sur les capacités finale de MGO en tant que framework flexible, extensible, autonome; mais également sur les capacités tout autres attendues pour une utilisation de ce framework dans un outil spécialisé dans la distribution transparente des calculs, openMOLE.

Concevoir des logiciels capable de s'exectuer sur des environnement distribués est une spécialité informatique en tant que telle, usant de technologie et de jargon déjà difficile d'accès à des développeurs spécialisé, et donc encore moins accessible à un public plus inter-disciplinaire comme les chercheurs en SHS.

Il n'est donc pas question d'associer, du moins dans un premier temps, une myriade d'interface au framework MGO à destination d'un couplage direct avec les technologies \textit{HPC} ou de \textit{Grid Computing}. Il me semble être beaucoup plus intéressant de ne pas complexifier outre mesure la librairie MGO, et de profiter du fait qu'il s'agit d'un \textit{framework} pour composer des métaheuristiques plus adaptés à l'usage maximum qui peut être fait des ressources disponibles. En soit, MGO peut dans sa version autonome satisfaire un public scientifique exigeant sur le volet d'une recherche théorique dans le domaine des méta-heuristiques, tout en proposant dans un plugin pour openMOLE, des métaheuristiques adaptés à un tout autre public, attiré par l'efficience de ces algorithmes pour des cas d'utilisation très spécifique qui n'ont que très peu avoir avec les \textit{benchmarks} que l'ont trouve par exemple dans la littérature des EA. Dans cette configuration, composer ces propres métaheuristiques reste toujours possible, mais demande un peu plus de connaissance technique pour modifier le code du plugin faisant l'interface entre MGO et openMOLE. Le tableau \ref{tab:resume_public_cible} aborde de façon synthétique ces deux points de vue utilisateurs par rapport aux principales qualités attendues du programme : Flexibilité, Extensibilité, Utilisabilité.

% http://www.tablesgenerator.com/latex_tables#
% Ajouter une couleur sur la partie +++ a mettre en valeur coté plugin MGO

\begin{table}[!htbp]
\begin{sidecaption}[fortoc]{Résumé des avantages et des inconvénients à atteindre selon le public cible. \parbox{\marginparwidth}{
\begin{enumerate}[label={},noitemsep,  parsep=0pt, partopsep=0pt, labelindent=0pt,leftmargin=*]
		\item $-$ assez difficile
		\item $-{}-$ difficile
		\item $-{}-{}-$ très difficile
		\item $+$ assez facile
		\item $++$ facile
		\item $+++$ très facile
\end{enumerate}}}
	[tab:resume_public_cible]
	\centering
	\begin{tabular}{llllll}
		\toprule
		\multicolumn{2}{l}{\multirow{2}{*}{}} & \multicolumn{2}{l}{modélisateur} & \multicolumn{2}{l}{informaticien} \\ \cline{3-6}
		\multicolumn{2}{l}{} & Plugin Mgo & Mgo & Plugin Mgo & Mgo \\
		\midrule
		(1) & Flexible & $-{}-$ & $+$ & $+$ & $+++$ \\
		(2) & Extensible & $-{}-{}-$ & $-$ & $+$ & $++$ \\
		(3) & Utilisable & $+++$ & $+$ & $+++$ & $+++$ \\
		\bottomrule
		\end{tabular}
  \end{sidecaption}
\end{table}

\begin{enumerate}[label=(\arabic*)]
\item Du point de vue de la flexibilité des métaheuristiques proposés, c'est à dire de la possibilité de combinaisons de composants offertes par MGO, elle est censé être identique que cela soit dans le plugin ou directement avec MGO. Toutefois, cette flexibilité est beaucoup plus difficile à mettre en oeuvre dans le plugin pour des non informaticiens, car la logique d'exécution des métaheuristiques est éclatée dans différentes tâches d'un workflow openmole afin que certaines d'entre elles soient parallélisés (voir la section suivante \ref{p:prototype_fonctionel}). De nouveaux algorithmes métaheuristiques spécialisés pour une utilisation sur grille de calcul font également leur apparition dans le plugin, et constitue des worflow beaucoup plus complexes, assez difficilement accessibles à la modification par des modélisateurs géographes.

\item Ajouter ou modifier de nouveaux algorithmes dans le plugin openMOLE nécessite de maitriser sufisament bien à la fois l'ensemble du vocabulaire (aussi apellé \textit{Domain Specific Language} DSL openMOLE) nécessaire pour définir des workflows, mais également le langage de programmation Scala qui dans le plugin est mélé de façon beaucoup plus complexe à ce vocabulaire. Une fois ces premières connaissances acquise, et avant même de pouvoir créer de nouvelles méta-heuristiques, il faudra comprendre en observant les métaheuristiques déjà implémenté comment d'une part les composants MGO sont encapsulés et chainés dans un workflow complexe mélant Scala et le DSL openMOLE, et d'autre part comment ce workflow voit sa complexité encapsulé dans un élément de vocabulaire nouveau accessible directement au niveau du DSL. Si ce travail peut effectivement être réalisé sans trop de difficulté par un informaticien un peu expérimenté, l'exercice paraitra très difficile voire hors de portée à un modélisateur géographe.

\item L'utilisation du plugin dans openMOLE doit être rendu le plus simple possible pour le modélisateur et l'informaticien. En revanche l'utilisation du framework de façon autonome demande pour un modélisateur quelque compétences techniques supplémentaires pour installer les logiciels adéquats permettant de modifier et compiler MGO. Une fois cette barrière technique dépassé, la connaissance du langage Java ou Scala est certe nécessaire pour modifier ou ajouter des composants, mais elle ne l'est pas forcément lorsqu'il s'agit d'imbriquer et de paramétrer les composants correctement.

\end{enumerate}

En effet, si les métaheuristiques se doivent effectivement d'être modifiable, ce n'est pas forcément cet aspect qui prime pour une utilisation dans la modélisation; qui subit une fois la base du modèle réalisé des cycles de développements assez courts en définitive, ceux ci se limitant la plupart du temps à des ajouts, des modifications, ou des retraits d'hypothèse. L'utilisation de méta-heuristique, même canonique, est un premier palier d'utilisation pour tenter de discriminer au plus vite les modèles de simulation. Dans ce cas c'est bien les propriétés d'efficacité et d'indépendance au problème que l'on mobilise dans l'utilisation de métaheuristique. La modification plus fine est un cas d'utilisation qui vise une adaptation spécifique de la réponse d'une métaheuristique à un problème, ce qui nécessite de la part de celui-ci une variabilité moindre que dans le cas d'un construction de modèle.


La mise à disposition de ce framework dans le cadre d'un plugin pour openMOLE fait intervenir de tout nouveaux objectifs, cette fois ci relatifs aux intéréts des modélisateurs.

Si le framework MGO est parfaitement utilisable de façon autonome, celui-ci ne bénéficie pas dans ce mode d'utilisation des algorithmes dédiés pour une utilisation sur des environnements distribués.

Or c'est bien cette utilisation que nous voulons rendre disponible aux modélisateurs, à la différence des librairies intégrés existantes, comme par exemple behaviorSearch que l'on a déjà décrit dans la \ref{section xx}.

On a vu précédemment que la recherche d'une architecture logicielle modulaire était avant tout guidé par la nature elle même très modulaire des métaheuristique.



L'histoire MGO tel qu'avancé dans la section \ref{ssec:historique_mgo}, un autre objectif à guider la réalisation d'un framework aussi modulaire.

% evaluate et express sont dans la meme tache dans OpenMOLE

\subsection{Premiere prototype fonctionel}
\label{p:prototype_fonctionel}

Avant mars 2012, le couplage entre MGO et openMOLE est réalisé par l'utilisateur lors de la définition des workflows. Grâce à la nature autonome et modulaire des composantes fournies par l'univers du framework MGO, la logique d'execution d'une métaheuristique peut facilement être retranscrite dans le référentiel d'openMOLE. Les tâches d'un workflow vont tout simplement encapsuler les appels et les objets spécifiques aux différentes composantes de la métaheuristique, en apportant la logique nécessaire à une execution de certaines de ces composantes dans un environnement de calcul distribué.

Autrement dit, la logique d'execution générique d'une métaheuristique décrite dans la composante \keyword{Evolution} est instancié (par exemple on choisit d'implémenter NSGA2) puis éclaté dans un ensemble de tâches dont l'organisation est guidé à la fois par la reproduction de cette optimisation, mais aussi par la parallélisation de certaines de ces tâches.

Un tel workflow doit également s'aclimater de toute la logique d'experimentation pour un modèle de simulation. Des tâches intermédiaires font donc leur apparition dans le workflow pour gérer un plan d'expérience particulier.

La nature intrinsèquement parallèle des algorithmes evolutionnaires tient dans la possibilité d'évaluer non pas les solutions candidates une par une, mais bien d'un seul coup, par l'évaluation de l'ensemble des solutions candidates d'une population à un instant $t$. La première étape a donc été de distribuer l'évaluation des solutions candidates sur un environnement distribué, afin de bénéficier d'une execution quasi simultanée de l'ensemble de la population.

\begin{figure}[H]
	\begin{sidecaption}[fortoc]{wf openmole \sqbox{tangoBlue1}}[fig:openmole_wf]
		\centering
		\includegraphics[width=1.1\linewidth]{wf_openmole_mgo_1.pdf}{
		}
  \end{sidecaption}
\end{figure}

On considère les paramètres suivant, une population initiale de génomes $n$, et le nombre de réplication à executer pour chacun de ces génomes équivalent à $r$; Autrement dit, à chaque itération $t$ de l'algorithme d'optimisation, c'est $n$ jeu de valeur de paramètres différentes (équivalent génome) qui vont chacune être testé $r$ fois avec des graines aléatoires différentes. Soit un total final de $n * r * t$ simulations.

Avant de rentrer dans les détails du workflow, il est également important de rapeller que dans celui-ci, un \keyword{Genome}, et par la suite un \keyword{Individu} dans une \keyword{Population}, sont deux dénominations qui désignent avant tout un jeu de valeur de paramètres, même si un individu représente on va le voir un peu plus que çà. Autrement dit, quant on discute ici de la qualité d'un \keyword{Individu} et de son positionnement dans l'espace des objectifs $\mathbb{Y}$, c'est sous entendu en référence au jeu de valeur de paramètre qui a permis d'obtenir ce même \keyword{Individu}.

Le premier niveau de workflow (étape \circled{1} dans la figure \ref{fig:openmole_wf}) opérant de façon locale sur la machine ou le serveur de l'utilisateur est décrit ainsi :

%\begin{itemize}[label=\textbullet]

\begin{myitemize}

\item[G] Cette tâche définit comment le problème va être représenté par le \keyword{Genome} durant l'optimisation. C'est aussi à ce moment là que le mapping est réalisé avec les paramètres du modèles, avec par défaut 1 gène par paramètre $p$ du modèle. Cette tâche génère ensuite un ensemble de génomes $g_i \in \{g\}, i \in \{1 \dotsc n\}$ chacun étant initialisé par des valeurs de paramètres aléatoires.

\item[Eg] Cette première tâche d'exploration execute un plan d'expérience qui associe à chaque \keyword{Genome} $g_i$ une liste de graines aléatoires  - ou \textit{seeds} - $\{s_i\}$. Cette liste d'éléments $s_{i,r} , s_{i,r} \in \{s_i\}, r \in \{1 \dotsc r\}$ est de taille égale au nombre de réplications $r$ définies en paramètre de l'expérimentation. De ce plan d'expérience résulte la création d'un ensemble $\{w\}$ de sous workflows équivalent au nombre de génomes initial $n$. Chaque $w_i$ est organisé selon la description du deuxième niveau de workflow décrit ci-dessous, distribué de façon transparente sur un des noeud de la grille de calcul par openMOLE (étape \circled{2} dans la figure \ref{fig:openmole_wf}) avec pour paramètre d'entrée le vecteur $(g_i, \{s_i\})$.

\item[Ag] Chaque sous workflow $w_i$ s'executant sur grille renvoie un \keyword{Individu} évalué, une structure de donnée qui associe pour chaque \keyword{Genome} $g_i$, un \keyword{Phenotype} et un vecteur d'objectifs $f_i$. Cette tâche d'aggrégation collecte ces individus auprès de l'ensemble des sous workflow de façon à former un ensemble d'\keyword{Individu} formant une nouvelle \keyword{Population} $P$. Cette dernière est ensuite transmise pour examen à la tâche suivante \textit{Ev}.

\item[Ev] Cette tâche contient le coeur de l'algorithme d'optimisation, dont le comportement est fonction de l'algorithme évolutionnaire selectionné ou composé, et des paramètres choisi par l'utilisateur pour celui-ci. Dans une première phase, la population nouvellement constituée est fusionnée avec la population d'individu de la génération précédente. Une fitness est attribuée à chaque individu de cette nouvelle population, ce qui permet de caractériser la qualité de chacune de ces solutions dans l'espace des objectifs de façon relative à l'algorithme utilisé. S'ensuit alors une première phase élitiste de selection qui opère sur la base de ce score. Les individus selectionnés participent ensuite de nouveau à un tirage au sort pour tenter d'intégrer le pool d'individu (\textit{mating pool}) participant à la reproduction, étape durant laquelle emerge un ensemble de nouveaux génomes à évaluer, transmis à la tâche \textit{Ev} pour une nouvelle distribution.
\end{myitemize}

Le deuxième niveau de workflow (étape \circled{3} dans la figure \ref{fig:openmole_wf}), celui qui s'execute sur un noeud distant de la grille de calcul, contient les tâches suivantes :

\begin{myitemize}

\item[X] Cette tâche extrait les valeurs de paramètre du génome $g_i$ qui est donné en entrée de ce sous workflow, et les transmets avec l'ensemble des $r$différentes \textit{seeds} de l'ensemble $\{s_i\}$ à une nouvelle tâche d'exploration \textit{Es}

\item[Es] définit un nouveau plan d'expérience pour exécuter l'ensemble des réplications $s_{i,r}$ sur ce noeud local de la grille. A chaque réplication du modèle de simulation est associé un jeu de valeur de paramètre tel qu'il a été extrait du génome $g_i$ (toujours identique donc), ainsi qu'une \textit{seed}, prise dans $s_{i,r}$ avec $r$ différent pour chaque réplication.

\item[M] exécute le modèle de simulation avec la graine aléatoire $s_{i,r}$ et le jeu de valeur de paramètre fourni. Cette étape est équivalente à la constitution du Phénotype.

\item[Obj] applique aux résultats de la simulation les fonctions objectifs définis par l'utilisateur dans le workflow

\item[As] récupère les vecteurs de valeurs objectifs associées à chaque réplication du modèle de simulation, et les aggrege selon une fonction statistique choisi par l'utilisateur, une moyenne ou une médiane par exemple.

\item[Ind] crée un \keyword{Individu}, une structure de donnée associant le \keyword{Genome} $g_i$ évalué et le vecteur objectifs précédemment agregé. La \textit{seed} n'est pas conservé dans cette expérience ou on recherche avant tout un comportement robuste à l'aléa.

\end{myitemize}

Les premiers workflows \Anote{cas_utilisation_wfom}, assez complexes et uniquement accessibles au format scripts, ont rapidement constituées des fichiers de plusieurs centaines de lignes. Cette évolution dans la taille des workflows est rapidement devenu problématique à la fois pour la maintenance mais également pour la lisibilité de tels expérimentations. Un obstacle vis à vis de notre principal objectif, mettre à disposition des modélisateurs un outil dont le premier avantage est sa facilité de mise en oeuvre et sa réutilisabilité.

Si l'execution d'un tel workflow est déjà un premier pas vers une systématisation dans l'application de ces techniques d'optimisation à l'exploration des modèles de simulations, il faut soulever un autre point important aux yeux de l'utilisateur. Quelle performance peut on en effet attendre d'un tel workflow d'optimisation distribué lorsqu'il est appliqué sur un modèle de simulation relativement rapide en Netlogo ?

Si $n=200$, $r = 30$, $t=2000$, cela équivaut donc à 12 millions de simulations. Pour nous donner une durée d'execution, il faut encore multiplier par la durée d'exécution du modèle : un modèle jugé relativement rapide s'executant en moyenne en 2 minute sous Netlogo revient à cumuler environ 46 années de calcul sur un seul processeur. Sur une grille de calcul disposant de 1000 processeurs, cette durée descend à 17 jours. Sachant que de tels algorithmes évolutionnaire ne garantissent pas d'optimum global, ceux ci doivent également être répliqué afin de vérifier si l'algorithme, malgré notre vigilance sur les paramètres de celui-ci, ne serait pas malheureusement tombé dans un optimum local ?

On comprend dès lors que chaque minutes qui peut être gagné sur l'exécution de ce modèle est autants de temps gagné sur la rapidité avec laquelle on est capable d'effectuer un retour d'expérience sur notre modèle à partir de ces résultats.


Un triple retour d'experience, pour MGO, pour openMOLE, pour le modèle.

Un choix difficile s'offre au modélisateur voulant explorer de façon plus systématique

Quelles questions ?
- Comment effectuer une recherche plus exhaustive des comportements des modèles ?
- Comment discriminer les résultats d'un front de Pareto, quelle expertise humaine ?

Quels apprentissages ?
- Quels biais dans les modèles stochastique ?
- Nombre de réplications nécessaires, suffisantes ?
- Quel retour sur la thématique ?

Quelques pistes ?
- quels critères d'arrets ?
- quel espilon sur les objectifs ?
- quelle possibilité de réévaluation ?
- aller vers des algorithmes utilisant la grille de façon plus efficace
- réécrire les modèles
- réduire le nombre d'objectif
- réduire le nombre de réplication
- réduire la population d'individu
- réduire la durée de l'expérimentation





\textbf{Après + description !!}



%Le choix est fait ici de développé une librairie MGO utilisable de façon indépendante, ou de façon couplé à openMOLE.

\section{Une brique logicielle dédiée à la visualisation de résultats}
