% -*- root: These.tex -*-

\Anotecontent{hd_confirmatif}{\enquote{L’une des forces de la THDC (théories hypothético-déductives de la confirmation) est qu’elle semble rejoindre assez largement la pratique méthodologique des sciences empiriques: elle restitue l’idée que pour évaluer une théorie ou une hypothèse, on en extrait d’abord certaines \enquote{prédictions} et que lorsque ces prédictions sont correctes, la confiance dans la théorie ou l’hypothèse s’en trouve confortée.} \autocite[13]{Cozic2009}}

\Anotecontent{confirmation}{\foreignquote{english}{Whenever observational data and evidence speak in favor of, or support, scientific theories or everyday hypotheses, the latter are said to be confirmed by the former. The positive result of an allergy test speaks in favor of, or confirms, the hypothesis that the tested person has the allergy that is tested for. The dark clouds on the sky support, or confirm, the hypothesis that it will be raining soon.[...] Historically, confirmation has been closely related to the problem of induction, the question of what to believe regarding the future in the face of knowledge that is restricted to the past and present.} (définition partielle tirée de \href{http://www.iep.utm.edu/conf-ind/}{@IEP} )}

\Anotecontent{information_confirmatif}{Ces notions sont assez complexes à déméler et mettent en jeu des démonstrations logiques assez poussées. On trouvera plus d'information sur les travaux d'Hempel sur la \enquote{confirmation} dans le cours de \textcite{Cozic2009}, et sur les sites de philosophie encyclopédique de l'\href{http://www.iep.utm.edu/conf-ind/}{@IEP}, de \href{http://plato.stanford.edu/archives/fall2015/entries/confirmation/}{@Standford}}

\Anotecontent{paradoxe_raven}{Dans un modèle logique tel que celui utilisé par ces philosophes, \enquote{Tous les corbeaux sont noirs } est considéré comme logiquemement équivalent à \enquote{ Tout objet non noir est autre chose qu'un corbeau}. Si je vais dans la nature, plus je vois de corbeau noir, et plus cela renforce mon sentiment que tous les corbeaux sont noirs. Maintenant, si je regarde la contraposée, je peux chercher dans la pièce ou je me situe pour vérifier que tout objet non noir n'est effectivemet pas un corbeau. Ce qui se confirme. Or, du fait de l'équivalence logique entre les deux propositions, plus je fait ce constat absurde qu'un objet non noir est autre chose qu'un corbeau (une boite de mouchoir blanche par exemple) dans la pièce ou je me trouve, et plus ce sentiment se renforce et renforce également la contraposée. Ce qui est totalement absurde.}

\Anotecontent{stanislas}{Un modèle de cognition remis au goût du jour ces dernières années, et qui parait assez bien à ce phénomène, est celui du \enquote{cerveau statisticien}, \enquote{cerveau prédictif}, ou \enquote{cerveau bayésien} pour qui \enquote{penser c'est avant tout prédire}. Cette machine à inférer permanente, construisant des logiques qui lui sont propres à partir du peu d'informations qui lui sont données directement ou indirectement, quitte à rappeler en urgence d'anciens schémas, fournit comme on pourrait s'en douter plus de mauvaises prédictions que de bonnes. Mais peu importe, ce qui est important ici, c'est sa capacité à apprendre rapidement de ces erreurs. Ainsi pour Stanislas Dehaene, partisan de ce modèle cognitif, \enquote{Ce que Pierce appelle l'abduction n'est rien d'autre que ce que les sciences cognitives contemporaines nomment l'inférence bayésienne et qui consiste à mener un raisonnement probabiliste en sens inverse afin de remonter aux causes cachées d'une série d'observations.} Cette théorie qui touche à l'ensemble des disciplines oeuvrant dans le champ des sciences cognitives sont mieux décrites par exemple par Stanislas Dehaene dont les cours sont disponibles sur le \href{http://www.college-de-france.fr/site/stanislas-dehaene}{@site} du Collège de France.}

\Anotecontent{idee_refutation}{ Ces deux auteurs sont proches des idées d'\textcite{Hedstrom2010}, qui présente une idée similaire dans son analyse des \textit{mechanisms}.

\foreignblockquote{english}[\cite{Hedstrom2010}]{In itself, a mechanism scheme provides a \textbf{how-possible explanation}; it tells us how the effect could in principle be produced. As it is possible that similar effects can be produced by a number of different (known or unknown) mechanisms, a crucial element in any mechanism-based ex- planation of empirical facts is the collection of empirical evidence about the assumed entities, activities, relations, etc. The empirical evidence turns a possible mechanism into a plausible mechanism and may eventually lead to the identification of the actual mechanism. \textbf{By presenting evidence in support of the assumptions of one mechanism and showing the absence of evidence for the assumptions of competing mechanisms, we increase the plausibility of the explanatory hypothesis.}}

Cette idée on la retrouve également dans la méthodologie \textit{Pattern Oriented Modelling} de Volker Grimm \autocites{Grimm2005, Railsback2012} et son équipe. Cette méthodologie de construction des modèles extrêmement bien documentée \autocite{Grimm2011} est celle qui se rapproche la plus de nos pratiques. Nous aurons l'occasion de revenir plus longuement sur cette méthodologie dans une relecture critique en conclusion de cette thèse (section \ref{ssec:critique_pom}).

\foreignblockquote{english}[\cite{Grimm2005}]{Many studies of ACSs try only one model of decision-making and attempt to show that it leads to results compatible with a limited data set. This practice, however, may lead to the impression that bottom-up models include so many parameters that they can be fitted to data whether or not their structure and processes are valid. A more rigorous strategy for modeling agent decisions, or other bottom-up processes, is to use \enquote{strong inference} (J.R. Platt 1965) by contrasting alternative decision models, or \enquote{theories} (S. Y. Auyang 1998, Grimm et al.2005). First, alternative theories of the agent’s decisions are formulated. Next, characteristic patterns at both the individual and higher levels are identified. The alternative theories are then implemented in a bottom-up model and tested by how well they reproduce the patterns. Decision models that fail to reproduce the characteristic patterns are rejected, and additional patterns with more falsifying power can be used to contrast successful alternatives.}}

\Anotecontent{note_slocal_verification}{La réplication d’un modèle est un effort qui n’est pas nécessairement entrepris par un chercheur ou une équipe de chercheurs de façon externe au projet. En effet, le modèle SimpopLocal \autocite{Schmitt2015} a du être réimplémenté de Netlogo vers Scala pour des raisons d’efficiences, le code source étant devenu trop difficile à maintenir dans Netlogo, et le temps d’exécution du modèle trop important pour sa calibration, il a été fait le choix de réimplémenter le modèle dans un autre langage, plus adapté. Cet événement, tous les projets peuvent y être confrontés, dès lors que le modèle devient trop complexe à maintenir ou trop long à s’exécuter sur Netlogo. Cette réplication a permis de trouver de nombreux bugs, même après plusieurs milliers d’exécutions du modèle, et plusieurs années de développements.}

\Anotecontent{horizon_naif}{Il s'agit bien ici d'assumer cet horizon un peu naïf, car tous les raisonnement ne méritent pas reproduction. L'idée sous-jacente développée de façon parcimonieuse et ciblée dans l'argumentation et les exemples de \autocites{Millington2012} me semble assez importante.}

\Anotecontent{m2m}{Depuis presque 20 ans, et malgré les efforts fournis au travers des workshops M2M (2003 - 2008) \autocite{Rouchier2013} par divers membres de la communauté des systèmes multi-agents, la réplication des modèles de simulation reste une pratique assez rare.  En effet, selon l'enquête de \textcite{Heath2009}, la mise à disposition des modèles dans les publications est encore inférieure à 20\% dans l’ensemble des publications étudiées en 2008. En France de nouveaux réseaux organisant des écoles d’été ont émergés ces dernières années, comme le RNSC MAPS (2009 - 2014), ou le réseau MEXICO, MISS-ABMS, etc. où la formation à des techniques de plus en plus avancées alterne avec la création ou la réplication de modèle de simulation agent.}

\Anotecontent{explication_potentielle}{Cette dernière se réfère au modèle logique D-N de Hempel (1965), dont on a vu dans la section \ref{sssec:realite_neopositiviste} qu'il était complétement dépassé. Nous aurons également l'occasion de revenir sur ce point en proposant avec l'analyse de \textcite{Besse2000} une autre déconstruction de cette notion, plus compatible avec nos pratiques que celle proposée par Elsenbroich et les sociologues détaillée ci-dessous.}

\Anotecontent{TCR}{Boudon exprime son désaccord avec Coleman qui au fil de sa vie défendra une vision de la TCR de plus en plus utilitariste : \enquote{Pour ma part, je me suis d’emblée senti en désaccord avec Jim sur le degré de généralité qu’il convient d’accorder à la théorie du choix rationnel. J’ai toujours considéré la TCR comme un modèle puissant [...] C’est pourquoi j’ai toujours été un peu déconcerté par les croisades « anti-utilitaristes » (en fait anti-TCR) qui sont conduites ici ou là dans les milieux des sciences sociales. Mais ce modèle ne doit pas être utilisé à contre-emploi, car son axiomatique ne peut être tenue pour généralement valide. Je dois reconnaître toutefois que, si j’ai tout de suite perçu ce point, je n’ai pas vu d’emblée comment définir le cadre théorique qui permettait de dépasser le particularisme de la TCR. [...] Pourtant, André Davidovitch et moi-même avions proposé, dès 1964, un modèle de simulation qui esquissait, par l’exemple, une réponse à cette question (Boudon et Davidovitch, 1964).[...] La théorie déductive construite à partir de ces argumentations schématiques imputées à un juge idéal-typique relève bien de l’individualisme méthodologique, mais dans une version que l’on peut qualifier de « cognitiviste », car elle prête à la notion de rationalité un sens, non seulement instrumental, mais cognitif. [...] Le modèle générateur que j’ai, dans la même veine, proposé (Boudon, 1973) pour expliquer la structure d’un ensemble de données statistiques relatives à l’éducation relève, lui aussi, de la version cognitiviste de l’individualisme méthodologique. \autocite{Boudon2003}}}

\Anotecontent{individu_complexe}{\enquote{Deux éléments nous semblent alors justifier le qualificatif d’\enquote{ individualisme méthodologique complexe } que nous attribuons à la forme de base de tout \enquote{  modèle générateur }. Premièrement, de tels modèles tâchent de démêler les jeux d’imbrication d’une pluralité de mécanismes relevant de différents niveaux analytiques, et dont les effets s’entremêlent dynamiquement. On peut dire qu’un modèle générateur se propose de représenter de manière stylisée la \enquote{  complexité des mécanismes } sous-tendue par toute régularité macrosociale que le sociologue souhaite expliquer, et non seulement décrire. Deuxièmement, un \enquote{  modèle générateur } attribue une importance particulière aux \enquote{  mécanismes d’agrégation complexe }, c’est-à-dire ceux qui renvoient aux multiples systèmes d’interdépendance (directe et indirecte) qui relient les acteurs. En cela, de tels modèles renvoient alors à l’un des traits distinctifs de l’approche dite de la \enquote{  complexité } (approche par ailleurs fort hétérogène, cf. Axelrod, Cohen 2000 : 46-53), à savoir l’attention constate que celle-ci a consacrée à l’interdépendance entre les entités constitutives d’un système ainsi qu’aux phénomènes d’émergence qui en découlent (Atlan 1991 ; Morgan 2005 ; Morin 1999 : chap. 4 ; Simon 1996 : chap. 7, 1999 ; Weisbuch 2003). Sur ce point, notons cependant que notre discipline ne manque pas de précurseurs illustres : \enquote{  les sociétés doivent être considérées comme des enchevêtrements complexes de systèmes d’interaction }, lit-on, dès la fin des années soixante-dix, chez Boudon (1979b : 113).}\autocite[12]{Manzo2007}}

\Anotecontent{mecanism_ambigu}{\foreignquote{english}{There is an ambiguity in the use of the notion of a mechanism. Sometimes it is used to re- fer to a causal process that produces the effect of interest and sometimes to a representation of the crucial elements of such a process. This should not be a cause of concern, however, be- cause the latter presupposes the former. When onemakes a claim that a certain mechanism ex- plains some real world events, one commits to the existence of the entities, properties, activ- ities, and relations that the description of the mechanism. \autocite{Hedstrom2010}}}

\Anotecontent{instrumentalisme_refute}{\foreignquote{english}{The idea of social mechanisms is quite often associated with rational choice theory (Abbott 2007, Gross 2009)b. However, from a philosophical and general sociological point of view, the connection between the two is quite weak. There is nothing in the idea of a mechanism-based explanation that would require the explanation to be articulated in terms of rational choice theory. On the contrary, the requirement that mechanism-based explanations cite actual causes of the phenomenon to be explained often makes rational choice explanations unacceptable, as they are built upon implausible psychological and sociolog- ical assumptions. Empirically false assumptions about human motivation, cognitive processes, access to information, or social relations cannot bear the explanatory burden in a mechanism-based explanation. It is not enough that the model \enquote{saves the phenomena}; it should represent the essential features of the actual causal structure that produces the observed phenomena.} \autocite{Hedstrom2010}}

\Anotecontent{manzo_mecanisme}{\enquote{Du premier point de vue, un mécanisme a la fonction d’éclairer comment et pourquoi une relation (ou une structure de relations) a été engendrée (Harré 1972 : 6, 118). Il s’agit de s’intéresser au « mode de production des phénomènes » (Cherkaoui 1998 : chap. 3). Concevoir le mécanisme de cette manière, c’est saisir son trait distinctif : la « générativité » (Fararo 1989 : 39-43). Cette notion pousse à rechercher l’émergence, l’engendrement ou la genèse de ce qui est observé. C’est d’ailleurs pourquoi on qualifie le plus souvent les mécanismes de \enquote{ générateurs }.}\autocite{Manzo2007}}

\Anotecontent{regularite_machamer}{An entity acts as a cause when it engages in a productive activity.[....] The intellibility consists in the mechanisms being portrayed in term's of a field's bottom out entities and activities. [...] The understanding provided by a mechanistic explanation renders a phenomenon intelligible. Mechanism descriptions \textbf{show how possibly, how plausibly, or how actually things work}. Intellibility arises not from an explanation's correctness, but rather from an elucidative relation between the explanans [...] and the explanandum [...] We should not be tempted to follow Hume and later logical empiricists into thinking that the intelligibility of activities (or mechanisms) is reducible to their regularity. [...] It is not the regularities that explains but the activities that sustain the regularities.}

\Anotecontent{besse_comprendre}{Pour mieux comprendre en quoi la notion de causalité d'Hempel et/ou Popper (modèle logique ND ou Nomologique-Déductif) se rapporte à une forme d'observation des régularités, on peut citer les propos de \textcite{Besse2000} : \enquote{C'est cela qui constitue la nouvelle conception de la causalité, dont Popper et les néo-positivistes se veulent les héritiers : la cause n'est plus définie désormais, comme puissance de production, mais comme liaison de phénomènes, comme loi ou régularité phénoménales. Pour reprendre la distinction devenue classique, on ne cherche plus le \textit{Pourquoi ?} des phénomènes, mais uniquement leur \textit{Comment ?} [...] Les partisans du réalisme philosophique ont sévèrement critiqué cette conception \enquote{positiviste} de l'explication scientifique, qui ne permet pas en vérité, selon eux, de fournir une intelligibilité aux phénomènes dont il s'agit de rendre compte. [...] Un partisans du réalisme reprocherait en effet à l'explication nomologique son caractère à la fois conventionel et descriptif, ne permettant pas véritablement de rendre \textit{raison} du phénomène à expliquer. Un réaliste récuserait, d'une certaine manière, l'opération \textit{d'euphémisation} de la notion de cause, qui réduit celle-ci à la simple notion d'une \enquote{condition initiale}, et surtout, qui conduit à identifier l'explication à la simple constatation de relations régulières de succession entre des phénomènes antécédents et des phénomènes conséquents. (Vrin, 1998, 130) }}

% \Anotecontent{hedstrom_fondamentaux}{\textcite{Hedstrom2010} revient plus en détail dans son texte sur la différence opérant entre les deux cadres explicatifs : la \textit{covering law} d'Hempel et la notion de mecanisme qu'il supporte.}

% \Anotecontent{hedstrom_fondamentaux}{
% \begin{enumerate}
% \item Un mécanisme est identifié par le type d'effet ou de phénomène qu'il produit.
% Un mécanisme est toujours un mécanisme pour quelque chose (Darden2006, p273). Le modélisateur doit faire preuve d'attention sur ce point, car à mécanisme égal, le résultat ou les effets produits peuvent être différents.

% \item Un mécanisme est une notion causale.
% Il se réfère aux entités menant au processus en cause pour produire l'effet en question. Un mécanisme n'est pas forcément invisible, et celui-ci peut être d'une nature stochastique, ce qui a pour effet de modifier la probabilité d'apparition du phénomène.

% \item Un mécanisme possède une structure.
% Ainsi, de la même façon qu'on ouvre une boite noire pour en révéler les éléments, les mécanismes possède une structure permettant de fractionner le questionnement : quels sont les entités participantes ? Quels sont leur propriétés ? Comment sont les intéractions de ces entités organisés temporellement et spatialement? Quels facteurs préviennent ou modifient the outcome ? Etc.

% \item Les mécanismes forment une hierarchie 
% Un mécanisme a un niveau donné présupose ou prend en compte l'existence de certaines entités et propriétés qui sont eux – même expliqués dans un niveau inférieur. La notion d'interdisciplinarité est pleinement compatible avec une telle définition, car les mécanismes et les entités qui les composent peuvent tout à fait provenir d'autre champs disciplinaires. Pour être explicatif un mécanisme n'a nul besoin que ces entités, ces propriétés ou ces activités soient elle même expliqué, la seule contrainte réside dans la nécessaire existance de ces éléments, leur explication est une tout autre question.
% \end{enumerate} }

\Anotecontent{pouvreau_teleologique}{A rapprocher aussi de l'analyse faite par \textcite[114]{Pouvreau2013} sur la finalité des processus à l'oeuvre dans l'auto-organisation des systèmes ouverts : \enquote{Tout processus d’équilibre peut être formulé téléologiquement [...] Toutes les lois systémiques ont la particularité que ce qui apparaît pour l’ensemble du système comme un processus causal d’équilibre peut être formulé téléologiquement pour les parties. Ce qui correspond à un processus causal d’équilibre apparaît pour la partie comme un événement téléologique, en ce que l’action de cette dernière semble dirigée vers le \enquote{ but } consistant à prendre sa place \enquote{ convenable } dans le tout.}}

\Anotecontent{verification_passeron}{Le terme \enquote{vérification} \foreignquote{english}{[...] stands for absolute thruth } \autocites{David2009, Oreskes1994} et se rapporte avant tout ici à la notion d'équifinalité \autocite{OSullivan2004}.}

\Anotecontent{manzo_empirie}{\textquote[\cite{Manzo2007}]{On peut en effet faire reposer sur cette confrontation entre les données simulées et les données empiriques une partie essentielle de la procédure d’évaluation des résultats d’un système multi-agents et de la pertinence des mécanismes générateurs qu’il tâche d’« animer ». Par rapport à une conception des « modèles informatiques » comme des purs « laboratoires virtuels » (cf. Carley 1999), une alliance, en aval, entre la technique statistique descriptive et des « modèles générateurs » à visée explicative animés par simulation pourrait ainsi satisfaire, du moins en partie, l’exigence, ressentie de plus en plus fortement, d’une meilleure « validation » des « modèles simulés » (cf. Boero, Squazzoni 2005 ; Moss, Edmonds 2005 ; Hedstrom 2005 : chap. 6).}}

\Anotecontent{manzo_journal}{Un journal qui n'avait pas abordé le thème de la simulation depuis les années 1965 (voir section \ref{ssec:disciplines_touches}).}

\Anotecontent{embryon}{\enquote{[...] le Viennois qualifia d’« équifinalité », c’est-à-dire ici la capacité de l’embryon à atteindre un même état final de développement quelles que soient les conditions initiales et les modalités intermédiaires particulières de son développement.} \autocite[83]{Pouvreau2013}}

\Anotecontent{desuet_as}{Un terme dont il faut dire qu'il est désuet en 2008, étant donnée la diversité de modèles opérant déjà à cette époque dans la simulation en sciences humaines et sociales \autocite{Chattoe2011}.}

\Anotecontent{naylor_etonnement}{On pourra peut être être étonné de retrouver la démarche de Naylor dans les approches subjectives sachant la description qu'on en a fait au préalable. Mais il y a bien une part de subjectivité dans cette démarche, l'application de chacune des étapes de la multi-stage validation faisant quand même appel à une forme d'expertise pour constituer le jeu des hypothèses que l'on estime valable en vue du test final de comparaison aux données.}

\Anotecontent{conte_deccorele}{\foreignquote{english}{I will suggest that producing causes and their link to effects must be hypothesized independent of generation: rather than wondering \enquote{which are the sufficient conditions to generate a given effect?}, the scientist should ask herself what is a general, convincing explanation, and only afterwards, she should translate it into a generative explanation.}}

\Anotecontent{sous_determination}{\enquote{Elle qualifie le rapport
multivoque qui peut exister entre plusieurs modèles théoriques de processus et une loi ou régularité empirique que ces modèles visent à expliquer.[...] C’est à partir de cette pratique aussi – aujourd’hui démultipliée par la computation sur ordinateur - que l’on peut concevoir ce que nous appellerons une sous-détermination P/L par calcul : elle se définit comme la sous-détermination des processus d’interaction susceptibles de mener par calcul à une forme approchée d’une loi empirique donnée.}\autocite{Varenne2014}}

\Anotecontent{conte_bystander}{Conte prend pour exemple le travail de Latané et Darley sur le \foreignquote{english}{Witness Effect} (WE) (ou \foreignquote{english}{Bystander Effect}). Ce phénomène intervient lorsqu'une personne est en danger, la règle observée par Latané et Darley dit qu'au-delà de trois personnes présentes sur les lieux, la possibilité d'une prise en charge de la victime chute de façon inversemment proportionelle au nombre de personnes présentes. La reproduction de ce phénomène par l'implémentation de règles locales au niveau des agents est assez aisée selon Conte, en utilisant une règle de décision basée sur la majorité (\textit{majority-rule}). Toutefois, le pouvoir explicatif de cette implémentation est faible, pourquoi ? La différence est subtile et importante, ce n'est pas la capacité à produire le WE qui est intéressante d'un point de vue explicatif, mais le fait que les agents ont une \textit{majority-rule} opérant quelque part dans le cerveau : \foreignquote{english}{This is not hair-splitting: while looking for a causal theory, scientists do not content themselves with any producing factor. They look for an informative explanation, which incorporates additional understanding of the level of reality that the phenomena of study belong to. In our example, this means an explanation adding further understanding of social individuals. That a majority rule leads to WE under specified conditions tells us nothing new about agents' behaviours. On the contrary, that agents are governed by an internal majority rule, and consequently may interfere negatively with one another under specified conditions, is interesting news. This we learned from the work of Latané and Darley, Darley, independent of generative simulation.} \autocite{Conte2007}}

\Anotecontent{openshaw_critique_math}{\foreignquote{english}{So What is Wrong with Mathematical Modelling? The criticisms can be summarized under four headings: wrong objectives, insoluble problems in model construction, lack of empirical validation, and little evidence of success or of markets for the traditional produce.}}


\Anotecontent{lee_forrester}{\foreignquote{english}{He removed about two-thirds of the model without altering the remaining parts, which were left intact, and the model performance was not significantly altered.}\textcite[176]{Lee1973}}

\Anotecontent{source_amoral}{Le code source du modèle étant disponible dans une seule publication, celle du rapports transmis à la DATAR en 1983 \autocite{AMORAL1983}}


\Anotecontent{Kleijnen_def}{\foreignquote{english}{This paper uses the definitions of V \& V given in the classic simulation textbook by Law and Kelton (1991, p.299): \enquote{\textbf{Verification} is determining that a simulation computer program performs as intended, i.e., debugging the computer program .... \textbf{Validation} is concerned with determining whether the conceptual simulation model (as opposed to the computer program) is an accurate representation of the system under study}. Therefore this paper assumes that verification aims at a \enquote{perfect} computer program, in the sense that the computer code has no programming errors left (it may be made more efficient and more user friendly). Validation, however, can not be assumed to result in a perfect model, since the perfect model would be the real system itself (by definition, any model is a simplification of reality). The model should be \enquote{good enough}, which depends on the goal of the model.}}

\Anotecontent{HackingCartwright}{\textquote[\cite{Hacking1989}]{Nos deux livres ont plus d'un point commun. L'un et l'autre accordent peu d'importance à la vérité des théories et avouent un faible pour certaines entités théoriques. Cartwright soutient que seules les lois phénoménologiques de la physique parviennent à la vérité tandis que, dans la partie B de ce livre, je fais remarquer que la science expérimentale est plus indépendante de la théorie que ce que l'on veut bien généralement admettre. Nous ne partons pas des mêmes postulats anti-théoriques car elle considère les modèles et les approximations alors que c'est surtout l'expérience qui m'intéresse, mais nos conceptions convergent.}}

\Anotecontent{def_cartwright}{\enquote{Disons qu'il y a des théories, des modèles et des phénomènes. Il serait normal de penser que les modèles sont doublement des modèles. Ils sont modèles pour les phénomènes et modèles pour la théorie. [...] Le réalisme scientifique est ici tout particulièrement concerné. Cartwright est pour l'essentiel anti-réaliste à propos des théories. Pour cela, elle s'appuie en partie sur les modèles. Elle fait remarquer que non seulement les modèles ne peuvent être déduits de la théorie qui les englobe, mais plus encore que les physiciens utilisent à leur gré divers modèles qui, sans pourtant se recouper, cohabitent tous au sein de la même théorie. Et cependant, ces modèles sont les seules représentations formelles disponibles des lois phénoménologiques que nous tenons pour vraies. Elle affirme que seules ces lois phénoménologiques nous permettent d'avancer. Toutes les modélisations de ces lois ne peuvent être vraies ensemble puisqu'elles ne sont pas compatibles. Et rien ne permet de penser qu'un modèle est supérieur à un autre. Aucun n'est vraiment justifié par la théorie qui le porte. Plus encore, les modèles ont tendance à résister aux changements de théorie, c'est-à-dire que le modèle est conservé même si la théorie s'avère inadéquate. Il y a plus de vérité locale dans les modèles incompatibles que dans les théories, pourtant plus sophistiquées.[...] L'idéal de la science n'est pas l'unité mais dans une abondance et diversité de plus en plus grandes.} \autocite[350]{Hacking1989}}.

\Anotecontent{winsberg_exper_simu_link}{\foreigntextquote{english}[\cite{Winsberg2009}]{Another unique feature of the epistemology of simulation is the ease with which it can draw inspiration from the epistemology of experiment.}}


\Anotecontent{sanders_couplage_spirale}{\textcite{Sanders2013} propose d'observer dans la discipline l'apparition de modèle opérant la synthèse de deux vagues d'innovations successives, Equation différentielle et ABM : \enquote{Sur cette dernière décennie cependant, on enregistre un nombre croissant de travaux combinant, comparant, couplant des modèles issus des deux vagues de modélisation discutées dans cette contribution.} Sanders apelle l'image d'une \enquote{avancée en spirale} pour imager \enquote{Les allers-retours que l’on peut constater entre les différentes familles de modèles, les essais de couplages, amènent à l’image de la spirale pour rendre compte de l’évolution de ce champ de recherche.} Les modèles intégrant ces différents formalismes en fonction de leur capacité à représenter au mieux les objets, ou les processus vis à vis d'un questionnement donné, la différence de point de vue entre les différents acteurs de ce champ de recherche menant à une spirale de progression qui n'est pas sans rapeller l'importance de la discussion, l'acceptation et la réutilisation des modèles au sein de la communauté comme d'un processus participant à la Validation de ceux-ci, tel que supporté par \textcite{Rouchier2013} ou \textcite{OSullivan2004}.}

\Anotecontent{varenne_quine}{\textquote[\cite{Varenne2014}]{La thèse de la sous-détermination énonce en substance que les données expérimentales ne sont jamais susceptibles de servir à la détermination univoque d’une théorie rendant compte de ces données, qu’elles ne peuvent donc jamais servir univoquement ni à la fonder, ni à la vérifier, ni à la corroborer. Cette thèse est même plus forte. Elle affirme que, pour une théorie possible, il en existe toujours une infinité d’autres qui pourrait venir formaliser, déduire et/ou expliquer les données expérimentales disponibles. Il faut remarquer que la sous-détermination dont se persuade Quine est fondée sur des raisonnements à base d’analyse conceptuelle. Cependant, une telle analyse conceptuelle se fonde dans le texte de Quine sur une hypothèse explicite, hypothèse explicite certes d’apparence recevable mais qui cache une hypothèse implicite bien plus problématique car de nature métaphysique.}}


\Anotecontent{winsberg_mereformal}{\foreigntextquote{english}[{\cite[18-19]{Winsberg2013}}]{Still, despite rejecting Gilbert and Troitzsch's characterization of the difference between simulation and experiment, Guala and Morgan both reject the identity thesis. Drawing on the work of Simon (1969), Guala argues that simulations differ fundamentally from experiments in that the object of manipulation in an experiment bears a material similarity to the target of interest, but in a simulation, the similarity between object and target is merely formal.[...] the idea that the existence of a formal similarity between two material entities could mark anything interesting is conceptually confused. Given any two sufficiently complex entities, there are many ways in which they are formally identical, not to mention similar. There are also ways in which they are formally completely different. Now, we can speak loosely, and say that two things bear a formal similarity, but what we really mean is that our best formal representations of the two entities have formal similarities. In any case, Winsberg and Parker both reject both the Gilbert and Troitzsch and the Morgan and Guala grounds for distinguishing experiments and simulations.}}

\Anotecontent{simuland}{Le \textit{Simuland} étant pour Doran le système ou la situation d'intérêt sur lequel le modélisateur s'appuie pour dériver des modèles mathématiques ou des simulations algorithmiques, Doran étant depuis le départ \autocite{Doran1970} un grand promoteur de cette dernière option (voir le premier chapitre \ref{sec:apparition_simu_science_sociales}, et les annexes \ref{sec:annexe}).}

\Anotecontent{Winsberg_critique_morvan}{\foreigntextquote{english}[{\cite[18]{Winsberg2013}}]{Interestingly, while Morgan accepts this argument against the identity thesis, she seems to hold to a version of the epistemological dependency thesis. She argues, in other words, that the difference between experiments and simulations identified by Guala implies that simulations are epistemologically inferior to real experiments - that they have intrinsically less power to warrant belief in hypotheses about the real world.} }

\Anotecontent{guala_morgan_reality_experiments}{\foreigntextquote{english}[{\cite[841]{Winsberg2009}}]{The identity thesis itself has drawn criticism from Guala (2002) and Morgan(2002). Guala begins by dismissing what he takes to be a poor argument against it. The poor argument goes something like this : simulations are not at all like real experiments because real experiments manipulate the real-world systems that are the very target of the investigation, while simulation merely manipulate \enquote{models} of the target system. What both Guala and Morgan correctly point out is that it is, quite generally speaking, false.}}

\Anotecontent{gilbert_critique}{\foreignquote{english}{\enquote{[t]he major difference is that while in an experiment, one is controlling the actual object of interest (for example, in a chemistry experiment, the chemicals under investigation), in a simulation one is experimenting with a model rather than the phenomenon itself.} \autocite[14]{Gilbert2005}. But this doesn't seem right. [...] It is false that real experiments always manipulate exactly their targets of interest. In fact, in both real experiments and simulations, there is a complex relationship between what is manipulated in the investigation on the one hand, and the real-world systems that are the targets of the investigation on the other. In cases of both experiment and simulation, therefore, it takes an argument of some substance to establish the ‘external validity’ of the investigation – to establish that what is learned about the system being manipulated is applicable to the system of interest. Mendel, for example, manipulated pea plants, but he was interested in learning about the phenomenon of heritability generally \autocite{Winsberg2013}}}


\Anotecontent{guala_phan_winsberg}{Winsberg résume le point de vue de Guala(2002) ainsi \foreignquote{english}{Guala argues that simulation differ fundamentally from experiments in that the object of manipulation in an experiment bears a material similarity to the target of interest, but in a simulation, the similarity between object and target are merely formal.}, mais on peut trouver une version réactualisé en 2008 dans l'article de \textcite[4.2]{Phan2010} \foreignquote{english}{In a simulation, one reproduces the behavior of a certain entity or system by means of a mechanism and/or material that is radically different in kind from that of a simulated entity (...) In this sense, \enquote{models simulate} whereas \enquote{ experimental systems} do not. Theoretical models are conceptual entities, whereas experiments are made of the same \enquote{stuff} as the target entity they are exploring and aiming at understanding}\autocite[14]{Guala2008}}

\Anotecontent{maki_phan}{\foreignquote{english}{For Mäki, abstractions in models are similar to abstractions in experiments as they both can be interpreted as a kind of isolation [...] This analogy between models and experiments is called \enquote{isolative analogy} by Guala (2008). From Mäki’s standpoint, a model can be said to be experimented in its explanatory dimension: the finality of such a model is to explore the explanatory power of some causal mechanism taken in isolation.} \autocite{Phan2008}}

\Anotecontent{moto_hacking}{Une remarque qui renvoie d'ailleurs explicitement à sa lecture du moto d'Hacking \foreignquote{english}{experiments have a life of their own} et à la notion d'autonomie (\textit{autonomous}) de son manifeste (\textit{downward, autonomous, motley}). Cette dernière s'appuie sur le fait que dans certains cas (impossibilité d'observation, manque de données), la simulation doit faire la preuve des connaissances (\textit{background knowledge}) apportées sur appel de ses propres ressources.}

\Anotecontent{experimental_warranting_belief}{\foreignquote{english}{The central idea of this thread is that experiments are the canonical entities that play a central role in warranting our belief in scientific hypotheses, and that therefore the degree to which we ought to think that simulations can also play a role in warranting such beliefs depends on the extent to which they can be identified as a kind of experiment} \autocite{Winsberg2009}}

\Anotecontent{varenne_autonome}{Il y a probablement un point intéressant à développer entre cet argument du modèle autonome, et les récents travaux en sciences sociales pour qualifier au travers d'une grille de lecture \autocites{Banos2013a, Sanders2013} le positionnement \autocites{Banos2013, Schmitt2013} et le déplacement des modèles de simulation au travers d'une part de leur construction \autocite{Cottineau2014b}, mais également de leur réutilisation. C'est aussi là d'ailleurs que le travail de Varenne réalisé au cours des années 2000 \autocites{Varenne2008, Varenne2013b} apparait assez audacieux, en proposant une typologie de fonctions épistémiques flexible et cumulable, il propose une grille de lecture permettant d'intégrer à la fois la diversité des approches dans les disciplines (inter) mais également l'évolution de ces mêmes approches à l'intérieur d'une discipline (intra). Un découplage qui permet une définition plus fine des rapports que peuvent entretenir chacune des disciplines entre le modèle et la simulation. Il pourrait être intéressant de faire appel à cette grille pour analyser la trajectoire disciplino-temporelle de certains modèles : daisyWorld \autocite{Dutreuil2013}, Schelling \autocite {Bulle2005}, SugarScape, etc.).}

\Anotecontent{def_hacking}{\enquote{Le \textit{réaliste à propos des entités} affirme que bon nombre d'entités théoriques existent vraiment. L'anti-réaliste s'oppose à ces entités qui ne sont pour lui que fictions, constructions logiques ou éléments d'un processus intellectuel d'appréhension du monde. Un anti-réaliste moins dogmatique dirait que nous n'avons pas, et ne pouvons avoir, de raison de supposer que ces entités ne sont pas des fictions. Peut-être existent-elles, mais le présupposer n'est pas nécessaire à notre compréhension du monde.

Le \textit{réaliste à propos des théories} dit que les théories
sont soit vraies, soit fausses et ce indépendamment de ce que nous percevons : la science, elle au moins, vise à obtenir la vérité et la vérité est le monde tel qu'il est. L'anti-réaliste dit des théories qu'elles sont au mieux prouvées, adéquates, opératoires, acceptables - quoi que incroyables, entre autres qualificatifs possibles.} \autocite[59]{Hacking1989}}


\Anotecontent{note_barlas}{\foreigntextquote{english}[{\cite[164]{Barlas1990}}]{As a philosophical term, validation refers to a purely logical problem, dealing with the internal consistency of a set of propositions with respect to a set of logic rules. The philosophical problem indicated by verification, on the other hand, deals with justification of knowledge claims and corresponds to validation as used in modeling literature. Verification in modeling literature deals with the internal consistency of a computer program. One must be careful in interpreting these two terms. as they switch meanings from one literature to the other. We adopt the use of validation common in modeling literature. Readers with a philosophical background should read this to mean verification.} Le terme Vérification renvoie à une représentation de la vérité, du \enquote{réel}, ce qui n'est pas vraiment l'objectif de la modélisation...}

\Anotecontent{Phan_Varenne_theorie}{\foreigntextquote{english}[\cite{Phan2010}]{Consequently, in the first neo-positivist epistemology, models were viewed not as autonomous objects, but as theoretically driven derivative instruments. Following the modelistic turn in mathematical logic, the semantic epistemological conception of scientific models persisted to emphasize on theory.} }

\Anotecontent{originalite_epistemologie}{Il propose de résumer l'originalité d'une telle épistémologie en évoquant l'inférence spécifique permise par l'étude simultanée de trois points sur la simulation. \foreignblockquote{english}[\cite{Winsberg2013}]{ \textcite{Winsberg2001} argued that, unlike the epistemological issues that take center stage in traditional confirmation theory, an adequate EOCS [Epistemology of Computer Simulation] must meet three conditions. In particular it must take account of the fact that the knowledge produced by computer simulations is the result of inferences that are downward, motley, and autonomous.[...] These three features were meant to be offered as conditions of adequacy; for which any adequate epistemology of simulation must account. Against the background of the growing use of simulation in the sciences, an adequate epistemology for the philosophy of science needs to explain the fact that simulation results and computational models are often taken to be reliable despite these three features. Winsberg (2001) argues that simulation requires a new epistemology precisely because traditional stories in philosophy of science about how knowledge claims get credentialed cannot explain them.}}

\Anotecontent{vogelback_marble}{ Extrait d'un entretien avec Duane F. Marble daté de juillet 2015 : \foreignquote{english}{You ask about Vogelback at Northwestern in the 1960s. This was a beautiful new building on the northern part of the Evanston campus. It housed the physically massive CDC 6400 and its support staff. The staff was extensive and broken into two groups: machine operators (it ran 24/7) and programmers. The programmers worked on the machines operating system but largely on the 12 small peripheral computers that handled the printers, tape drives, etc. They also taught Fortran programming classes and worked on academic projects. This facility was open to all on campus at no charge as long as they were doing academic work. It was run by Dr. Benjamin Mittman, a great computer scientist and a really nice and helpful guy. He was a chess freak and was the first to get computers playing chess against each other. He had a faculty advisory committee and I served on this for some time. Vogelback maintained an extensive (for its day) library of useful programs that we used for statistical analysis, solution of matrix based analysis problems, linear programming, etc. To these, most of the users had their own specialized program library such as the on I set up for the Department of Geography. The departmental library was communal in nature in that anyone who created a useful program general enough to of use to others added his or her to the departmental \enquote{pot.}

I used Vogelback extensively both in the early years when you had to take boxes of punch cards back and forth and later when we had simple terminals set up in my lab. I used the facility for both research and teaching. I taught some of the first geography classes (upper division and graduate) to make use of the computer as a learning tool. Much of this did not involve student creation of programs but rather their use of special, existing programs such as the LANDUSE \autocite{Marble1972b} one that developed out of some of my early research in computational geography. }}

\Anotecontent{critique_positionnement}{Cette typologie et cette appel à une différence de traitement entre modèles et simulations a soulevé un certain nombre de critiques chez les philosophes des sciences, dont la plus longue et la plus argumentée est sûrement celle de \textcite{Frigg2009}. On trouve le résumé des points saillants de cette critique dans les publications de \textcites{Winsberg2009, Winsberg2013} mais également dans celles de \textcites{Yanoff2010, Eckhart2010} qui se réfèrent régulièrement à ce débat pour eux-mêmes se positionner.}

\Anotecontent{frilosite_philoScience}{\foreignquote{english}{As computer simulation methods have made their way into novel disciplines, the issue of their trustworthiness for generating new knowledge has often loomed large, especially when they have competed for attention with experiments or analytically tractable modeling methods. The relevant question is always whether or not the results of a particular computer simulation are accurate enough for their intended purpose.[...] Given our long-standing preoccupation with issues of confirmation, it might seem obvious that philosophers of science would have the resources to easily approach these questions.} \autocite{Winsberg2013}}

\Anotecontent{VV_philout}{ \foreignquote{english}{During the last two decades a workable and constructive approach to the concepts, terminology, and methodology of V\&V has been developped, but it was based on pratical realities in business and government, \textbf{not} the issue of obsolute thruth in the philosophy of nature} \autocite{Oberkampf2010}
\foreignquote{english}{A very old philosophical question is: do humans have accurate knowledge of reality or do they have only flickering images of reality, as Plato stated? In this paper, however, we take the view that managers act as if their knowledge of reality were sufficient. Also see Barlas and Carpenter (1990), Landry and Oral (1993), and Naylor, Balintfy, Burdick and Chu (1966, pp.310-320).} \autocite{Kleijnen1995}
\foreignquote{english}{With the strong interest in verification from the software engineering community, this contrasting but complementary explanation of the term was quite important. The effort to place valida- tion in a cost-risk framework moved the concept from a philosophical explanation in earlier works to a form more useable for simulation practitioners.} \autocite[165-166]{Nance2002}}

\Anotecontent{Sargent_def}{\foreignquote{english}{\textbf{Model verification} is often defined as \enquote{ensuring that the computer program of the computerized model and its implementation are correct} and is the definition adopted here. \textbf{Model validation} is usually defined to mean \enquote{substantiation that a computerized model within its domain of applicability possesses a satisfactory range of accuracy consistent with the intended application of the model} \autocite{Schlesinger1979} and is the definition used here. A model sometimes becomes accredited through model accreditation. Model accreditation determines if a model satisfies specified model accreditation criteria according to a specified process. A related topic is model credibility. Model credibility is concerned with developing in (potential) users the confidence they require in order to use a model and in the information derived from that model. A model should be developed for a specific purpose (or application) and its validity determined with respect to that purpose [...]A model is considered valid for a set of experimental conditions if the model’s accuracy is within its acceptable range, which is the amount of accuracy required for the model’s intended purpose.}}

\Anotecontent{naylor_nance}{\foreignquote{english}{Thomas Naylor, a coauthor of the book cited above, deserves credit for drawing major attention to the validation issue in the 1960s: Is the model actually representing the truthful behavior of the referent system? His work, above and in later publications (Naylor 1971, Naylor and Finger 1967), exerted a major influence in framing validation within different philosophical perspectives. Numerous techniques that can be used were identified or developed. While the issues of both verification and validation were of concern from the early days of simulation, often no clear distinction was made between the two terms.} \autocite[165]{Nance2002}}

\Anotecontent{first_time_validation}{La citation de Churchman par \textcite{Naylor1966} est tirée de \autocite[165]{Nance2002} : \foreignquote{english}{\foreignquote{english}{X simulates Y} is true if, and only if, (a) X and Y are formal systems, (b) Y is taken to be the real system, (c) X is taken to be an approximation to the real system and (d) the rules of validity in X are non-error-free.} \autocite{Nance2002} }

\Anotecontent{balci_standard}{\foreignquote{english}{A uniform, standard terminology is yet nonexistent. A recent literature review \autocite{Balci1984} indicated the usage of 16 terms [...] Except some early papers which appearead between 1966 and 1972, model verification and model validation have been most of the time consistently defined reflecting the following differentiation : \textbf{model verification} refers to building the model right; and \textbf{model validation} refers to building the right model. \autocite{Balci1986}}}

\Anotecontent{batty_forrester}{\foreignquote{english}{An interesting, fruitful and amusing piece of work has been undertaken by \autocite{Stonebraker1972} who has simplified the model drastically by reducing the total number of model equations by two-thirds. The results of running the model in this fashion are much the same as Forrester's and this has led Smith and Sage (1973) to propose the use of hierarchical system theory as a tool for simplifying the model.}\autocite[308]{Batty1976}}

\Anotecontent{evolution_openmole}{Comme l'indiquent ses créateurs Romain Reuillon et Mathieu Leclaire, le logiciel OpenMOLE est clairement piloté par les besoins de la communauté des modélisateurs des systèmes complexes : \foreigntextquote{english}[\cite{Reuillon2013}]{The design of OpenMOLE has been driven by the practices of the complex system modellers. For instance, the quickly evolving implementations of complex-system models, in heterogeneous languages, and the need to experiment all along the modelling process made it crucial to easily embed continuously changing user software components in the platform.}}

\Anotecontent{batty_temps}{Exceptions dont on trouve une liste comparative réalisée par \textcite{Batty1972} incluant entre autre la gestion de l'espace, du temps, et la nature des dynamiques : Le modèle TOMM (Time Oriented Metropolitan Model) de Crecine, le modèle EMPIRIC, les travaux de Paelinck, ou encore ceux de Wilson qui adapte les travaux existants en démographie, ou hérités de la dynamique des populations en écologie/biologie}

\Anotecontent{end_mathematical_area}{\foreignquote{english}{The problem is that this emphasis on mathematical modelling, as a largely data-free activity, has established a style of modelling which has continued despite massive changes in both computer hardware and data environments. Few of the texts on models or modelling methods in geography mention the word \enquote{data} or \enquote{computers}; see for example, Wilson (1974), Bennett (1979), and Thomas and Huggett (1980). Nor do they describe any of the models in sufficient algorithmic detail to allow for easy programming; the exception is perhaps Batty (1976). Studies containing details of empirical evaluations and applications are still the exception rather than the rule. [...] There is no disputing the great wealth of rich and varied models that are \enquote{on the shelf in the shop window}, awaiting application, or that a comprehensive tool kit of useful mathematical methods now exists. However, it is reasonable to question whether the real need now is for developing yet more, and ever more refined theoretical models, or whether more needs to be clone with the models and methods that already exist. It is certainly a great disadvantage to have so much of the intellectual capital of quantitative human geography encapsulated in theoretical models that seemingly serve no useful purpose outside of geography itself.[...]
The great age of mathematical geography put emphasis on the acquisition of formai mathematical skills as a prerequisite for understanding quantitative geography and as the basis for geographical theory. This perceived need still continues unabated.[...] There is no dispute about either the quality of the work or of the utility of mathematical methods; rather, criticism is focused on the lack of applied relevancy, the lack of attention to empirical study, and the absence of explicit geography. Can human geographers really expect to study the world in a data-free environment? If yes, then it implies a degree of sophistication that is inappropriate and where are the theories? If no, then it begs the question, where are the data-related studies? The criticism is made then that the deductive route to theory formulation has been taken to far and bas not been particularly successful. Far to many so-called \enquote{theories} have neyer been tested and many more are untestable! Yet prediction and empirical testing are not merely by products of theory, nor are they the logical next step that can for ever be postponed; they are an integral part of science. The ability to make correct predictions is the only relevant scientific test of the depth and quality of understanding afforded by a mathematical model. Currently there seems to be a gross lack of balance.} \autocite{Openshaw1989}}

\Anotecontent{pumain_gain}{ Pumain liste au moins trois intérêts qui découlent de cette phase d'acquisition du projet systémique : a) le dépassement de l'opposition idiographique et nomothétique, b) l'histoire et les particularités des entités géographiques vues comme expression originale de trajectoires et de bifurcations, c) le dépassement de la rigidité des trajectoires biographiques historiques par l'emploi des simulations}

\Anotecontent{marble_validation}{Ce témoignage est issu d'un échange privé par email en 2013 avec Duane Marble, un des pionniers modélisateurs américains, qui lorsqu'il est interrogé sur la suite des problématiques de validation des modèles de simulation détaillées dans son article de 1972 \autocite{Marble1972}, conforte notre point de vue sur les limitations encore en cours dans les années 1980 : \foreignquote{english}{As I recall, the situation in the 1980's had not changed very much. Simulation in human geography did not last long. Much of this was the result of a lack of computer capacity. Simply replicating Hagerstrand's diffusion model proved difficult and our attempt to inject a more explicit temporal element just would not work due to the computational load.}}

\Anotecontent{marble_decline}{Une citation importante de Duane F. Marble que l'on aurait également pu intégrer dans le chapitre 1, pour démontrer encore une fois l'importance des problématiques méthodologiques et techniques dans la construction des modèles et dans leur vérifications/validations : \foreigntextquote{english}[{\cite[386-387]{Marble1972}}]{Simulation modeling in human geography enjoyed a fair amount of popularity in the early sixties with the publication in English of some of Hägerstrand's early work. These early applications identified many critical research topics in an explicit fashion, but as a rule failed to produce results of the power and level of generality originally envisionned by their developpers. Problems of size, computer capability, and severe difficulties in verying the results of modelling efforts have led during the last half of the decade to a sharp decline of Monte Carlo simulation in geographic research. In the last several years, the only large geographic simulation to be constructed was the SIMCOM model outlined earlier, and Ramachandran's (1967) work represents the only large-scale empirical application of the Hägerstrand models.}}

\Anotecontent{fetichisme_spatial}{\foreigntextquote{english}[{\cite[712]{Gregory2009}}]{Any approach that treats space as sufficiently autonomous to social processes that ‘no change in the social process or spatial relations could alter the fundamental structure of space’}}

\Anotecontent{limitation_batty}{\foreignquote{english}{Computation has also been a perennial problem in urban modelling. In spatial interaction modelling especially, the storage required increases exponentially with the number of zones into which the system is divided. In most computers, models with greater than 200 zones are impracticable unless external storage on tapes is used, but such external storage increases the running time sometimes prohibitively.} \autocite[355]{Batty1976}}

\Anotecontent{programmes}{Particulièrement difficiles à trouver en dehors des Etats-Unis, les rapports disponibles dans les \href{http://findingaids.library.northwestern.edu/catalog/inu-ead-nua-archon-989}{@archives} de la \textit{Northwestern University Library} sont susceptibles de contenir les précieux programmes et les rapports d'avancements de ces ingénieurs géographes :
\begin{enumerate}[labelindent=\parindent,leftmargin=*]
\item \textit{Duane F. Marble and Sophia R. Bowlby, Computer Programs for the Operational Analysis of Hagerstrand Type Spatial Diffusion Models, Research Report No. 27, February, 1968}. \item \textit{Duane F. Marble, Some Computer Programs for Geographic Research, Special Publication No. 1, August, 1967 }. \item \textit{ Forrest R. Pitts, Hager III and Hager IV: Two Monte Carlo Computer Programs for the Study of Spatial Diffusion Problems, Research Report No. 2, October, 1965}. \end{enumerate}}

\Anotecontent{batty_dynamic}{\foreignquote{english}{Perhaps the most important concept involved in any dynamic approach revolves around the idea that urban structure is an inevitable reflection of several complex processes of change in the urban system (Batty, 1971). To understand structure, it is therefore essential to explore the processes, past and present, which have generated that structure.} \autocite[292]{Batty1976}}

\Anotecontent{problem_generation_model}{\foreignquote{english}{It is easy to blame excessive ambition, lack of time and money, and changing priorities for the failures in first generation urban modelling, but the quality and limitations of the models also had a great deal to do with the situation. At the beginning of the decade, urban modelling seemed to present a means for cutting through and tackling the complexity of the modern metropolis. [...] But the modellers failed to recognise the limitations of their models in helping to sort out ill-defined planning problems. Such models based on fairly well-defined formal structures had to rely upon fairly sparse theory, thus often appearing arbitrary and somewhat mechanistic in structure. The ill-defined nature of reality and the lack of behavioural content combined to limit the use of models to a much greater extent than was realised at the time. [...] A more fundamental problem of urban modelling was realised at this stage. The problem of observing the mechanisms of cause and effect in urban systems is compounded by the fact that such observation is clouded by a host of different factors. It is unusually difficult to test specific theories in the social sciences for it is impossible to hold all variables but one constant and trace the effects on the system. This is a problem which is basic to theory in the social sciences and there is little doubt that it is the major limitation on urban modelling.} \autocite[11]{Batty1976}}

\Anotecontent{observational_validate}{La Validation devient de fait beaucoup plus difficile au regard de l'\textit{observational dilemna}, comme le constate \textcite[313]{Batty1976} dans la construction de son propre modèle : \foreignquote{english}{In this chapter, the observational dilemma concerning the impossibility of monitoring change at its most elemental level, is faced directly. Even the most simple and crude of hypotheses to be built into the model are difficult to validate against data, thus demonstrating one of the major barriers to ambitious urban modelling.}}

\Anotecontent{observational_dilemna}{\foreignquote{english}{Yet there are severe problems in trying to develop dynamic theory, two of which are worthy of some discussion. Perhaps the major problem concerns the ability to observe or monitor the urban system. Unlike the physical sciences in which the effect of critical variables on the system of interest can be isolated in the laboratory, such a search for cause and effect is practically impossible in social systems. Thus, there are many instances when it is difficult, if not impossible, to disentangle one cause from another in the changing behaviour of such systems. This is a fundamental limitation which is referred to here as the \textbf{observational dilemma}. A second problem concerns that hoary perennial - data. [...] } \autocite[296]{Batty1976}}

\Anotecontent{marble_computer_historycdc}{ \textcite[3]{Marble1967} déclare dans son recueil de programme de 1967 avoir écrit des routines pour le CDC 3400, que l'on suppose rapidement traduit en CDC 6400. Une procédure qui semble courante, comme en témoigne \textcite{Goldberg1968} pour le package \textit{SPURT}, dédié à la simulation, créé et utilisé (apparemment même par des géographes) au \textit{Vogelback Computing Center} alors sous la direction de Mittman. En 2010, Marble écrit \foreignquote{english}{Northwestern, when I arrived, was just opening its new Vogelback Computing Center and had acquired high end computing technology in the form of a Control Data Corporation (CDC) 6400. Aside from the “super”computer, the most significant component of Northwestern's computing infrastructure, in my eyes, was clearly Vogelback's Director of Computing, Dr. Benjamin Mittman. Ben was the originator of computer chess as a competitive programming activity, and he put together a generally excellent support staff at Vogelback. He was immensely helpful on a personal level to those of us who were working on the CDC mainframe. Ben also made sure that a number of useful software packages (e.g., the BMD statistical analysis package, linear programming software for solution of the transportation problem, etc.) were made freely available to all Vogelback users.} \autocite{Marble2010} Anecdote amusante sur le personnage cité par Marble, Benjamin Mittman est aussi un acteur important dans le développement et la structuration de la communauté créant des programmes d'échecs sur ordinateur, et accueille au \textit{Vogelback Computing Center} les étudiants David J. Slate, Larry R. Atkin, et Keith Gorlen ayant donné naissance au programme pionnier \textit{CHESS} \autocite{Mittman1971} s'executant sur le tout récent \href{http://computerchess.tumblr.com/post/56345790213/playing-chess-at-vogelback-computing-center}{@CDC6400}, vainqueur plusieurs années d'affilée dans les premières compétitions d'échecs organisés à l'époque par l'\href{https://chessprogramming.wikispaces.com/ACM+North+American+Computer+Chess+Championship}{@ACM}}

\Anotecontent{exemple_slocal}{On trouve un exemple de ce type de mécanismes dans SimpopLocal, où la quantité maximum d'objets \enquote{innovations} dans le modèle de simulation a du être limité, pour éviter tout débordement de mémoire sur certaines plages de valeurs de paramètres \autocites{Schmitt2014,Schmitt2015}.}

\Anotecontent{programme_trouver}{Ces logiciels ou codes sources sont difficiles à trouver, et les publications qui les contiennent ne sont la plupart du temps disponibles que sous la forme d'archives numérisées non exploitables (\textit{Google Books}), ou sous format papier dans les universités correspondantes. Un travail reste à faire pour sauvegarder et mettre ce bien commun à disposition de tous les géographes.}

\Anotecontent{ordinateur_actuel}{En comparaison, les ordinateurs actuels contiennent au minimum 4Go de mémoire, soit 4 194 300 KB.}

\Anotecontent{numac}{Un témoignage recoupé par les administrateurs du centre \href{http://archive.michigan-terminal-system.org/discussions/how-did-sites-learn-about-and-make-the-decision-to-use-mts/3numac}{@NUMAC} (\textit{Northumbrian Universities Multiple Access Computer, UK}). La primo installation dans une université sur le territoire anglais, et probablement européen (si on s'en tient aux témoignages...) d'un IBM 360/67 au lieu des \textit{mainframes} jusque là anglais étant lié à la volonté des administrateurs d'utiliser et de se former sur un des premiers système de temps partagé américain MTS (Michigan Terminal System) alors compatible avec la série d'IBM 360/67.}

\Anotecontent{atlas}{On trouve une copie du \textit{Flower Reports} ainsi qu'un listing par années des équipements présents dans les universités de Grande Bretagne sur le site internet de \href{http://www.chilton-computing.org.uk/}{@Chilton} qui héberge depuis de nombreuses années des facilités de calcul pour les universitaires \href{http://www.chilton-computing.org.uk/acl/society/computing/1965.htm}{@ATLAS} }


\Anotecontent{ibm604650}{Voir \href{http://www.aag.org/cs/garrison}{@Garrison} et la page \href{https://www.washington.edu/uwit/history/}{@historique} du service IT (information technology) de l'université de Washington}

\Anotecontent{dyke_validation}{\foreignquote{english}{The initial enthusiasm for a newly acquired ability to model complex systems, characteristic of the early days of anthropological simulation, more often than not led to an exaggeration of the capabilities and usefulness of computer models.In retrospect it seems clear that much of this excess could have been avoided had more attention been paid to testing (particularly to validation). The literature of the past 4 or 5 years, however, gives ample evidence that the situation has changed. Those who continue to use simulation seem to have paid much more attention to the problem of validation and tend to be more modest in their claims of utility.}}

\Anotecontent{dyke_bilan}{ \foreignquote{english}{Since that time there has been a considerable increase in the number of publications whose results have depended on simulation studies. Despite this increase, it is probably fair to say that simulation has received at best only a cautious acceptance in anthropology.} \autocite{Dyke1981} }

\Anotecontent{temoignage_archeo_alden}{Pour ne prendre qu'un exemple des témoignages relevés chez les pionniers, celui d'Aldenderfer en 1988 expliquant que \foreigntextquote{english}[\cite{Aldenderfer1998}]{During the 1980s, relatively few archaeologists continued to advocate whole-society modeling [...] While much of Doran's work has been widely cited within the relatively small community of mathematically inclined archaeologists, his work has had relatively little influence beyond this small circle}}

\Anotecontent{tobler_michigan}{Le \textit{Logic of Computers Research Group} est créé en 1949 à l'université du Michigan par le philosophe spécialiste de Charles Sanders Peirce et également mathématicien / informaticien Arthur W. Burks \autocite{Goldstein1991}. Avant d'être autonome en 1957 avec son propre cursus/département inter-disciplinaire (\textit{Computer and Communication Sciences}), celui-ci démarre en étant associé au département de philosophie \autocite{Burks1981}. Par curiosité, et sachant la présence de scientifiques dans ce groupe (Edgar Frank \enquote{Ted} Codd, Arthur W. Burks, John Holland, Tommaso Toffoli) en filiation directe avec les travaux de Von Neumann sur les automates cellulaires, j'ai posé la question à Waldo R.Tobler pour son inspiration sur le papier introduisant les CA aux géographes en 1979 \autocite{Tobler1979} : \foreignquote{english}{When I wrote the Cellular Geography paper I was a visitor (on sabbatical) from the university of Michigan at IIASA outside of Vienna. I presented the paper \enquote{Schachbrett Modelle in der Geographie} at the University of Vienna, and later had the English version to which you refer, published. I was aware of Codd's work and Ulam's work from my reading, without format affiliation or contact with Burks. A paper by Martin Gardner in Scientific American introduced many people to cellular automata. Many of us in Geography at that time used matrix algebra so were familiar with grid structures. I attach one of my old papers : \autocite{Tobler1967b}.} Il m'indique également qu'il n'a pas travaillé avec Pitts et Marble, bien qu'ils les connaissent : \foreignquote{english}{I have known Duane Marble since graduate school in 1966 or 1967, but have never worked with him. I also knew F. Pitts from about that time but did not work with him. }(Extrait d'un entretien privé par email avec Waldo R. Tobler daté d'avril 2015)}

\Anotecontent{hagerstrand_lund}{ Sten Henriksson \autocite[32-33]{Lindgren2008} relate à propos d'Hägerstrand : \foreignquote{english}{First Torsten Hägerstrand , he was active then in the mid - 50s , he was , shall we say, one of the world's leading human geographers and devoted himself to simulate stuff on SMIL , he was a childhood friend of Carl-Erik Froberg and was one of the first to use SMIL -56 and there are others such as these early adopters who have been proactive.}, suivi du témoignage de Axel Ruhe plus précis sur ses premier travaux : \foreignquote{english}{I will mention two of them, I do not know how much research it has led to , and was the geographic data processing Torsten Hägerstrand 59 who was a professor of human geography , I remember we ran a program on SMIL for possible locations of the Öresund bridge , how much shipping would be developed if we had it here or there. And then it was the location of the cinemas, roughly the same as going over the Öresund Bridge but on a smaller scale. It was a study of school children going to school and then also examined if they used the nearest way or another} On trouve également une note de bas de page sur le calcul du modèle dans \autocite{Hagerstrand1965} : \foreignquote{english}{The runs were performed on the for taking care of the machine-program, electronic computer SMIL of Lund and I The random numbers were produced by am indebted to professor Carl-Erik Froberg a built-in routine.}}

\Anotecontent{tobler_650}{Waldo R. Tobler a manipulé l'IBM 709 \autocite{Tobler2004}, mais également l'IBM 650 \autocite[107,303]{Gould2002} avec les autres pionniers de l'université de Washington : \foreignquote{english}{Sometime in the late 1950's Duane Marble, I and others learned to program in the SOAP (Symbolic Optimal Assembly Program) on an IBM 650 in the attic of the chemistry building at the University of Washington (Seattle). Several years later came Fortran. I do not think that Professor Garrison (who died earlier this year) knew how to program but he knew about computers while in the US armed services during WWII.} (Extrait d'un échange privé par email avec Waldo R. Tobler daté de mai 2015)}

\Anotecontent{marble_hagerstrand}{A propos des implémentations, et des échanges entre cette équipe de géographes développeurs et Hägerstrand, voici plusieurs passages tirés d'échanges privés par email avec Duane Marble entre aout 2014 et avril 2015 : \foreignquote{english}{The development of the Hagerstrand model in Fortran was part of a joint research activity by Dr. Pitts and myself. At that time, you must understand, that there was no viable substitute for Fortran. This was an independent work although we both were in correspondence with Hagerstrand from time to time. The creation of the program was part of a larger project that involved field work in South Korea. I had left the University of Washington before Hagerstrand came to spend a term as a visiting professor, but I did meet him briefly while he was in the United States. My main contact with him occurred during the year I spent teaching in Sweden. This was not at Hagerstrand’s university but I did spend some time lecturing at Lund and I also saw him at various places around Sweden.[...] The programming work on the Hagerstrand model was done by Pitts and myself for the most part with some bits and pieces (e.g., output statistics, etc.) written by some of my graduate students.  Pitts and I were unhappy with some of the decisions had made about the model (necessary for manual computation) and our goal was to start incorporating some changes after we got his basic model working. Pitts was also a regional specialist on Japan and Korea and spoke both languages fluently. He wanted to do some agricultural field work in South Korea to provide new, raw data for studying spatial diffusion. We went to Korea in the summer of 1966 and established that the field work was too difficult to accomplish with the available resources. So that thought was put away. [...] Working on the digital model soon illuminated a large number of problems. Of the major ones, the question of how time was handled was, we felt, the most critical. We attempted to move beyond Hagerstrand's notion of \enquote{generations} but this ran into computer problems. Even with the excellent computer available, a CDC 6400, we could do either time or space but not both. Another dead end. More interesting and also quite important was, we felt, to turn Hagerstrand's simple barriers into a real 3D landscape. [...] This work was done in the mid-1960's but I do not recall the exact dates. There were two aspects to the 3D model. The first, of course, was terrain. The second involved the need for a directional component since the terrain varied all around the source of the message. One of the roadblocks was that there was no digital terrain data available and my lab had no digitizing equipment since none existed! The first digital terrain map was done by USGS in  the 1970's and it's creation took months of effort. [...]Hagerstrand dead ended on this due to the human vs physical geography structure of the discipline with both sides rejecting anything to do with the question. We had the same problem. And we still have it today within the discipline! I have been chasing this one for some time and I feel that perhaps I can at least interest others in this topic if I get around to writing it up. [...] So we ended up with lots of questions and no more resources. There were several internal reports written but nothing was published except an article by Woody on his initial, small implementation before we began working together \autocite{Pitts1963}.}}

\Anotecontent{barnes_ibm}{ \textcite{Barnes2006a} indiquent que le premier ordinateur sur le campus serait daté de 1955, un IBM 604}

\Anotecontent{hagerstrand_mc}{C'est via un voyage aux États-Unis que le physicien Karl Erik Frödberg, un ami d'enfance de Torsten Hägerstrand, récupère un texte polycopié présenté par John Von Neumman et Stanislas Ulam sur les méthodes de Monte-Carlo. Alors appliquées au calcul de l'épaisseur des chapes de béton pour les centrales nucléaires, la technique est utilisée pour pallier à une résolution impossible de ce problème via les approches mathématiques classiques. Hägerstrand ayant déjà travaillé à l'étude de l'émigration en 1949, trouvera dans cette technique un écho innovant à sa problématique d'alors, la propagation des idées et des innovations dans l'agriculture suédoise. \autocite[26-28]{Gould2004}]}

\Anotecontent{dynamique_pionnier}{On trouvera un aperçu plus large de cette histoire de la géographie quantitative américaine, ses rebondissements, son contexte informatique par des témoignages directs, ou des ouvrages plus généraux : \textcite{Haggett1969, Haggett1990}, \textcites{Gould1970, Gould1975, Gould1979, Gould2002, Gould2004}, \textcite{Glick1988}, \textcite{Morril2005}, \textcites{Marble1972, Marble2010}, \textcite{Bailly2000, Bailly2009}, \textcites{Barnes2001, Barnes2004, Barnes2006a}, \textcite{Racine1969}, \textcite{Berry1964b, Berry1993, Berry2001, Berry2005, Berry2005b}, \textcite{Kao1963}, \textcite{Scott2000}, \autocite{Johnston2004,Johnston2006}, \autocite{Kohn1970}, \textcite{Isard2003}, \textcite{Hagerstrand1970}, \autocite{Unwin1992}, \autocite{Griffith2013}, \textcite{Tobler1967}, \autocite{Hudson1979}, etc.}

\Anotecontent{premier_ordinateur}{Considéré comme l'un des premiers à voir l'intérêt général de l'usage de l'ordinateur pour la géographie, le premier cours de Garrison serait daté de 1954 sous l'intitulé (Geog 426: Quantitative Methods in Geography) \autocite{Barnes2004}.}

\Anotecontent{temoignagne_lake}{\foreigntextquote{english}[Lake2013]{However, as already noted, archaeological simulation did not entirely die out during the 1980s, so it is worth considering the exact nature of this resurgence. In fact, I estimate that approximately ten archaeological simulation studies were undertaken in the 1980s and thirteen in the 1990s. Clearly neither is a large number in absolute terms, but nor is the increase anything approximating an order of magnitude.[...]  What we learn from them is that the resurgence of simulation in the 1990s was more a matter of perception that of the actual numbers of models being built.}}

\Anotecontent{isomorphismes}{On retiendra pour ce travail une définition relative au premier exposé de la \enquote{systémologie générale} par Bertalanffy : \enquote{C’est dans un séminaire organisé par Morris, où il est invité à faire une conférence, qu’il expose pour la première fois fin 1937 les grandes lignes de son projet de \enquote{ systémologie générale }. Bertalanffy constate dans toutes les sciences une même promotion d’une conception dynamiste, des concepts de \enquote{ totalité } et d’\enquote{ organisation }, ainsi que la récurrence d’une identité formelle de principes et de lois dans des domaines \textit{a priori} complètement différents (\enquote{ isomorphismes }). Pour expliquer cette évolution, il postule l’existence de principes, de modèles et de lois s’appliquant à n’importe quel type de système, indépendamment de la nature de ses éléments et de ses propriétés particulières. D’où son projet d’une \enquote{ systémologie générale }, dont le but est leur formulation. } \autocites{Pouvreau2006}[670]{Pouvreau2013}. L'histoire du terme et de ses différentes acceptations, ou évolutions est assez complexe, on pourra se référer sur ces points aux travaux très complet de \textcite{Pouvreau2013}.}

\Anotecontent{varenne_modele}{Franck Varenne propose un panorama beaucoup plus large et générique de la notion de modèle dans son ouvrage \textit{Théorie, Réalité, Modèle} paru en 2012. \autocite{Varenne2012}}

\Anotecontent{footnote_kant}{Edgar Kant (1902-1978) un géographe déjà rompu aux méthodes statistiques en Estonie \autocite{Chabot1937} - où il avait déjà pu appliqué ses méthodes - s'est expatrié d'Allemagne avec dans ses bagages les travaux de Christaller, Lösch, etc. Tuteur d'Hägerstrand en Suède, il le forme aux différentes méthodes qui vont se répercuter sur ses travaux de thèse.}

\Anotecontent{RO_collecte}{On en trouve trace également dans des collectes bibliographiques à destination des enseignements comme \autocite{Greer1972}}

\Anotecontent{or_bertalanffy}{Une discipline proche du projet Bertalanffien en bien des aspects, comme le défend \textcite[801]{Pouvreau2013}.}

\Anotecontent{systemique_sousjacente}{A condition de ne pas oublier qu'une partie de ces concepts existent de façon sous-jacente aux disciplines, ce qui explique parfois leur rapidité d'acceptation. C'est le cas de l'approche systémique développée par la cybernétique quand elle ne fait pas qu'apposer un nom commun sur des concepts déjà étudiés, mais fait alors écho à des révolutions méthodologiques en attente d'être activées. \textcite[5]{Batty1976} résume la situation ainsi \foreignquote{english}{The idea of systems being described in terms of structure and behaviour, in terms of input and output, and the notion of purposeful control of such systems in terms of negative and positive feedbacks, appeared to many social scientists an ideal description of their systems of interest and thus the approach has come to be used in more-or-less all of the social sciences}.}

\Anotecontent{cour_orain}{Voir les notes de \href{http://www.esprit-critique.net/article-12642840.html}{@cours}, dispensés sur le blog \enquote{esprit critique} de Olivier Orain}.

\Anotecontent{carnap}{Voir la définition du programme donnée par Carnap dans la note précédente.}

\Anotecontent{ouelbani_positivisme}{Le programme de Carnap tient en quatre points selon Dahms, cités par \textcite{Ouelbani2006} : \begin{enumerate*}[label=(\alph*)] \item la réduction de la philosophie à une théorie de la connaissance; \item la distinction des sciences, non plus en sciences de la nature et sciences humaines, mais en sciences empiriques et analytiques: \item le logicisme comme programme de réduction des sciences analytiques; \item le réductionnisme comme programme de réduction des sciences synthétiques ou empiriques.\end{enumerate*}}

\Anotecontent{plateforme_dynasim} {Le modèle de simulation DYNASIM est implémenté à la fois sur la plateforme de simulation interactive \textit{Microanalytic Simulation of Households} MASH et la plateforme de simulation portable et plus générique \textit{Microanalytic Simulation System} MASS. On trouve plus de détails sur celles-ci dans le volume 1 des conférences \textit{Microeconomic Simulation Models for Public Policy Analysis}, accompagné d'un historique plus complet des modèles réalisés à cette période racontée par \textcite{Orcutt1978}}.

\Anotecontent{retrospective_archeo}{On pourra trouver plus d'informations sur les premiers travaux dans les ouvrages cités ci-dessous, mais également dans des rétrospectives plus récentes comme celle de \autocite{Kohler2011}, ou \autocite{Lake2013}}.

\Anotecontent{retrospective_caa}{Une bibliographie des conférences CAA (1975-1988) avec un index simulation a été réalisée par \textcite{Ryan1988}.}

\Anotecontent{gardin}{\foreignquote{english}{While some archaeologists had used machine-sorted punched cards much earlier, the earliest archaeological applications of electronic data processing I know were by Peter Ibm and by Gardin in France around 1958 or 1959. The earliest use I know of in this country was by James Deetz (Deetz 1965) in 1960. Subsequently there have been about 5 or 10 major projects involving computers and archaeology in North America, about the same number in the rest of the world, and many smaller ones.} \autocite{Cowgill1967}
\\
\\
\foreignquote{english}{The growing concern in archaeology with the new methods made possible by computers is reflected in the increasing size, frequency, and importance of sessions on analytical and statistical methods at professional meetings. Indeed, in each of the three years that have passed since Chenhall's review article in this journal, a special meeting has been held, devoted only to such problems:
\begin{enumerate}[parsep=0pt, partopsep=0pt]
\item{1969-Marseille:} International Symposium on the Application of Computers in Archaeology (Gardin, 1970).
\item{1970-Mamaia:} Anglo-Romanian Conference on Mathematics in the Archaeological and Historical Sciences (Hodson, Kendall, and Tautu, 1971).
\item{1971-Marseille:} Seminar on the Mathematical Methods of Archaeology (In press).
\end{enumerate}} \autocite{Whallon1972}}

\Anotecontent{gremy}{Cette curiosité et cette ouverture en France pour la simulation en sociologie, on la trouve également dans les écrits de \textcite{Gremy1971} : \enquote{Simuler un phénomène social, c'est en quelque sorte construire un modèle réduit de ce phénomène, une maquette sur laquelle il est possible de procéder à divers essais; on parvient ainsi à réaliser des expériences que l'on ne pourrait effectuer dans la réalité. [...] En conclusion, les techniques de simulation rendent possible la solution d'un problème auquel se heurtent les sociologues depuis le début de la sociologie : celui de la formalisation opératoire de théories, permettant d'éprouver la cohérence et la compatibilité de ces théories avec les observations. La simulation est un bon substitut des méthodes de calcul lorsque celles-ci sont inapplicables : formalisations incomplètes, nombreuses interactions entre les variables, intervention d'aléas. Elles s'adaptent particulièrement bien aux théories dont le sociologue a besoin pour rendre compte des faits sociaux, puisque les modèles simulables peuvent être très complexes, et au faible niveau de formalisation qui est celui d'une science à ses débuts, puisqu'ils admettent une formalisation incomplète. La simulation est l'outil dont le sociologue avait besoin pour essayer d'expliquer les faits sociaux. Dans l'incapacité de recourir aux méthodes expérimentales pour éprouver la valeur de théories simples et devant la quasi-impossibilité de disposer de modèles calculables assez complexes pour rendre compte des données d'observation, le socio- logue manquait d'un outil commode lui permettant de passer du plan des observations à celui de la théorisation.} Celui-ci revient en détail sur tous les bénéfices potentiels que peut apporter l'utilisation de la simulation en sociologie, avec la présentation de divers exemples, en sociologie (Abelson, Coleman, Boudon, etc.), mais également dans d'autres disciplines (Hägerstrand, Orcutt, etc.)}


\Anotecontent{gulahorn}{Avec plusieurs tentatives pour la construction d'une machine universelle de résolution de problème (\foreignquote{english}{Logic Theorist program} en 1957 et \foreignquote{english}{General Problem Solver} en 1959). Ce programme s'avère également être la première pierre posée de l'intelligence artificielle, en formation à l'intersection de la naissance encore récente des sciences cognitives et de l'informatique. Cette machine est conçue pour mimer les capacités de résolution de l'esprit humain, et permet enfin d'exprimer et de questionner les théories comportementales dans un langage informatique alors plus précis et moins ambigu que le langage naturel. Le programme est ainsi capable de résoudre des problèmes aussi différents que de jouer aux échecs, de résoudre des problèmes mathématiques, ou de retrouver des motifs dans des données.}

\Anotecontent{homonculus}{Les applications sont menées à des échelles très diverses, ainsi alors que le modèle Homonculus développé par le couple Gullahorn tente de mieux comprendre les stratégies de résolution de conflits avec la programmation de comportements au niveau individuel \autocite{Gullahorn1965}, le projet \textit{Simulmatics} mené par \textcite{Abelson1961} vise quand à lui l'étude du comportement de groupes d'électeurs en cas de conflit d'opinion (ou \foreignquote{english}{cross-pressure}) pour tenter en fonction d'un échantillon de population d'analyser l'impact de stratégies politiques, une demande de J.F.Kennedy pour la campagne de 1960 aux États-Unis}.

\Anotecontent{archeo_stat}{ \textcite{Haggett1989} pointe les ressemblances avec \textit{Models in Geography}, ouvrage en partie résultat des conférences de Madingley (1963) : \foreignquote{english}{What was happening in geography appeared also to be happening in other subjects. The late David Clarke's Analytical Archaeology, published in 1968, paralleled in many ways the theme and structure of the models volume.} D'autres citations, comme celle de \textcite{Doran1970} vont également dans ce sens : \foreignquote{english}{Similar trends are apparent in allied subjects such as anthropology and social geography. In particular, location analysis has influenced archaeologists, with its emphasis on the study of all aspects of a population and its environment, and on the use of quantitative methods and models (Haggett 1963)} }

\Anotecontent{archeo_systemique}{Une analyse a posteriori confirme l'apport de la systémique dans la construction des modèles de simulation, comme en témoigne \textcite[5]{Lake2013} et de façon plus précoce \textcite{Aldenderfer1998} en 1988. \foreignquote{english}{One of the theoretical hallmarks of the \textit{New Archaeology} was the systems approach \autocite{Aldenderfer1991}, and a result of its adoption was the use of computer simulation to model whole societies or significant portions of them.}}

\Anotecontent{whallon_simulation}{\foreigntextquote{english}[Whallon1972, 38]{The techniques and procedures of computer simulation so closely parallel the current thinking and processes of model-building of many archaeologists that the lateness and limits of their application are surprising.}}

\Anotecontent{lund_croisee}{Une analyse croisée entre des modèles de différentes disciplines sur la diffusion des innovations, contenant notamment les modèles d'Hägerstrand et de Rapoport a été publiée en 1968 dans la revue \textit{Lund Studies in Geography} par \textcite{Brown1968}}

\Anotecontent{boudon}{Selon \textcite[61]{Manzo2005}, Boudon a très tôt supporter l'idée des modèles de simulation comme support à l'explication, comme il témoigne à propos de ses écrits des années 60-80 : \enquote{À ce moment, j’avais publié divers écrits sur l’individualisme méthodologique, la théorie de l’action, la rationalité et les \enquote{modèles générateurs}. Mes travaux sur l’éducation m’avaient en effet convaincu que ni l’analyse multivariée ni les méthodes statistiques d’\enquote{analyse des données} ne permettaient d’expliquer les régularités statistiques qui sont le pain quotidien du sociologue : il fallait tenter plutôt de les engendrer à partir d’hypothèses sur les logiques de comportement des acteurs.} \autocite[391]{Boudon2003}
}

\Anotecontent{columbia_coleman}{C'est à l'université de Columbia sous la direction du sociologue Robert Merton et du mathématicien-sociologue Paul Lazarsfeld, des acteurs influents dans l'application des méthodes quantitatives à la sociologie \autocite{Lazarsfeld1954} aux États-Unis mais également en France (il collabore avec Boudon sur plusieurs projets, d'enseignements et de publications) et à l'international \autocite{Lecuyer2002}, que \textcite{Coleman1964} publie en 1964 \textit{Introduction to Mathematical Sociology}, un ouvrage devenu une référence en sociologie quantitative dont on peut lire un résumé élogieux dans la \textit{Revue française de Sociologie} réalisé par \textcite{Boudon1966} en 1966. Dans cette republication éditée par \textcite{Guetzkow1962}, \textcite{Coleman1962} écrit à propos de la simulation pour les sociologues, \foreignquote{english}{The aim is to program into the computer certain theoretical processes, and then to see what kind of behavior system they generate. The aim is to put together certain processes at the individual and interpersonal level and then to see what consequences they have at the level of larger systems} En 1972, pour \textcite[36]{Guetzkow1972} James S. Coleman considère la simulation \foreignquote{english}{[...] as a half-way point between verbal speculative theory and formal theory, aiding in the development of such theory through concretizing the functioning of \foreignquote{english}{social processes}}.}

\Anotecontent{boudon_abelson}{Un auteur connu également pour avoir échangé avec \textcite{Boudon1967} sur la simulation à la même période, voir le récit qu'en fait \textcite[204]{Padioleau1969}}

\Anotecontent{legende_carnegie}{La légende veut que l'idée d'appliquer la simulation aux \textit{Behavioral Science} viendrait d'un déjeuner entre Guetzkow et des physiciens nucléaire lors de son séjour au Carnegie, pour en savoir plus : \href{http://www.hawaii.edu/intlrel/pols635f/Guetzkow/hg.html}{@Harold} }

\Anotecontent{ouvrage_1962}{L'ouvrage de \textcite{Guetzkow1962}, difficile à trouver, contient des re-publications de publications inédites dans plusieurs disciplines : Orcutt en économie \foreignquote{english}{Simulation of economic systems}, Coleman en sociologie \foreignquote{english}{Analysis of social structures and simulation of social processes with electronic computers}, Abelson en psychologie et science politique \foreignquote{english}{Simulmatics project}, Hovland en psychologie sociale avec \foreignquote{english}{Computer simulation of thinking}.}

\Anotecontent{mediation_facilitante}{\enquote{Oui, tout à fait. Plus précisément : \begin{itemize}
\item Il s’agit bien d’une opération effectuée par et pour un sujet ou un collectif de sujets humains (un labo, un réseau ou une communauté de chercheurs,...) visant à connaître (dans tous les sens du terme “connaître” : connaissance de contemplation (theoria), de figuration (images, représentations), de savoir­faire (cognition pratique), etc.)
\item De plus, dans la mesure où elle recourt à une médiation (c’est sa caractéristique, en effet ce n’est peut­ être pas le cas de toutes les opérations de connaissance : dans la tradition philosophique, certains auteurs (Descartes par exemple) soutiennent qu’il y a des connaissances directes, intuitives, donc sans médiation), cette opération cognitive orientée “sujets” doit également être comprise comme augmentée car
instrumentée (comme on parle de réalité augmentée, de corps augmentée ou de cerveau augmenté par les technologies de greffe d’organes artificielles, d’implantation de stimulateurs cérébraux, etc.).\end{itemize}}}

\Anotecontent{varenne_temps}{Plusieurs auteurs, comme \textcite[462]{Gullahorn1965}, \textcite[296]{Doran1970}, \textcite[294-295]{Batty1976} semblent faire référence implicite ou explicite à cette action de \enquote{plonger le modèle dans le temps}. Hors \textcite[31]{Varenne2013b} indique que cette dénotation se rapporte principalement au temps du système cible, et non pas au temps du modèle, qui peut être simulé autrement (en usant par exemple d'un tirage probabiliste). Cette référence n'est donc pas un marqueur suffisant permettant de caractériser en elle-même la notion de \enquote{simulation de modèle}}

\Anotecontent{laboVirtuel}{Les récentes et au moins tout aussi récurrentes critiques sur l'apport d'une telle expérimentation dans les sciences sociales montrent qu'il est intéressant de développer quels sont véritablement ces points de similitudes et de divergences entre l'expérimentation physique et virtuelle, ne serait ce que pour construire une argumentation lisible à destination des nouveaux modélisateurs. Des sociologues des sciences comme Bruno Latour ou Ian Hacking ont développé ces vingt dernières années une véritable épistémologie des pratiques de laboratoire centrées autour de la démarche expérimentale, des réflexions qu'il nous faut prendre absolument prendre en compte pour toute analyse qui se voudrait plus poussée sur cette notion, comme en témoignent les travaux récents des épistémologues spécialisé dans la simulation comme Winsberg, ou \textcite[204]{Varenne2012}}

\Anotecontent{shubik_jeux}{voir sa \href{http://blogs.library.duke.edu/rubenstein/2012/12/18/the-martin-shubik-papers-from-early-game-theory-to-the-strategic-analysis-of-war/}{@Biographie}}

\Anotecontent{shubick_sympo}{Shubick est aussi présent à un des tout premiers symposiums sur le sujet organisés par \textit{American Economic Review} \autocite{Shubik1960b}, où il retrouve d'autres pionniers de son époque, comme \textcite{Orcutt1960}, et Clarkson aidé de Simon \autocite{Clarkson1960}}

\Anotecontent{polysemie_simu}{ \textcite[343-350]{Morgan2004} propose une analyse intéressante de la diversité d’acception pouvant sous tendre l'emploi du terme \enquote{simulation} en se basant sur l'état de l'art réalisé par \textcite{Shubik1960a} en 1960, mais on peut aussi citer des sources plus directes comme les rapports fait par les instituts scientifiques militaires proche du courant de l'\textit{Operationnal Research} (OR) : \foreignquote{english}{ The term \enquote{simulation} has recently become very popular, and probably somewhat overworked. There are many and sundry definitions of simulation, and a review and study of some of these should help in gaining a better perspective of the broad spectrum of simulation.} \autocite{Harman1961}}

\Anotecontent{wilcock_stat}{On trouve un récit plus détaillé de l'arrivée des méthodes statistiques en archéologie dans la publication de \autocite{Wilcock1997}}

\Anotecontent{doran_intuition}{\foreignquote{english}{There has now been a wide variety of experiments involving computer processing of archaeological data. Clarke (1968) describes many of them, and another valuable source is Cowgill (1967). I do not propose to discuss these experiments here, important though they are. [...] In this final section I shall briefly present the computer in what seems to me to be a much more promising and interesting role, which has as yet received rather little attention from archaeologists, even though in some ways it can be regarded as the practical equivalent of systems theory. I mean the use of a computer to construct and test a \enquote{simulation} of some complex system evolving in time. [...] Indeed, one of the great advantages of using a computer program to simulate evolving systems is that a much wider range of possibilities can be accommodated than can be expressed mathematically.} \autocite{Doran1970}}

\Anotecontent{caa}{Il est intéressant de noter que ces quelques archéologues pionniers en informatique ont très vite créé leur propres canaux de diffusion en Angleterre. Si de multiples conférences pour le développement des aspects computationels en archéologie existent à la charnière 1960-1970 (Rome, New-York, Marseille) \autocite{Wilcock1997}, ce n'est qu'en 1973 que se forme sous le patronage de quelques chercheurs anglais la première \foreignquote{english}{Computer Applications and Quantitative Methods in Archaeology Conference} \href{http://caaconference.org/about/}{@CAA}. Celle-ci tient sa première édition à Birmingham, et deviendra par la suite en 1992 une conférence à portée internationale. La particularité de cette conférence, qui existe toujours, est son interdisciplinarité; le comité d'organisation militant toujours pour la rencontre et le dialogue entre  archéologues, mathématiciens et informaticiens. A l'ocasion des 40 ans de la conférence en 2012, le projet \foreignquote{english}{Personnal-Histories Project} à permis la collecte et la mise à disposition de témoignages vidéo des pionniers sur le site de \href{http://www.sms.cam.ac.uk/collection/750864}{@Cambridge}}



\Anotecontent{ptvuebesher}{Un point de vue parmi les nombreux dans ce livre, celui de l'éditeur \textcite[194]{Beshers1965} : \foreignquote{english}{The development of a simulation model must by by persons intimately familiar with the subject matter. This principle has been violated in the past by excessive delegation of responsability to mathematicians and programmers interested primarly in questions of structure and style.}}

\Anotecontent{kaplan}{Voir l'article et le schéma plus détaillé du projet \textit{Venice Time Machine} sur son \href{http://fkaplan.wordpress.com/2013/03/14/lancement-de-la-venice-time-machine/}{@blog}}

\Anotecontent{sheps}{\foreignquote{english}{To understand how changes in the size and composition of human populations occur, it is essential to study the determinants of these changes and the interrelations among them. The impossibility of investigating these relationships experimentally stimulates the formulation of models, as a means of enhancing our understanding of the process.} \autocite{Sheps1971}}

\Anotecontent{rand}{La \textit{Research ANd Development corporation} (RAND) est une organisation de recherche et développement non lucrative, fondée après guerre. Celle-ci a joué un rôle très important dans l'établissement de grands projets de recherche opérationnelle à visée militaire ou concernant la sécurité nationale, et cela dans tous les domaines y compris en science sociale \autocite{Rand106}. La simulation et le développement de projets autour de la simulation sont également des domaines très financés, avec par exemple le développement du langage pour la simulation comme SIMSCRIPT (1962-63) \autocite{Nance2013}, et le support de plusieurs projets de planification urbaine dans les années 1960 (modèle TOMM de Crecine (1964), modèle de Lowry (1964), etc.) \autocite[6-9]{Batty1976}. Trevor Barnes a également beaucoup analysé les implications de ces institutions et complexes militaro-industriels dans l'établissement de la géographie quantitative américaine \autocite{Barnes2006a,Barnes2008}.}

\Anotecontent{as_amblard}{\textcite[113]{Amblard2006} pointe bien la double utilité des analyses de senbilités pour tester la robustesse des modèles pendant sa construction. \enquote{Les propriétés liées à la robustesse des modèles, évaluées en général par l'utilisation d'analyses de sensibilité (voir chapitres 3 et 12), sont en pratique plus souvent réalisées. [...] L'analyse de sensibilité, si elle peut s'appliquer pour tester la robustesse des résultats d'un modèle, peut également être utilisée pour tester la robustesse de la structure du modèle.}}

\Anotecontent{survey_heath}{\foreignquote{english}{It could be argued that validation is one of the most important aspects of model building because it is the only means that provides some evidence that a model can be used for a particular purpose. Without validation a model cannot be said to be representative of anything real. However, 65\% of the surveyed articles were not completely validated. This is a practice that is not acceptable in other sciences and should no longer be acceptable in ABM practice and in publications associated with ABM. One of the other potential reasons why models are not being completely validated is that the authors may consider that just conceptually or operationally validating their model is good enough. This survey found that overall 36\% (the majority) of the articles only validated one aspect of the model. Our position is that both conceptual and operational validity are required for complete validity. [...] Not enough scientists using ABM as an analysis tool are properly validating and documenting their model. It is absolutely essential that all models be completely validated and that the articles associated with them clearly document the validation techniques used and the validation results. Likewise, publication outlets and reviewers should be stringent in their validation requirements in order to produce better models and to advance not only their field of interest but also the field of ABM.} \autocite{Heath2009}}

\Anotecontent{serpent_mer}{On pensera nottament à cette fameuse question, récurrente lors des assemblées : \enquote{Avez vous validé votre modèle?} et si oui, \enquote{Quelle connaissances pouvez vous en tirer vis à vis de votre discipline ?} \autocite{Amblard2006} }

\Anotecontent{survey_thiele}{\foreigntextquote{english}[\cite{Thiele2014}]{A brief survey of papers published in the Social Simulation and in Ecological Modelling in the  years  2009–2010  showed  that  the  percentages  of  simulation  studies  including  parameter fitting  were  14  and  37\%,  respectively,  while  only  12  and  24\%  of  the  published  studies included some type of systematic sensitivity analysis.}}

\Anotecontent{survey_cottineau}{\foreignquote{english}{Even in one of the most recent issues of JASSS (March 2014), among the 7 papers exposing ABM results, only 2 mentioned a specific focus on evaluation, and 1 covered the quantitative aspects of such a process, however based on a few simulations (see supplement 1)} \autocite{Cottineau2015}}

\Anotecontent{hewitt_metaphore_sociale}{\textcite{Hewitt1976} reprend dans un paragraphe \emph{Modelling and Intelligent Personn} la vision dominante de l'Intelligence Artificielle tel que vue par \textcite{Newell1962}, un auteur référence dans le domaine de la résolution de problème avec Herbert Simon et J. C. Shaw : \foreignquote{english}{The problem solver should be a single personality, wandering over a goal net much as an explorer wanders over the countryside, having a single context and taking it with him wherever he goes}
\textcite{Hewitt1976} propose d'aller plus loin et donne dans ce papier quelques nouvelles pistes de réflexion, inspiré à la fois par les travaux théoriques réalisé en amont par Seymour Papert et Minsky (society of mind) dont Hewitt est l’élève, mais également par la métaphore des sociétés scientifiques comme il le rappelle ci dessous \foreignquote{english}{We are investing the problem solving model of a society of experts to supplement the model of a single very intelligent human. We submit that this change in focus has several benefits. It provides a better basis for naturally introducing parallelism into problem-solving since protocols of individual people do not seem to exhibit much parallelism. [...] Psychologists have found it extremely difficult to discover the communication that occur in the mind of an individual expert during problem solving.[...] In this ways we hope to develop the communication mechanisms that are necessary to achieve cooperation between expert modules for various micro-worlds in order to perform larger tasks which call for the expertise of more than one micro-world. Our work is attempting to build on the analysis that has been done by philosophers of science in recent years on the structure of the processes used by scientific societies.} Enfin la citation de Edward Osborne Wilson, un zoologiste connu pour sa théorie de sociobiologie animale (et humaine, le passage de l'animal à l'humain étant très largement critiqué \autocites{Ruelland2004,Pumain2003}) et son expertise reconnu sur les sociétés d'insectes tel que les termites et fourmi, révèle également l’intérêt que porte Hewitt au protocoles de communications dans la structuration de telle sociétés : \enquote{Reciprocal communication of a cooperative nature is the essential intuitive criterion of a society.}}

\Anotecontent{blackboard}{\foreignquote{english}{Imagine a group of human specialists seated next to a large blackboard. The specialists are working cooperatively to solve a problem, using the blackboard as the workplace for developing the solution. Problem solving begins when the problem and initial data are written onto the blackboard. The specialists watch the blackboard, looking for an opportunity to apply their expertise to the developing solution. When a specialist finds sufficient information to make a contribution, she records the contribution on the blackboard, hopefully enabling other specialists to apply their expertise. This process of adding contributions to the blackboard continues until the problem has been solved.}\autocite{Corkill1991}}

\Anotecontent{mace_systeme}{\enquote{Autre \enquote{monstre sacré} de l’IAD, le système Mace développé par L. Gasser eut un impact considérable sur l’ensemble des recherches ultérieures en IAD \autocite{Gasser1987}. [...] L. Gasser, en reliant ses travaux a ceux de Hewitt sur les acteurs, montrait non seulement qu’il était possible de réaliser un SMA à partir de la notion d’envoi de message, mais aussi que cela n’était pas suffisant, une organisation sociale ne pouvant se ramener à un simple mécanisme de communication. Il faut en plus introduire des notions telles que les représentations d’autrui et faire en sorte qu’un agent puisse raisonner sur ses compétences et ses croyances. En outre, on doit distinguer, comme le faisait Mace, la compétence effective, le \enquote{savoir-faire} directement applicable, de la connaissance qu’un agent peut avoir de sa propre compétence. On peut dire que, peu ou prou, toutes les plates-formes actuelles de développement de SMA sont des descendants directs ou indirects de Mace.} \autocite[30]{Ferber1995}}

\Anotecontent{critique_popper}{\enquote{D'un point de vue poppérien, les énoncés d'observation qui forment la base sur laquelle on peut évaluer le mérite d'une théorie scientifique sont eux-mêmes faillibles. [...] Mais ce qui affaiblit le point de vue falsificationiste tient précisément au fait que les énoncés d'observation sont faillibles et que leur acceptation ne peut avoir lieu qu'à titre d'essai et qu'elle est sujette à révision. Les théories ne peuvent être falsifiées de façon convaincante parce que les énoncés d'observation qui forment la base de la falsification peuvent eux-mêmes se révéler faux à la lumière de développements ultérieurs. }\autocite[111]{Chalmers1987}}

\Anotecontent{new_zeland}{Pour plus d'informations sur la diffusion du néo-positivisme en Nouvelle-Zélande, on pourra se référer plus spécifiquement à la thèse de \textcite{Hammond1992}}

\Anotecontent{inspiration_double_small}{Une inspiration qui opère dans les deux sens, comme en témoigne les annexes de \autocite[20-21]{Hewitt2014}, mais également ce passage des remerciements dans l'article introductif du formalisme \autocite{Hewitt1973} : \foreignquote{english}{We would like to acknowledge the help of the following colleagues: Bill Gosper who knew the truth all along: \enquote{A data structure is nothing but a stupid programming language.} Alan Kay whose FLEX and SMALL TALK machines have influenced our work. Alan emphasized the crucial importance of using intentional definitions of data structures and of passing messages to them. This paper explores the consequences of generalizing the message mechanism of SMALL TALK and SIMULA-67; the port mechanism of Krutar, Balzer, and Mitchell; and the previous CALL statement of PLANNER-71 to a universal communications mechanism. Alan has been extremely helpful in discussions both of overall philosophy and technical details.}

Une ressemblance entre les deux projets que l'on retrouve également exprimé dans le témoignage d'Alan Kay sur l'histoire du premier langage orienté objet (\textit{Object Oriented Programming}) SMALLTALK. PLANNER est ainsi évoqué comme une influence de Kay après son doctorat en 1969, période durant laquelle il developpe plusieurs langages informatiques : \foreignquote{english}{ [...] I went to the Stanford AI project and spent much more time thinking about notebook KiddyKomputers than AI. But there were two AI designs that were very intriguing. The first was Carl Hewitt's PLANNER, a programmable logic system that formed the deductive basis of Winograd's SHRDLU [Sussman 69, Hewitt 69] I designed several languages based on a combination of the pattern matching schemes of FLEX and PLANNER [Kay 70].}

Mais également une influence réciproque courant des années 1972, alors que Kay développe SMALLTALK au  fameux \textit{Xerox Palo Alto Research Center (PARC)} \foreignquote{english}{[...] In November, I presented these ideas and a demonstration of the interpretation scheme to the MIT AI lab. This eventually led to Carl Hewitt's more formal \enquote{Actor} approach \autocite{Hewitt1973}. In the first Actor paper the resemblance to Smalltalk is at its closest. The paths later diverged, partly because we were much more interested in making things than theorizing, and partly because we had something no one else had: Chuck Thacker's Interim Dynabook (later known as the \enquote{ALTO}).} \autocite{Kay1993}}

\Anotecontent{futur_histoire_acteur}{Il faut comprendre que le formalisme \enquote{Acteur} est un terrain de recherche théorique riche de sa propre histoire et de ses propres influences dans le domaine de l'intelligence artificielle distribué, dont on trouve récit dans les publications récentes des auteurs \autocite{Hewitt2014}. Dans cette dernière, Hewitt définit le \enquote{modèle Acteur} comme \foreignquote{english}{[...] a mathematical theory that treats \enquote{Actors} as the universal primitives of digital computation. The model has been used both as a framework for a theoretical understanding of concurrency, and as the theoretical basis for several practical implementations of concurrent systems. [...] An Actor is a computational entity that, in response to as message it receives, can concurrently: send messages to addresses of Actors that it has; create new Actors; designate how to handle the next message it receives.[...] The Actor model can be used as a framework for modelling, understanding, and reasoning about, a wide range of concurrent systems.}}

\Anotecontent{inspiration_ferber}{ \enquote{Mais quelques travaux ont voulu rester dans les idées initiales que prônaient Hewitt et qu’il confirma avec ses notions de “sémantique des systèmes ouverts” (Hewitt 1991; Hewitt 1985). P. Carle (Carle 1992), S. Giroux (Giroux et Senteni 1992) et J. Ferber (Ferber 1987), tout en estimant que les langages d’acteurs sont effectivement de trés bons outils pour l’implémentation de calculs parallèles, considèrent néanmoins qu’ils présentent des caractéristiques tellement originales qu’ils modifient par leur présence la notion même d’architecture multi-agent en envisageant les agents et les systèmes multi-agents comme des extensions naturelles de la notion d'acteur.} \autocite[145]{Ferber1995} Par Systèmes Ouverts d'Information (\textit{Open Information Systems}) il faut comprendre un système \enquote{[...] dans lequel la connaissance n'est pas la somme des connaissances de tous les agents, mais la résultante de l'interaction de plusieurs micro-théories, c'est-à-dire de savoirs et savoir-faire associés à des agents.} \autocite[238]{Ferber1995}}

\Anotecontent{acteur_definition_ferber}{\enquote{Mais qu’est-ce qu’un acteur? Un acteur est une entité informatique qui se compose de deux parties: une structure qui comprend l’adresse de tous les acteurs qu’il connaît et a qui il peut envoyer des messages et une partie active, le script, qui décrit son comportement lors de la réception d’un message. Le comportement de chaque acteur, qui s’exécute indépendamment et en parallèle avec les autres, se résume à un ensemble d’actions extrêmement réduit: envoyer des messages, créer
des acteurs et modifier son état (ou déterminer un nouveau comportement pour le message suivant). C’est tout! Et c’est suffisant pour pouvoir exprimer n’importe quel calcul parallèle comme une combinaison de ces actions primitives.} \autocite[145]{Ferber1995}}

\Anotecontent{inspiration_wooldridge}{\foreignquote{english}{Throughout the 1970s, several other researchers developed prototypical multiagent systems. The first was Carl Hewitt, who proposed the Actor model of computation \autocite{Hewitt1973}}. \autocite[399]{Wooldridge2009}}


\Anotecontent{artificial_societies}{ \hl{Un terme aujourd'hui désuet si on en crois chattoe brown ... }}

\Anotecontent{gilbert_confidence}{Le fait que \textcite{Gilbert2000a} fasse cette confidence dans un article intitulé \foreignquote{english}{Modelling Sociality : The View from Europe} dans l'ouvrage \foreignquote{english}{Dynamics in Human and Primate Societies} de Kohler et Gumerman \autocite{Kohler2000} n'est probablement pas anodin, et révèle l'existence passé d'une forme de cloisonement entre les deux foyers Européen et Américain sur ce sujet. Car face à l'ambition affiché de l'ouvrage d'Epstein et Axtell \foreignquote{english}{Growing artificial societies} \autocite{Epstein1996}, il est fort dommageable de voir que les projets européens, riche pourtant à cette époque d'un certain avancement sur le sujet, ne sont qu'à peine évoqués en introduction, la plupart des références n'allant pas en deça des années 1990. Le modèle Anasazi étant initialement inspiré des travaux sur Sugarscape \autocite{Rauch2002}, la participation des européens à cet ouvrage pourrait en définitive être interprété comme le signe d'une collaboration retrouvée.}

\Anotecontent{histoire_sugarscape}{Le journaliste \textcite{Rauch2002} a interviewé Joshua Epstein à ce sujet en 2002 : \foreignquote{english}{One day in the early 1990s, when he was giving a talk about his model of arms races, he met Axtell, who was graduate student. He wound up bringing Axtell to Brookings, in 1992. Not long after, Epstein attended a conference at the Santa Fe Institue [...] At Santa Fe juste then a big subject was artificial life, often called A-Life. \enquote{All of the work was about coral reefs, ecology, growing things that look like trees, growing things that look like flocks of birds, schools of fish, coral, and so on,} Epstein told me. \enquote{And I thought, jeez, why don't we try to use these techniques to grow societies?} Fired up, he returned to Brookings and discussed the idea with Axtell. There followed the inevitable napkin moment, when the two of them sat in the cafeteria and sketched out a simple artificial world in which little hunter-gatherer creatures would move around a landscape finding, storing, and consuming the only resource, sugar.} Le modèle est écrit sur sa propre base logicielle \textit{Object Pascal}, c'est à dire découplé d'une plateforme agent existante. Le code source n'est pas disponible en libre accès, et la plupart des versions que l'on trouve aujourd'hui sur les plateformes sont donc des réimplémentations basé sur la description faites des auteurs du modèle.}

\Anotecontent{varela_modele_ca}{Varela, Hersini, Bourgine \autocite{Bourgine1992} organise à Paris en 1991 la première conférence européenne ECAL sur la vie artificielle, mais il faut savoir que Varela développe aussi un modèle d'automate cellulaire appuyant sa théorie dès 1974 en Fortran, décrit par \textcite{McMullin1997b}. Sa \enquote{redécouverte} a permis de le sauvegarder sous une autre forme, dans un projet mené par \textcite{McMullin1997}.}

\Anotecontent{smith_bio}{Voici ce que Alvy Ray Smith, un informaticien pionnier dans le domaine de l'infographie, titulaire d'une thèse sur les automates cellulaires en 1970 et également connu pour ses travaux sur la représentation graphique des L-Systems, écrit sur son site internet vis à vis d'un état de l'art qu'il publie en \autocite{SmithIII1976} suite à ce qu'il apelle une des toutes premières conférences sur la VA : \foreignquote{english}{This is an extensive survey and bibliography of the field of CA (I was calling them \enquote{polyautomata} at the time) up to 1975. I added the words \enquote{Cellular Automata and} to the original title when I transcribed the original article into online form. It was originally written as the introduction to the German edition of Theory of Self-Reproducing Automata, by John von Neumann, edited (posthumously) by Arthur W Burks, University of Illinois Press, Urbana, 1968. Von Neumann performed the work, completed by Burks, in 1952-53. The German publishers never published the German edition, but gave me permission to publish my survey in the book listed above, the proceedings of a conference held in Noordwijkerhout, The Netherlands, Apr 1975. I like to call this conference ALife0 - for Artificial Life 0 conference - since it was the first attempt I know to cross-fertilization between biologists and computer scientists. Many of the players at this conference were present for ALife1, ALife2, etc - cf Simple Nontrivial Self-Reproducing Machines . Other participants at ALife0 were Karel Culik, Pauline Hogeweg, John Holland, Aristid Lindenmayer, and Stanislaw Ulam.}}

\Anotecontent{helmreich_IA}{L'Anthropologue \textcite[10]{Helmreich1998} ayant fait du SFI son terrain d'étude est tout à fait lucide sur cette question, et met le lecteur en garde au debut de sont ouvrage : \foreignquote{english}{Not all Artificial Life scientists are happy with how the recent history of the field is told, with how this shapes the terrain of inquiry, or with how the Santa Fe Institute is privileged in popular accounts [...] Many Europe-based researchers argue that Artificial Life was not created ex nihilo in New Mexico but has descended from tangled international lineages of cybernetics, systems theory, Artificial Intelligence, self-organization theory, origins of life research, and theoretical biology. The Chilean-born and Paris-based biologist Fransisco Varela [...] has argued that the materialization of Artificial Life in New Mexico has focused attention on overly computational views of life and that the naming of Artificial Life on the analogy to Artificial Intelligence has only made this more intense. U.S narrations of Artificial Life history are notorious in the international community for their erasure or marginalization of European and Latin American precedents and scientists, of transnational collaborations, and of the social, intellectual, and economic contexts that produced the science in some places and not others. [...] Indeed, my own decision to do field-work on the community at Santa Fe was powerfully guided by readings of popular science, and this book runs the risk of reinforcing the mainstream myth that \enquote{Artificial Life was born out of Zeus' head in Santa Fe, New Mexico, in the 1980's} -- as one Europe-based scientist sardonically summarized it to me.}}

\Anotecontent{echelle_optimization}{\foreignquote{english}{Evolutionary computation, the field of simulating evolution on a computer, provides the basis for moving toward a new philosophy of machine intelligence. Evolution is categorized by several levels of hierarchy: the gene, the chromosome, the individual, the species, and the ecosystem. Thus there is an inevitable choice that must be made when constructing a simulation of evolution. Inevitably, attention must be focused at a particular level in the hierarchy, and the remainder of the simulation is to some extent determined by that perspective. Ultimately, the question that must be answered is, \enquote{What exactly is being evolved?}} \autocite{Fogel1998}}

\Anotecontent{livret_CREA}{Le lecteur intéressé trouvera sur cette généalogie complexe de la notion des éléments de reflexion dans un livret édité par le CREA en 1985. Celui-ci relate un travail de trois ans de recherches sur la \enquote{Genealogie de l'auto-organisation} basé sur le dépouillements des archives du BCL. Ce livret traite des recherches réalisé par les chercheurs Isabelle Stengers, Pierre Livet, Pierre Lévy (lecture commenté des archives du BCL); et contient également des interviews menés par Isabelle Stengers de Fogelman-Soulié, Atlan, Von Foerster, Weisbuch, Varela. \autocite{CREA1985}}

\Anotecontent{lowry}{ Un point de vue très bien illustré par cet extrait tiré de la partie \textit{Evaluation} de l'article \textit{A short course in model design} de \textcite[62]{Lowry1968}, paru pour la première fois en 1965 dans \textit{Journal of the American Institute of Planners} : \foreignquote{english}{Above all, the process of model building is educational. The participants invariably find the perceptions sharpened, their horizons expanded, their professional skills augmented. The mere necessity of framing questions carefully does much to dispel the fog of slopply thinking that surrounds our effort at civic betterment. My parting advice to the planning profession is : If you do sponsor a model, be sure your staff is deeply involved in its design and calibration. The most valuable function of the model will be lost if it treated by the planners as a magic box which yields answers at the touch of a button.} L'article expose également plusieurs objectifs guidant la construction des modèles. La valeur scientifique des modèles dit de \enquote{descriptions} y est subtilement reconnue comme une source à mieux prendre en compte lors de modélisations plus risquées pour la prédiction : \foreignquote{english}{Good descriptive models are of scientific value because they reveal much about the structure of the urban environment, reducing the apparent complexity of the observed world to the coherent and rigorous language of mathematical relationships. They provide concrete evidence of the ways in which \enquote{everything in the city affects everything else}, and few planners would fail to benefit from exposure to the inner workings of such models.} }


\Anotecontent{pouvreau_livre1949}{\enquote{Ce dernier ouvrage est la synthèse des réflexions de Bertalanffy au cours des deux décennies passées, une systématisation des thèmes qu’il a jusqu’alors développés dans ses divers écrits. [...] L’originalité de ce livre par rapport à ses écrits antérieurs tient au fait que Bertalanffy, dans l’esprit du projet de \enquote{ systémologie générale } qui constitue désormais le cœur de ses préoccupations intellectuelles, ne se limite pas à y argumenter la nécessité d’un point de vue \enquote{ organismique } dans l’ensemble des domaines de la biologie – en particulier, à démontrer la pertinence et la fécondité des concepts de \enquote{ système ouvert } et d’\enquote{ ordre hiérarchique }. Il s’efforce aussi d’y établir l’évolution convergente de l’ensemble des sciences naturelles, sociales et humaines, ainsi que de la philosophie, vers une épistémologie centrée sur les concepts de système et d’organisation dynamique. La logique de Das biologische Weltbild est l’incarnation de l’idée fondamentale de Bertalanffy, et l’aboutissement d’un chemin initié dès sa thèse doctorale de 1926 : le dépassement de l’organicisme en direction du systémisme. Son livre se concluant en conséquence par un exposé des grandes lignes de sa \enquote{ systémologie générale }.}\autocite[46]{Pouvreau2006}}

\Anotecontent{taylor_openended}{\textcite{Taylor1999} parle pour ce cas de \textit{Open-Ended Evolution} : \foreignquote{english}{This term refers to a system in which components continue to evolve new forms continuously, rather than grinding to a halt when some sort of `optimal' or stable position is reached[...]Note that open-ended evolution does not necessarily imply any sort of evolutionary progress.[...]Also, by using the term `open-ended' I wish to imply that an indefinite variety of phenotypes are attainable through the evolutionary process, rather than continuous change being achieved by, for example, cycling through a finite set of possible forms.}}

\Anotecontent{taylor_reproduction}{Les deux termes réplication et reproduction sont souvent utilisés de façon synonyme mais renvoient en réalité à des études différentes, la première évoquant la capacité à reproduire une copie conforme, alors que la seconde renvoie au processus d'évolution naturel par la mise en oeuvre d'opération et de matériel génétique \autocites{Sipper1998,Taylor1999}}




\Anotecontent{premier_ouvrage_gilbert}{Premier ouvrage publié sur ce thème par Nigel Gilbert \autocite{gilbert1994}, « Simulating Societies » réalise une première, sinon la première tentative de publication réunissant d'emblée autant d'acteurs usant du multi-agent dans leur disciplines. Reprenant la plupart des interventions réalisés àla conférence de Guilford en 1992, il réunit pour la première fois une multitude de point de vue en abordant le thème de la modélisation agent à la fois sur des aspects méthodologique \autocite{drogoul1994multi} et thématique avec la présentation de cas d'application dans divers domaines centrés autour de la simulation de modèles de «sociétés humaines» : archéologie, développement pour l'écologie, sociologie, etc. Si le livre n'est pas spécifiquement centré autour des ABM car d'autres types de modèles sont également présentés, il est clair dès la présentation des chapitres que c'est ce nouveau formalisme qui suscite le plus d'intérêt et de discussion. Si le formalisme agent est connu et reconnu par la suite pour sa capacité intégratrive dans des publications ultérieures, force est de constater dans cet ouvrage qu'il se présente déjà en 1994 comme un outil d'expression priviligié et immédiatement inter-disciplinaire. L'archéologie et l'anthropologie sont ici majoritaire, conséquence directe selon les auteurs de la capacité des chercheurs opérants dans ces disciplines a se positionner plus facilement comme observateur de notre société, facilitant ainsi l'extraction des éléments clés à intégrer dans les modèles. Cette remarque n'est pas anodine, et témoigne d'une observation faite par la suite en lisant ce premier chapitre introductif. Celui çi s'appuie sur un exemple de modèle archéologique (effondrement Maya) que les auteurs utilisent comme un fil conducteur pour desambiguiser un certain nombre de termes et de concepts propre au process de simulation , et table en conclusion sur un espoir non dissimulé d'arriver à formuler au travers  de ce travail un début de cadre de modélisation commun qui réunit les différentes approches existantes. Les réflexion sur un certain nombre de concepts montre un recul étonnant pour un premier ouvrage de réflexion sur la question. Ainsi, la partie \emph{key concepts in modelling and simulation} aborde de façon succinte les points suivants : définition d'un modèle, de simulation, explicitation de la différence entre modèle spécifié et modèle implementé, précisions sur la notion d'équifinalité des modèles, introduction à la validation.}


\Anotecontent{foerster_interview}{ Dans l'interview relate dans le cahier 8 du CREA \autocite[257-258]{CREA1985}, Foerster rapelle les motivations premières à l'origine du projet BCL, dont le projet initial ne visait pas forcément le rattachement au projet cybernétique, comme il a eu lieu par la suite : \enquote{ [...] Il y a eu trois conférences sur l'auto-organisation. Les deux premières organisées par Yovits et Cameron. La troisième année, c'est moi qui ai organisé la conférence sur \enquote{Principles of Self-Organizing Systems}, et cette fois Ross Ashby a participé à la rencontre. Pas avant. Ross n'était pas encore au BCL à l'époque. En fait, mon laboratoire, à l'origine, n'était pas consacré à la cybernétique. J'étais associé au groupe \enquote{cybernétique} de Macy, d'accord, mais j'ai apellé notre laboratoire non pas Cybernetic Computer Laboratory, mais Biological Computer Laboratory. Notre intérêt était indépendant des notions principales de la cybernétique qui, je dois le dire, m'apparaissaient à l'époque [...] assez peu fascinantes. Ce qui me fascinait c'était la causalité circulaire.[...] Ce qui m'intéressait vraiment, c'était les principes de computation des organismes vivants. [...] Nous n'étions pas des cybernéticiens. Chacun venait avec sa compétence. Ma compétence était en physique, et donc je jouais le rôle de l'avocat de la physique. Je prenais garde à ce que des lois de la physique ne soient pas violés. Il y avait donc un effort coopératif. C'est seulement plus tard que la cybernétique a pris un grand intérêt, [...] cinq ou six ans après le début du BCL, peut-être plus.}}

\Anotecontent{connexionisme_symbolisme}{Dans la littérature il n'est pas rare de trouver cette tension entre connexioniste et cognitiviste se cristalliser principalement autour du livre \enquote{Perceptron} écrit par Minsky et Papert en 1969. Ces derniers auraient par une argumentation délètere enterré tout possibilité de recherche dans ce qui est aujourd'hui considéré comme un archéo-connexionisme; et qui ne reviendrai ensuite sur le devant de la scène que dans les années 1980. La réalité historique semble bien différente, et plusieurs auteurs s'attachent aujourd'hui à comprendre les causes multi-factorielles réelles qui ont conduit à l'abandon temporaire de cette branche de recherche : mort de rosenblatt, argumentation exhaustive de Papert et Minsky, manque de puissance informatique, suppressions des financements, absence de résultats, etc. Une chose est sure, autant Minsky que Papert ont contribués par leur filiations universitaire avec McCulloch à l'élaboration de cet archéo-connexionisme, et continue d'intégrer encore aujourd'hui les différentes approches dans leurs travaux plutot que de les opposer de façon stérile. Si effectivement il semble y'avoir plusieurs ruptures (\hl{ref Levy}) avec la démarche de McCulloch, on retiendra comme seul exemple ici l'éloignement du substrat neurobiologique opéré par les promoteurs de l'IA devenu par la suite \enquote{classique}, plus intéressé par l'expression de la cognition pour la résolution de problème que par la compréhension de celle ci en terme de mécanismes biologique. Une problématique qui a pourtant toujours été moteur dans les recherches et la trajectoire de recherche de McCulloch. \hl{Ajout Citation Crevier, Culloch interview, et ce que j'ai lu dans Restnick, plus citation du papier de Minsky sur le perceptron.}} %pour construire des programmes de résolution de problème, La notion de connexionisme est en un certain sens intéressante, car mis à l'écart pendant des années, elle revient avec la notion forte de décentralisation en IA. L'analyse de Restnick sur ce point...}}

\Anotecontent{liaison_prigogine_foerster}{\hl{Voir ce que dit Foerster dans son interview au CREA...}}

%\Anotecontent{nature_ccs}{The program therefore developed its own core curriculum, establishing courses in automata theory, information and probability theory, analog and digital computers; and in natural language, psychology, and biology treated from an information-processing point of view. Students were also required to take a course in modern algebra and, when it became available, an advanced course in programming. \href{http://um2017.org/2017_Website/History_of_Computer_%26_Communications_Sciences.html}{@History of CCS}}

\Anotecontent{influence_turing}{\enquote{McCullough lui-même fut mis sur sa voie à la suite de la démonstration en 1936 par le logicien anglais Alan Turing de l’« isomorphisme » entre toute machine capable de réaliser un calcul fondé sur une procédure algorithmique et une « machine universelle » abstraite dotée d’un « programme » où figurent des instructions et des données que la machine lit sur un ruban de longueur infinie et où elle inscrit ses résultats }\autocite[777-778]{Pouvreau2013} On trouve également des éléments sur cette relation avec Turing dans les écrits de \textcite{Husbands2012} et l'analyse de \textcite{Dupuy2005} et \textcite{Levy1985}. \hl{Ajouter extrait texte de Dupuy et Levy ? } }

\Anotecontent{mcculloch_ratioClub}{\hl{A raffiner} Une initiative que l'on peut rattacher à la relation complexe qu'il a tissé avec les nombreux membres du \textit{Ratio Club} fondé en 1949 par John Bates, et parmis lesquels vont figurer plusieurs scientifiques aux travaux notoires. Membres qui par ailleurs n'ont pas attendu l'attribution officielle de Wierner pour esquisser des idées similaires à ce qui va devenir par la suite la \enquote{pensée cybernétique}. McCulloch entretient des rapports avec les différents membres, dont Ashby ne fut qu'un des membres parmis d'autres invités, preuve aussi de l'influence des penseurs anglais dans la structuration du mouvement cybernétique américain. Des relations qui commence d'ailleur bien avant, car les travaux de Turing ont déjà traversé l'atlantique et marque d'une influence - peut être réciproque - les travaux de ce dernier avec ceux de McCulloch, Pitts et Von Neummann. Plusieurs de ces membres seront amenés à participer par invitation de McCulloch à des séjours aux Etats Unis. Ashby, dont le premier contact écrit avec McCulloch daterai d'avant 1946 (si on en croit la lettre de Bateson à Ashby daté de décembre 1946) va ainsi être amené à effectuer plusieurs voyages aux Etats-Unis entre 1949 et 1958. Sur ces différents points on pourra se référer aux travaux répétés et très intéressant menés par \textcite{Husbands2012}, dont la plupart de ces réflexions sont tirés.}

\Anotecontent{sous_discipline_biologie}{ \enquote{ Surtout dans les années 1920 et 1930, les sciences biologiques furent en effet le lieu d’un mouvement dont la vocation était de réhabiliter la légitimité d’approches holistiques et néanmoins non métaphysiquement vitalistes de ce que Bertalanffy appelait les \enquote{ problèmes de la vie }, que ce soit d’une manière générale, à partir de considérations épistémologiques, ou sur un mode spécifique et empiriquement fondé, relatif à des problèmes bien circonscrits. Sept domaines furent plus spécifiquement concernés : l’embryologie, la théorie de l’évolution phylogénétique, la morphologie, la théorie de l’hérédité, l’étude du comportement de l’organisme individuel dans son environnement (soit selon la perspective du système formé par l’organisme et son environnement, soit selon la perspective de l’autonomie acquise par l’organisme par rapport à cet environnement) et, enfin, celle des relations entre espèces biologiques (biocénologie). } \autocite[153]{Pouvreau2013}}

\Anotecontent{ordre_desordre}{\enquote{En réalité, et Ashby fut explicite sur ce point en 1962, toute sa cybernétique justifiait l’idée de l’impossibilité d’une \enquote{auto-organisation} dans un système n’interagissant pas avec son environnement, les changements organisationnels devant tirer leur source de l’extérieur du système – il critiqua d’ailleurs la pertinence même du concept d’\enquote{ auto-organisation }, en toute rigueur \enquote{ auto-contradictoire } : le système improprement dit \enquote{ auto-organisé } détecte au moyen de ses échanges avec son environnement et sous la forme de perturbations affectant ses \enquote{ variables essentielles } la \enquote{ variété } de cet environnement, et ne peut gagner lui-même de \enquote{ variété } qu’en collectant de l’information sur cet environnement ou en tentant de contrôler les échanges de matière et d’énergie qu’il entretient avec lui. La théorie de la \enquote{ variété } apportait en fin de compte un fondement logico-mathématique à l’idée dont l’origine se trouve chez Fechner et que nous avons vue opposée par Bertalanffy à Schrödinger dès 1949, selon laquelle l’ordre \enquote{ organismique } ne doit pas être pensé comme \enquote{ issu de l’ordre }, mais comme émergeant \enquote{ épigénétiquement } du chaos selon des principes inhérents aux systèmes dynamiques : Ashby donna une impulsion significative à ce qui allait devenir, notamment par l’intermédiaire de Prigogine et Atlan, le fameux principe d’\enquote{ ordre à partir du bruit }} \autocite[800]{Pouvreau2013}}

\Anotecontent{conrad_model}{\foreignquote{english}{Largely because of the limits of memory, computational power, and available runtime (at Stanford University in the late 1960s there was only one IBM 360 mainframe for the entire campus), Michael’s first computer model of an evolving ecosystem was highly simplified and could represent only three hierarchical levels, the genetic, the organismic, and the population in a discrete, finite one-dimensional workld. Populations were limited to a few hundred individuals, and runs were limited to a few hundred generations. In keeping with the ineffability of fitness in real ecosystems, Michael was careful not to explicitly define any fitness function in his program, but allowed the reproductive success or failure of the biota to depend on the ecosystem interactions at every level so that reproduction, competition, and niche selection were subject to variation as the ecosystem itself evolved. For this reason, the behavior of the model was difficult to analyze in any detailed causal terms. }\autocite{Pattee2002}}

\Anotecontent{def_biomathematique}{\enquote{Afin de lever d’emblée toute ambiguïté quant à ce dont il sera question dans ce chapitre, une
définition s’impose au préalable : j’appelle \enquote{ biologie mathématique } (ou \enquote{ biomathématique }) ce qui se veut le parfait analogue pour la biologie de la physique mathématique : une entreprise de construction mathématique de concepts et de lois biologiques, où les mathématiques sont vouées à entretenir avec la biologie un rapport \enquote{ non plus descriptif, mais formateur } (Bachelard), ou mieux encore, \enquote{ constituant } (Lévy-Leblond). En ce sens, on ne peut parler de biologie mathématique ni dans le cas où les mathématiques n’ont pour fonction que de résumer statistiquement ou d’ajuster des données empiriques, ni dans celui où elles ne s’introduisent que par le biais des principes et lois physico-chimiques tels qu’on peut les mettre en œuvre dans l’analyse des objets biologiques.} \autocite[515]{Pouvreau2013}}

\Anotecontent{piaget_cloture}{\enquote{L’œuvre de Jean Piaget représente une étape cruciale dans l’histoire des modèles de la circularité biologique. En allant au-delà du contexte de l’embryologie, Piaget élabore une approche conceptuelle générale qui vise explicitement à cerner les spécificités de l’autodétermination biologique, par la jonction théorique entre circularité, autodétermination et dimension thermodynamique. En particulier, il développe un concept théorique fondamental dans cette tradition, celui de \enquote{ clôture organisationnelle } [...] , entendu comme complémentaire à celui d’ouverture thermodynamique de von Bertalanffy. L’objectif de Piaget est de rendre compatible l’idée d’un flux constant de matière et énergie entre l’organisme et l’environnement avec celle d’un ordre circulaire constitutif, qui maintient le système au cours du temps. Le concept de clôture de Piaget décrit la dynamique propre du vivant comme une forme d’autodétermination, dans le sens fondamental d’une connexion entre \textit{activité} et \textit{existence}, réalisée par un réseau circulaire de relations entre les composants de l’organisme, dont dépendent son unité et individuation.La distinction entre clôture organisationnelle et ouverture thermodynamique est le pivot théorique sur lequel les élaborations plus récentes de l’autodétermination biologique s’appuieront, plus ou moins explicitement. Cela vaut non seulement par rapport à la relation profonde entre stabilité de l’organisation et variations des processus sous-jacents, mais également en relation avec les interactions adaptatives de l’organisme avec l’environnement. Sur ce point, Piaget réinterprète et généralise les concepts d’assimilation et adaptation – initialement formulés par l’embryologie de Waddington – et décrit les interactions d’un organisme avec l’environnement en termes d’adaptation, conçue comme une assimilation des perturbations qui induit une \enquote{ autorégulation } interne (accommodation). Ainsi, l’organisation adapte le réseau circulaire de relations en fonction des perturbations, tout en maintenant la clôture qui réalise l’autodétermination} \autocite{Mossio2014}}

\Anotecontent{etude_pouvreau_mossio}{ A la suite d'un échange privé avec David Pouvreau daté du premier octobre 2014, il est apparu qu'une nette préfiguration de la notion de \enquote{cloture organisationelle} apparaisse dans les travaux de Bertalanffy au travers de sa théorie \enquote{bionomogénétique} de l'évolution.  Sur ce point, des travaux sont en cours, notamment avec le philosophe Matteo Mossio, pour tenter de reconstruire un historique de la notion de cloture qui tiennent compte à juste titre des travaux préalable, notamment ceux de Bertalanffy.
\textcite[619]{Pouvreau2013} définit et replace ce terme dans le contexte des débats sur l'évolution dans les années 1930 : \enquote{s’exerceraient des \enquote{ lois internes de forme }, ou \enquote {de structures }, qui s’expriment chez les organismes par des \enquote{ caractéristiques d’organisation } autonomes n’ayant \enquote{ rien à voir avec l’adaptation } ; des \enquote{ lois de la morphogenèse immanentes } qui codétermineraient l’évolution phylogénétique en opérant une sélection parmi les variations contingentes admissibles au titre de cette évolution, et conféreraient à celle-ci une \enquote{ directivité intérieure }. Typiquement \enquote{ organismique } par l’expression du principe d’\enquote{ activité primaire } qu’elle manifestait, il s’agissait d’une conception dynamique interprétant tout processus morphogénétique comme une \enquote{ réaction entre les ‘puissances’ inhérentes à l’organisme et les conditions extérieures }. Tout en demeurant sur le \enquote{ sol ferme } de la science, elle semblait à Bertalanffy fournir le cadre adéquat pour répondre à l’ensemble des critiques adressées aux théories \enquote{ sélectionniste } et \enquote{ mutationniste } }.
L'\enquote{activité primaire} étant entendu selon \textcite[60]{Pouvreau2013} comme \enquote{Un schème anti-mécaniciste qu’il opposa tout au long de sa carrière à celui de la \enquote{ réactivité primaire }, selon lui à l’oeuvre aussi bien en biologie (par exemple dans la théorie des tropismes de Jacques Loeb ) qu’en psychologie (avec le behaviorisme) et en théorie de la connaissance (empirismes) – avec cette nuance importante que la combinaison de ce schème avec celui du \enquote{ système ouvert } faisait diverger Bertalanffy de la monadologie leibnizienne : \enquote{L’organisme, même sous des conditions extérieures constantes, donc en l’absence de stimulation extérieure, ne représente pas un système au repos, mais un système actif, mû intérieurement [innerlich gewegt] [...] Il faut considérer comme primaire l’activité autonome, et non la réactivité (le réflexe) }. C’est ce schème, en particulier, qui se devine en amont de sa conception épigénétique de la morphogenèse organique, lorsqu’il postula en 1928 afin d’expliquer ce phénomène un \enquote{ principe de formation immanent à l’organisation de la matière } } Ces définitions dont la complexité est apparente demande pour être mieux comprise de se plonger pleinement dans les écrits de \textcites[451-453, 158-161]{Pouvreau2013} se rapportant spécifiquement à la \enquote{bionomogénétique}.}


\Anotecontent{deux_principes_autoorganisation}{La théorie organismique de Bertalanffy fait état de deux grands principes, qui reprennent en unifiant les développements passés et multiples de nombreuses influences exposés en détail dans les publications de Pouvreau et Drack \autocites{Pouvreau2013, Pouvreau2007}. Le premier principe repris et travaillé dans la théorie de Bertalanffy est celui bien connu de système ouvert en équilibre de flux, éloigné de l'équilibre;  le deuxième moins connu est le principe de hierarchisation. Le couplage de ces deux principes ... on a une expression du vivant qui découle d'un principe d'auto-organisation, dont Pouvreau montre qu'il s'accord assez bien avec celui évoqué par Ashby dans son homéostat.}

\Anotecontent{Pouvreau_secondprincipe}{Le « second principe » de Bertalanffy était le schéma suivant de développement épigénétique d’un système, dont on peut remarquer qu’il correspond bien à ce que Chauvet a récemment posé comme une « caractéristique de la vie dans la matière ». Dans une étape « primaire », le système est « unitaire » : il forme une « totalité équipotentielle » ayant des capacités maximales de régulation. Aucune de ses parties n’y est encore investie d’une fonction spécifique. Dans un second temps survient un processus de « ségrégation » au cours duquel le système se « scinde » en sous-systèmes dont le développement spécifique ultérieur se prédétermine. Un processus de « différenciation progressive » engage alors chaque sous-système dans la voie de développement qui lui a été ainsi assignée. Il se caractérise par l’attribution de fonctions déterminées à ces sous-systèmes et la constitution de structures plus ou moins rigides. C’est un processus d’autonomisation relative et de spécialisation des parties et des processus, qui implique pour le système dans son ensemble une « perte de régulabilité » (ou de « plasticité ») et que Bertalanffy appela à partir de 1937 la « mécanisation progressive ». \autocite[476-477]{Pouvreau2013}}

\Anotecontent{piaget_mossio}{\enquote{L’oeuvre de Jean Piaget représente une étape cruciale dans l’histoire des modèles de la circularité biologique. En allant au-delà du contexte de l’embryologie, Piaget élabore une approche conceptuelle générale qui vise explicitement à cerner les spécificités de l’autodétermination biologique, par la jonction théorique entre circularité, autodétermination et dimension thermodynamique. [...] L’objectif de Piaget est de rendre compatible l’idée d’un flux constant de matière et énergie entre l’organisme et l’environnement avec celle d’un ordre circulaire constitutif, qui maintient le système au cours du temps. Le concept de clôture de Piaget décrit la dynamique propre du vivant comme une forme d’autodétermination, dans le sens fondamental d’une connexion entre activité et existence, réalisée par un réseau circulaire de relations entre les composants de l’organisme, dont dépendent son unité et individuation. La distinction entre clôture organisationnelle et ouverture thermodynamique est le pivot théorique sur lequel les élaborations plus récentes de l’autodétermination biologique s’appuieront, plus ou moins explicitement. Cela vaut non seulement par rapport à la relation profonde entre stabilité de l’organisation et variations des processus sous-jacents, mais également en relation avec les interactions adaptatives de l’organisme avec l’environnement. Sur ce point, Piaget réinterprète et généralise les concepts d’assimilation et adaptation – initialement formulés par l’embryologie de Waddington – et décrit les interactions d’un organisme avec l’environnement en termes d’adaptation, conçue comme une assimilation des perturbations qui induit une « autorégulation » interne (accommodation). Ainsi, l’organisation adapte le réseau circulaire de relations en fonction des perturbations, tout en maintenant la clôture qui réalise l’autodétermination} \autocite[12]{Mossio2014}}

\Anotecontent{terme_bioinformatique}{Paulien Hogeweg définit son objet de recherche \foreignquote{english}{on multilevel evolution aims to understand how different levels of selection, complex genotype phenotype mapping, and interactions over different timescales contribute to the evolution of complexity at the level of individuals and/or ecosystems.} On comprend pourquoi ces travaux ont eu autant d'impact du coté de la biologie, de l'écologie \autocite{Hogeweg1988}, que de l'informatique \autocites{Hogeweg1979, Hogeweg1983, Drogoul1993}. Dans cet extrait d'article \autocite{Hogeweg2011} elle témoigne de l'origine du mot \enquote{bioinformatique} début 1970. Un terme qui, au contraire de son usage actuel \autocite{Giavitto2002}, se rapproche initialement en bien des aspects de ce que \textcite{Giavitto2002} décrit comme ce processus d'aller retour entre les deux approches de \textit{Biological computing} et \textit{Computational Biology} : \foreignquote{english}{In the beginning of the 1970s, Ben Hesper and I started to use the term \enquote{bioinformatics} for the research we wanted to do, defining it as \enquote{the study of informatic processes in biotic systems}. [...] It seemed to us that one of the defining properties of life was information processing in its various forms [...] At a minimum, we felt that that information processing could serve as a useful metaphor for understanding living systems. We therefore thought that in addition to biophysics and biochemistry, it was useful to distinguish bioinformatics as a research field (or what we termed a “work concept”).[...]
While evolutionary models mainly dealt with invasion of mutants and changing allele frequencies, the question of how evolution leads to complex organisms was not addressed. [...] To meet the challenge of a \enquote{constructive evolutionary biology} became another long-term goal of bioinformatics as we envisioned it. Research in artificial intelligence at this time was exploring new representations of information processing systems, often inspired by biological systems, [...] demonstrating the power of an individual self-centered approach to generating and/or understanding more global structures. We felt that the re-introduction of biologically inspired computational ideas back into biology was needed in order to begin to understand biological systems as information processing systems. In particular, a focus on local interaction leading to emergent phenomena at multiple scales seemed to be missing in most biological models.[...] In short, under the heading of bioinformatics we wanted to combine pattern analysis and dynamic modeling and apply them to the challenge of unraveling pattern generation and informatic processes in biotic systems at multiple scales.} \autocite{Hogeweg2011}}

\Anotecontent{conrad_explanation}{\foreignquote{english}{\textcite{Conrad1970} also studied general properties of evolution systems but from a different perspective that involved the simulation of a hierarchic ecosystem. A population of cell-like individual organisms placed in an array of environmental cells was subjected to a strict materials conservation law that induced competition for survival. The organisms were capable of mutual cooperation, as well as executing biological strategies that included genetic recombination and the modification of the expression of their genome. No fitness criteria were introduced explicitly as part of the program. Instead, the simulation was viewed as an ecosystem in which genetic, individual, and populational interactions would occur and behavior patterns would emerge.}\autocite[62]{Fogel2006b}}

\Anotecontent{patte_deception}{\foreignquote{english}{The hope of \enquote{strong} artificial life was stated by Langton : \enquote{We would like to build models that are so lifelike that they would cease to be models of life and become examples of life themselves.} Very little has been said at this workshop about how we would distinguish computer simulations from realizations of life, and virtually nothing has been said about how these relate to theories of life, that is, how the living can be distinguished from the non-living. The aim of this paper is to begin such a discussion.} \autocite[63]{Pattee1988}}

\Anotecontent{piquant_weiss}{\enquote{Or, très tôt dans sa carrière, la cybernétique devait se trouver confrontée à une autre conception des totalités organisées, plus riche et en un sens dirigée contre elle. Cette confrontation eut lieu au symposium Hixon, en 1948, et elle mit aux prises, d’un côté, Warren McCulloch, de l’autre un ensemble de chercheurs composé des neurophysiologistes Karl Lashley et Ralph Gerard, des psychologues Wolfgang Köhler et Heinrich Klüver, et surtout de l’embryologiste Paul Weiss. John von Neumann était également présent, il dut lui aussi subir les assauts de Weiss, mais il faut surtout retenir de ses interventions la critique que lui-même adressa à McCulloch, dont nous avons déjà parlé et sur laquelle nous reviendrons. La conception présentée par Weiss nous intéresse au plus haut point parce qu’elle annonce dans une certaine mesure les théories de l’autopoiése et de l’autonomie qui seront développées par l’école chilienne de l’auto-organisation (Maturana et Varela). Du point de vue de l’histoire des idées qui est ici le nôtre, il est particulièrement piquant de voir la cybernétique mise en accusation par un mouvement de pensée que sa descendance contribuera à alimenter} \autocite{Dupuy2005}}

\Anotecontent{reductionisme_pouvreau_macy}{\enquote{Ce réductionnisme consistait à glisser de l’idée légitime que des correspondances entre les comportements des machines \enquote{ finalisées }, des systèmes biologiques et des groupes sociaux peuvent être établies et guider de manière féconde la recherche dans les différents champs scientifiques concernés, à l’idée beaucoup plus contestable que les méthodes, concepts et principes permettant effectivement bien de construire des machines cybernétiques et de rendre compte de leur fonctionnement peuvent capturer les propriétés systémiques scientifiquement accessibles des organismes vivants, de leur cerveau, de la psyché humaine et des sociétés, voire qu’ils y suffisent.} \autocite[784]{Pouvreau2013}}

\Anotecontent{dupuy_causalite}{ Spécialiste des conférences de Macy, le témoignage de \textcite{Dupuy2005} est sans appel : \enquote{Ce qu’il faut ici retenir, c’est que la cybernétique aura manqué cela, alors même qu’elle plaçait ses conférences fondatrices sous le signe de la \enquote{ causalité circulaire }. On est d’ailleurs effaré, à la lecture des Actes des conférences Macy, de constater combien le thème de la circularité est absent. Bien sûr, on parle de feedback, bien sûr on mentionne régulièrement l’existence et le rôle des circuits neuronaux \enquote{ réverbérants }, mais ce ne sont là que les formes les plus élémentaires de la causalité circulaire.} Une constatation qui rejoint les critiques importantes subit par McCulloch et Neumann au symposyum de Hixon de 1948 lorsqu'il sont confrontés au reste du corps biologique, notamment Paul Weiss, un embryologiste et acteur important du mouvement organiciste. Il n'est pas étonnant de voir dans la critique de Bertalanffy, proche de Paul Weiss, une relecture très pertinente des insuffisances de la \enquote{causalité circulaire} tel qu'elle a été développé jusqu'alors par la cybernétique pour caractériser un certain nombre de processus biologique. Sur ce point on peut également se référer à la lecture croisée des arguments de Dupuy, et de Pouvreau dont une partie des arguments sont repris en Annexe 1. }


%% NOTE BULLE RUFAT SUGDEN

\Anotecontent{bulle_modele_explicatif}{Par explicatif, \textcite{Bulle2005} parle de modèles qui \enquote{ visent à rendre compte des mécanismes et processus qui sous-tendent concrètement les phénomènes observés. Appliqués à caractériser des mécanismes ou des processus opérant véritablement, les modèles théoriques tendent à isoler des relations causales particulières et à s’abstraire des phénomènes totaux observables. Dans une optique essentiellement explicative, un modèle ne retient que les éléments nécessaires à la représentation des relations élémentaires en cause.}}

\Anotecontent{ruffat_samuel_ville}{En géographie, on connait et on étudie depuis plusieurs années le parallèle fait entre la simulation de ville opérée dans le cadre ludique, et dans le cadre scientifique; comme en témoigne des travaux sérieux, comme ceux de \textcite{Rufat2012} dont le texte dans Cybergéo cite Anne Bretagnolle, une géographe spécialisée dans l'étude des systèmes de villes : \foreignquote{english}{Of course simulation models can be conceived as entertainment tools, and designed for building games, or imagining fictive worlds, as utopias always did. But we want to learn something about the real world from such exercises, by confronting the results of simulation with observation.} En archéologie, l'introduction donnée par Kohler \autocite{Kohler2000} dans le fameux livre \textit{Dynamics in Human and Primate Societies : Agent-bade modeling in social and spatial processes}s'ouvre sur cette relation ambigüe qu'entretient la simulation avec le jeu \foreignquote{english}{ Often when I demonstrate the simulation of Anasazi settlement discussed in chapter 7 of this volume someone will say, \enquote{This is just a game isn't it?} I'm happy to admit that it is, so long as our definition of games encompasses child's play—which teaches about and prepares for reality—and not just those frivolous pastimes of adults, which release them from it.} Enfin et de façon plus générale, si toutes les simulations contiennent forcément une part d'apprentissage qui dérivent de leur construction, cet objectif existe également de façon autonome et donne lieu à la construction de nombreuses simulations spécifiques, comme en témoigne le toujours très vigoureux journal \textit{Simulation and Gaming}}

\Anotecontent{bulle_modele_autonome}{\enquote{Formellement, le modèle est autonome par rapport au réel. Les éléments qu’il met en œuvre sont définis opérationnellement par les seules liaisons retenues, et exclusivement par elles, c’est-à-dire par l’ensemble des relations qu’ils entretiennent avec les autres éléments du modèle. Ils ne sont donc pas simplement isolés de l’ensemble des facteurs intervenant en réalité, mais ils sont littéralement reconstruits. Autrement dit, le sens des variations marquées par les facteurs du modèle ne renvoie pas à des variations réelles des objets que ces facteurs dénotent, mais aux relations avec les autres facteurs, c’est-à-dire à la théorie que le modèle exprime.} \autocite{Bulle2005} }

\Anotecontent{phan_livet_modele}{\enquote{Le modèle est une manière d'expérimenter dans un \enquote{ monde virtuel } la puissance explicative de quelques hypothèses empiriquement choisies. Partant d’une situation concrète, mais sélectionnant seulement quelques facteurs, un tel modèle abstrait est évidemment \enquote{ faux par la construction } (non réaliste quand à ses hypothèses), mais cela ne signifie pas qu’il ne puisse être pertinent, ou plus exactement \enquote{ convainquant } pour l'explication des phénomènes empiriques du \enquote{ monde réel } }\autocites{Livet2006, Phan2008} }


\Anotecontent{denise_geopoint}{Ce qui rejoint aussi cette remarque de Denise Pumain, faite en 1984 dans les discussions de Geopoint \autocite[202]{Geopoint1984} \enquote{On ne met jamais à l'épreuve le fait que le système est pertinent ou non. Ce qu'on met à l'épreuve, c'est les connaissances qu'on a sur le système pris en compte. Est-ce que les règles de fonctionnement que je considère importantes pour le système produisent effectivement les configurations que j'observe. Mais cela ne me dit en rien si mon choix qui m'a conduit à m'intéresser à ce système et le découper de cette manière est fondé.} }

%% NOTES HERMANN

\Anotecontent{verification_philo}{A ne pas confondre avec la Vérification de la Validation \& Verification. C'est du fait de cette contiguïté entre approche philosophique, et les approches pratiques de la validation qu'opèrent une relecture ou une appropriation des termes responsables aujourd'hui de la plupart des ambiguïtés provoquant des débats terminologiques sans fin. \autocite{David2009}}

\Anotecontent{Herman_parcimonie}{
Ces deux remarques tirés du texte d'Hermann nous proposent d'explorer l'évolution négative de la représentativité comme une conséquence parfaitement assumé de l'activité \enquote{simplifiante} opérée durant l'activité de modélisation.

\foreignquote{english}{Parsimony is a significant contributor to a model’s value as a device for prediction and explanation. On the other hand, simplification and abstraction of processes and structures assumed to exist in the referent system reduce the credibility of claims that correspondence exists between an operating model and its intended referent.} \autocite[217]{Hermann1967}

\foreignquote{english}{Simplification in model building increases the uncertainty of a simulation’s ‘representativeness’, and thus adds to the necessity for establishing validity.} \autocite[217]{Hermann1967} }

\Anotecontent{durand_observation}{Comme le rapelle Durand-Dastès dans une discussion à Géopoint en 2000, \autocite{Dupont2002} l'observation est intégré au moins à \enquote{ [...] trois stades du cheminement logique : Au stade de la conception du modèle [...] Au stade de la simulation proprement dite [...] Au stade de la validation du modèle [...] } }

%% NOTES SUR LA PARTIE C
\Anotecontent{kauffman}{Il en résulte une activité permanente qui passe en partie inapercu, et qui fait assez bien écho par exemple à que dit Kauffman à propos de l'image statique que l'on se fait en général du \enquote{je}, alors que celui-ci est l'objet d'une fluctuation et d'une remise en cause permanente, dont nous ne percevons qu'une infime partie sous l'angle de nos décisions prise de façon consciente.}

\Anotecontent{style_hacking}{Pour ce concept de \enquote{styles de raisonnement scientifique}, Ian Hacking s'est inspiré d'une relecture philosophique des travaux de l'historien des sciences Alistair Crombie. \enquote{Voici une liste provisoire des styles de raisonnement scientifique,semblable à la liste de Crombie. Les noms sont toujours arbitraires, visant à être plutôt  des aides mémoire que des descriptions précises.1) Les mathématiques, (1a) Le style géométrique, (1b) Le style combinatoire.2) Le style du laboratoire (des  instruments, de la création des phénomènes, de la mesure). 3) Le style  galiléen (de  la  modélisation  hypothétique). 4) Le style  taxinomique. 5)  (5a) Le  style  des  probabilités, (5b) Le  style  statistique.6) Le  style \enquote{historico-génétique»}}  Ce cadre d'analyse permet de penser l'histoire des raisonnements en sciences non pas dans la rupture mais dans l'accumulation et la continuité. \enquote{ On parle de  l’augmentation des connaissances, mais ce sont aussi les modes de recherches qui s’accumulent, les techniques d’argumentation, les conceptions de la preuve, les critères d’objectivité. Ce sont des modes de recherche presque sans bornes. Les mathématiques pures ont leur province spécifique, mais elles sont appliquées dans toutes les sciences. Le raisonnement historique-génétique s’applique à la géologie  et à la  philologie. Nous faisons des études statistiques de tous les phénomènes qui nous fascinent ou dont nous avons peur. Et nous essayons de transformer l’univers entier en laboratoire.}

Ainsi, par exemple, \enquote{Ce qu’on appelle \enquote{la méthode scientifique } n’est que la combinaison fructueuse des styles (2) et (3). On dit trop  souvent que la science se résume entièrement au laboratoire et aux modèles théoriques. Non : l’un des mérites des catégories de Crombie est que les mathématiques pures, la statistique et la taxinomie, figurent toutes parmi  les sciences.}

Sur l'abduction, qu'il considère comme faisant partie de la logique, il explique \enquote{Chaque style de raisonnement est fondé sur des capacités typiquement humaines,à la fois cognitives et physiologiques, et il en découle. Nous ne doutons pas que ces capacités soient le produit de l’évolution par sélection naturelle. Les styles de raisonnement scientifique, de leur côté, ont été développés au cours de l’évolution des cultures humaines. Chacun a un commencement, qui bien souvent ne reste dans les mémoires que sous forme de mythe, et chacun a son  propre rythme de développement. La  logique, au sens le plus général,  tient lieu de condition préalable à l’émergence des styles de raisonnement scientifique, mais n’est pas elle-même un style.}

Enfin, \enquote{Faire de la logique, au sens le plus généreux de Peirce – la déduction, l’induction et l’abduction – suppose d’être humain. [...] Aucune de ces trois branches de la logique ne figure dans notre liste canonique des six styles de pensée scientifique. Mais en sont-elles vraiment absentes ? Pourquoi n’inclurait-on pas (1) l’hypothèse ou l’abduction sous la rubrique modèles hypothétiques, (2) la déduction dans le style mathématique, et (3) l’induction
dans le raisonnement statistique et probabilitaire ? Parce que (1) l’abduction, ou inférence à une explication, est une opération beaucoup plus générale que de concevoir des structures abstraites qui modélisent un univers sous-jacent. Parce que (2) il n’y a pas que les mathématiciens qui font des déductions : nous en faisons tous. Parce que (3), même si les probabilités peuvent donner une analyse de certaines inductions, et affiner notre sens notoirement infirme de ce qu’on peut inférer à partir de données incomplètes, l’usage avisé des statistiques est au mieux un raffinement de l’induction, et non la totalité de l’induction.}

Pour lui la logique ne tient pas d'un style de raisonnement, car contrairement aux autres, \enquote{ Elle  ne  crée  pas  d’objets  nouveaux, elle  n’introduit pas de critères nouveaux de la vérité. Peirce avait raison, la  logique a la vertu de préserver la vérité dans le raisonnement, mais elle ne la crée pas. La logique est universelle — (quoiqu’il faille reconnaître avec  Jack Goody que l’écriture a eu une influence profonde sur la pratique du raisonnement) — et nos styles de raisonnement et leurs normes sont des produits culturels}

Ces extraits sont tirés de deux cours passionants sur ce sujet \autocites{Hacking2002, Hacking2006} disponible sur la \href{http://www.college-de-france.fr/site/ian-hacking/index.htm}{@page} dédiée à la Chaire de Philosophie et histoire des concepts scientifiques tenu par Ian Hacking entre 2001 et 2006 au Collège de France. D'autres cours sont disponibles en français sur le site, dans la continuité de ce thème.}


\Anotecontent{methode_hermann}{Toutes ces méthodes reposent sur l'évaluation du système simulé en fonction de critères identifiés comme pertinents pour décrire les comportements du système cible. Si la \textit{face validity} repose sur le critère très vague et subjectif de ressemblance entre système simulé et système étudié, les autres critères s'appuient en revanche sur une comparaison statistique classique; ainsi l'\textit{internal validity} s'intéresse à la variabilité du système dans le cas de réplications, la \textit{variable-parameter validity} prend pour critère la comparaison entres variables/paramètres du modèles et leurs équivalents assumés dans le système réel, l'\textit{event validity}, enfin dans le cas de l'\textit{hypothesis validity} le critère repose sur l'analyse et la comparaison des interactions entre unités composant le système.}

\Anotecontent{stonedahl_netlogo}{ \foreignquote{english}{BehaviorSearch follows in the tradition of NetLogo [Wilensky, 1999, 2001; Tisue \& Wilensky,2004], and Logo [Papert, 1980] before it, in embracing the twin design goals of \enquote{low threshold} and \enquote{high ceiling}. By this we mean that the BehaviorSearch tool should be both easy for beginners to learn and use \enquote{low threshold}), while also providing advanced features that will allow expert modelers to engage in cutting-edge research and analysis ( \enquote{high ceiling}). To be clear, the  \enquote{low threshold} goal for NetLogo, which aims to support use by elementary school students, is lower than that of BehaviorSearch, which primarily targets NetLogo’s research audience. However, increasingly NetLogo is being used by undergraduates or even high school or middle school students who are developing agent-based models for research projects, and we would like BehaviorSearch to be accessible to these audiences, as well as researchers from various disciplines who are non-expert programmers but have adopted ABM methodologies for their research. Just as NetLogo strives to make the creation of agent-based models accessible to children and novices, BehaviorSearch aims to facilitate model analysis by making search and optimization techniques accessible to all modelers.} \autocite[336]{Stonedahl2011a}}

\Anotecontent{railsback_POM_resume}{\foreignquote{english}{The basic idea of POM is to use \enquote{multiple} patterns to design and analyze models (Grimm et al. 1996,2005; Grimm and Berger 2003; Wiegand et al. 2003; Grimm and Railsback 2005, chapter 3). Each pattern serves as a filter of possible explanations so that after using, say, three to five patterns we are more likely to have achieved structural realism. An unrealistic model might pass one filter, but probably not also the second and third one. POM thus uses \enquote{multi-criteria assessment} of models using multiple patterns that have been observed in the real system, often at different scales and at both the agent and systems levels. [...] The basic idea of POM is not new but fundamental to any science; it is to use multiple patterns as indicators of a system's internal organization. By trying to reproduce these patterns simultaneously we try to decode this internal organization. Using POM for agent-based modeling includes using patterns to design a model's structure (chapter 18), develop and test theory for agent behavior (chapter 19), and find good parameter values (chapter 20)}\autocite[228]{Stonedahl2011a}}

\Anotecontent{railsback_POM_structure}{\foreignquote{english}{The general idea of POM is to use multiple observed patterns as indicators of a system's internal organization. Trying to reproduce such patterns with our ABMs makes us decode the internal organization of the real system. Experienced modelers use patterns in exactly this way all the time [...] but often the use of the patterns is not explicit or systematic. The point of POM is to consciously think in terms of pattern, regularities or stylized facts right from the beginning and to select model structure accordingly. For beginners, it can take some time to get familiar with POM and to understand that it is, in fact, quite simple. Like a neophyte detective, you will learn to identify patterns in data and expert knowledge and to focus not juste on \enquote{strong}, striking patterns but also on weak ones: combinations of weak patterns can be very powerful for designing and testing model structure. POM is not a technique, like programming or a statistical test, but a general strategy and attitude, a continous awareness of the fact that we as scientists are working very much like detectives. And, by being pattern oriented we can avoid two extremes: simplistic models that are too poor in structure and mechanisms and therefore likely to reproduce a very few patterns but for possibly the wrong reasons, and models that are too complex because they were designed without the guidance of characteristic patterns (as well as the model's purpose) as a guide to what can be left out as well as what must be included.} \autocite[240]{Railsback2012}}

\Anotecontent{railsback_POM_nulltheories}{ \foreignquote{english}{You should always start with \enquote{null theories}, that is, theories that do not use any specific mechanism or assumption. Typical null theories are to just assume that agents make decisions randomly, and (the opposite) to assume agents always do exactly the same thing. Why start with null theories? One reason is that it allows you to develop and test the rest of the ABM before focusing on behavior traits. Another reason is that it helps you learn how sensitive the output of the ABM is to how you represent the key behavior. You might find that some key patterns emerge robustly even without detailed behavior; then you will have learned that behavior is not key to explaining the patterns (an example in section 19.3.3 illustrates this nicely). Or you might learn that some important patterns are not reproduced, showing that the theory devopment exercise is in fact important.} \autocite[246]{Railsback2012} }

\Anotecontent{agent_grimm}{ \foreignquote{english}{ The most important reason we use ABMs is that they allow us to explicitly consider how a system's behavior is affected by the \enquote{adaptive behavior} of its individual agents - how agents make decisions and change their state in response to changes in their environment and themselves. [...] By agent \enquote{behavior} we refer to something an agent does during a simulation : it makes a decision, uses some resources, moves, produces offspring, etc. [...] \enquote{Traits} are the rules we give agents for how to behave. Traits can be thought of as the programs that agents execute, and behavior (of the agents and the whole system) as the results of the agents executing those programs as a simulation proceeds. \enquote{Adaptive behaviors} are decisions that agents make, in response to the current state of themselves and their environment, to improve (either explicitly or implicitly) their state with respect to some objective. An \enquote{adaptive trait} is then the rules we given an agent for making the decisions that produce adaptive behavior. Finally we refer to \enquote{theory} for adaptive behaviors (adressed in chapter 19) as adaptive traits that have been tested and found useful in particular contexts.} \autocite[143]{Railsback2012}}

%DEBUT ORTHOGRAPHE EMILIE
\Anotecontent{batty_donnees}{ Un point sur lequel \textcite{Batty1989} semble encore s'accorder dans les années 1980 pour les modèles urbains. Dans un paragraphe intitulé \foreignquote{english}{Data, Data Everywhere and Not a Thought to Think} il écrit d'abord un premier paragraphe sur l'accès à l'information qui s'avère être d'une actualité étonnante quand on le compare au discours plus récent sur les enjeux de l'\textit{openData} \foreignquote{english}{The diffusion of computer technology and the rise of data banks which contain facts and information about individuals across whole arrays of issues in one of the more sinister features of the emerging information society. There is a sense in which computers attract data in that the availability of information systems leads to their rather unselective use.[...] This proliferation of data, and the many banks which now exist; are available only to those who have the power to buy it, or control it; and although there are clearly better data sets available than there were a generation ago to aid our understanding of cities, for example, access to these data is restricted. There is something rather strange about a situation where data can be collected about ourselves \enquote{freely} but then only made available to us at a cost.} Suite à quoi il déclare \foreignquote{english}{Models had always run into data problems, and lack of data, or at least the right kind of data, clearly did influence the development of the field. But lack of data has never really been a central issue, and in any case it is an open question as to whether any of the appropriate data is available now.[...] Thus data availability has never been the key issue in modelling. As Shubik (1979) says: \enquote{Ours is a data-rich and information-poor society}. The real critique of models relates to more substantive issues, to the question which models are able to respond to, not to the data which are available.} Et, si il est évident qu'il existe de nombreuses questions pour lequel il n'existe pas encore de données, cela n'est pas du fait de l'absence de système de collecte, mais bien parce que \foreignquote{english}{Once again there is the obvious point that the useful information can be collected only if some idea of what it is likely to be used for has been established.} }
%FIN ORTHOGRAPHE EMILIE

\Anotecontent{batty_ok}{Batty semble assez d'accord avec cette direction donnée par Forrester, \foreignquote{english}{Forrester (1969) states 'In the social sciences failure to understand systems is often blamed on inadequate data. The barrier to progress in social systems is not lack of data....The barrier is deficiency in the existing theories of structure.' There is some truth in Forrester's statement, for many theories and models would never have been proposed if data were a prerequisite. This book has also presented models based on easily available data, and at the present time, it seems that more work is needed on model structure and design rather than data collection.}\autocite[355]{Batty1976}}

\Anotecontent{note_varenne_hybride}{Par hybridation, on entend les modèles dont le support informatique n'est pas exclusivement tourné vers une résolution discrete mathématique. \textcite{Couclelis1985} ose par exemple dès 1985 le rapprochement entre les travaux d'Allen et de l'école de Bruxelle avec les AC. Les modèles de simulation comme ceux de\textcite{Allen1978} ne peuvent pas être considéré comme faisant partie de façon exclusive à la classe des modèles d'équation différentielles. En effet, à la différence par exemple du modèle de Forrester, ce type de modèle s'appuie sur un langage informatique (Fortran) pour orchestrer différents modules mathématiques, ce qui laisse volontairement ou involontairement (sous forme d'effet de bord) la place d'intégrer d'autres types d'hypothèses dans le modèles. Hagerstrand est encore un autre cas, probablement développé en langage machine dans ses premières versions sur l'ordinateur SMIL, puis indépendamment en Fortran par l'équipe de Marble, dont l'execution ne peut être réduit à la seule technique monte-carlo mis en oeuvre. Ces réflexions sont principalement issue de plusieurs échanges par mail avec Franck Varenne daté de 2014, durant lesquels l'auteur a appris que certains de ces points de discussions était analysé de façon beaucoup plus exhaustive dans une HDR en cours d'écriture \autocite{Varenne2014b}.}

\Anotecontent{point_un_openshaw}{ \foreignquote{english}{\textbf{Absence of software} An emphasis on mathematical methodes does not necessarily preclude computer modelling, and computers were available twenty years ago. However, there is nevertheless a paradox that many mathematical modellers have never made such use of computers in their work. It is hardly surprising, then, that the availability of software for working models is extremely limited and bears no relationship whatsoever, either to the range of theoretical models that exist in the literature or to the efforts that have been expended in this area of geography. There may be a large number of models outlined in excellent books by Wilson (1974, 1981b) and Batty(1976), but there is virtually no software readily available for any of them. The same sad situation also applies to many geographical analysis techniques.} \autocite[78]{Openshaw1989}}

\Anotecontent{point_deux_openshaw}{\foreignquote{english}{\textbf{Absence of empirical testing} Many model builders see no need to test or evaluate their models and theories against data [...] There are no standard data sets useful for benchmarking models, no generally agreed methodology for model evaluation, and no history of experience of these activities.} \autocite[78]{Openshaw1989}}

\Anotecontent{pluriformaliser}{\enquote{La pluriformalisation, entendue comme pratique de formalisation plurielle assistée par ordinateur peut avoir principalement deux types d’objectifs. Soit elle intègre deux ou plusieurs types d’aspects formels pour deux ou plusieurs parties différentes (ex. : bourgeons, feuilles, ...) à un même niveau d’abstraction. Soit elle intègre deux formalisations différentes de deux objets ou parties évoluant à deux échelles différentes d’un même système (ex. : le zooplancton et sa population). Ces deux objectifs ne donnent pas lieu à des infrastructures informatiques fondamentalement différentes puisque la notion de niveau d’abstraction ou d’échelle est relative aux points de vue adoptés par les utilisateurs. Mais la pluriformalisation, fondée sur une discrétisation préalable — et souvent des langages à objets —, ne tend plus à formuler, en elle-même, un seul point de vue ; car elle entraîne de fait l’intégration de plusieurs points de vue. Apparaissant comme le lieu d’un croisement des points de vue, et du fait de son appui prioritaire sur le pouvoir dénotationnel des symboles, elle est une pratique formelle de levée d’ambiguïté fondée sur une relative réification des modèles.} \autocite{Varenne2012a}}

\Anotecontent{pumain_decouverte}{A l'origine ce sont deux conférences qui amènent Denise Pumain à intégrer l'univers systémique à ses analyses, \enquote{la première organisée à Boston au MIT en 1981, encore très largement dominée par l’analyse de systèmes de type Forrester, et la seconde à Bruxelles en 1982 (AFCET, SOGESCI, 1982) déjà largement consacrée aux théories de l’auto-organisation.} \textcites[27]{Pumain2003}{Mathieu2014}
}

\Anotecontent{abduction_definitions}{
D'autres formulations de l'abduction sont également intéressantes, comme celle logique de Jean-Pierre Desclés  (a) \autocite[239]{Mathieu2014}, mais également celle retenues par (b) \textcite{Phan2006} et (c) \textcite{Livet2014} qui mettent en avant la subtile différence qui lie l'abduction et l'inférence à la meilleure explication.  : \begin{enumerate}[label=(\alph*)]
\item \enquote{Je rappelle ce qu’est l’abduction, par opposition à la déduction : le raisonnement déductif de la logique se ramène à : si je postule ‘p’ et si j’accepte que ‘p implique q’, alors comme j’ai accepté ‘p’, je dois accepter ‘q’ ; le raisonnement par abduction ou rétroduction se ramène à : si je constate que j’ai ‘q’ et que, par ailleurs, je sais que ‘p implique q’ (loi théorique, par exemple, ou savoir commun), alors ‘p’ est une hypothèse qui est simplement plausible – je ne dis pas probable ou possible – du fait de la présence de l’indice ‘q’ (des faits constatés), sachant que l’on a les implications correspondantes ‘p implique q’, renforce la plausibilité d’une hypothèse ‘p’. L’abduction est un procédé argumentatif qui permet de remonter des observables constatés jusqu’à des hypothèses plausibles, par l’intermédiaire de relations explicites (définitions et lois implicationnelles) mises en œuvre dans une organisation théorique.}
\item \enquote{L’abduction consiste à proposer une hypothèse explicative (conjecture provisoire) pour expliquer un fait surprenant qui active le doute. De l’hypothèse, on tire par déduction des conséquences que l’on soumet à l’épreuve, dans une phase inductive, qui n’est pas ici un processus de construction d’hypothèses par généralisation, mais une évaluation empirique. Des résultats négatifs conduisent à reformuler une nouvelle hypothèse (abduction) alors que des résultats positifs diminuent le doute. L’ensemble de cette démarche est parfois qualifiée d’\enquote{ abductive } dans un sens large [WAL 03]. Elle est anti-inductiviste dans le sens où le point de départ n’est pas la généralisation d’observations, mais l’art de former des conjectures pertinentes à partir de l’observation, comme le font le médecin avec les symptômes, ou le détective avec les indices. Peirce, justifie cet \enquote{ art } de manière naturaliste par un argument évolutionniste : si nous ne savions pas formuler des hypothèses pertinentes, l’espèce humaine aurait disparu. L’abduction (limitée au sens étroit d’une \enquote{ inférence en faveur de la meilleure explication }) n’est pas plus que l’induction un raisonnement logiquement valide. Une hypothèse explicative peut mobiliser plusieurs conditions liées dont certaines ne sont pas nécessaires ou encore dont plusieurs peuvent avoir isolément pour conséquence le phénomène observé. De plus, une hypothèse qui semble convaincante n’exclue pas la possibilité d’hypothèses concurrentes.}
\item \enquote{ Selon Charles S. Peirce, l’abduction consiste à produire des conjectures : partant d’un fait observé surprenant, on envisage une proposition P telle que si P était vraie, F irait de soi (cela, une déduction le montrerait), si bien que l’on a des raisons de sélectionner P comme conjecture, comme hypothèse probable (Campos, 2011). C’est, selon Lorenzo Magnani (2009, p. 95-126), la partie créative de l’abduction. Vient ensuite une partie sélective. Comme plusieurs conjectures ou hypothèses peuvent être avancées, il faut pouvoir faire une sélection entre elles, l’hypothèse sélectionnée étant alors utilisée pour une inférence \enquote{à la meilleure explication}. Afin de réaliser cette sélection, on peut penser appliquer des idées des théories de la révision et choisir d’éliminer les conjectures qui remettraient en question les régularités jugées les plus normales par nos théories (Walliser et al., 2005). Mais, par ailleurs, on pourrait aussi préférer celles qui ne se bornent pas à des remises en cause trop minimales, si l’on est plus sensible à ce qui surprend. Ces diverses voies ne sont pas des plus aisées à formaliser.} \end{enumerate}}

\Anotecontent{pumain_cumulativité}{Denise Pumain cite Berthelot pour définir cette notion : \enquote{Par cumulativité on entend classiquement la possibilité d’intégrer les résultats d’un grand nombre d’observations et d’expériences dans l’unité d’un modèle susceptible de les déduire » (Berthelot, 1996, p.74)} \autocite{Pumain2005}}

\Anotecontent{pumain_methode}{\textcite{Pumain2005} voit au moins deux méthodes permettant de construire des savoirs dans l'articulation de concepts issus de disciplines différentes. Sachant, que \enquote{Dans les deux cas, il s’agit de déconstruire des approches disciplinaires en vue de reconstruire un autre savoir à un niveau plus englobant}.

- \enquote{La première méthode consisterait à partir d’un savoir disciplinaire, enraciné dans un point de vue particulier, mais à le mettre en représentation dans un contexte plus large, mobilisant éventuellement des interrogations de sens commun (à partir d’une \enquote{bonne histoire } par exemple) et restituant l’argumentaire et la force de conviction de l’enquête scientifique.}

- \enquote{La seconde méthode consisterait au contraire à se placer aux marges des disciplines et à y rechercher des concepts partageables par d’autres sciences sociales, concepts \enquote{exportables} , ou \enquote{ négociables }, qu’il s’agit d’enrichir par le croisement interdisciplinaire.}

Ainsi dans l'exposition de ces deux méthodes, on observe bien \enquote{[...] comment une véritable articulation de concepts peut favoriser une intégration des processus observés à des échelles de temps et d’espace différents, et permettre que des aspects du social jusqu’ici approfondis séparément retrouvent leur cohérence dans un jeu d’interactions.}}

\Anotecontent{yanof_equi_a}{\foreignquote{english}{As well as disregarding the social science context within which AS research occurs, Grüne-Yanoff also fails to acknowledge that AS already has an explanatory methodology laid out succinctly in the standard textbook for the field (Gilbert and Troitzsch 2005, 15-18). What distinguishes AS from statistics, in particular, is that a model is not simply \enquote{fitted} to data but that the individual level assumptions that generate the data to be compared with  \enquote{reality} are \enquote{more or less} validated independently.

This is what researchers were trying to do in the Anasazi study (Grüne-Yanoff, 544) when they drew on behavioural data from \enquote{comparable} traditional studies (in the absence of the Anasazi themselves) and although Grüne-Yanoff discusses this as a weakness, he does not give it credit as part of a recognised methodology.

Of course, as Grüne-Yanoff points out, in the context of an extinct society, this independent validation may itself be problematic (we do not know how to choose modern societies which are “like” the Anasazi because the whole point is that we don’t know enough about the Anasazi) but that remains a point against the example and not the methodology per se.} \autocite{Chattoe2011}}
