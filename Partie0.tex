
\graphicspath{{Figure1/}}



\chapter*{Introduction}

\epigraph{Nous sommes comme un patient qui sort d'un coma aussi long que la vie d'une étoile.
Ce dont nous ne pouvons nous souvenir, nous devons le redécouvrir }{---  \textup{Robert Charles Wilson}  Axis}

La géographie est partie prenante des bouleversements considérables introduits par la numérisation dans l’ensemble des pratiques scientifiques depuis à peine deux décennies, et cela à plusieurs titres. Les manifestations les plus évidentes tiennent à la prolifération des informations individuelles " géolocalisées " désormais disponibles sur toutes sortes de support, et notamment, ce qui est entièrement nouveau, en situations de mobilité 	\todo{ (Fen-Chong, 2012) } (Fen-Chong, 2012).  Les dispositifs techniques de repérage comme le GPS et l’ouverture des systèmes d’information géographique à l’interactivité grâce à la version 2.0 d’Internet donnent lieu au développement d’une " géographie volontaire "\todo{ ( Goodchild, XXX, Openstreetmap…)}, qui conduit à diffuser auprès du grand public des pratiques et des savoir-faire jusqu’ici réservés aux professionnels. Le très grand nombre des institutions privées ou publiques qui partagent ce nouvel engouement pour l’inscription spatiale de leurs activités, tout comme la croissance fabuleuse des " réseaux sociaux " sur Internet  contribuent à l’immense développement de ce qu’il est convenu d’appeler, sans traduction en français, les " big data " \todo{ (ref dossier special Nature ? )}. Ces masses de données très labiles, évoluant souvent en temps réel, qu’il est relativement facile de collecter à différents niveaux d’agrégation, posent de nouveaux défis aux géographes en termes de traitement des ces informations, tout autant qualitatives que quantitatives. Les méthodes classiques de résumé des connaissances par la modélisation et la visualisation doivent être considérablement transformées pour s’adapter à cette nouvelle donne. 

Heureusement les capacités de calcul informatique et les méthodes de modélisation multi-échelles des informations spatio-temporelles se sont bien développées, en parallèle et en réponse à ces nouveaux défis. Parmi les sciences humaines et sociales, les géographes sont particulièrement présents dans l’expérimentation et la mise au point méthodologique, qu’il s’agisse des les appliquer à des données environnementales (Kanevski, Delahaye, Douvinet) ou à des problèmes de géographie urbaine (Batty, Banos, …Ruas) ou régionale (Wilson, Dauphiné, White, Engelen, Daudé) ou encore à l’analyse de réseaux (Rozenblat). 

Le laboratoire Géographie-cités a été pionnier dans l’application à la géographie des systèmes multi-agents, notamment avec la série des modèles Simpop réalisés en collaboration avec des informaticiens (Bura et al., 1996, Sanders et al., 2007, Pumain, 2012) mais aussi avec des modèles d’agents développés directement avec des logiciels simplifiés comme Netlogo (Banos…). L’évolution des modèles ne se limite pas à l’emploi de logiciels mais participe plus généralement des réflexions de la communauté scientifique intéressée par les systèmes complexes. Le GDRE S4 (Simulation Spatiale pour les Sciences Sociales) a par exemple inscrit son action parmi les premiers projets de l’Institut des Systèmes Complexes de Paris-Ile-de-France (ISC-PIF) fondé par Paul Bourgine \todo{ (ref) }. L’UMR  Géographie-cités est devenue en 2011 l’antenne  de l’Institut des systèmes complexes à l’Université Paris I et acquiert du matériel pour sa propre autonomie de fonctionnement.

Les systèmes complexes sont bien entendu au cœur des interrogations en sciences humaines et sociales, dans la mesure où il est connu depuis très longtemps dans ces disciplines que les interactions en jeu ne sont pas linéaires \todo{ (Morin, 200 ) }  (Morin, 200 ), que des individus engagés dans des processus collectifs sont susceptibles de donner lieu à des phénomènes originaux à l’échelon macroscopique (Schelling, 1978) \todo{ (Schelling, 1978) }, et que les situations initiales ou les effets de contexte sont parfois aussi déterminants que les modalités qui règlent les échanges entre les individus, suscitant des évolutions où la " path dependence " \todo{ (Arthur, 1994) } (Arthur, 1994) ou "enchaînement historique " joue un rôle très important, en limitant les expressions des dynamiques possibles.

Les modèles d’agents employés en géographie ont la particularité d’avoir été développés non seulement pour des simulations d’agents individuels, par exemple pour simuler la diffusion d’épidémies dans un territoire urbain ou régional ou sur un réseau (Banos SRAS, Badariotti, Laperrière, Eubank…) ou la diffusion d’innovations (Daudé), la propagation d’incendies (Langlois), ou encore pour éclairer les choix résidentiels (Bonnefoy…) mais ont démontré l’intérêt de modélisations fondées sur des entités plus vastes (villes, régions, voire états du monde) pour examiner les possibilités d’émergence à partir de leurs interactions à des niveaux géographiques plus larges. 

\subsubsection{Une nouvelle forme d’explication en géographie par les modèles de simulation}

\textbf{Introduction de la causalité dans les modèles => } Ces pratiques de modélisation sont le résultat des multiples inspirations ayant contribué à l’évolution de la géographie, cela depuis les années 1970 en France, et 1950-60 pour les Etats Unis, la Grande Bretagne et la Scandinavie. \autocite{Banos2013} La mise en application rapide de la méthode hypothético-déductive et le rapprochement de la discipline avec la démarche systémique \autocite{Garison1960, Chorley 1962}, puis la théorie générale des systèmes ont permis à la géographie de s'ouvrir à l'interdisciplinarité et d'opérer un glissement dans la justification motivant la construction des modèles : de modèles d’abord uniquement statiques et descriptifs, on est passé à des modèles de simulation géographiques \parencite{Hagerstrand1952, Orcutt1957, Morrill1965, Wilson1970} qui permettent d'intégrer la notion de causalité \parencite[137]{ Pumain2002}.

\textbf{La new geography voit l'arrivée de l'individu methodologique au travers d'Orcutt} L'étude et la compréhension des dynamiques qui fondent les interactions entre les objets géographiques deviennent alors possibles, réalisant l’autre ambition de la " New Geography " qui était de fonder la théorie sur des observations faites au niveau des individus, selon des propositions qui sont rétrospectivement assez proches des recommandations de R. Boudon \autocite{Raymond1984} par exemple. Une intuition qui se concrétise très tôt, dès la fin des années 1950 dans les premiers modèles de micro-simulation  réalisé par Orcutt \autocite{Orcutt1957} dont l'approche "sous-entendait déjà l'idée qu'il est plus intéressant de saisir un changement de niveau macro-économique comme émergeant à partir des comportements individuels qu'en recherchant des lois opérant au niveau macroscopique du système" \autocite{Sanders2007}

% A caser ici Bulle2005
% A caser ici : Ostrom1988 1970-1980 Third Symbol System / différent du Third way of doing science de tel que décrit Troitzsch2013 p17 : Ihrig2013 )
%Cette période pleine d'agitation intellectuelle peut être lus au travers d'au moins deux angles, le premier oppose dimension descriptive et explicative, la deuxième modèles "classiques" et les modèles dit complexes qui viennent impacter l'appréciation de cette causalité, deux opposition qui impacte la construction des modèles.

\textbf{Les enseignements des années 1970 et le retour à l'explicatif dans la simulation} Au début des années 1970, alors que l'euphorie retombe quant aux capacités largement surévaluées des large-scale models  tels ceux développés pour les transports urbains par la Rand Corporation aux Etats-Unis, 
 \autocite{Lee1973} met en exergue au travers de ces fameux "Seven Sins of Large-Scale Models"  ce qui pour lui est en cause dans l'échec d'une décennie de développement. Parmis ces différentes causes, il cite \textbf{... a completer ...} De plus, l'échec des théories statiques faisant état de systèmes trop simple, à l'équilibre, mobilisé le plus souvent à l'écart d'un regard scientifique ou dans des délai d'action ne permettant d'activer pas celui ci, tout cela peine à rendre compte d'une complexité des systèmes urbains qui apparaît de plus en plus comme évidente \autocite[293]{Batty1976}. Les affirmations tenu jusqu'au alors comme "connaissance générale valide" sont mise en défaut et/ou invalidé par l'usage des simulations  \autocite[10]{Batty1994} \cite{Starbucks1983} , et la notion même de "prédiction" doit être repensé à l'aube de ce constat d'échec. 

Si le requiem de Lee a bien eu un effet non négligeable sur la construction et la publication de tel modèles du coté des planifieurs (seulement trois modèles seront publiés dans le même journal à la suite de cet article..), force est de constater que la construction de simulation pour la théorie urbaine  lui n'a pas disparu, et s'appuie au contraire sur l'apprentissage de ces échecs pour se réinventer dans les années qui suivent. A ce titre, \autocite{Harris1994} soulève dans une relecture très critique de l'article de Lee, l'ofuscation ou la méconnaissance de l'auteur vis à vis des débats qui agite déjà depuis plusieurs années la simulation de modèle urbains \autocite{Wilson1970, Orcutt1957, Harris1968}. Ce faisant, Harris accuse Lee d'enfoncer des portes ouvertes et de porter des accusation que certains jugeront par la suite prématuré vis à vis du préjudice subit, touchant à coeur une discipline encore bien jeune, qui selon \autocite[p11]{Batty1994} sort à peine d'une periode d'apprentissage d'une décennie.

 Car le milieu universitaire, bien que peu sollicité pour la construction de modèles qui sont avant tout concu  dans le cadre d'une stratégie efficace de plannification, n'est pas en reste des bouleversements apportés par l'apports de la simulation informatique et la construction/ diffusion de certains de ces modèles en dehors des USA. En témoigne encore la longue lignée de modèles inspirés du modèle de Lowry, un modèle créé dans le cadre de la RAND qui a rencontré un large succès avant d'être repris par la suite par l'école de Leeds. Déjà annoncés en 1961 par \autocite{Cohen1961} dans le journal "Quaterly Economics" différentes équipes de développement  sont déjà bien identifiés par la communauté universitaire. Le mouvement du professeur Orcutt, mais aussi celui de Forrester vont par la suite amener chacun à leur manière une nouvelle dimension explicative et l'ajout d'une dimension temporelle \autocite[p295]{Batty1976} plus à même d'aborder cette complexité qui fait défaut aux théories actuelles.

Le courant de la micro simulation initié par Orcutt semble effectivement passer outre l'extinction annoncé par Lee en 1973, et rencontre même un certain succès durant toute les années 1970 comme en témoigne la mise en place de nombreux programme nationaux au début des années 1980. \cite{Baroni2007} Une réponse peut être avancé dans le positionnement innovant d'Orcutt pour face aux résultats décevant des "Large Scale Models" de son époque, opérant pour la plupart à un niveau macro et fournissant des résultats hautement aggrégés difficile à exploiter dans un cadre prédictif, et finalement peu représentatif de la diversité des systèmes économiques \cite{Birkin2012} \cite{Baroni2007}. Si les critiques de Lee restent pour la plupart mobilisables pour les modèle issue de la micro-simulation (complexités des modèles, absence d'objectifs clairement posés, volume des données à mobiliser, complexité des calculs, coût de construction, absence de résultats, etc.), il n'en reste pas moins que la proposition d'Orcutt introduit avec une approche plus \textit{bottom-up} une dimension explicative absente jusque là. En répondant à l'observation de Lee sur l'absence d'extraction de connaissance micro quelque soit la complexité injecté dans les modèles macro, Orcutt ouvre d'une certaine facon la voie à des développements théoriques beaucoup plus riche que ne le permettait à l'époque les seuls modèles Macro, faisant ainsi de son modèle un instrument pour "consolidating past, present, and future research efforts of many individuals in varied areas of economics and sociology into one effective and meaningful model; an instrument for combining survey and theoretical results obtained on the micro-level into an all-embracing system useful for prediction, control, experimentation, and analysis on the aggregate level" \autocite[122]{Cohen1961}\autocite{Orcutt?}.

Il faut attendre 1975 pour qu'un premier "Large Scale Model" nommé DYNASIM, financé par les USA, intègre la vision d'Orcutt. L'ancrage thématique apporté par l'étude de la dimension micro permet alors de supporter cette double casquette d'un outil à vocation à la fois décisionnel et de recherche pour les sciences sociales\ref{}.  Il faudra toutefois attendre l'évolution des techniques informatiques des années 1980 pour que le coût de développements de ces modèles baisse réellement, permettant à ces modèles d'être conçu plus rapidement et de se diffuser internationalement \ref{}. De plus bien que porteur d'une dimension explicative ces modèles restent principalement à visée essentiellement prédictives, et n'intègre pas les interactions entre individu, pour cela il faudra attendre des développement beaucoup plus récent. \autocite{Sanders2007}.

A ce titre \autocite{Batty2001} range encore les modèles issues du courant de micro-simulation d'Orcutt dans la génération des modèles qu'il dit "classiques", au même titre que ceux issue de la Rand Corporation. A ces modèles il oppose les modèles dit "complexe" et cite Forrester comme le premier ayant polarisé le débat sur 

Dans ce contexte, le modèle de Forrester fait en 1968 une entrée très remarquée dans le milieu des "policies" \autocite{Lee1983} en proposant un modèle abstrait de ville pour l'étude de scénarios urbains. Celui ci est non spatialisé, ne fait appel à aucune donnée, et donc il ne peut pas être considéré comme un modèle prédictif sérieux pour les " policy analysis " de l'époque\autocite{Lee1973}. 

 Le discours contraire de Forrester, affirmant la véracité de ses prédictions sur 250 ans, ne manquera pas de soulever de nombreuses critiques sur son travail. Pourtant ce travail ouvre la voie avec les systèmes dynamiques à des modèles causaux, qui peuvent s'avérer tout aussi explicatifs que prédictifs, permettant l'étude beaucoup plus fine des dynamiques et des boucles de rétro-action à l'oeuvre dans des systèmes.

% Toujours en rapport avec l'échec des modèles des années 1960, et qui permet d'amener aux sciences complexes, forrester étant le premier à polariser le débat sur cette approche..
% Echec ne sont pas toujours des échecs pour tout le monde ! et donc on peut s'appuyer sur le fait que la découverte de la non correspondance des théories nous amene à voir la complexité du réel finalement.
% Introduction forrester,puis wilson pour la non linéarité et sys dynamique.

% Forrester polarise le débat, fait émerger le fait qu'il y a des systèmes non linéaires, et qu'il ya d'autres type de modèles possibles, qui ne necessite pas forcément de données pour être explicatif.
 
% Amene un questionnement sur la structure des modèles, et donc sur la facon de les construire non ?
% => chgt dans la structuration des modèles

 Il n'en reste pas moins que l'article de Lee amène avec lui l'idée d'un retour urgent à la simplicité dans la formulation des modèles, %appuyé par l'échec de l'application de beaucoup de théories sociales les unes avec les autres, sans réelles résultats... En ce sens, Lee pose implicitement la question de l'utilité de ces modèles qui peuvent égalemement tout faire.... cf modele lowry, "back to empirie" cf starbucks1983 => questionne violamment la prediction et ce que l'on peut en dire. Retour à la simplicité, ok. Oui mais simplicité ne marche pas vraiment en fait, dimension complexe est ouverte,et si on s'en tienta ce que dit batty . \autocite{Batty2001}

%Ouverture dimension explicative via Forrester, modèle ouvrant perspective sur la complexité (Batty), et cela meme si le modèle original se concentre autour d'un point d'équilibre.  

Toutefois l'application de cette méthode scientifique pour la construction de modèles de simulation mathématiques ou entités-centrés (informatiques), si elle a permis l'accélération dans la formulation et le test des hypothèses, a aussi engendré la construction de modèles plus abstrait. 

Si on se tient à l'opposition décrite par Emmanuelle Bulle, qui décrit très bien la dichotomie qui existe entre modèle purement explicatif comme Schelling, SugarScape, ou 
 
 \textbf{Une opposition / mise en tension des modes de construction }
%ARTICLE DE LEE PERMET NOUS PERMET D'INTRODUIRE L'OPPOSITION ENTRE DEUX APPROCHES pour implique des modalités de construction différentes la construction des modèles DONC, avec la confrontation entre d'un coté des modèles causaux descriptif classiques, principalement orienté vers la prédiction, et d'autre part un autre type de modèle, qui ne base pas forcément sur des données, ou du moins qui ne sont pas validable en entier, et que Batty qualifie modèles complexes ... (à voir qui ouvrent beaucoup plus la voie à l'explication. (a valider ? semble complètement faux ... ))

 Ces modèles plus concentrés alors sur le réalisme des effets que sur le réalisme des causes usent d'abstractions mathématiques déconnectées de toute empirie pour mieux observer les données. Or en sciences humaines et sociales nous ne somme pas dans le cadre d'un modèle de simulation météorologique pouvant finalement mobiliser n'importe quelle abstraction mathématique du moment que le temps de demain est correctement prédit \autocite[\nopp 3.8]{Elsenbroich2012} \autocite{Kuppers2005}]. L'utilisation d’un modèle prédictif dans le cadre d'une application décisionnelle, s’il mobilise des hypothèses trop éloignées de la réalité dans le but de se rapprocher " artificiellement " des données, attire très logiquement la critique. Ce qui est critiqué ici, c'est d'abord cette simplification par la voie mathématique, limitante, et qui doit être impérativement contrôlée pour pouvoir ensuite espérer raccrocher le modèle à une réalité.

D’autre pratiques encourent le même reproche, leur logique étant d’ajouter de façon gloutonne une infinité  d'hypothèses, même validées empiriquement, aux modèles,  cela jusqu'à obtenir la correspondance avec une série de données. Des modèles dits « ad hoc » ont en particulier commencé à proliférer avec l’apparition des systèmes d’information géographique dans les années 1980. Ce sont ces pratiques, qui entre autres causes viendront limiter le développement de la modélisation dans certaines disciplines des sciences humaines et sociales, car elles remettent en cause la scientificité de cette approche. 

D’ailleurs, cette critique sur la " scientificité des modèles simulés" est une constante dans l'histoire de la modélisation en science sociales, et a pris différentes formes en fonction de l'époque, et des techniques alors disponibles.

\textbf{Une abstraction mathématique qui ne convient pas à tout les disciplines }

Mais ce retour à l'abstraction mathématique, nécessaire pourtant pour expliciter de manière non ambigüe et inscrire les théories sociales dans les simulateurs, ne convient pas à toutes les disciplines, telle l'archéologie qui a du mal à faire correspondre ces équations simples avec des systèmes d'interaction très complexes \autocite{Kohler2011} dans un contexte de rareté des données et de populations peu nombreuses. S’il existe bien dans toutes les disciplines des sciences sociales des chercheurs motivés par la simulation sous une forme non mathématique, grâce à l’informatique, dont on a très vite l'intuition qu'elle permettra d'aller beaucoup plus loin dans l'expression de nos problématiques \autocite{Doran1970}, c'est sans compter avec les limitations techniques des ordinateurs de l'époque, l'absence de méthodologies, la diversité des langages et la technicité importante nécessaires pour écrire les programmes, le coût et la durée de telles recherches, les stratégies de publication et finalement l’extrême rareté des publications faisant état de modèle utile dans la prédiction ou l'explication.

Si certaines disciplines comme la géographie semblent profiter d'un bouleversement théorique, méthodologique (rapprochement avec la systémique, application de la méthode hypothético-deductive) et institutionnel (formation des chercheurs aux outils statistiques et mathématiques) mettant la discipline plus à la portée de ces techniques mathématiques \cite{Pumain2002},  il faudra attendre les années 1980-90 pour voir apparaître un regain d'intérêt pour les modèles de simulation dans de nombreuses disciplines en sciences sociales. C'est l'avènement de formalismes informatiques plus expressifs, plus accessibles et moins limitants que les mathématiques (automates cellulaires, méta-formalisme agent), permettant de décrire nos objets d'études et leur interactions dans leurs aspects multi-niveaux, qui induit le développement de bon nombres de modèles. Toutefois, la plupart des problématiques semblent demeurer, tant sur les aspects techniques, que sur les aspects épistémologiques, toujours cristallisées dans cette question de la scientificité des modèles de simulation \todo{ref JASS}

En effet, le transfert de l'activité de modélisation dans un contexte informatisé, si elle permet  d'accélérer l'activité de raisonnement, ne touche en rien à la réflexion qu'il est nécessaire de mobiliser pour guider intelligemment la construction des modèles et des théories associées ! On pourrait donc affirmer un peu malicieusement que l'outil mathématique ou informatique n'est pas un outil de construction de connaissance d'usage plus dangereux, réductionniste et moins scientifique que ne le sont nos professeurs d'universités… Pourtant, s’il est certain que la vérité ne s’inscrit pas automatiquement dans des chiffres ou des symboles manipulés par des machines,  il reste que, dans le processus de production de connaissances, la médiatisation par des modèles requiert toujours un arsenal d’éléments de probation qu’on n’exige pas souvent avec autant de force de la part du simple discours dans un argumentaire scientifique.

% Transition ici sur la nature des modèles ... POurrait fairel le lien ici entre le contexte (prédiction / explication) et le fait que la structuration, et donc quelque part la construction aussi des modèles à tout à voir avec la validation qui la touche.Batty 2001 toujours ...

\subsubsection{Comment construire des modèles de systèmes complexes qui participe à la production de connaissance}

En effet, la question de la " Vérification " des modèles, au sens philosophique du terme (valeur de vérité), reste indépassable du fait des multiples biais amenant l'observateur à toujours questionner la valeur de cette connaissance qui résulte d'un transfert entre les résultats d'un modèle volontairement imparfait (" simplifié ", donc réducteur par définition), et la " réalité " dans toute sa complexité  \autocite{OSullivan2004}.  Les termes " vérification " et  " validation " sont couramment rencontrés dans notre discipline, mais sous des acceptions différentes tenant souvent au transfert des terminologies entre ingénierie \autocite{Sargent1984} d\autocite{Balci1998}  et philosophie, ce qui conduit à  des débats terminologiques sans fin \autocite{David2009}. 

Ainsi dans le cadre de notre étude, le terme " vérification "  \enquote{[...] stands for absolute thruth } \autocite{David2009} \autocite{Oreskes1994} et se rapporte avant tout ici à la notion d'équifinalité \autocite{OSullivan2004} En dehors de toute considération technique, cette équifinalité qui décrit le fait que m-modèles créés par les scientifiques peuvent représenter la même réalité ( ou modèle de la réalité ), est tout à la fois un moteur et une limitation dans notre capacité de construction des connaissances. 

L’existence de théories alternatives multiples est une constante dans l’histoire des sciences humaines. L'étude de l'objet social est un construit contextuel qui se nourrit d'une multiplicité des point de vues. C'est à ce titre que Jean-Claude Passeron \autocite{Passeron2006} nous met en garde contre une tentative de vérification des modèles qui serait décorrélée de tout contexte historique. Pour lui le faillibilisme poppérien qui se cache derrière la méthode hypothético déductive ne peut pas s'appliquer à la construction de théorie dans le cadre des sciences humaines et sociales. L'équifinalité est à ce titre un moteur permettant de confronter nos théories sur un objet social  qu'il est impossible de tout façon impossible de voir dans son unicité. A ce titre, " les questions pour  la validation des modèles ne devraient jamais être abordées en dehors des questions relatives à leurs usages " \todo{Amblard}(Amblard)

Le processus de modélisation apporte une dimension supplémentaire à l'analyse de chacun de ces points de vue. Car  il est hélas  impossible de prouver par les modèles qu'il n'y a pas un tout autre ensemble de fait stylisés ou d'interactions qui soit capable d'arriver à la même observation, enlevant de fait toute unicité d’une explication " scientifique " au point de vue représenté par le modèle. L'équifinalité est donc à ce titre une limitation indépassable à la connaissance qui peut être déduite de nos modèles.

Le terme " validation " quant à lui est souvent entendu pour définir un état qualifiant la correspondance entre des observations empiriques et les sorties de la simulation. Compte tenu de la notion d'équifinalité, cet état de correspondance ne suffit pas à prouver que le modèle représente bien la " réalité ", dans la mesure où l’unicité de  cette adéquation peut être remise en cause par le jeu de nouvelles hypothèses.

\begin{quotation} In fact, utility of simulation is sometimes confused with validity. The one refers to its usefulness for some purposes, whereas the other refers to its degree of correspondence with the real world. Since utility requires some degree of validity, some authors speak of a model as havng been « validated » by some use to which it has been put. Validity of a model, however, is not and end in itself but merely a means of enhancing the utility of the model – and usually only up to a point. Both validity and utility are commonly matters of degree. […] While validity is the ultimate test of a theory, the ultimate test of a model is its utility.  \\ \sourceatright{ \autocite{Schultz1972}}\end{quotation}

Comme \todo{cite{Amblard200x}} le propose, nous remplacerons donc le terme de " validation ", qui prête à confusion,  par celui d’ " évaluation ", qui n'est pas sans rappeler la notion d'utilité telle que définie dans la citation ci dessus.

\subsubsection{Une démarche pour systématiser l'évaluation des modèles}

Dans le cadre de cette thèse, nous défendrons une « évaluation » de modèle qui se confond presque complètement avec la méthodologie de construction qui la soutient. Cette « validation interne » doit selon nous être systématisée au regard de la « validation externe » qui mesure classiquement la correspondance entre données simulées et observées face à la question posée. C’est en cela que la démarche que nous proposons est « systématique ». Les opérations nécessaires à la « validation interne » telles que l'introduction, la modification, ou la suppression d'hypothèses, s’effectuent donc à la mesure de leur apport qualitatif et quantitatif dans l'explication de la dynamique globale sur laquelle se fonde la « validation externe ». Autrement dit, c'est la recherche d'une cohérence qualitative autant que quantitative de la dynamique interne qui nous guide dans notre recherche de correspondance avec les données observées.

A ce titre, le recours au calibrage, et la recherche de cohérence interne dans les dynamiques pourraient passer pour une tentative de mieux définir par ce biais les processus en jeu dans un contexte réel. Pour \autocite{OSullivan2004} cet argument est encore un leurre, car toujours au vu de l'équifinalité, si ces procédures améliorent bien la connaissance du modèle, absolument aucune garantie ne peut être donnée sur la qualité et la transférabilité de cette connaissance pour l'étude de processus réel. Cela est d'autant plus vrai lorsqu'il s'agit de système complexes, dont la nature même empêche toute  mesure des dynamiques à l'oeuvre lors des processus d'émergence, et rend donc discutable toute comparaison possible avec des dynamiques simulées. 

\begin{quotation} It is clear that assessment of the accuracy of a model as a representation must rest on argument about how competing theories are represented in its workings, with calibration and fitting procedures acting as a check on reasoning. So, while we must surely question the adequacy of a model that is incapable of generating results resembling observational data, we can only make broad comparisons between competing models that each provide ‘reasonable’ fits to observations. Furthermore, critical argument and engagement with underlying theories about the processes represented in models is essential: no purely technical procedure can do better than this.  \\ \sourceatright{ \autocite{OSullivan2004}} \end{quotation}

% Un point de vue partagé par {Batty2001} ce qui permettrai d'introduire la notion de système complexe également !

Ainsi plus que les solutions techniques, c'est dans le processus de discussion et d'échange autour des hypothèses admises dans les modèles que notre connaissance sur les phénomènes réels est amenée à progresser. Par la mobilisation, l'hybridation, la confrontation de modèles ou briques de modèle issues d'angles de vues inter-disciplinaires,  on met en œuvre une grande discussion à même d'éclairer cette dynamique globale qui serait de toute façon insaisissable dans sa globalité. {cf transcidisciplinarité de morin ?}

 \autocite{Rouchier2013} s'appuyant sur une définition de \todo{Gilbert et Artweiler} décrit cette forme de validation basée sur la réutilisation et l'enrichissement collectif des modèles comme étant post-moderne, \endquote{ dans la mesure ou elle base la valeur d'un modèle au regard de son usage par une communauté d'usagers }. Il y a donc dans le processus d'évaluation des modèles de simulation une dimension collective qui ne peut plus être niés dans l'établissement d'outil et de méthodologie . De façon plus générale, \autocite{Rouchier2013} évoque et décrit bien dans un article récent «  Construire la discipline « Simulation Agent » » la nature de ce mouvement structurant qui oeuvre dans la construction de communauté scientifique. Celui ci prend forme autour de revues revendiquant une large ouverture inter-disciplinaire, tel que JASSS, qui font alors office de catalyseur en supportant, relayant ces discussions de fond, à la fois sur le plan méthodologique et technique.

Dans sa conclusion \autocite{Rouchier2013} mise sur le développement de la crédibilité de cette discipline dans les années à venir, grâce aux revues, aux règles de conduites edictées, et aux modèles repris et discutés au coeur de cette communauté \autocite{Hales2003}. Même si il est bon de garder une vision du futur optimiste du fait des avancés qui ont émergé des discussions ces dernières années, les problématiques que l'on rencontrent encore aujourd'hui dans le cadre de la simulation de modèles agents en géographie continue de faire écho à celles déjà mainte fois relayées par diverses publications ces dernières décennies\todo{ref JASS} \autocite{Squazzoni2010}  \autocite{Richiardi2006} \autocite{Windrum2007}. Sachant cela, il est difficile alors de ne pas sentir naître un sentiment plus mitigé sur cet avenir, car si la communauté n'arrive pas à dépasser tout ou partie des problèmes qui enrayent la diffusion des pratiques de simulation, comme cela semble être le cas, alors c'est toute la reconnaissance de ce champ comme une discipline scientifique à part entière qui reste limité.

Et sur ce point, un regard sur l'histoire passée de la simulation dans les sciences sociales n'est pas fait pour nous rassurer, la plupart des problèmes cités comme facteurs limitants dès les années 1970 recoupent encore aujourd'hui tout ou partie de nos problèmes actuels. En forte interaction, ceux ci peuvent être rapportés à au moins trois dimensions explicatives, une dimension technique, une dimension méthodologique et une dimension institutionelle, ce qui peut être explique pourquoi ceux ci n'ont jamais pu être totalement résolus dans le cadre d'une seule politique, d'une seule projet, ou d'une seule publication : faible nombre de modèles publiés et reproductibles, absence de publication décrivant des protocoles d'évaluation de modèles et des mises en application de ces protocoles, difficulté d'accès à l'information et à la ressource technique nécessaire pour l'exploration des modèles, stratégie de publication misant sur la publication de modèles déjà finalisés mais jamais appliqués de nouveau, manque de formations adaptées ou dédiées, confrontation avec des courants disciplinaires " mainstream " ignorant l'activité modélisante, etc.

Dans notre volonté de proposer une démarche systématisant la construction et l'évaluation des modèles, ce contexte historique déceptif doit être pris en compte, et il nous faut prendre le partie que ce n'est pas la proposition d'une n-ième méthodologie à vocation englobante, universalisante qui sera capable à elle seule de lever ces barrières, et cela même si elle prend acte de cette dimension collective dans sa formulation (reproductibilité des modèles, des expérimentations).

 A ce sujet, il existe une histoire drôle chez les informaticiens, qui peuvent être régulièrement confronté à des états de l'art comportant pléthore d'approches (méthodologique ou technique) pour la résolution d'un même problème. Ainsi l'informaticien zélé, acteur de notre histoire, allume son ordinateur en arrivant dans son laboratoire et part à la recherche d'une solution pour son problème du moment. Mécontent de ne pas trouver un outil satisfaisant pour son problème à la fin de sa journée, celui ci se dit alors dans un éclair de lucidité " Tentons de créer une nouvelle méthodologie pour unifier toute ces approches hétérogènes en une seule !". Ce n'est que quelques mois plus tard, et au terme d'un développement difficile mais enrichissant, que la solution prend finalement forme. A ce moment là, force est de constater que ce ne sont plus 14 mais 15 solutions concurrentes qui s'affronte alors sur le marché des méthodologies pour la résolution de ce problème. Moralité ? Projeter la construction d'une n-ème méthodologie dans une volonté unificatrice (et donc forcément réductrice) peut certes être un exercice constructif (le protocole ODD qui tente d'unifier la description des modèles est en ce sens une expérience intéressante), mais force est de constater que celui ci a peu de chance d'enclencher le processus de standardisation tant attendu, d'autant plus lorsque cet effort s'exerce dans un cadre largement inter-disciplinaire dont les frontières tant sur les aspects méthodologiques que techniques ne peuvent pas être imaginé/intégré par une seule et même personne.

Sur ce dernier point, une première réflexion révélatrice de cette expérience a ainsi été mené par Thomas Louail et Sébastien Rey au laboratoire Géographie-Cités en 2010 (?). L'objectif de ce travail était de lever les limites des méthodologies et outils existants autour des modèles de la famille de modèle Simpop2 afin d'infléchir une réflexion et des premiers outils prototype pour la construction et l'évaluation automatisé de modèle dans le cadre d'une utilisation collective. Si ce projet a permis de fonder la base d'une réflexion plus large qui nous motive encore aujourd'hui dans la présentation de ce projet, force est de constater que l'ampleur de la tâche une fois décrite rendait difficilement réalisable sa concrétisation en dehors d'une équipe pluri-disciplinaire, mobilisé sur plusieurs années sur ce sujet.

Bharathy2010

\subsubsection{D'une démarche systématique à une démarche intégrée}

Cette réflexion menant à la construction d'une démarche systématique pour l'évaluation et la construction de modèle de simulation doit certes être mené dans le cadre d'une amélioration de nos pratiques, mais nous avons vu que cet effort n'avait pas pour vocation première l'établissement d'un standard. En effet, la diversité de ces même pratiques rend impossible et réducteur une telle approche. 
Un autre point de vue défendu ici, montre qu'il est plus intéressant de retranscrire cette diversité par un ensembles de couplage entre des outils conçu sur une base autonome et standard;

Autrement dit, ce projet s'inscrit dans un objectif double, il s'agit à la fois de garantir l'indépendance et la réutilisation des outils dans de multiples configurations tout en problématisant leur utilisation dans des constructions méthodologique (ou cas d'utilisation) que nous jugeont pertinent pour l'exploration et la construction de modèles en géographie. De ce fait ils participent à la création d'un écosystème appropriable par tout les points de vues, non réducteur car flexible dans le cadre de nos pratiques, et appuyant en plusieurs points cette dimension collective pour la construction et l'évaluation de modèle.

Deux niveaux de discussion doivent être envisagé, le modèle d'une part, et l'exploration de ce modèles d'autres part.

Une réflexion en terme d'outils, une réflexion en terme de couplage entre les outils, une réflexion en terme de plateforme support garantissant une dimension collective à cette réflexion.

L'objectif est la mise en place d'un outil qui fait office d'attracteur,  capable d'intégrer des outils et des méthodes, mais aussi d'incubateur capable de catalyser un processus de standardisation des outils ou méthodes qui s'appuient dessus. 

L'intégration des méthodes permet d'envisager la construction d'une base de discussion

 Celui ci au contraire ne peut que s'enrichir du fait des échanges qui se produisent à l'orée de chacune des disciplines, promesse ici d'une démarche compatible avec l'ouverture propre aux "système complexe", souvent avancé mais encore difficile à concrétiser.
 
 Les freins historiques à la diffusion de méthodologies et d'outils sur la validation que nous avons ainsi identifiés précédemment peuvent alors être intégré dans une vision plus élargie en accord avec les derniers prérequis technique et méthodologique qualificatif d'un travail dit "scientifique"  

Nous pensons qu’une stratégie d’organisation de ce champ peut s’inspirer  de ce qui a été pratiqué au cours des années 1960 à 1980 par les mathématiciens et les informaticiens qui ont acculturé les sciences humaines et sociales aux pratiques de l'analyse des données, en développant des méthodes autour de logiciels d'accès facile et d'utilisation standardisé.

\subsubsection{Un exemple historique pour l’émergence de méthodes standardisées}

Il n’est pas nécessaire de remonter très loin dans l’histoire de notre discipline, pour découvrir comment ont pu émerger, à partir  de pratiques localisées, des outils et des méthodes qui sont aujourd'hui largement diffusés et considérés comme acquis et standardisés. Ainsi, bien qu’il fasse partie de notre outillage de « tous les jours » pour analyser des données multi-dimensionnelles, le terme ACP (Analyse en Composantes Principales) n'apparaît concrètement qu'à l'automne 1962 ( selon Benzécri 1977  p9, lors d'une conférence au Collège de France). 

Quelques indications de Jean-Paul Benzécri (Benzécri 1977) sont particulièrement éclairantes à cet égard lorsqu'il relate la publication fondatrice en 1973 de son ouvrage « l'analyse des données » 

a)  plus de 70 auteurs ont participé aux tomes I et II et la construction des méthodes s'est faite par confrontation avec les écoles internationales existantes (pxx) ; Michel Armatte (Armatte 2008) soulignera dans le préambule de l'analyse rétrospective de ce livre à quel point ces techniques alors innovantes ont pu susciter l'engouement tout autant que le rejet de la part de la communauté statistique.

 b)  les quelque douze chapitres que comporte ce livre sont largement issus et enrichis d'un travail autour de données et de chercheurs de toutes disciplines ; ainsi « le codage, la critique et l'interprétation des résultats s'étant perfectionnés au fur et à mesure que se diversifiaient les données » (p28) 

Si ces deux points relèvent bien l'importance de l'aspect interdisciplinaire dans la construction des méthodes en Analyse de données, un autre récit de Ludovic Lebart (Lebart 2008) souligne l'importance de la mise à disposition des connaissances dans des manuels pour promouvoir leur diffusion. Ainsi Jean-Pierre Fénélon et Ludovic Lebart, « deux timides et  maladroits apôtres de Jean-Paul Benzécri » sont convaincus lors de la publication de leur livre « Statistique et Informatique Appliquées » d'être en 1970  les pionniers d'une « révolution censée embraser toute la science statistique ». Dans la droite ligne de leur précurseur, qui avait déjà mis à disposition dans son livre « Analyse de Données » un programme en FORTRAN (pour les heureux possesseurs d'un 1620 IBM !), Ludovic et son acolyte proposent eux aussi les codes sources et des méthodes des programmes. Ils cristallisent ainsi des pratiques, qui à une époque ou l'omniprésence des « MainFrame » et la quasi absence de système d'exploitation pour l’exécution de programmes compilés (donc échangeables) contraint à la libre diffusion des codes sources entre les équipes. Loin de s’arrêter là, cette équipe fonde l'ADDAD (Association pour la Diffusion et le Développement de l'analyse des données) et publiera une revue qui fait la promotion de ces pratiques jusqu'en 1997. Cet esprit perdure encore dans une revue fondée en 1988 par des disciples statisticiens, celle ci est toujours publiée au format électronique sous le nom de  MODULAD. 

De par ces deux mécanismes agissant de concert, construction inter-disciplinaire et libre diffusion des méthodes via des manuels fondateurs, les techniques d'Analyse de Données percolent rapidement dans les publications en géographie française dès les années 1970 (Beguin 1979, vous pouvez citer mon article de 1976 dans l’Espace Géographique et celui écrit avec Madeleine Brocard et Violette Rey en 1973, et puis aussi le manuel Chadule autre ref fondatrice en fr ?) avec l'aide de cette poussée révolutionnaire venant de la géographie anglo-saxonne, et la traversée de leurs idées largement favorable à ces outils ( en témoigne la traduction française de l'article l’ouvrage  de Peter Haggett (1967) en 1973 par Hubert Fréchou sous l’impulsion de Philippe Pinchemel). La multiplication des publications bravant, décortiquant, adaptant les nouvelles méthodes génère vite des discussions et des améliorations autour de ces nouveaux outils et de la manière de les utiliser dans ce qui laisse très vite préfigurer les méthodes de la géographie quantitative moderne.

Ainsi si l'apport inter-disciplinaire a été un moteur très fort dans la construction des outils en Analyse de données, il a également très largement participé à sa diffusion par le biais de la ré-appropriation des méthodes et outils par les communautés.

Si le contexte est aujourd'hui bien différent, les vecteurs principaux de communication dans la recherche ayant muté pour s'adapter à la révolution Internet, de nouveaux outils et plateformes apparaissent tous les jours sur Internet pour compléter une offre existante déjà extrêmement riche.


Les objectifs pour cette thèse

Voir en, quoi on peut faire évoluer les pratiques pour systématiser l’évaluation dans les modèles d’agents

Revue des pratiques existantes (chapitre 1)

Les fonctionnalités d’un laboratoire virtuel étendu (construction des modèles, exploration, visualisation) (chap 2)

Démonstration de l’intérêt pour le calibrage (chapitre 3)

Réalisation d’analyses de sensibilité (chapitre 4)

Conclusion



\listoftodos


